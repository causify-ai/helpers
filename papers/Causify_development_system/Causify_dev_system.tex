% From https://github.com/kourgeorge/arxiv-style

\documentclass{article}

\usepackage{arxiv}

% Allow utf-8 input.
\usepackage[utf8]{inputenc}
% Use 8-bit T1 fonts.
\usepackage[T1]{fontenc}
% Hyperlinks.
\usepackage{hyperref}
% Simple URL typesetting.
\usepackage{url}
% Professional-quality tables.
\usepackage{booktabs}
% Blackboard math symbols.
\usepackage{amsfonts}
% Compact symbols for 1/2, etc.
\usepackage{nicefrac}
% Microtypography.
\usepackage{microtype}
% Can be removed after putting your text content.
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{doi}

\title{The Causify Dev System (v0.1)}

%\date{September 9, 1985}   % Here you can change the date presented in the paper title
%\date{}                    % Or removing it

\author{ The Causify Team
\thanks{Use footnote for providing further information about author (webpage,
alternative address)---\emph{not} for acknowledging funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA
15213
\\ \texttt{hippo@cs.cranberry-lemon.edu} \\
%% examples of more authors
\And
Elias D.~Striatum \\
Department of Electrical Engineering\\
Mount-Sheikh University\\
Santa Narimana, Levand \\
\texttt{stariate@ee.mount-sheikh.edu} \\
%% \AND
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
%% \And
%% Coauthor \\
%% Affiliation \\
%% Address \\
%% \texttt{email} \\
}

% Uncomment to remove the date
%\date{}

% Uncomment to override  the `A preprint' in the header
%\renewcommand{\headeright}{Technical Report}
%\renewcommand{\undertitle}{Technical Report}
\renewcommand{\shorttitle}{\textit{arXiv} Template}

%%% Add PDF metadata to help others organize their library
%%% Once the PDF is generated, you can check the metadata with
%%% $ pdfinfo template.pdf
\hypersetup{
  pdftitle={A template for the arxiv style},
  pdfsubject={q-bio.NC, q-bio.QM},
  pdfauthor={David S.~Hippocampus, Elias D.~Striatum},
  pdfkeywords={First keyword, Second keyword, More},
}

\begin{document}
  \maketitle

  \begin{abstract}
  \end{abstract}

  % keywords can be removed
  \keywords{First keyword \and Second keyword \and More}

  % #############################################################################
  \section{Introduction}
  Software development workflows are becoming more complex as they adapt to the
  demands of large-scale systems and modern collaborative development practices.
  As teams and codebases grow, companies face the challenge of organizing both
  effectively. When it comes to structuring the codebase, two main approaches
  emerge: monorepos and multi-repos. Monorepos consolidate all code into a
  single repository, simplifying version control but carrying a risk of
  scalability and maintainability issues. Conversely, multi-repos store the code
  in logically separated repositories, easier to manage and deploy but more
  difficult to keep in sync.

  In this paper, we propose Causify dev system, an alternative hybrid solution: a
  modular system architecture built around runnable directories. Although
  independent, these directories maintain cohesion through shared tooling and
  environments, offering a straightforward and scalable way to organize the
  codebase while ensuring reliability in development, testing, and deployment.

  %In this paper, we first outline the current state-of-the-art (Section 2), then
  %describe our approach, with a particular focus on the containerized workflows
  %that support it (Section 3). We then discuss the strengths and limitations of
  %our approach compared to existing practices (Section 4), and conclude by
  %presenting potential avenues for future improvement (Section 5).

  % #############################################################################
  \section{Runnable directories}

  \subsection{The monorepo approach}

  The monorepo approach involves storing all code for multiple applications within
  a single repository. This strategy has been popularized by large tech companies
  like Google[2], Meta[3], Microsoft[4] and Uber[5], proving that even codebases
  with billions of lines of code can be effectively managed in a single
  repository. The key benefits of this approach include:

  \begin{itemize}
    \item Consistency in environment: with everything housed in one repository, there's
    no risk of projects becoming incompatible due to conflicting versions of
    third-party packages.
    \item Simplified version control: there is a single commit history, which makes it
    easy to track and, if needed, revert changes globally.
  \item Reduced coordination overhead: developers work within the same repository,
    with easy access to all code, shared knowledge, tools and consistent coding
    standards.
  \end{itemize}

  However, as monorepo setups scale, users often face significant challenges. A
  major downside is long CI/CD build times, as even small changes can trigger
  massive rebuilds and tests throughout the entire codebase. To cope with this,
  extra tooling, such as [Buck](https://buck2.build/) or
  [Bazel](https://bazel.build/), must be configured, adding complexity to
  workflows. Even something as simple as searching and browsing the code becomes
  more difficult, often requiring specialized tools and IDE plug-ins.

  Additionally, when everything is located in one place, it is harder to separate
  concerns and maintain clear boundaries between projects. Managing permissions
  also becomes more difficult when only selected developers should have access to
  specific parts of the codebase.

\subsection{Multi-repo approach}

The multi-repo approach involves splitting code across several repositories,
with each one dedicated to a specific module or service. This modularity allows
teams to work independently on different parts of a system, making it easier to
manage changes and releases for individual components. Each repository can
evolve at its own pace, and developers can focus on smaller, more manageable
codebases.

However, the multi-repo strategy comes with its own set of challenges,
particularly when it comes to managing dependencies and ensuring version
compatibility across repositories. For instance, different repositories might
rely on two different versions of a third-party package, or even conflicting
packages, making synchronization complex or, in some cases, nearly impossible.
In general, propagating changes from one repository to another requires careful
coordination. Tools like [Jenkins](https://www.jenkins.io/) and
[GitHub Actions](https://github.com/features/actions) help streamline CI/CD
pipelines, but they often struggle when dealing with heterogeneous environments.

\subsection{Runnable directories}

An ideal strategy would combine the best of both worlds:

\begin{itemize}

  \item The modularity of multi-repos, to keep the codebase scalable and
    simplify day-to-day development processes.

  \item The environment consistency of monorepos, to avoid synchronization
    issues and prevent errors that arise from executing code in misaligned
    environments.
\end{itemize}

Both are achieved through the hybrid approach proposed in this paper, which will
be discussed in Section 3.

\begin{itemize}

  \item This section describes the design principles in our approach to create Git
    repos that contain code that can be:

    \begin{itemize}

      \item Composed through different Git sub-module

      \item Tested, built, run, and released (on a per-directory basis or not)
    \end{itemize}

  \item The technologies that this approach relies on are:

    \begin{itemize}

      \item Git for source control

      \item Python virtual environment and \texttt{poetry} (or similar) to
        control Python packages

      \item \texttt{pytest} for unit and end-to-end testing

      \item Docker for managing containers
    \end{itemize}

  \item The approach described in this paper is not strictly dependent of the specific
    package (e.g., \texttt{poetry} can be replaced by \texttt{Conda} or another
    package manager)
\end{itemize}

\subsection{Design goals}

The proposed development system supports the following functionalities

\subsubsection{Development functionalities}

\begin{itemize}

  \item Support composing code using a GitHub sub-module approach

  \item Make it easy to add the development tool chain to a ``new project'' by simply
    adding the Git sub-module \texttt{//helpers} to the project

  \item Create complex workflows (e.g., for dev and devops functionalities) using
    makefile-like tools based on Python \texttt{invoke} package

  \item Have a simple way to maintain common files across different repos in sync
    through links and automatically diff-ing files

  \item Support for both local and remote development using IDEs (e.g., PyCharm,
    Visual Studio Code)
\end{itemize}

\subsubsection{Python package management}

\begin{itemize}

  \item Carefully manage and control dependencies using Python managers (such as
    \texttt{poetry}) and virtual environments

  \item Code and containers can be versioned and kept in sync automatically since
    a certain version of the code can require a certain version of the container
    to run properly

    \begin{itemize}

      \item Code is versioned through Git

      \item Each container has a \texttt{changelog.txt} that contains the
        current version and the history
    \end{itemize}
\end{itemize}

\subsubsection{Testing}

\begin{itemize}

  \item Run end-to-end tests using \texttt{pytest} by automatically discover
    tests based on dependencies and test lists, supporting the dependencies needed
    by different directories

  \item Native support for both children-containers (i.e., Docker-in-Docker) and
    sibling containers
\end{itemize}

\subsubsection{DevOps functionalities}

\begin{itemize}

  \item Support automatically different stages for container development

    \begin{itemize}

      \item E.g., \texttt{test} / \texttt{local}, \texttt{dev}, \texttt{prod}
    \end{itemize}

  \item Standardize ways of building, testing, retrieving, and deploying
    containers

  \item Ensure alignment between development environment, deployment, and CI/CD systems
    (e.g., GitHub Actions)

  \item Bootstrap the development system using a ``thin environment'', which has
    the minimum number of dependencies to allow development and deployment in
    exactly the same way in different setups (e.g., server, personal laptop, CI/CD)

  \item Manage dependencies in a way that is uniform across platforms and OSes,
    using Docker containers

  \item Separate the need to:

    \begin{itemize}

      \item Build and deploy containers (by devops)

      \item Use containers to develop and test (by developers)
    \end{itemize}

  \item Built-in support for multi-architecture builds (e.g, for Intel \texttt{x86}
    and Arm) across different OSes supporting containers (e.g., Linux, MacOS,
    Windows Subsystem for Linux WSL)

  \item Support for developing, testing, and deploying multi-container applications
\end{itemize}

\subsubsection{Runnable directory}

The core concept of the proposed approach is a \textbf{runnable directory} --- a
self-contained, independently executable directory with code, equipped with a
dedicated DevOps setup. A repository is thus a special case of a runnable directory.
Developers typically work within a single runnable directory for a given
application, enabling them to test and deploy code without affecting other parts
of the codebase.

A runnable directory can contain other runnable directories as subdirectories.
For example, Figure 1 depicts three runnable directories: \texttt{A}, \texttt{B},
and \texttt{C}. Here, \texttt{A} and \texttt{C} are repositories, with \texttt{C}
incorporated into \texttt{A} as a submodule, while \texttt{B} is a subdirectory
within \texttt{A}. This setup provides the same accessibility as if all the code
were hosted in a single monorepo. Note that each of \texttt{A}, \texttt{B}, and \texttt{C}
has its own DevOps pipeline --- a key feature of our approach, which is discussed
further in Section 3.2.

%```mermaid
%graph RL
%    subgraph A [Runnable Dir A]
%        direction TB
%        subgraph C1 [Runnable Dir C]
%            DevOpsC1[DevOps C]
%        end
%        subgraph B [Runnable Dir B]
%            DevOpsB[DevOps B]
%        end
%        DevOpsA[DevOps A]
%    end
%    subgraph C [Runnable Dir C]
%        DevOpsC[DevOps C]
%    end
%
%    C -->|Submodule| C1
%
%    style A fill:#FFF3CD
%    style C fill:#FFF3CD,stroke:#9E9D24
%```
% Figure 1. Sample architecture of Causify's runnable directories.

\subsubsection{Docker}

Docker is the backbone of our containerized development environment. Every
runnable directory contains Dockerfiles that allow it to build and run its own
Docker containers, which include the code, its dependencies, and the runtime system.

This Docker-based approach addresses two important challenges. First, it ensures
consistency by isolating the application from variations in the host operating system
or underlying infrastructure. Second, a specific package (or package version) can
be added to the container of a particular runnable directory without affecting other
parts of the codebase. This prevents ``bloating'' the environment with packages
required by all applications --- a common issue in monorepos --- while also
effectively mitigating the risk of conflicting dependencies, which can arise in a
multi-repo setup.

Our approach supports multiple stages for container release:

\begin{itemize}

  \item Local: used to work on updates to the container; accessible only to the developer
    who built it.

  \item Development: used by all team members in day-to-day development of new
    features.

  \item Production: used to run the system by end users.
\end{itemize}

This multi-stage workflow enables seamless progression from testing to system
deployment.

It is also possible to run a container within another container's environment in
a Docker-in-Docker setup. In this case, children containers are started directly
inside a parent container, allowing nested workflows or builds. Alternatively, sibling
containers can run side by side and share resources such as the host's Docker daemon,
enabling inter-container communication and orchestration.

%```mermaid
%graph TD
%    host[Host]
%    docker_engine[Docker Engine]
%    subgraph sibling_container["Sibling Containers"]
%        container_1[Container 1]
%        container_2[Container 2]
%    end
%    subgraph children_container["Children Containers"]
%        container_1a[Container 1a]
%        container_1b[Container 1b]
%    end
%    host --> docker_engine
%    docker_engine --> container_1
%    docker_engine --> container_2
%    container_1 --> container_1a
%    container_1 --> container_1b
%
%    style sibling_container fill:#FFF3CD,stroke:#9E9D24
%    style children_container fill:#FFF3CD,stroke:#9E9D24
%```

% Figure 2. Docker container flow.

\subsubsection{What is needed}

An ideal strategy would combine the best of both worlds:

\begin{itemize}

  \item The modularity of multi-repos, to keep the codebase scalable and
    simplify day-to-day development processes.

  \item The environment consistency of monorepos, to avoid synchronization
    issues and prevent errors that arise from executing code in misaligned
    environments.
\end{itemize}

Both are achieved through the hybrid approach proposed in this paper, which will
be discussed in Section 3.

\subsubsection{Thin environment}

To bootstrap development workflows, we use a thin client that installs a minimal
set of essential dependencies, such as Docker and invoke, in a lightweight
virtual environment. A single thin environment is shared across all runnable directories
which minimizes setup overhead (see Figure 3). This environment contains
everything that is needed to start development containers, which are in turn specific
to each runnable directory. With this approach, we ensure that development and deployment
remain consistent across different systems (e.g., server, personal laptop, CI/CD).

%```mermaid
%graph RL
%  thin_env[thin environment]
%  subgraph A [Runnable Dir A]
%    direction TB
%        B[Runnable Dir B]
%        C1[Runnable Dir C]
%    end
%    subgraph C [Runnable Dir C]
%    end
%
%  C -->|Submodule| C1
%  A -.-> thin_env
%  B -.-> thin_env
%  C1 -.-> thin_env
%  C -.-> thin_env
%
%  style A fill:#FFF3CD
%  style C fill:#FFF3CD,stroke:#9E9D24
%```
%
%Figure 3. Thin environment shared across multiple runnable directories.

\subsubsection{Submodule of ``helpers''}

All Causify repositories include a dedicated ``helpers'' repository as a submodule.
This repository contains common utilities and development toolchains, such as the
thin environment, Linter, Docker, and invoke workflows. By centralizing these resources,
we eliminate code duplication and ensure that all teams, regardless of the project,
use the same tools and procedures.

Additionally, it hosts symbolic link targets for files that must technically reside
in each repository but are identical across all of them (e.g., license and
certain configuration files). Manually keeping them in sync can be difficult and
error-prone over time. In our approach, these files are stored exclusively in ``helpers'',
and all other repositories utilize read-only symbolic links pointing to them.
This way, we avoid file duplication and reduce the risk of introducing accidental
discrepancies.

%```mermaid
%graph RL
%    subgraph A [Runnable Dir A]
%        direction TB
%        B[Runnable Dir B]
%        H1[Helpers]
%    end
%    subgraph H [Helpers]
%    end
%
%    H -->|Submodule| H1
%
%    style A fill:#FFF3CD
%    style H fill:#FFF3CD,stroke:#9E9D24
%```
%
%Figure 4. "Helpers" submodule integrated into a repository.

\paragraph{3.4.1. Git hooks}

Our ``helpers'' submodule includes a set of Git hooks used to enforce policies
across our development process, including Git workflow rules, coding standards, security
and compliance, and other quality checks. These hooks are installed by default when
the user activates the thin environment. They perform essential checks such as
verifying the branch, author information, file size limits, forbidden words,
Python file compilation, and potential secret leaks\ldots etc.

\subsubsection{Executing tests}

Our system supports robust testing workflows that leverage the containerized environment
for comprehensive code validation. Tests are executed inside Docker containers
to ensure consistency across development and production environments, preventing
discrepancies caused by variations in host system configurations. In the case of
nested runnable directories, tests are executed recursively within each directory's
corresponding container, which is automatically identified (see Figure 5). As a result,
the entire test suite can be run with a single command, while still allowing tests
in subdirectories to use dependencies that may not be compatible with the parent
directory's environment.

%```mermaid
%graph LR
%    start((start))
%    start --> A
%    subgraph A[Runnable Dir A]
%        direction LR
%        pytest_1((pytest))
%        B[Runnable Dir B / Container B]
%        C[Runnable Dir C / Container C]
%        dirA1[dir1 / Container A]
%        dirA2[dir2 / Container A]
%        dirA11[dir1.1 / Container A]
%        dirA12[dir1.2 / Container A]
%        pytest_1 --> B
%        pytest_1 --> C
%        pytest_1 --> dirA1
%        pytest_1 --> dirA2
%        dirA1 --> dirA11
%        dirA1 --> dirA12
%    end
%
%style A fill:#FFF3CD,stroke:#9E9D24
%style B font-size:15px
%style C font-size:15px
%```
%
%Figure 5. Recursive test execution in dedicated containers.

\subsubsection{Dockerized executables}

Sometimes, installing a package within a development container may not be
justified, particularly if it is large and will only be used occasionally. In
such cases, we use \emph{dockerized executables}: when the package is needed, a
Docker container is dynamically created with only the specific dependencies
required for its installation. The package is then installed and executed within
the container, which is discarded once the task is complete. This prevents the development
environment from becoming bloated with dependencies that are rarely used. If
necessary, for example during test execution, a dockerized executable can be run
inside another Docker container, whether using the children or sibling container
approach, as discussed in Section 3.2.

\subsection{Discussion}

Causify's approach presents a strong alternative to existing code organization solutions,
offering scalability and efficiency for both small and large systems.

The proposed modular architecture is centered around runnable directories, which
operate as independent units with their own build and release lifecycles. This
design bypasses the bottlenecks common in large monorepos, where centralized workflows
can slow down CI/CD processes unless specialized tools like Buck or Bazel are used.
By leveraging Docker containers, we ensure consistent application behavior across
development, testing, and production environments, avoiding problems caused by
system configuration discrepancies. Dependencies are isolated within each directory's
dedicated container, reducing the risks of issues that tight coupling or package
incompatibility might create in a monorepo or a multi-repo setup.

Unlike multi-repos, runnable directories can utilize shared utilities from ``helper''
submodules, eliminating code duplication and promoting consistent workflows
across projects. They can even reside under a unified repository structure which
simplifies codebase management and reduces the overhead of maintaining multiple repositories.
With support for recursive test execution spanning all components, runnable directories
allow for end-to-end validation of the whole codebase through a single command,
removing the need for testing each repository separately.

There are, however, several challenges that might arise in the adoption of our
approach. Teams that are unfamiliar with containerized environments may need time
and training to effectively transition to the new workflows. The reliance on
Docker may introduce additional resource demands, particularly when running multiple
containers concurrently on development machines. This would require further
optimization, possibly aided by customized tooling. These adjustments, while ultimately
beneficial, can add complexity to the system's rollout and necessitate ongoing
maintenance to ensure seamless integration with existing CI/CD pipelines and
development practices.

\subsection{Future directions}

Looking ahead, there are several areas where the proposed approach can be
improved. One direction is the implementation of dependency-aware caching to ensure
that only the necessary components are rebuilt or retested when changes are made.
This would reduce the time spent on development tasks, making the overall process
more efficient. Further optimization could involve designing our CI/CD pipelines
to execute builds, tests, and deployments for multiple runnable directories in
parallel, which would allow us to take full advantage of available compute resources.

Additional measures can also be taken to enhance security. Integrating automated
container image scanning and validation before deployment would help guarantee
compliance with organizational policies and prevent vulnerabilities from
entering production environments. In addition, fine-grained access controls
could be introduced for runnable directories in order to safeguard sensitive
parts of the codebase. These steps will bolster both the security and efficiency
of our workflows as the projects continue to scale.

\section{Buildmeister: Daily Accountability for CI Stability}

\subsection{Motivation}

Automated test pipelines are essential, but without accountability, they often fall
into disrepair. The Buildmeister routine introduces a rotating, human-in-the-loop
system designed to enforce green builds, identify root causes, and ensure high-quality
CI/CD hygiene. This mechanism aligns technical execution with team
responsibility, fostering a culture of operational ownership.

\subsection{Core Responsibilities}

The Buildmeister is a rotating role assigned to a team member each week. Their primary
duties are:

\begin{itemize}

  \item Monitor build health daily via the Buildmeister Dashboard

  \item Investigate failures and ensure GitHub Issues are filed promptly

  \item Push responsible team members to fix or revert breaking code

  \item Maintain test quality by analyzing trends in Allure reports

  \item Document breakage through a structured post-mortem log
\end{itemize}

The Buildmeister ensures builds are never ``temporarily broken'', our policy is:
``Fix it or revert within one hour.''

\subsection{Handover and Daily Reporting}

The routine begins each day with a status email to the team detailing:

\begin{itemize}

  \item Overall build status (green/red)

  \item Failing test names and owners

  \item GitHub issue references

  \item Expected resolution timelines

  \item A screenshot of the Buildmeister dashboard
\end{itemize}

At the end of each rotation, the outgoing Buildmeister must confirm handover by receiving
an ``Acknowledged'' reply from the incoming one, ensuring continuity and
awareness.

\subsection{Workflow in Practice}

When a build breaks:

\begin{itemize}

  \item The team is alerted via Slack (\#build-notifications) through our GitHub
    Actions bot

  \item The Buildmeister triages the issue:

    \begin{itemize}

      \item Quickly reruns or replicates the failed tests if uncertain

      \item Blames commits to identify the responsible party

      \item Notifies the team and files a structured GitHub Issue
    \end{itemize}

  \item All information including test names, logs, responsible engineer are transparently
    shared and tracked
\end{itemize}

If the issue is not resolved within one hour, the Buildmeister must escalate and,
if needed, disable the test with explicit owner consent.

% #############################################################################
\subsection{Tools and Analysis}

\subsubsection{Buildmeister Dashboard}

A centralized UI provides a real-time view of all builds across repos and branches.
It is the Buildmeister's daily launchpad.

\subsubsection{Allure Reports}

\begin{itemize}

  \item Every week, the Buildmeister reviews trends in skipped/failing tests, duration
    anomalies, and retry spikes

  \item This process:

    \begin{itemize}

      \item Surfaces hidden test instability

      \item Provides historical context to new breaks

      \item Enables preventive action before regressions cascade
    \end{itemize}
\end{itemize}

\subsubsection{Post-Mortem Log}

Every build break is logged in a shared spreadsheet, capturing:

\begin{itemize}

  \item Repo and test type

  \item Link to the failing GitHub run

  \item Root cause

  \item Owner and fix timeline

  \item Whether the issue was fixed or test was disabled
\end{itemize}

This living record forms the basis for failure mode analysis and future automation
improvements.

\subsection{Why It Matters}

The Buildmeister is not just a rotating duty, it is a system of shared accountability.
It transforms test stability from an abstract ideal into a daily operational
habit, backed by clear expectations, defined processes, and human enforcement.
By combining automation with ownership, we achieve sustainable reliability in a complex,
multi-repo ecosystem.

% #############################################################################
\section{Coverage Tracking with Codecov: A Layer of Continuous Accountability}

\subsection{Motivation}

Maintaining comprehensive test coverage across a growing codebase requires more
than just writing tests, it demands visibility, automation, and enforcement. Our
integration with Codecov provides a system-wide view of test coverage, structured
into fast, slow, and superslow test suites. This setup ensures that all code
paths are exercised and that test coverage regressions are identified early and
reliably.

\subsection{Structured Coverage by Test Category}

We categorize coverage tests into three suites based on runtime and scope:

\begin{itemize}

  \item Fast tests run frequently (e.g., daily) and provide immediate feedback on
    high-priority code paths

  \item Slow tests cover broader logic and data scenarios

  \item Superslow tests are comprehensive, long-running regressions executed on
    a weekly cadence or on-demand
\end{itemize}

Each suite produces its own coverage report, which is flagged and uploaded independently
to Codecov, enabling targeted inspection and carryforward of data when some
suites are skipped.

\subsection{CI Integration and Workflow Behavior}

Coverage reports are generated and uploaded automatically as part of our CI pipelines.
The workflow:

\begin{itemize}

  \item Fails immediately on critical setup errors (e.g., dependency or
    configuration issues)

  \item Continues gracefully if fast or slow tests fail mid-pipeline, but
    surfaces those failures in a final gating step

  \item Treats superslow failures as critical, immediately halting the workflow
\end{itemize}

This behavior ensures resilience while preventing silent test degradation.

\subsection{Enforced Thresholds and Quality Gates}

Coverage checks are enforced at both project and patch levels:

\begin{itemize}

  \item Project-level threshold: Pull requests fail if overall coverage drops
    beyond a configured margin (e.g., \textgreater1\%)

  \item Patch-level checks: Changes are required to maintain or improve coverage
    on modified lines

  \item Flags and branches: Checks are scoped per test suite and only enforced
    on critical branches
\end{itemize}

Together, these gates maintain coverage integrity while avoiding noise from
unrelated code paths.

\subsection{Visibility and Developer Experience}

Codecov is integrated tightly into the developer workflow:

\begin{itemize}

  \item PRs show inline coverage status and file-level diffs

  \item Optional summary comments detail total coverage, changes, and affected files

  \item Reports can be viewed in Codecov's UI or served locally as HTML

  \item Carryforward settings retain historical data when full test suites aren't
    executed
\end{itemize}

Developers can also generate and inspect local reports for any test suite using
standard coverage commands.

\subsection{Best Practices and Operational Consistency}

To ensure effective usage:

\begin{itemize}

  \item Coverage is always uploaded---even if tests fail---ensuring no blind spots

  \item Developers are encouraged to monitor coverage deltas in PRs

  \item The system defaults to global configuration, but supports fine-tuning
    via repo-specific overrides

  \item Weekly reviews of coverage trends and flags help spot regressions and low-tested
    areas
\end{itemize}

\subsection{Beyond the Basics}

Our setup also supports:

\begin{itemize}

  \item PR commenting: Optional automated comments on test impact

  \item Badges: Live indicators of coverage status

  \item Custom reporting: Layouts and thresholds can be adjusted to align with
    evolving policies
\end{itemize}

\subsection{Summary}

Coverage tracking is more than a checkbox---it's an enforcement mechanism, a
feedback loop, and a source of engineering discipline. With structured test
categories, resilient workflows, and project-level gates, our Codecov-based system
transforms coverage data into actionable insights, reinforcing test quality
across all levels of the stack.

% #############################################################################
\section{GitHub Automation: Scalable Metadata Management Through Declarative Workflows}

\subsection{Motivation}

As organizations grow, so does the surface area of their GitHub ecosystem---more
repositories, more contributors, and more operational complexity. Without strong
conventions and enforcement, labels diverge across projects, and team boards
lose structural consistency. This leads to disjointed workflows, unclear
ownership, and fragmented planning. To mitigate this, we introduced a declarative
automation system for GitHub metadata. It enables centralized definitions for
labels and project structures and propagates them across the organization in a
reproducible and scalable way.

\subsection{Standardized Label Infrastructure}

Issue labels form the foundation for triage, workflows, and reporting. At scale,
inconsistent naming, coloring, or descriptions can fragment automation and reduce
clarity. To address this, we maintain a centralized manifest that defines the entire
organization's label taxonomy. Each label includes a name, color code, and
description. Repositories synchronize against this manifest using a containerized
process that:

\begin{itemize}

  \item Creates missing labels using manifest definitions

  \item Updates outdated labels (e.g., if the description or color changes)

  \item Backs up existing labels before applying changes

  \item Supports dry-run execution for visibility and confidence

  \item Optionally prunes unused labels not defined in the manifest
\end{itemize}

This guarantees all repositories speak a consistent language for issues and pull
requests, improving developer experience and enabling reliable automation.

\subsection{Template-Based Project Synchronization}

GitHub Projects (Beta) are a powerful tool for planning and tracking, but manually
configuring project fields and views across teams is tedious and error-prone. To
streamline setup and reduce drift, we introduced a project templating system.
Project metadata, such as fields and views is defined in a canonical source project
and synced into destination projects. The system:

\begin{itemize}

  \item Adds missing global fields to ensure functional parity across projects

  \item Logs discrepancies in view structure (e.g., missing ``Backlog'' or ``Current
    sprint'' views)

  \item Preserves existing configurations to avoid accidental data loss

  \item Operates in a dry-run mode to preview proposed changes safely
\end{itemize}

Due to current API limitations, view creation, layout configuration, and field
ordering cannot yet be automated. However, warnings are logged for manual follow-up,
and the system is designed to evolve as GitHub expands its GraphQL support.

\subsection{Design Principles and Workflow Behavior}

These automation tools follow several core principles:

\begin{itemize}

  \item Declarative Configuration: Metadata is defined in structured YAML files
    stored in version control, serving as the source of truth

  \item Reproducible Execution: All sync operations run in isolated Docker
    containers, ensuring consistent behavior across machines and CI pipelines

  \item Non-Destructive by Default: No changes are applied unless explicitly
    confirmed, and current states can be backed up for rollback

  \item Extensibility: The architecture is modular and designed to incorporate
    future GitHub API enhancements, including view creation and layout syncing
\end{itemize}

The workflows support both interactive execution and automated pipelines,
allowing integration into CI/CD systems or local tooling as needed.

\subsection{Declarative Repository Settings Synchronization}

As our GitHub footprint has grown, so has the need to manage repository
configuration such as metadata fields, merge policies, and branch protection
rules in a reliable and consistent manner across dozens of projects.
Manual configuration inevitably leads to drift, accidental misconfiguration,
and wasted effort during onboarding or project setup. To address this, we
introduced a \textit{declarative} synchronization system for repository
settings, inspired by the principles of infrastructure-as-code. Repository
policiesâ€”including key settings (description, default branch, visibility,
merge strategies, etc.) and fine-grained branch protection rules are defined
centrally in a YAML manifest. This manifest serves as the single source of
truth for desired state. A containerized executable (run locally or in CI) can
then \textbf{export} the current settings of any repository for inspection,
backup, or review, and can \textbf{apply} (sync) the manifest to one or more
repositories, ensuring alignment with organizational standards. The system also
supports a dry-run mode for safe preview of all proposed changes.

This approach delivers several benefits:
\begin{itemize}
    \item \textbf{Consistency:} All repositories adhere to the same baseline policies,\\
    reducing configuration drift and error.
    \item \textbf{Auditability:} Changes to settings are reviewed and version-controlled\\
    like code, supporting transparent change management.
    \item \textbf{Scalability:} New projects can be onboarded with a single command,\\
    inheriting standardized policies from day one.
    \item \textbf{Recoverability:} Before each sync, the system backs up the existing configuration,\\
    enabling rollbacks if needed.
    \item \textbf{Confidence:} Dry-run and logging capabilities provide full visibility into\\
    what changes would be made, supporting both local experimentation and safe automation in CI/CD pipelines.
\end{itemize}


\subsection{Summary}

Our GitHub automation system codifies repository metadata into a version-controlled,
declarative format. By automating label and project synchronization, we
eliminate manual drift, accelerate onboarding, and enforce consistency at scale.
This infrastructure-as-code approach brings the same discipline to GitHub configuration
that we apply to code and deployment, enabling confident, scalable collaboration
across all teams.

  % #############################################################################
  \section{}

%  \begin{figure}
%    \centering
%    \fbox{\rule[-.5cm]{4cm}{4cm} \rule[-.5cm]{4cm}{0cm}}
%    \caption{Sample figure caption.}
%    \label{fig:fig1}
%  \end{figure}

  \subsection{Tables}
  See awesome Table~\ref{tab:table}.

  The documentation for \verb+booktabs+ (`Publication quality tables in LaTeX')
  is available from:
  \begin{center}
    \url{https://www.ctan.org/pkg/booktabs}
  \end{center}

  \begin{table}
    \caption{Sample table title}
    \centering
    \begin{tabular}{lll}
      \toprule \multicolumn{2}{c}{Part} \\
      \cmidrule(r){1-2} Name           & Description     & Size ($\mu$m)  \\
      \midrule Dendrite                & Input terminal  & $\sim$100      \\
      Axon                             & Output terminal & $\sim$10       \\
      Soma                             & Cell body       & up to $10^{6}$ \\
      \bottomrule
    \end{tabular}
    \label{tab:table}
  \end{table}

  \bibliographystyle{unsrtnat}
  %%% Uncomment this line and comment out the ``thebibliography'' section below to use the external .bib file (using bibtex) .
  \bibliography{references}

%- [1]
%  [Mono vs. multi-repo](https://free.gitkraken.com/hubfs/Mono_v_Multi-Repo_debate_2023.pdf)
%- [2]
%  [Why Google stores billions of lines of code in a single repository](https://dl.acm.org/doi/10.1145/2854146)
%- [3]
%  [What it is like to work in Meta's (Facebook's) monorepo](https://blog.3d-logic.com/2024/09/02/what-it-is-like-to-work-in-metas-facebooks-monorepo/)
%- [4]
%  [Microsoft: How "Mono-repo" and "One Infra" Help Us Deliver a Better Developer Experience](https://devblogs.microsoft.com/appcenter/how-mono-repo-and-one-infra-help-us-deliver-a-better-developer-experience/)
%- [5]
%  [Uber: Faster Together: Uber Engineering's iOS Monorepo](https://www.uber.com/blog/ios-monorepo/)


  %%% Uncomment this section and comment out the \bibliography{references} line above to use inline references.
  % \begin{thebibliography}{1}

  %     \bibitem{kour2014real}
  %     George Kour and Raid Saabne.
  %     \newblock Real-time segmentation of on-line handwritten arabic script.
  %     \newblock In {\em Frontiers in Handwriting Recognition (ICFHR), 2014 14th
  %             International Conference on}, pages 417--422. IEEE, 2014.

  %     \bibitem{kour2014fast}
  %     George Kour and Raid Saabne.
  %     \newblock Fast classification of handwritten on-line arabic characters.
  %     \newblock In {\em Soft Computing and Pattern Recognition (SoCPaR), 2014 6th
  %             International Conference of}, pages 312--318. IEEE, 2014.

  %     \bibitem{hadash2018estimate}
  %     Guy Hadash, Einat Kermany, Boaz Carmeli, Ofer Lavi, George Kour, and Alon
  %     Jacovi.
  %     \newblock Estimate and replace: A novel approach to integrating deep neural
  %     networks with existing applications.
  %     \newblock {\em arXiv preprint arXiv:1804.09028}, 2018.

  % \end{thebibliography}
\end{document}
