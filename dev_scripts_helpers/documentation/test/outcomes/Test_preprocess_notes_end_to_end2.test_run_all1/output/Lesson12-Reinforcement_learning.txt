---
fontsize: 10pt
---
\let\emph\textit
\let\uline\underline
\let\ul\underline

::: columns
:::: {.column width=15%}
![](lectures_source/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\textcolor{blue}{\text{Probabilistic Reinforcement Learning}}}$$**
\endgroup
\vspace{1cm}

**Instructor**: Dr. GP Saggese - `gsaggese@umd.edu`

**References**:

- AIMA Chap 17: Making complex decisions
- AIMA Chap 22: Reinforcement Learning

#### Sequential Decision Problems
- **\textcolor{purple}{Sequential Decision Problems}**
  - Utilities Over Time
  - Algorithms for MDPs
- Reinforcement Learning



#### Sequential Decision Problems
- **\textcolor{red}{Agents need to make decisions in the real world}**:
  - In a stochastic environment (randomness, unpredictability)
    - E.g., weather conditions affecting a delivery route
  - Where utility depends on a sequence of decisions
    - E.g., planning a multi-step journey where each step influences the next

- **\textcolor{blue}{What is involved}**
  - Uncertainty
    - Represent the lack of certainty in outcomes, modeled using probabilities
    - E.g., weather forecasts often include uncertainty (70% chance of rain)
  - Utility functions
    - Measure the desirability of outcomes by quantifying preferences
    - E.g., assign higher values to outcomes with more profit and lower risk
  - Rewards
    - Yielded by the environment as feedback for actions taken
    - E.g., receive points in a game for completing a level
  - Sensing
    - Gather information about the environment using sensors
    - E.g., a robot using a camera to detect obstacles in its path
  - Search and planning
    - Find a sequence of actions to achieve a goal
    - E.g., a GPS system planning the shortest route to a destination

#### Markov Decision Process
- **\textcolor{red}{MDPs}** are a formal model for sequential decision making

- **\textcolor{blue}{Assumptions}**
  - Fully observable but stochastic environment
    - Sensors give agent the complete state of the environment $\forall t$
    - Next state is not completely determined by current state and agent's action
  - Initial state $s_0$
  - An agent takes action $a \in Actions(s)$ in each state $s$
  - Transition model
    - $\Pr(s' | s, a)$ is probability of reaching state $s'$, if action $a$ is
      done in state $s$
    - Markov assumption: probability depends on $(s, a)$, not on history
  - Reward function
    - For every transition $s \to s'$ via $a$ the agent receives a reward
      $R(s, a, s')$
    - Total reward depends on sequence of states and actions, i.e., environment
      history
  - Goal states

::: columns
:::: {.column width=70%}
- E.g., a robot navigating a slippery surface
  - It knows its exact location and the map (fully observable)
  - Its wheels may slip unpredictably (stochastic outcome)
::::
:::: {.column width=25%}
![](lectures_source/figures/Lesson12_Star_Wars_droid.png)
::::
:::


#### MDP: Solution

- The solution of an MDP is a **\textcolor{red}{policy}** _"in state $s$ take action $a$"_
  $$
  \pi(s): s \to a \in Actions(s)
  $$
  - Any execution of the policy leads to a different **\textcolor{darkgreen}{environment history}**
    because of the stochastic nature of the environment
  - Environment history is a sequence of states and actions
    $(s_0, a_0) \to (s_1, a_1) \to ... \to (s_i, a_i) \to ...$

- A policy is measured by the **\textcolor{blue}{expected utility}** of the environment history
  $$
  U(\pi(s)) = \EE[f(s_0, \pi(s), R(s, a, s'))]
  $$
- The **\textcolor{teal}{optimal policy}** $\pi^*(s)$ yields the highest expected utility
  $$
  \pi^*(s) = \argmax_{\pi} \; \EE[f(s_0, \pi(s), R(s, a, s'))]
  $$
  - Note: the optimal policy is a function of the reward function


- MDP is often solved with **\textcolor{violet}{dynamic programming}**
  1. Break the problem in smaller pieces recursively
  2. Solve the sub-problems
  3. Remember solutions of the pieces

#### MDP: 4x3 Environment Example

::: columns
:::: {.column width=70%}
- **\textcolor{red}{Environment}**
  - A 4 x 3 grid world
  - Fully observable: the agent always knows its location
  - Non-deterministic: actions are not reliable
    - Pr(intended action) = 0.8
    - Pr(move right/left angle) = 0.1
- **\textcolor{darkgreen}{Agent}**
  - Begin at the `START` cell
  - Choose actions `Up`, `Down`, `Left`, `Right` at each step
  - Aim to reach goal states marked `+1` or `-1`
- **\textcolor{blue}{Transition Model}**
  - Result of each action in each state $\Pr(s' | s, a)$
- **\textcolor{violet}{Utility Function}**
  - The reward for each state transition $s \to s'$ via action $a$ is
    $R(s, a, s')$
    - -0.04 for all transitions (to encourage reaching terminal states swiftly)
    - +1 or -1 upon reaching terminal states
  - Total utility is the sum of all received rewards
::::
:::: {.column width=25%}

![](lectures_source/figures/Lesson12_4x3_environment1.png)

\vspace{0.5cm}

\begin{center}
\includegraphics[width=0.5\textwidth]{lectures_source/figures/Lesson12_4x3_environment2.png}
{\scriptsize\text{Valid actions}}
\end{center}

\vspace{0.5cm}

\begin{center}
\includegraphics[width=1.0\textwidth]{lectures_source/figures/Lesson12_4x3_environment3.png}
{\scriptsize\text{Example of optimal policy}}
\end{center}
::::
:::

#### Utilities Over Time
- Sequential Decision Problems
  - **\textcolor{purple}{Utilities Over Time}**
  - Algorithms for MDPs
- Reinforcement Learning


#### Utility Function
- The **\textcolor{red}{utility function}** for environment histories (finite or infinite) is
  expressed as:
  $$
  U_h([\textcolor{blue}{s_0, a_0}, \textcolor{darkgreen}{s_1, a_1}, ..., s_n, ...])
  $$

- A **\textcolor{darkgreen}{finite horizon}** indicates a fixed time $N$ after which nothing matters:
  $$
  U_h([s_0, a_0, s_1, ..., \textcolor{red}{s_N, ...s_{N+k}}])
  = U_h([s_0, a_0, s_1, ..., \textcolor{red}{s_N}]) \; \forall k > 0
  $$
  - Actions are chosen based on the current state and remaining steps
  - Lead to non-stationary policies

- **\textcolor{blue}{Infinite horizon}**
  - No fixed time limit, i.e., the process continues indefinitely
  - Utility is often defined using a discount factor $\gamma < 1$ for
    convergence
  - The optimal policy can be **\textcolor{violet}{stationary}**
    - Policies do not depend on the specific time step
    - Same action is chosen whenever the agent visits the same state

#### Additive Rewards
- **\textcolor{red}{Additive rewards}**:
  - Rewards for each transition $s_i \xrightarrow{a_i} s_{i+1}$ are summed:
    $$
    U_h([s_0, a_0, s_1, a_1, \ldots]) = \sum_{i=0} R(s_i, a_i, s_{i+1})
    $$

- **\textcolor{darkgreen}{Additive discounted rewards}**:
  - Include a discount factor $\gamma \in [0, 1]$:
    $$
    \begin{aligned}
    U_h([s_0, a_0, s_1, a_1, \ldots])
    &= R(s_0, a_0, s_1) + \gamma R(s_1, a_1, s_2) + \gamma^2 R(s_2, a_2, s_3) + \ldots \\
    &= \sum_{i=0} \gamma^i R(s_i, a_i, s_{i+1}) \\
    \end{aligned}
    $$
    where:
    - $\gamma = 1$: purely additive rewards
    - $\gamma \to 0$: future rewards are negligible
    - $\gamma \to 1$: future rewards significant

- **\textcolor{blue}{Pros of discounted rewards}**:
  - Reflect human tendency to prioritize near-term rewards
  - In economics, early rewards can be reinvested, compounding further rewards
  - Support infinite horizons, preventing unbounded rewards

#### Expected Utility of a Policy
- We said that a policy leads to different environment histories
- The expected **\textcolor{red}{utility of executing policy}** $\pi$ from state $s$:
  $$
  U^\pi(s) = \EE[ \sum_{i=0}^\infty \gamma^t R(S_i, \pi(S_i), S_{i+1}) ]
  $$
  where the expectation $\EE[\cdot]$ is:
  - Over state sequences determined by $s$, $\pi(s)$
  - The environment's transition model $\Pr(s' | s, a)$

- The agent should choose the **\textcolor{blue}{optimal policy}**:
  $$
  \pi_s^* = \argmax_{\pi} U^{\pi}(s)
  $$
  - With discounted utilities and infinite horizons, the optimal policy is
    independent of the starting state: $\pi_s^* = \pi^*$
  - This is not true for finite-horizon policies or other reward combinations

#### Principle of Maximum Expected Utility (MEU)

- MEU posits: _"A rational agent should choose the action that maximizes its
  expected utility based on its beliefs"_

- **\textcolor{red}{Formal Definition}**
  - Possible actions: $a \in A$
  - Possible outcomes: $s'$
  - Probability distribution: $\Pr(s'|a)$ for each action
  - Utility function: assign a numerical value $U(s')$ to each outcome
  - The expected utility of action $a$ is (recursive):
    $$
    EU(a) = \EE[U(a)] = \sum_{s'} U(s') \Pr(s'|a)
    $$
  - Choose the action $a^*
    = \argmax_{a \in A} \sum_{s'} U(s') \Pr(s'|a)$

- **\textcolor{blue}{Example}**:
  - E.g., an agent must choose between:
    - `Action A`: 80% chance of reward 10; 20% chance of reward 0
    - `Action B`: 100% chance of reward 6
  - By MEU, choose `Action A`, since $EU(A) = 0.8 \cdot 10 + 0.2 \cdot 0 = 8$ >
    $EU(B) = 1.0 \cdot 6 = 6$

#### MDP: Tetris Example

::: columns
:::: {.column width=20%}
![](lectures_source/figures/Lesson12_Tetris.png)
::::
:::: {.column width=80%}

- **\textcolor{red}{States $S$}**
  - Current board configuration and falling piece
- **\textcolor{darkgreen}{Actions $A$}**
  - Valid final placements of the piece
    - Rotation (0â€“3 positions)
    - Horizontal movement (left, right)
    - Hard drop (instant placement)
- **\textcolor{blue}{Transition Model $T(s, a, s')$}**
  - Deterministic or stochastic based on next piece modeling
  - Piece generation often random (uniform or "bag" system)
- **\textcolor{teal}{Reward $R(s, a, s')$}**
  - +1 for each cleared line
  - Negative reward for new block addition or height increase
  - Game over may have large negative reward
- **\textcolor{violet}{Discount Factor $\gamma$}**
  - Close to 1 for valuing long-term survival and line-clearing
::::
:::

#### Utility of a State

- The utility of a state $s$, $U(s)$, reflects the long-term desirability of a
  state under optimal behavior
  - I.e., $U(s) = U^{\pi^*}(s)$, the expected sum of discounted rewards under an
    optimal policy from $s$
  - To remove the dependency from the policy, use the optimal policy
  - Calculated based on the expected rewards and the discount factor

::: columns
:::: {.column width=70%}
- **\textcolor{red}{Example}**:
  - In a 4x3 environment, the utility of a state is:
    - Higher closer to the +1 state, as fewer steps are needed to reach it
    - Lower for the one close to the -1 state, since the agent needs to go
      around it
  - E.g., if the agent is two steps away from the +1 state, the utility will be
    higher compared to being four steps away
  - This assumes certain reward (e.g., $\gamma = 1$ and $r = -0.04$ for
    non-terminal transitions)
::::
:::: {.column width=30%}
![](lectures_source/figures/Lesson13_4x3_environment_utility_states.png)
::::
:::

#### Bellman Equation
- The **\textcolor{red}{utility of a state}** $s$ is the expected reward for the next transition
  plus the discounted utility of the next state, assuming the agent chooses the
  optimal action:
  $$
  U(s) = \max_{a \in A(s)} \sum_{s'} \Pr(s' | s, a)[R(s, a, s') + \gamma U(s')]
  $$
  where:
  - $A(s)$: set of actions available in state $s$
  - $\Pr(s' | s, a)$: probability of transitioning to state $s'$ from state $s$
    by action $a$
  - $R(s, a, s')$: reward after transitioning from state $s$ to $s'$ using $a$
  - $\gamma$: discount factor, where $0 \leq \gamma < 1$

- Writing Bellman equations for all states gives a system of equations
  - Each state has its own equation based on its possible actions and
    transitions
  - Each equation is recursive: utility of $s$ depends on utilities of its
    successor states

- Under certain conditions (e.g., finite state/action spaces, $\gamma < 1$):
  - This system has a unique solution
  - The utility function $U(s)$ is well-defined
  - E.g., in a grid world with a finite number of cells and actions

#### Bellman Equation: Intuition
- The **\textcolor{red}{Bellman Equation:}** says
  $$
  \textit{"Utility of a state = Best immediate action + Future potential"}
  $$
  - Balances short-term gain and long-term value when outcomes are partly under
    the control of a decision-maker and partly random

- E.g., to find the fastest path to the goal in a maze, the Bellman equation
  prescribes:
  - _"Your current position is only as valuable as the best path out of it"_
  - Best path combines current proximity (reward now) and future position
    quality (reward later)
  - Value backs up from future to present, similar to tracing a route from
    finish to start

- E.g., in a chess game, the optimal strategy involves making the best move at
  each turn while considering future moves and potential outcomes

#### Q-Function
- Aka "Action-utility function"
- The Q-function $Q(s, a)$ is the expected utility of taking an _action_ in _a
  given state_
  - Gives the expected value of choosing action $a$ in state $s$, and then
    acting optimally afterward

- Utility of actions $Q(s, a)$ is the "dual" view of utility of states $U(s)$
  - Express the utility of a state in terms of utility of actions:
    $$
    U(s) = \max_a Q(s, a)
    $$
  - Bellman equation for Q-functions
    $$
    Q(s, a) = \sum_{s'} \Pr(s'|s, a)[R(s, a, s') + \gamma \max_{a'} Q(s', a')]
    $$
  - An optimal policy picks the "best" action:
    $$
    \pi^*(s) = \argmax_a Q(s, a)
    $$

#### Shaping Theorem
- For discounted sums of rewards, the **\textcolor{red}{scale of utilities}** is arbitrary:
  - An affine transformation $U'(s) = m \cdot U(s) + b$ does not change the
    optimal policy $\pi^*(s)$
  - What matters for decision-making is the relative ordering of utilities
    (which is preserved)

- More generally, a **\textcolor{darkgreen}{potential-based reward shaping}**, i.e., using a function
  $\Phi(s)$ of the state $s$ doesn't change the optimal policy:
  $$
  R'(s, a, s') = R(s, a, s') + \gamma \Phi(s') - \Phi(s)
  $$
  - It ensures the difference in value between states remains consistent

- **\textcolor{blue}{Pros}**
  - Speed: Can significantly speed up learning by guiding the agent
    - E.g., adding a potential function that increases with proximity to a goal
      can encourage faster convergence
    - E.g., animal trainers provide a small treat to the animal for each step in
      the target sequence
  - Safety: Prevents misleading the agent into a suboptimal policy
    - E.g., an agent might prioritize short-term rewards over long-term gains

#### Representing MDP
- The transition model $\Pr(s' | s, a)$ and the reward function $R(s, a, a')$
  can be represented with:
  - Three-dimensional tables of size $|S|^2 \cdot |A|$
  - For sparse MDPs (i.e., each $s$ transitions to only a few states $s'$), the
    table size is $O(|S| \cdot |A|)$

- MDPs can be represented using **\textcolor{red}{Dynamic Decision Networks}** (DDNs):
  - DDNs are a type of probabilistic graphical model extending Bayesian networks
    for sequential decision problems
  - DDNs offer a factored representation, compactly encoding state variables and
    dependencies
  - They are more scalable and expressive than atomic (flat) representations
    - E.g., in a large MDP with many states, a DDN can efficiently represent the
      problem without explicitly listing every possible state transition

#### Dynamic Decision Networks: Tetris Example

::: columns
:::: {.column width=50%}

- DDN models Tetris in terms of time slices with the game's state, actions, and
  rewards
  - **\textcolor{red}{State variables}**:
    - $Board_t$: grid configuration at time $t$
    - $Piece_t$: current piece falling
    - $NextPiece_t$: upcoming piece
  - **\textcolor{darkgreen}{Decision variable}**:
    - $Action_t$: placement of $Piece_t$ (rotation and position)
  - **\textcolor{blue}{Chance nodes}** (transition):
    - $Board_{t+1}$: board after action
    - $Piece_{t+1}$: next piece, depending on $NextPiece_t$ or random selection
  - **\textcolor{violet}{Utility node}**:
    - $Reward_t$: derived from $Board_{t+1}$ (e.g., lines cleared, holes created)
::::
:::: {.column width=45%}
```graphviz
digraph TetrisDDN {
    rankdir=LR;

    // Time t
    subgraph cluster_t {
        label="Time t";
        Board_t [label="Board_t", shape=ellipse];
        Piece_t [label="Piece_t", shape=ellipse];
        Action_t [label="Action_t", shape=box, style=filled, fillcolor=lightblue];
        Reward_t [label="Reward_t", shape=diamond, style=filled, fillcolor=lightgreen];

        Board_t -> Action_t;
        Piece_t -> Action_t;
        Board_t -> Reward_t;
        Piece_t -> Reward_t;
        Action_t -> Reward_t;
    }

    // Time t+1
    subgraph cluster_t1 {
        label="Time t+1";
        Board_t1 [label="Board_{t+1}", shape=ellipse];
        Piece_t1 [label="Piece_{t+1}", shape=ellipse];
        NextPiece_t1 [label="NextPiece_{t+1}", shape=ellipse];
    }

    // Temporal connections
    Action_t -> Board_t1;
    Board_t -> Board_t1;
    Piece_t -> Piece_t1;
    Board_t1 -> Reward_t;
}
```
::::
:::

#### Algorithms for MDPs
- Sequential Decision Problems
  - Utilities Over Time
  - **\textcolor{purple}{Algorithms for MDPs}**
- Reinforcement Learning


#### Value Iteration (1/2)
- **\textcolor{red}{Value iteration}** solves MDPs using 2 steps:
  - Compute optimal utility for each state $U(s)$
  - Extract optimal policy $\pi^*$ from utilities $U(s)$

- **\textcolor{blue}{Step 1}**: compute optimal utility for each state
  - There are $n$ possible states, so $n$ Bellman equations, one per state
    $$
    U(s) = \max_{a \in A(s)} \sum_{s'} \Pr(s' | s, a)[R(s, a, s') + \gamma U(s')]
    $$
  - Each equation relates the utility of a state to the utilities of its
    successors
  - The state utilities $U(s)$ are $n$ unknowns
  - Solve these equations $n$ equations with $n$ unknowns simultaneously
    - Problem: equations are non-linear due to max operator
    - Solution: use an iterative approach

#### Value Iteration (2/2)

- **\textcolor{red}{Solve system of Bellman equations}**
  - Start with arbitrary values for utilities $U(s) = 0$
  - Perform Bellman updates:
    $$
    U_{i+1}(s) \leftarrow \max_a \sum_{s'} \Pr(s'|s,a)[R(s,a,s') + \gamma U_i(s')]
    $$
  - Calculate the right-hand side and plug it into the left-hand side
  - No strict update order required for convergence, but intelligent ordering
    can improve speed, especially in large or structured MDPs
  - Repeat until equilibrium or close to convergence
    $|| U_{i+1} - U_i || < \epsilon$
  - Guaranteed to converge to the unique fixed point (optimal policy) for
    additive discounted rewards and $\gamma < 1$

- **\textcolor{blue}{Step 2}**: compute optimal policy
  - Derive optimal policy by choosing action $a$ that maximizes expected utility
    for each state $s$:
    $$
    \pi^*(s) = \argmax_a \sum_{s'} \Pr(s'|s,a) [R(s,a,s') + \gamma U(s')]
    $$

#### Policy Iteration

- **\textcolor{red}{Policy iteration}** solves MDPs by iteratively improving a policy
  - Alternates between evaluating the current policy and improving it
  - Uses the simplified Bellman equation with a fixed action per state

- **\textcolor{darkgreen}{Algorithm steps}**
  - Start with an initial (random) policy $\pi$
  - Policy Evaluation: compute $U^\pi(s)$ by solving:
    $$
    U^\pi(s) = \sum_{s'} \Pr(s'|s,\pi(s))[R(s,\pi(s),s') + \gamma U^\pi(s')]
    $$
  - Policy Improvement: for each state, find:
    $$
    \pi'(s) = \argmax_a \sum_{s'} \Pr(s'|s,a)[R(s,a,s') + \gamma U^\pi(s')]
    $$
  - Repeat until policy is unchanged or close to convergence

- **\textcolor{blue}{Convergence Guarantee}**
  - Each iteration strictly improves or maintains policy performance
  - Guaranteed to terminate with an optimal policy for finite MDPs

- **\textcolor{violet}{Efficiency Considerations}**
  - Policy evaluation involves solving linear equations
  - Typically converges in fewer iterations than value iteration

#### Off-Line vs On-Line Solution of MDPS

- **\textcolor{red}{Offline methods}** (e.g., value iteration, policy iteration) precompute full
  solutions
  - Pros:
    - Compute the entire optimal policy $\pi^* \; \forall s$ before taking any
      action
  - Cons:
    - Assumes full knowledge of transition probabilities $\Pr(s'|s,a)$ and
      reward function $R(s,a,s')$
    - Not feasible for large MDPs (e.g., Tetris with $10^{62}$ states)

- **\textcolor{blue}{Online methods}** compute actions at runtime, using only reachable parts of
  the state space
  - Interleave planning and acting
  - Agent explores the environment and updates estimates (e.g., Q-learning)
  - Pros:
    - Focuses computation only on relevant parts of the state space
    - Scales to large problems with appropriate heuristics and approximations
    - Allows adaptive, real-time decision-making
    - No need for full model of the MDP
  - Cons
    - Requires fast and accurate state evaluation functions
    - May require significant computation at each decision point
    - Needs exploration and careful tradeoff with exploitation
    - Sensitive to model accuracy and search depth

#### The $n$-Bandit Problem

::: columns
:::: {.column width=60%}

- A simplified reinforcement learning scenario
  - There are $n$ different actions (arms)
  - Each arm $a_i$ yields a reward drawn from an unknown probability
    distribution $R_i$
  - At each timestep $t$, agent selects an arm $a_t$ and receives reward
    $r_t \sim R_{a_t}$
  - No state transitions: the environment is static and memoryless
  - Goal: maximize total reward over a sequence of pulls

::::
:::: {.column width=35%}
![](lectures_source/figures/Lesson12_Multi_armed_bandits.png)
::::
:::

- **\textcolor{red}{Exploration vs. Exploitation}**
  - Exploration: try different arms to learn their rewards
  - Exploitation: choose the best-known arm to maximize immediate reward

- **\textcolor{blue}{Applications}**
  - Online advertising (choosing ads to show)
  - Clinical trials (testing treatments)
  - A/B testing in web development

#### Partially Observable MDPs (POMDPs)

- **\textcolor{red}{Motivation}**
  - Traditional MDPs assume full observability of the environment
  - The agent knows in which state it is in
  - In real-world situations, agents often lack precise knowledge of the current
    state
  - POMDPs (read "pom-dee-pees") extend MDPs to handle uncertainty in state
    perception

- **\textcolor{darkgreen}{Definition}**
  - A POMDP is defined by:
    - States $S$
    - Actions $A$
    - Transition model $\Pr(s'|s,a)$
    - Reward function $R(s,a,s')$
    - Sensor model $\Pr(e|s)$: probability of observing evidence $e$ in state
      $s$

- **\textcolor{blue}{Belief States}**
  - A belief state $b(s)$ is a probability distribution over possible actual
    states $s$ (i.e., the probability of being in $s$)
  - The agent maintains $b(s)$ as its internal representation of the environment
  - Optimal policies depend on belief states: $\pi^*(b)$

#### POMDP: 4x3 World with Noisy Four-Bit Sensor

::: columns
:::: {.column width=75%}
- The world is the 4x3 grid with partial and probabilistic information about the
  environment

- Use a noisy four-bit sensor, instead of knowing where the agent is
  - Detect obstacles in four directions: North, East, South, West
  - Produces a four-bit string (e.g., `1010`), each bit indicating presence
    (`1`) or absence (`0`) of a wall in one direction

- **\textcolor{red}{Error Model}**
  - Each bit is correct with probability $1 - \epsilon$, incorrect with
    probability $\epsilon$
  - Errors are assumed to be independent across bits
  - Example: true config is `1100`, observed is `1110`

- **\textcolor{blue}{Localization Rule}**
  - Helps infer the robot's position by comparing sensor output with map-based
    expectations (integrated into belief state updates)
  - Localization is achievable with high error rate by aggregating observations
    over time
  - E.g., if the robot believes to be in `(3, 2)`, moves left and the sensor reads
    `NESW = 1100`, it's likely that it's in `(3, 1)` (although not guaranteed
    because motion and sensor are noisy)
::::
:::: {.column width=25%}
![](lectures_source/figures/Lesson12_4x3_environment1.png)
::::
:::

#### Belief State Transitions and Value of Information

- **\textcolor{red}{Belief Update}**
  - After action $a$ and observation $e$, belief state $b$ is updated:
    $$
    b'(s') = \alpha \Pr(e|s') \sum_s \Pr(s'|s,a) b(s)
    $$
    where $\alpha$ normalizes the distribution
  - Same equation as the filtering task to calculate the new belief state
    $b'(s)$ from the previous belief state $b(s)$ and the new evidence $e$

- **\textcolor{darkgreen}{Belief space}**
  - Everything (policy, transition and reward models) is now function of belief
    state
  - It can't be function of the actual state the agent is in, since the agent
    doesn't know the actual state
  - Intermediate belief states have lower utility due to uncertainty
  - Information-gathering actions can improve future decision quality

- **\textcolor{blue}{Transition and Reward Models in Belief Space}**
  - Transition: $\Pr(b'|b,a)$ defined using:
    $$
    \Pr(b'|b,a) = \sum_e \Pr(b'|e,a,b) \Pr(e|a,b)
    $$
  - Expected reward in belief state:
    $$
    \rho(b,a) = \sum_s b(s) \sum_{s'} \Pr(s'|s,a) R(s,a,s')
    $$

- **\textcolor{violet}{Decision cycle for POMDP}**
  - Repeat
    1. Given the current belief state $b$, execute action $a = \pi(b)$
    2. Observe percept $e$
    3. Update belief state $b'$

#### Solving POMDPs
- **\textcolor{red}{Observable MDP over Belief Space}**
  - A POMDP on an actual state space can be converted into an MDP on the belief
    space

- **\textcolor{darkgreen}{Value Iteration for POMDPs}**
  - Maintains a set of conditional plans $p$ with associated utility vectors
    $\alpha_p$
  - Expected utility of a plan in belief state $b$ is $b \cdot \alpha_p$
  - Optimal utility is piecewise linear and convex over belief space

- **\textcolor{blue}{Recursive Plan Evaluation}**
  $$
  \alpha_p(s) = \sum_{s'} \Pr(s'|s,a) \left[ R(s,a,s') + \gamma \sum_e \Pr(e|s') \alpha_{p.e}(s') \right]
  $$

- **\textcolor{violet}{Challenges}**
  - Number of plans grows exponentially with depth
  - Even small problems generate many plans (e.g., $2^{255}$ plans for a
    two-state POMDP at depth 8)
  - Approximation Techniques

#### Reinforcement Learning
- Sequential Decision Problems
- **\textcolor{purple}{Reinforcement Learning}**
  - Passive Reinforcement Learning
  - Active Reinforcement Learning
  - Generalization in Reinforcement Learning
  - Policy Search



#### Problem with Supervised Learning
- **\textcolor{red}{In supervised learning}**
  - An agent learns by observing examples of input / outputs
  - It's hard to find labeled data for all situations

- E.g., apply supervised learning to play chess
  - Take a board position as input $\vx$ and return a move $m$
  - Build a DB of grandmaster games with positions and winner (assuming moves by
    winner are good)
  - Problems
    - In a new game, positions differ from DB, as we have few examples compared
      to possible positions ($10^{40}$)
    - The agent doesn't understand the game's goal (i.e., checkmate) or valid
      moves of each piece

- _"The AI revolution will not be supervised"_ (Yann LeCun)

#### Reinforcement Learning

- **\textcolor{red}{Reinforcement Learning (RL) Paradigm}**
  - Agent learns from direct interaction with the environment
  - Periodically receives reward signals indicating success or failure
    ("reinforcements")
  - Learns a policy to maximize cumulative future rewards
  - Goal: maximize expected sum of rewards

- **\textcolor{darkgreen}{RL vs supervised learning}**
  - Providing a reward signal to the agent is easier than providing inputs /
    outputs
  - RL is active since the agent explores the environment and learn from actions
    and consequences

- **\textcolor{blue}{RL vs MDP}**
  - The goal of both is to maximize the expected sum of rewards
  - In RL the agent:
    - Doesn't know the transition model or the reward function (doesn't know the
      rules)
    - Needs to act to learn more

#### Sparse vs Immediate Rewards
- Sparse rewards = in the vast majority of states the agent is not given
  informative reward
  - E.g., win/lose at the end of a chess game
  - The agent must explore many states to find the few that provide rewards
  - Often requires more sophisticated exploration strategies

- Immediate / intermediate rewards help guide learning
  - E.g.,
    - In tennis, you can get rewards for every point scored
    - Learning to crawl, any forward motion is a reward
    - In a video game, collecting coins or power-ups can serve as intermediate
      rewards
  - Provides continuous feedback to the agent

#### Applications of Reinforcement Learning

- **\textcolor{gray}{Games and Simulations}**
  - RL has achieved superhuman performance in games like Go, Chess, and Dota2
  - Algorithms learn strategies through self-play and reward-driven improvement

- **\textcolor{orange}{Robotics}**
  - RL enables learning of complex control policies for walking, grasping, and
    manipulation
  - Applications include robotic arms, quadrupeds, and autonomous drones

- **\textcolor{blue}{Autonomous Vehicles}**
  - RL used for decision-making and control in self-driving cars
  - Handles tasks like lane merging, navigation, and obstacle avoidance

- **\textcolor{magenta}{Recommendation Systems}**
  - Adaptive recommendation based on user interactions (e.g., Netflix, YouTube)
    to optimize long-term engagement and satisfaction

- **\textcolor{teal}{Finance and Trading}**
  - Portfolio management and trading strategies learned through market
    simulations
  - Agents aim to maximize returns under uncertainty and risk constraints

- **\textcolor{olive}{Healthcare}**
  - Personalized treatment policies learned from patient data
  - Applications in dynamic dosing, diagnostics, and medical decision support

- **\textcolor{darkgreen}{Industrial Control}**
  - Optimization of manufacturing processes, energy grids, and logistics

- **\textcolor{cyan}{Natural Language Processing}**
  - Dialogue systems and chatbots that learn optimal conversational strategies
  - Use RL to improve response quality and user satisfaction over time

#### Model-Based Reinforcement Learning

- **\textcolor{gray}{Definition}**
  - Learns an explicit model of the environment's dynamics and uses it to make a
    decision about how to act
  - **\textcolor{orange}{Transition model}**: estimates $\Pr(s'|s,a)$, i.e., probability of reaching
    state $s'$ from $s$ after action $a$
  - **\textcolor{blue}{Reward model}**: estimates $R(s,a)$, i.e., expected reward after taking
    action $a$ in state $s$
  - Intuition: learn to drive by studying the manual and physics

- **\textcolor{magenta}{Learning Process}**
  - Collects experience tuples $(s, a, r, s')$
  - Updates the model of the environment (transition and reward)
  - Plans using the model to improve policy (e.g., via value iteration or policy
    iteration)
  - Dyna-Q algorithm: combines model-free updates with simulated planning steps

- **\textcolor{teal}{Advantages}**
  - Efficient sample usage: fewer real-world interactions required
  - Enables planning by simulating outcomes

- **\textcolor{olive}{Disadvantages}**
  - Learning an accurate model is challenging
  - Errors in the model can propagate and lead to poor decisions

- **\textcolor{darkgreen}{Examples}**

#### Model-Free Reinforcement Learning

- **\textcolor{red}{Definition}**
  - Learns directly from interactions with the environment without building a
    model of dynamics
  - Agent observes $(s, a, r, s')$ and updates value or policy estimates based
    on observed outcomes
  - No attempt to predict $P(s'|s,a)$ or $R(s,a)$
  - Intuition: learn to drive by trial and error

- **\textcolor{darkgreen}{Learning Process}**
  - Value-based methods: Learn state or state-action values (e.g., $Q(s,a)$)
    - E.g., Q-learning
  - Policy-based methods: Learn the policy directly
    - E.g., REINFORCE, actor-critic

- **\textcolor{blue}{Advantages}**
  - Simpler to implement when environment model is unknown or too complex
  - Robust to model inaccuracies since no model is used

- **\textcolor{violet}{Disadvantages}**
  - Requires more environment interactions (sample inefficient)
  - Harder to incorporate planning or long-term reasoning

#### Model-Based vs Model-Free Reinforcement Learning

- **\textcolor{gray}{Core Distinction}**
  - Model-Based RL: Learns a model of environment dynamics $P(s'|s,a)$ and
    $R(s,a)$ and uses it for planning
  - Model-Free RL: Learns value functions $Q(s,a)$ or policies $\pi(a|s)$
    directly from experience

- **\textcolor{blue}{Sample Efficiency}**
  - Model-Based: Generally more sample efficient due to simulated planning
  - Model-Free: Typically needs more environment interactions

- **\textcolor{teal}{Computation}**
  - Model-Based: Higher planning overhead; simulations required
  - Model-Free: Simpler computations per step; often more scalable

- **\textcolor{darkgreen}{Flexibility and Robustness}**
  - Model-Based: Sensitive to model inaccuracies
  - Model-Free: More robust to model errors (since it doesn't learn one)

- **\textcolor{brown}{Typical Use Cases}**
  - Model-Based: Robotics, planning tasks, known environments
  - Model-Free: Games, large-scale unknown or stochastic environments

- **\textcolor{darkgray}{Examples}**
  - Model-Based: Dyna-Q, PILCO
  - Model-Free: Q-learning, Deep Q-Networks (DQN), REINFORCE
  - Hybrid Approaches combine both strengths: learn models for planning and use
    model-free learning for robustness (e.g., AlphaZero)

#### Active vs Passive Reinforcement Learning

- **\textcolor{gray}{Basic Distinction}**
  - Passive RL: Learns value of a fixed policy; does not choose actions
  - Active RL: Learns both the value function and the optimal policy through
    exploration

- **\textcolor{orange}{Policy Handling}**
  - Passive: Follows a given policy $\pi(s)$ and estimates $V^\pi(s)$ or
    $Q^\pi(s,a)$
  - Active: Improves policy over time, aiming for $\pi^*(s)$ that maximizes
    reward

- **\textcolor{blue}{Exploration}**
  - Passive: No exploration â€” strictly evaluates the given policy
  - Active: Explores actions to improve the policy (e.g., $\epsilon$-greedy,
    softmax)

- **\textcolor{magenta}{Learning Goal}**
  - Passive: Accurate value function for a known policy
  - Active: Optimal policy and value function via interaction

- **\textcolor{teal}{Algorithms}**
  - Passive: Temporal Difference Learning (TD), Adaptive Dynamic Programming for
    a fixed policy
  - Active: Q-learning, SARSA, policy iteration methods

- **\textcolor{olive}{Use Cases}**
  - Passive: Evaluation of policies from human demonstrations or expert systems
  - Active: Autonomous agents discovering optimal strategies from scratch

- **\textcolor{darkgreen}{Sample Efficiency}**
  - Passive: Can converge faster if policy is close to optimal
  - Active: Slower due to need for exploration and policy improvement

- **\textcolor{cyan}{Challenges}**
  - Passive: Limited by the quality of the provided policy
  - Active: Balancing exploration and exploitation effectively

#### Passive Reinforcement Learning
- Sequential Decision Problems
- Reinforcement Learning
  - **\textcolor{purple}{Passive Reinforcement Learning}**
  - Active Reinforcement Learning
  - Generalization in Reinforcement Learning
  - Policy Search


#### Passive Learning Agent
- Consider a fully observable environment with a small number of actions and
  states

- **\textcolor{red}{The agent:}**
  - Has a fixed policy $\pi(s)$ to determine its action
  - Needs to learn $U^\pi(s)$, the expected discounted reward if policy $\pi$ is
    executed starting in state $s$
  - Doesn't know the transition model $\Pr(s'|s, a)$ and the reward function
    $R(s, a, s')$

- The agent executes a set of trials using the policy $\pi$:
  - Starts from an initial state and experiences state transitions until
    reaching terminal states
  - Stores actions and rewards at each state $(s_0, a_0, r_1, s_1, ..., s_n)$
  - Estimates:
    $$
    U^{\pi}(s) = \EE[ \sum_{t=0}^{\infty} \gamma^t R(S_t, \pi(S_t), S_{t+1})]
    $$

- Direct utility estimation
  - For each state $s$, average the returns from all episodes in which $s$ was
    visited:
    $$
    U^\pi(s) = \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_i(s)
    $$
    where $G_i(s)$ is the return after state $s$ in episode $i$

- **\textcolor{darkgreen}{Advantages}**
  - Unbiased estimator of utility under $\pi$
  - Simple

- **\textcolor{blue}{Limitations}**
  - High variance: requires many samples
  - Inefficient if state visits are rare or episode lengths vary greatly
  - (Important): Ignore the connections between states given that the utility
    values obeys the Bellman equation for a fixed policy

- **\textcolor{violet}{Example}**
  - Evaluating a delivery drone's route utility based on completed delivery
    missions without changing its behavior

#### Adaptive Dynamic Programming

- **\textcolor{gray}{Objective}**
  - Learn utility estimates $U^\pi(s)$ for a fixed policy $\pi$ using an
    estimated model of the environment

- **\textcolor{blue}{Key Components}**
  - Model learning: Estimate transition probabilities $\Pr(s'|s,a)$ and reward
    function $R(s,a)$ from experience
  - Utility update: Solve the Bellman equations for the fixed policy:
    $$
    U^\pi(s) = R(s, \pi(s)) + \gamma \sum_{s'} \Pr(s'|s, \pi(s)) U^\pi(s')
    $$

- **\textcolor{teal}{Learning Process}**
  - Collect transitions $(s, \pi(s), r, s')$ during execution
  - Update model estimates:
    - $\Pr(s'|s,a) \approx$ empirical frequency
    - $R(s,a) \approx$ average observed reward
  - Use dynamic programming to compute $U^\pi(s)$

- **\textcolor{darkgreen}{Advantages}**
  - More sample-efficient than direct utility estimation
  - Leverages structure of the MDP to generalize better

- **\textcolor{brown}{Limitations}**
  - Requires accurate model estimation
  - Computational cost of solving Bellman equations repeatedly

- **\textcolor{darkgray}{Example}**
  - Suitable when environment dynamics are stationary and can be learned from
    interaction
  - A thermostat estimates room temperature dynamics and uses them to predict
    comfort level under a fixed heating schedule

#### Temporal-Difference Learning

- **\textcolor{gray}{Objective}**
  - Estimate utility values $U^\pi(s)$ for a fixed policy $\pi$ using experience
    without a model

- **\textcolor{orange}{Key Idea}**
  - Combine benefits of Monte Carlo methods and Dynamic Programming
  - Update estimates after every transition using bootstrapping

- **\textcolor{blue}{TD(0) Update Rule}**
  - When a transition occurs from state $s$ to state $s'$ via action $\pi(s)$,
    we apply the update:
    $$
    U^\pi(s) \leftarrow U^\pi(s) + \alpha [ r + \gamma U^\pi(s') - U^\pi(s) ]
    $$
    where:
    - $s$ is the current state
    - $r$ is the immediate reward
    - $s'$ is the next state
    - $\alpha$ is the learning rate
    - $\gamma$ is the discount factor

- **\textcolor{magenta}{Characteristics}**
  - Online and incremental: updates occur after each step
  - Does not require knowledge of model $P(s'|s,a)$ or $R(s,a)$

- **\textcolor{teal}{Advantages}**
  - More efficient and lower variance than Monte Carlo methods
  - Simpler and more memory-efficient than model-based methods

- **\textcolor{olive}{Limitations}**
  - Introduces bias due to bootstrapping
  - May require careful tuning of learning rate $\alpha$

- **\textcolor{darkgreen}{Example}**
  - Real-time systems, streaming environments, or when dynamics are too complex
    to model
  - An autonomous cleaner estimates long-term dirtiness reduction for each room
    it passes through, adjusting based on immediate results

#### Active Reinforcement Learning
- Sequential Decision Problems
- Reinforcement Learning
  - Passive Reinforcement Learning
  - **\textcolor{purple}{Active Reinforcement Learning}**
  - Generalization in Reinforcement Learning
  - Policy Search


#### Active Reinforcement Learning

- Passive RL assumes agent has a fixed policy and passively receives reward
  signals
  - In many real-world cases, agent needs to decide what actions to take and
    rewards must be actively sought or queried

- Active RL includes cost-sensitive decisions about when to query for rewards
  - Useful when querying is expensive or limited (e.g., human feedback)
- Key problem: balancing cost of querying against benefit of accurate reward
- Formal model:
  - Agent observes state $s$ and selects action $a$
  - Decides whether to query for reward $r$
  - Cost $c$ incurred if query is made
- Objective:
  - Maximize cumulative reward minus query costs
  - $\sum (r_t - c_t)$ where $c_t = c$ if query made, 0 otherwise
- Optimal policy needs to learn both:
  - What actions to take
  - When it is worth querying for reward
- Applications:
  - Robotics with costly sensors
  - Interactive systems with human-in-the-loop feedback

#### Greedy Agent in Reinforcement Learning

- A greedy agent always selects the action with the highest estimated value
  based on current knowledge or Q-values
  $$
  a = \argmax_a Q(s, a)$$ for state $s$
  - No exploration: purely exploits known information
  $$
- An agent must make a tradeoff between
  - Exploitation of current best action to maximize its short-term reward
  - Exploration of unknown states to gain information that can lead to a change
    in policy (and greater rewards in the future)
  - E.g., in life you need to decide continuing a comfortable existence, or try
    something unknown in the hopes of a better life

- Goal: efficient learning with minimal queries to maximize information gain per
  unit cost

- Strategies include:
  - Random follow greedy policy or explore
  - Cost-aware exploration: modify exploration bonus based on query cost
  - Confidence-based querying: only query when uncertain about reward
  - Give weight to actions not tried very often and avoid actions (believed to
    be) of low utility
  - Use an exploration function $f(u, n)$ to trade-off greed (preference for
    high values of utility) vs curiosity (preference for actions not tried
    before)

- Example:
  - An agent exploring a maze may avoid querying in familiar areas
  - Queries are focused on uncertain or novel states

#### Safe Exploration in Reinforcement Learning
- In idealized settings, agents can explore freely and learn from negative
  outcomes (e.g., losing in chess or simulations)
  - E.g., a self-driving car in simulation can crash without consequences

- In the real world, exploration has risks:
  - Irreversible actions may lead to states that cannot be recovered from
  - Agents can enter "absorbing states" where no further rewards or actions are
    possible
  - E.g., a crash that destroys a self-driving car permanently limits its future
    learning

- Safer Policy Approaches
  - **\textcolor{red}{Bayesian Reinforcement Learning}**: Maintain a probability distribution
    over possible models
    - Compute a policy that maximizes expected utility across all plausible
      models
    - In complex cases, leads to an "exploration POMDP" which is computationally
      intractable but conceptually useful
  - **\textcolor{blue}{Robust Control Theory}**: Optimize for the worst-case scenario among all
    plausible models
    - Resulting policies are conservative but safe
    - E.g., agent avoids any action that could possibly lead to death
  - Impose constraints to prevent the agent from taking dangerous actions
    - E.g., safety controllers can intervene in risky states for autonomous
      helicopters

#### Temporal-Difference Q-Learning
- Q-learning is a model-free reinforcement learning algorithm
  - Learns the value of taking an action in a given state, denoted $Q(s, a)$
  - Does not require a model of the environment

- Temporal-difference (TD) learning updates estimates based on other learned
  estimates
  - Unlike Monte Carlo methods, it updates after every step using bootstrapping

- **\textcolor{red}{Q-learning update rule:}**
  $$
  Q(s,a) \leftarrow Q(s,a) + \alpha [ r + \gamma \max_{a'} Q(s',a') - Q(s,a) ]
  $$
  - $\alpha$: learning rate
  - $r$: reward received after action $a$
  - $\gamma$: discount factor for future rewards
  - $s'$: next state
  - $a'$: next action

- The update aims to reduce "the TD error"
  $r + \gamma \max_{a'} Q(s',a') - Q(s,a)$, i.e., the difference between current
  estimate and observed return

- Converges to the optimal action-value function $Q^*$ under certain conditions
  - Requires exploration of all state-action pairs
  - Learning rate must decay appropriately

- E.g., in a gridworld, Q-learning learns the optimal path to the goal by
  updating $Q$ values as it explores

- Exploration strategy (e.g., $\epsilon$-greedy) is crucial for balancing
  learning and exploitation

#### Generalization in Reinforcement Learning
- Sequential Decision Problems
- Reinforcement Learning
  - Passive Reinforcement Learning
  - Active Reinforcement Learning
  - **\textcolor{purple}{Generalization in Reinforcement Learning}**
  - Policy Search


#### Generalization in Reinforcement Learning (1/2)
- Tabular representations become infeasible for large state spaces
  - Real-world problems often have millions or more distinct states
  - Example: Backgammon has ~$10^{20}$ states, but successful agents visit only
    a small fraction

- Function approximation enables scalability and generalization
  - Replace large tables with parameterized functions: $\hat{U}_\theta(s)$ or
    $\hat{Q}_\theta(s, a)$
  - Linear example:
    $\hat{U}_\theta(s) = \theta_1 f_1(s) + \cdots + \theta_n f_n(s)$

- Benefit: Generalizes from visited states to unvisited ones
  - Allows efficient learning with fewer examples

- Temporal-Difference (TD) and Q-learning adapt to function approximation
  - TD update:
    $$
    \theta_i \leftarrow \theta_i + \alpha [r + \gamma \hat{U}_\theta(s') -
    \hat{U}_\theta(s)] \frac{\partial \hat{U}_\theta(s)}{\partial \theta_i}
    $$
  - Q-learning update:
    $$
    \theta_i \leftarrow \theta_i + \alpha [r + \gamma \max_{a'}
    \hat{Q}_\theta(s',a') - \hat{Q}_\theta(s,a)] \frac{\partial \hat{Q}_\theta(s,a)}{\partial \theta_i}
    $$
  - Issues and solutions:
    - **\textcolor{red}{Divergence}**: parameters can grow uncontrollably
    - **\textcolor{darkgreen}{Catastrophic forgetting}**: important knowledge can be lost
    - **\textcolor{blue}{Solution}**: experience replay reuses old data to stabilize learning

- Deep reinforcement learning:
  - Uses deep neural networks to approximate $U$ or $Q$
  - Learns features automatically from raw inputs (e.g., images)

- Reward shaping:
  - Adds auxiliary (pseudo)rewards to guide learning
  - Must be designed carefully to avoid misleading the agent

- Hierarchical RL:
  - Structures behavior into reusable subprograms and subgoals
  - Decomposes complex tasks to improve learning efficiency

#### Policy Search
- Sequential Decision Problems
- Reinforcement Learning
  - Passive Reinforcement Learning
  - Active Reinforcement Learning
  - Generalization in Reinforcement Learning
  - **\textcolor{purple}{Policy Search}**


#### Policy Search in Reinforcement Learning
- A policy $\pi(s)$ maps states to actions
  - Use a parameterized representation with fewer parameters than states (e.g.,
    linear, deep neural network): $\pi_\theta(s)$
  - Directly optimizes parameters $\theta$ of the policy $\pi_\theta(s)$ rather
    than value functions
  - Pick the value with highest predicted value
    $$
    \pi_\theta(s) = \argmax_a \hat{Q}_\theta(s, a)
    $$
  - Useful in high-dimensional or continuous action spaces

- Even if learning a function replaces the Q-function, it is not an
  approximation of Q-function (i.e., Q_learning)
  - Seek a function that gives good performance and might differ from the
    optimal Q-function $Q^*$

- To avoid jittery policy for discrete actions, use stochastic policies for
  smoother optimization:
  - E.g., softmax over Q-values
    $$
    \pi_\theta(s,a) = \frac{e^{\beta \hat{Q}_\theta(s,a)}}{\sum_{a'} e^{\beta \hat{Q}_\theta(s,a')}}
    $$
    where $\beta$ controls exploration vs exploitation

- If everything is continuous and differentiable, use gradient descent to find
  optimal policy
  - If environment (or policy) is not deterministic, compute averages to smooth
    out the gradient
  - High variance in trial outcomes makes gradients noisy
  - Requires many trials to ensure reliable gradient estimation
