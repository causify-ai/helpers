// # 21, Deep learning (p. 763)

::: columns
:::: {.column width=15%}
![](lectures_source/UMD_Logo.png)
::::
:::: {.column width=75%}

\vspace{0.4cm}
\begingroup \large
MSML610: Advanced Machine Learning
\endgroup
::::
:::

\vspace{1cm}

\begingroup \Large
**$$\text{\blue{Deep Learning}}$$**
\endgroup
\vspace{1cm}

**Instructor**: Dr. GP Saggese - `gsaggese@umd.edu`

**References**:

# Neural networks

* Deep learning
- Deep learning is a family of ML models and techniques with complex expressions
  and tunable connection strengths
  $$\text{Deep Learning} \subseteq \text{Machine Learning}$$
  - "Deep" as circuits are organized in layers with many connection paths
    between inputs and outputs
  - Represent hypotheses as computation graphs with learnable weights
  - Fit the model to the training data by computing the gradient of the loss
    function with respect to those weights

- Deep learning is extremely effective for:
  - Machine translation
  - Image recognition/synthesis
  - Speech recognition/synthesis

* DL vs ML
- Many ML methods can handle a large number of input variables
  - **But**
    - The path from input to output is very short (e.g., multiply and sum)
    - There are no variable interactions

- E.g., decision trees
  - Allow long computation paths
  - Only a small fraction of variables can interact

- The expressive power of such models is very limited
  - Real-world concepts are far more complex

## Biological inspiration

* Biological Inspiration for Neural Networks
- To perform a function like in a biological system, replicate its structure

- There is a leap of faith: the _structure_ matters to achieve a _functionality_

- **Examples**
  - You want to fly, birds fly, birds have wings $\implies$ build a contraption
    with wings
  - You want to learn, the brain learns, the brain has many neurons and synapses
    $\implies$ build a system with many simple units connected together

* Neurons in the brain
::: columns
:::: {.column width=55%}
- The brain is jam-packed with neurons
- Each neuron has inputs (dendrites) and an output (axon)
- Neurons connect their output to inputs of different neurons, sending pulse of
  electricity
- Senses (e.g., eyes) send pulses to the neurons
- Neurons send pulses to the muscles to make them contract
::::
:::: {.column width=40%}
![](lectures_source/figures/Lesson11_Neuron.png)
::::
:::

* Neural Network
- Get inspiration from networks of neurons in the brain
  - Human brain consist of layers of interconnected nodes

- Resemblance with neural structures is superficial
  - Architecture is inspired by the brain but does not replicate its complexity
  - Each connection has a weight that adjusts as learning proceeds
  - Neural networks simplify the brain's processes to make them computationally
    feasible

- Deep learning encompasses a broader range of models and algorithms beyond
  neural networks
  $$\text{Neural Networks} \subseteq \text{Deep Learning}$$
  - Neural networks are building blocks for many deep learning models
  - E.g., a convolutional neural network (CNN) is used for image classification
    tasks
  - E.g., recurrent neural networks (RNNs) are used for sequence prediction
    tasks like language modeling

* The "one learning algorithm" theory
::: columns
:::: {.column width=55%}
- The brain can perform various tasks
  - E.g., process vision, sense of touch, do math, play pickleball
  - Doesn't have thousands of different programs
  - Seems to have a single learning algorithm

- The "one learning algorithm" idea has been experimentally verified
  - Re-route the connection from eyes to the brain's sound-processing area
  - After training, the brain can "see," e.g., visual discrimination

- The **AI dream**:
  - If you can implement a (simplified) version of the brain algorithm, you can
    have a machine that can learn anything
::::
:::: {.column width=40%}
![](lectures_source/figures/Lesson11_Brain.png)
::::
:::

* Why resurgence of neural networks?
- AI winter
  - Proposed in the 1950
  - Popular in '80s and '90s
  - Then fell out of fashion due to limitations

- **Key Reasons for Resurgence**

  - **Increased Computational Power**
    - GPUs and TPUs suited for matrix operations in neural nets

  - **Availability of Big Data**
    - Open-source frameworks (e.g., TensorFlow and PyTorch)
    - Massive amounts of labeled data (Internet, IoT, and digital storage)

  - **Algorithmic Improvements**
    - Better activation functions (ReLU)
    - Advanced optimization techniques (Adam, RMSprop)
    - Regularization methods (dropout, batch normalization)

  - **Breakthrough Architectures**
    - CNNs for image tasks
    - RNNs and LSTMs for sequences
    - Transformers for language and vision

  - **Demonstrated Success in Applications**
    - State-of-the-art performance in vision, speech, language, and games
    - Commercial impact in healthcare, finance, and autonomous vehicles
    - 2012: AlexNet outperformed methods on ImageNet using deep CNNs

* NN vs logistic regression + non-linear transform

::: columns
:::: {.column width=55%}
- Logistic regression with non-linear transformations might seem sufficient for
  any problem
  - However, the number of features increases rapidly
  - Neural networks synthesize their own features, offering an advantage

- E.g., in computer vision for $50 \times 50$ 256-color images
  - There are 7500 bytes available as features
  - Using all cubic terms for a non-linear model requires $\approx 7500 ^ 3$
    features $\propto (10^4)^3 = 10^{12} = 1$ trillion features
  - Which ones are really needed?
  - The features are predetermined and not learned

- Each neuron in a neural network performs logistic regression
  - Features used are computed by other neurons
  - You are not limited to using inputs $\vx$ or polynomial terms derived from
    $\vx$
  - You can infer features automatically
::::
:::: {.column width=40%}
![](lectures_source/figures/Lesson11_Shallow_vs_Deep.png)

- Shallow model has short computation path
- A decision tree has some long paths
- A deep learning network has long paths with many variables interacting
::::
:::

## Neural networks

* Neural Network Perceptron
::: columns
:::: {.column width=55%}
- Aka "artificial neuron", "logistic unit"
- A perceptron has $n$ inputs and 1 output (like a brain neuron)

- The inputs are combined using a non-linear activation function $\theta(s)$ to
  implement:
  $$
  y = h_{\vw}(\vx) = \theta(\vw^T \vx)
  $$
  - Same functional form as logistic regression ($\theta$ is logit) or linear
    classification ($\theta$ is sign)
- The parameters $\vw$ are typically called **weights** in neural network
  literature
::::
:::: {.column width=40%}

```tikz
% Input nodes
\node[draw=none, fill=none, text centered] (x1label) at (-3,2) {$x_1$};
\node[draw=none, fill=none, text centered] (x2label) at (-3,1) {$x_2$};
\node[draw=none, fill=none, text centered] (x3label) at (-3,-0.4) {$\vdots$};
\node[draw=none, fill=none, text centered] (xnlabel) at (-3,-2) {$x_n$};

% Dots and lines to perceptron
\node[circle, draw, thick, line width=1.2pt, minimum size=1.2cm] (sum) at (0,0) {$\sum$};

% Input to sum connections
\draw[->, thick] (x1label) -- (sum);
\draw[->, thick] (x2label) -- (sum);
\draw[->, thick] (x3label) -- (sum);
\draw[->, thick] (xnlabel) -- (sum);

% Weights
\node[draw=none, fill=none, text centered] at (-1.7,1.5) {$w_1$};
\node[draw=none, fill=none, text centered] at (-1.7,0.8) {$w_2$};
\node[draw=none, fill=none, text centered] at (-1.7,-1.6) {$w_n$};

% Activation function node
\node[circle, draw, thick, line width=1.2pt, minimum size=1.2cm, right=0.75cm of sum] (act) {$\theta$};
\draw[->, thick] (sum) -- (act);

% Output
\node[draw=none, fill=none, text centered, right=0.75cm of act] (y) {$y = \theta(\mathbf{w}^T \mathbf{x})$};
\draw[->, thick] (act) -- (y);

% Optional: label the transformation
\node[draw=none, fill=none, text centered, above=0.1cm of sum] {$\mathbf{w}^T \mathbf{x}$};
```
::::
:::

* Activation Functions

::: columns
:::: {.column width=55%}
- **Sigmoid Function**:
  - $\theta(s) = \frac{1}{1 + \exp(-s)}$
  - Output range: $[0, 1]$
  - Smooth, differentiable, and used for probabilistic outputs
  - Saturates at extremes; suffers from vanishing gradient

- **Hyperbolic Tangent (tanh)**:
  - $\theta(s) = \tanh(s) = \frac{e^s - e^{-s}}{e^s + e^{-s}}$
  - Output range: $[-1, +1]$
  - Zero-centered; improves convergence over sigmoid
::::
:::: {.column width=40%}

```tikz
  \begin{axis}[
    axis lines = middle,
    xmin=-6, xmax=6,
    ymin=-1.1, ymax=1.1,
    samples=200,
    domain=-6:6,
    legend pos=south east,
    grid=major,
    xlabel={$x$},
    ylabel={},
    legend style={draw=none, fill=none},
    legend cell align={left}
  ]

  % Sigmoid function
  \addplot[thick, blue] {1 / (1 + exp(-x))};
  \addlegendentry{Sigmoid}

  % Tanh function
  \addplot[thick, red, dashed] {tanh(x)};
  \addlegendentry{Tanh}

  \end{axis}
```
::::
:::

* Activation Functions

- Map the neuron's input signal $s$ to an output activation value $\theta(s)$
  - Approximately linear for small $s$
  - Saturate or threshold for large $|s|$
    - Non-linearity enable learning complex functions
  - Many functions cross the origin or have $\theta(0) = 0$

::: columns
:::: {.column width=55%}
- **ReLU (Rectified Linear Unit)**:
  - $\theta(s) = \max(0, s)$
  - Output range: $[0, \infty)$
  - Non-differentiable at $s = 0$; can cause dead neurons

- **Softplus Function**:
  - $\theta(s) = \log(1 + \exp(s))$
  - Smooth approximation of ReLU
  - Always differentiable; output in $[0, \infty)$
::::
:::: {.column width=40%}

```tikz
  \begin{axis}[
    axis lines=middle,
    xmin=-4, xmax=6,
    ymin=-1, ymax=6,
    samples=200,
    domain=-4:6,
    xlabel={$x$},
    ylabel={},
    legend pos=north west,
    grid=major,
    legend style={draw=none, fill=none},
    legend cell align={left}
  ]

  % ReLU function
  \addplot[thick, blue, domain=-4:0] {0};
  \addplot[thick, blue, domain=0:6] {x};
  \addlegendentry{ReLU}

  % Softplus function
  \addplot[thick, red, dashed] {ln(1 + exp(x))};
  \addlegendentry{Softplus}

  \end{axis}
```
::::
:::

* Neural networks to compute boolean functions
- Build `AND`, `OR`, `NOT` functions with linear perceptrons using proper bias
  and weights

- `AND` is implemented by a single neuron with 2 inputs $x_1$ and $x_2$ and bias
  - Implement in terms of $s$:
    ```
    x_1   x_2   AND
     -1    -1    -1
     -1    +1    -1
     +1    -1    -1
     +1    +1    +1
    ```
  - Plot a diagram and find a proper decision boundary:
    $$
    y = \theta(w_0 * 1 + w_1 * x_1 + w_2 * x_2) = \theta (-3 + 2 w_1 + 2 w_2)
    $$
    Weights are $(w_0, w_1, w_2) = (-3, 2, 2)$

- You cannot classify 4 points in a `XOR` position in a plane with a _single_
  perceptron
  - Use two perceptrons to separate two regions of space
  - Then combine the outputs of the 2 perceptrons together

* Universal Approximation in Feedforward Networks

- **Power of Composition**
  - Connecting perceptrons enables complex functions
  - A network of sufficient size and depth can approximate any Boolean or
    continuous function to arbitrary precision
  - Compositional structure: each layer builds on the previous one

- **Role of Nonlinearity**
  - Nonlinear activation functions (e.g., sigmoid, tanh, ReLU) are essential
  - Without nonlinearity, layers reduce to a single linear transformation
  - Nonlinearity allows modeling of complex, non-linear decision boundaries

- **Geometric Intuition**
  - To separate two classes with a circular boundary:
    - Use multiple perceptrons (e.g., 8–16) to approximate the circle with a
      polygon
    - Combine outputs logically for the final decision
  - This forms a "lookup table" over regions of input space—similar to decision
    trees

* Feedforward vs Recurrent Neural Networks

- **Feedforward Neural Networks**
  - Information flows in one direction from inputs to outputs without cycles in
    the computational graph
  - Can model static relationships between inputs and outputs
    - E.g., classifying a handwritten digit from an image
  - Limited in handling sequential dependencies (only fixed window of inputs)

- **Recurrent Neural Networks (RNNs)**
  $$z_t = f_w(z_{t-1}, x_t)$$
  - Allow cycles in the computational graph with delays
    - Each unit can take inputs from its previous output: adds memory
    - Process sequences: outputs depend on current and previous inputs
  - Suitable for sequential data (e.g., time series, language modeling) and
    model longer-range dependencies
  - E.g., predicting the next word in a sentence based on previous words

* Structure of feedforward neural network

- **Layered Architecture**
  - A feedforward network consists of an ordered set of layers indexed by $l$
    1. Input layer ($l = 0$)
    2. One or more hidden layers ($0 < l < L$)
    3. Output layer ($l = L$)
  - Each layer $l$ contains $d(l)$ units or neurons, and can vary in size

- **Input Layer ($l = 0$)**
  - Represents the input vector $(x_0=1, x_1, x_2, ..., x_d)$
  - $x_0 = 1$ acts as the bias input
  - Booleans are mapped to 0/1 or -1/+1
  - Numeric attributes are left unchanged (scaled to fit a fixed range, logged)
  - Categorical attributes can be one-hot encoded

* Structure of feedforward neural network

::: columns
:::: {.column width=55%}
- **Hidden Layers ($0 < l < L$)**
  - Each neuron computes a weighted sum of inputs from the previous layer
  - Applies a nonlinear activation function $\theta$
  - Includes a bias unit with constant output 1
  - Fully connected: every node in layer $l-1$ connects to every node in layer $l$

- **Output Layer ($l = L$)**
  - Final layer producing the output vector $\mathbf{y}$
  - Output can be:
    - Linear: for regression tasks
    - Nonlinear (e.g., softmax, sigmoid): for classification tasks
  - Choice of activation depends on the task type and desired output range
::::
:::: {.column width=40%}
```tikz
[
    neuron/.style={circle, line width=1.4pt,draw=black, minimum size=1.2cm},
    layerlabel/.style={font=\bfseries\small, text centered},
    ->, thick
]

% Vertical spacing
\def\vspacing{1.8}
\def\biasheight{3.5}

% Layer labels
\node[layerlabel] at (0,\biasheight+1.2) {$l=0$};
\node[layerlabel] at (3,\biasheight+1.2) {$l=1$};
\node[layerlabel] at (6,\biasheight+1.2) {$l=2$};
\node[layerlabel] at (9,\biasheight+1.2) {$l=L$};

% Input neurons
\foreach \i in {0,1,2,3,4} {
    \ifnum\i=0
        \node[neuron] (I\i) at (0,{4*\vspacing/2 - \i*\vspacing}) {$x_0$};
    \else
        \node[neuron] (I\i) at (0,{4*\vspacing/2 - \i*\vspacing}) {$x_{\i}$};
    \fi
}

% Hidden layer 1 neurons
\foreach \i in {1,2,3} {
    \node[neuron] (H1\i) at (3,{2*\vspacing - \i*\vspacing}) {};
}
\node[neuron] (H10) at (3,\biasheight) {1}; % Bias node on top

% Hidden layer 2 neurons
\foreach \i in {1,2,3} {
    \node[neuron] (H2\i) at (6,{2*\vspacing - \i*\vspacing}) {};
}
\node[neuron] (H20) at (6,\biasheight) {1}; % Bias node on top

% Output layer
\foreach \i in {1,2} {
    \node[neuron] (O\i) at (9,{1.5*\vspacing - \i*\vspacing}) {$y_{\i}$};
}

% Connections Input -> Hidden 1
\foreach \i in {0,1,2,3,4} {
    \foreach \j in {1,2,3} {
        \draw[->] (I\i) -- (H1\j);
    }
}

% Hidden1 -> Hidden2
\foreach \i in {1,2,3} {
    \foreach \j in {1,2,3} {
        \draw[->] (H1\i) -- (H2\j);
    }
}
\foreach \j in {1,2,3} {
    \draw[->] (H10) -- (H2\j);
}

% Hidden2 -> Output
\foreach \i in {1,2,3} {
    \foreach \j in {1,2} {
        \draw[->] (H2\i) -- (O\j);
    }
}
\foreach \j in {1,2} {
    \draw[->] (H20) -- (O\j);
}
```
::::
:::

* Conventions for neurons in a neural network
- Each neuron $x^{(l)}_j$:
  - Belongs to a layer with index $l$
  - Accepts inputs from the previous layer (scanning index $i$)
  - Has an index $j$ in the layer for its output

- Weights are identified by 3 indices $w^{(l)}_{ij}$ where:
  - $ij$ are organized as input-output
  - $0 \le l \le L$ denotes the layer
  - $0 \le j \le d(l)$ denotes the output of the layer (i.e., the neuron in the
    layer)
  - $0 \le i \le d(l-1)$ denotes the inputs: start from 0 to account for the
    bias
    - Use $l - 1$ in $d(l-1)$ since you look at the previous layer

// TODO: Add pic

* Feedforward propagation algorithm
- The output of the generic $l$-th layer is $\vx^{(l)}$

- The outputs of the input layer ($l = 0$) are the inputs of the network:
  $$\vx^{(0)} = (x^{(0)}_0, x^{(0)}_1, ..., x^{(0)}_{d(0)}) = \vx = (1, x_1, ..., x_d)$$

- The output of the $j$-th neuron of the $l$-th layer is $x_j^{(l)}$
  - Has $d(l-1)$ inputs from the previous layer combined with the
    weights
  - Computes the signal $s^{(l)}_j$
  - The activation function $\theta$ is applied:
    $$
    x^{(l)}_j = \theta(s^{(l)}_j) = \theta(\sum_{i=0}^{d(l-1)} w^{(l)}_{ij} x^{(l-1)}_i)
    $$

- The output of the network is the output of the neurons in the last layer
  $$
  y = h(\vx) = x^{(L)}_1
  $$
  - Outputs are $s^{(L)}_1$ for regression or $\theta(s^{(L)}_1)$ for classification

* Vectorized feedforward propagation algorithm
- Vectorize neuron evaluation:
  - The $j$-th neuron of the $l$-th layer uses the $d(l-1)$ outputs of the
    previous layer to compute its output:
    $$
    x^{(l)}_j = \theta(\sum_i w^{(l)}_{ij} x^{(l-1)}_i) = \theta( (\vw^{(l)}_j)^T \vx^{(l-1)} )
    $$

- Compute all inputs $\vs^{(l)}$ to the activation function as matrix-vector
  product:
  $$
  \vs^{(l)} = \mW^{(l)} \cdot \vx^{(l-1)}
  $$
  where $\mW^{(l)}$ is a matrix with weight vectors for each neuron in layer $l$
  as rows
  - Include the bias by adding a column to weight matrix $\mW$ and padding
    inputs $\vx^{(l)}$ with 1s

- Apply the activation function in vectorized form:
  $$
  \vx^{(l)} = \vtheta(\vs^{(l)}) = \vtheta(\mW^{(l)} \cdot \vx^{(l-1)})
  $$

* Cost Function for Single-Class NN Classification
- For binary classification using neural networks, use
  \red{logistic regression} cost function with a \blue{regularization term}:
  $$
  E_{in}(\vw)
  = - \frac{1}{N}
  \sum_{i=1}^N (\red{y_i \log h(\vx_i) + (1 - y_i) \log (1 - h(\vx_i))}) +
  \blue{\frac{\lambda}{N} \sum_{\green{j=1}}^p \|\vw_j\|^2}
  $$

- By convention, we don't regularize the \green{bias} ($j=1$ and not $j=0$), as
  it is constant and does not affect the minimum $\vw$

* Multi-output neural networks for multi-class classification
- **In one-vs-all approach:**
  - Train $n$ models, one per class, to recognize each class
  - Pick the model with the highest probability
  - The output is a one-hot encoding of each class
  - E.g.,
    - Use 4 output neurons to discriminate pedestrian, car, motorcycle, truck
    - Encode pedestrian = `(1, 0, 0, 0)`
    - Encode car = `(0, 1, 0, 0)`
    -

- Instead of training $n$ neural networks, train a single neural network with an
  output layer of $n$ nodes
  - Global optimization vs $n$ local optimizations
  - End-to-end learning

* Cost Function for Multi-Class NN Classification
- The loss function for multi-class classification using neural networks
  - encode one-hot
  - the expected outputs $\vy_i$
  - the outputs from the model $\vh(\vx_i)$

  \begin{alignat*}{2}
  E_{in}(\vw)
  & = - \frac{1}{N}
  \sum_i \sum_k \vy_i|_k \log \vh(\vx_i)|_k +
  (1 - \vy_i|_k) \log (1 - \vh(\vx_i)|_k) + \\
  & \frac{\lambda}{N}
  \sum_{l=1}^L \sum_{j=1}^{d(l)} \sum_{i=1}^{d(l-1)} (w^{(l)}_{ij})^2 \\
  \end{alignat*}

- Avoid to consider the inputs ($l \ne 0$) and the bias terms ($i \ne 0, j \ne
  0$)

* Issues with fitting a neural networks

- **Generalization**
  - Match model complexity to data resources
  - High flexibility $to$ lots of data is needed
  - Overly complex models can overfit if data is insufficient
  - E.g., a deep neural network with millions of parameters requires a large
    dataset to train effectively without overfitting

- **Optimization**
  - Several layers of perceptrons with a hard threshold turn the optimization
    problem into a combinatorial one
  - E.g., challenges in finding the global minimum due to the non-convex nature
    of the problem

* Fitting a neural networks for SGD
- Use Stochastic Gradient Descent (SGD) to determine the weights $\vw$
  - Consider the error on a single example $(\vx, y)$:
    $$
    E_{in}(\vw) = e(h_{\vw}(\vx), y) = e(\vw)
    $$
  - Same process for both regression and classification:
    $$
    \begin{aligned}
    & e(h_{\vw}(\vx), y) = (h_{\vw}(\vx) - y)^2 \\
    & e(h_{\vw}(\vx), y) = -y \log h_{\vw}(\vx) - (1 - y) \log (1 - h_{\vw}(\vx)) \\
    \end{aligned}
    $$
  - Need to compute $\nabla_{\vw} e(\vw_0)$ by computing all the partial
    derivatives
    $$
    \frac{\partial e(\vw)}{\partial w^{(l)}_{ij}} \; \forall i, j, l
    $$

- The entire formula for the hypothesis $h_{\vw}(\vx)$ is very convoluted:
  - Non-linearity $\theta$ of ...
  - Linear combinations of weights and $\theta$ of ...
  - Linear combinations of weights and $\theta$ of ...
  \begingroup \small
  \begin{alignat*}{2}
  h_{\vw}(\vx)
  &= \theta( (\vw^{(L)})^T \cdot \vx^{(L-1)} ) \\
  &= \theta( (\vw^{(L)})^T \cdot \vtheta( \mW^{(L-1)} \cdot \vx^{(L-2)} ) \\
  &= \dots \\
  \end{alignat*}
  \endgroup

* Computing the gradient
- Compute all partial derivatives to get:
  $$\nabla_{\vw} e(\vw)$$

- **You can:**
  - Compute the analytic expression of the derivatives by brute force
  - Approximate the derivatives numerically by changing each $w^{(l)}_{ij}$ and
    computing the variation of $e$
  - Use a very efficient algorithm (backpropagation) to compute the gradient

- **Backpropagation** (or "backprop")
  - Efficient algorithm for computing gradients of the loss function with
    respect to all weights in the network
  - Enables training of multi-layer neural networks via gradient descent
  - Based on the chain rule of calculus
  - Updates weights to minimize the overall prediction error

- **Intuition**
  - Forward pass computes predictions
  - Backward pass computes how much each weight contributed to the error
  - Adjust weights to reduce future errors

* Backpropagation in Neural Networks
- Initialize weights $\vw$ randomly
  - Avoid $\vv{0}$ as it is an unstable equilibrium point

- For each iteration:
  - Pick a random input $\vx_n$ (SGD setup)
  - Forward pass: compute outputs of all neurons $x^{(l)}_j$ given $\vx_n$
    and current weights $\vw(t)$
  - Backpropagation: compute all $\delta^{(l)}_j$ using backpropagation for
    current $\vx_n$ and $\vw(t)$
  - Compute derivatives of errors:
    $\frac{\partial e}{\partial w^{(l)}_{ij}} = \delta^{(l)}_j x^{(l-1)}_i$
  - Update weights using derivatives
    $$
    \vw(t + 1) \leftarrow \vw(t) - \eta \nabla_{\vw} e(\vw(t))
    $$
    $$
    w^{(l)}_{ij}(t + 1) \leftarrow w^{(l)}_{ij}(t) - \eta \frac{\partial e}{\partial w^{(l)}_{ij}}
    $$

- Iterate until termination

- Note that the cost function is not convex
  - No guarantee of finding the global minimum

* Backpropagation in Neural Networks
- If we want to use batch gradient descent (instead of SGD)
  - Accumulate the partial derivatives considering all the examples in the
    training set
  - Update the weights with the accumulated values

* Gradient checking
- For some algorithms the analytical expression of the gradient becomes
  complicated: mistakes are possible
  - E.g., back-propagation in neural networks

- One approach is:
  - Compute the gradient analytically
  - Compute the gradient numerically
    $$
    \frac{\partial E_{in}(\vw)}{\partial w_j} \approx
    \frac{E_{in}(\vw - \hat{w_j} \varepsilon) - E_{in}(\vw + \hat{w_j} \varepsilon)}
    {2 \varepsilon}
    $$
  - Compare the gradients within numerical approximation
  - Pick $\varepsilon$ small (e.g., $\varepsilon = 10^{-4}$) but not so small to
    cause too many numerical issues

- Automatic differentiation packages solve this issue

* Automatic Differentiation
- **Automatic Differentiation**
  - Computes gradients using calculus rules on numerical programs
  - Applies the chain rule efficiently from output to input
  - Avoids manual gradient derivation for new architectures

- **Practical Benefits**
  - Major deep learning frameworks (E.g., tensorFlow, PyTorch) implement
    automatic differentiation
  - Enables rapid experimentation with network structures, activation functions,
    and loss functions
  - Frees you from manually re-deriving learning rules

- **Encouragement of End-to-End Learning**
  - Complex tasks (E.g., machine translation) modeled as compositions of
    trainable subsystems
  - Trained on input-output pairs without explicit internal supervision
  - Requires minimal prior knowledge about internal components or roles

# Advanced Neural Network Architectures

## Convolutional Neural Networks

* Convolutional Neural Networks

- **Motivation**
  - Feedforward networks do not scale with high-dimensional inputs like images
  - Convolutional neural networks (CNNs) are designed to exploit spatial
    structure in data

- **Key Idea**
  - Local connectivity: adjacent pixels have meaning
  - Weight sharing to detect spatially local patterns
  - Convolutions act as learnable filters applied across input regions

- **Basic Components**
  - Convolutional layer: applies multiple filters across input
  - Activation function: non-linearity (e.g., ReLU) after convolution
  - Pooling layer: reduces spatial dimensions (e.g., max pooling)
  - Fully connected layers: typically at the end for classification

![](lectures_source/figures/Lesson11_CNN.png)

* Convolutional Neural Networks

- **Convolutional Layer Mechanics**
  - Filter (or kernel): small matrix of weights (E.g., $3 \times 3$)
  - Apply dot product between filter and local patch of input

- **Weight Sharing**
  - Each filter is reused across all spatial locations
  - Spatial invariance: things detectable in an image should be independent of
    the position in the image
  - Reduces number of parameters and improves generalization

- **Feature map **
  - Output of a convolutional layer after applying filters (kernels) to the
    input
  - Represents spatial patterns such as edges, textures, or more complex
    features learned by the network

- **Stacking Convolutions**
  - Multiple layers can detect increasingly abstract features:
    - Early layers detect edges, textures
    - Later layers detect object parts or entire objects

- **Example**
  - An image of size $32 \times 32 \times 3$ with a $5 \times 5$ filter creates a
    $28 \times 28$ feature map (ignoring padding)

* Convolutional Neural Networks

- **Pros**
  - Parameter efficiency due to local connectivity and weight sharing
  - Invariant to translation and small distortions in input
  - Scalable to large input sizes (E.g., high-resolution images)

- Pooling is a downsampling operation that reduces the spatial dimensions (width
  and height) of feature maps
  - Reducing computation
  - Controlling overfitting
  - Making features more translation-invariant
  - Common pooling: max pooling, average pooling

- **Common Architectures**
  - LeNet, AlexNet, VGG, ResNet

- **Applications**
  - Vision tasks
    - Image classification
    - Object detection
    - Face recognition
    - Medical imaging
  - NLP
  - Audio processing

* Residual Networks (ResNets)

::: columns
:::: {.column width=70%}
- **Motivation for Deep Networks**
  - Deeper networks perform better on complex tasks
  - Very deep networks suffer from vanishing/exploding gradient problems

- **Key Idea: Residual Learning**
  - Learn a residual function $F(x) = H(x) - x$ instead of a direct mapping
    $H(x)$
  - Original function becomes $H(x) = F(x) + x$
  - Easier to learn a small change from a function than a new function

- **Residual Block Structure**
  - Each block computes $F(x)$ and outputs $F(x) + x$
  - $x$ is the _shortcut connection_ or _skip connection_

- **Pros**
  - Enable training of deeper networks (e.g., hundreds of layers)
  - Help gradients flow during backpropagation

- **Empirical Success**
  - ResNets outperform plain networks of the same depth
  - Achieved state-of-the-art results on ImageNet and other benchmarks
::::
:::: {.column width=25%}
```graphviz[width=50%]
digraph ResidualBlock {
    rankdir=LR;
    splines=ortho;
    node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];
    nodesep=0.50;

    x       [label="x", shape=plaintext];
    join [shape=point];
    layer1  [label="Layer"];
    layer2  [label="Layer"];
    add     [label="+", shape=circle, width=0.3, fixedsize=true];
    out     [label="x + F(x)", shape=plaintext];

    {rank=min; layer1; layer2; x; join, add, out}
    
    x -> join [arrowhead=none];
    join -> {layer1; add};
    layer1 -> layer2;
    layer2 -> add;
    
    add -> out;
}
```
::::
:::

## Recurrent Neural Networks

* Recurrent Neural Networks (RNNs)
::: columns
:::: {.column width=55%}
- Maintain a hidden state that evolves over time
  - Process input sequences one element at a time, maintaining a memory of
    previous inputs
  - Output depends on current input and previous hidden state
  - Useful for tasks where the order of data is crucial
- Trained using backpropagation through time

- Pros
  - Designed for sequential data
    - E.g., time series, text
  - Can, in theory, capture temporal dependencies of arbitrary length

- Cons
  - Struggle with long-term dependencies (vanishing gradients)
  - Training is slow due to sequential processing (no parallelism over time steps)
  - Struggles with long-term memory retention
  - Suitable for simple sequences

- **Examples**:
  - Language modeling: Given a sequence of words, predict the next word
  - Time series prediction: Forecast future values based on past observations
  - Sentiment analysis: Determine the sentiment of a sentence by considering the
    sequence of words
::::
:::: {.column width=40%}
```graphviz
digraph FeedbackSystem {
    rankdir=TB;
    splines=ortho;
    node [shape=box, style="rounded,filled", fillcolor=white, fontname="Helvetica", fontsize=12, penwidth=1.4];
    nodesep=0.4;
    ranksep=0.8;

    Input [label="Input layer"];
    Block1 [label="Hidden layer 1"];
    Block2 [label="Hidden layer 2"];
    join1 [shape=point, width=0.1, height=0.1, label=""];
    Output [label="Output layer"];

    // Edges
    x -> Input;
    Input -> Block1;
    Block1 -> Block2;
    Block2 -> join1;
    join1 -> Block1 [label="Feedback", style=dashed];
    join1 -> Output [label="y"];

    // Rank hints
    {rank=same; x; Input; Block1; Block2; join1; Output}
}
```
// TODO(TA): Rotate the pic
::::
:::

* Vanishing and Exploding Gradient Problem

- Arise due to repeated multiplication of gradients in the chain rule

- **Vanishing Gradient**
  - Gradients become very small in early layers as they are propagated backward
  - Leads to extremely slow learning or no learning
  - Common with activation functions like sigmoid or tanh

- **Exploding Gradient**
  - Gradients grow exponentially during backpropagation
  - Causes unstable updates and potential overflow in weights
  - Often seen when weights are initialized with large values

- **Solutions**
  - Use ReLU or similar activations to mitigate vanishing gradients
  - Apply gradient clipping to handle exploding gradients
  - Normalize inputs
  - Apply batch normalization

- These issues motivated the design of architectures like LSTM and ResNet

* Long Short-Term Memory (LSTM) Networks

// https://medium.com/@ottaviocalzone/an-intuitive-explanation-of-lstm-a035eb6ab42c

// https://d2l.ai/chapter_recurrent-modern/lstm.html

- Special architecture with memory cells and gates
  - Forget gate: decides what information to discard from the cell state
  - Input gate: controls what new information to store
  - Output gate: determines what to output from the cell

- **Pros**
  - Designed to overcome vanishing gradient problem in standard RNNs
  - Captures long-range dependencies through gated memory cells
  - Flexible architecture for learning when to remember or forget

- **Cons**
  - More complex than RNNs: includes multiple gates and a memory cell
  - Higher computational cost due to increased number of parameters
  - Slower training and inference compared to simpler models (e.g., GRUs)
  - Difficult to parallelize over time steps

- Especially effective in natural language processing and time series tasks
- E.g., used in machine translation, speech recognition, and language modeling

// TODO(TA): Add a graphviz pic

* Gated Recurrent Units (GRUs)

// https://d2l.ai/chapter_recurrent-modern/gru.html

- Alternative to LSTMs for sequential modeling
  - Combine **forget** and **input** gates into a single **update gate**
  - Retain essential functionality with fewer components:
    - Update gate controls how much of the past state to keep
    - Reset gate controls how much of the past information to forget

- **Pros**
  - Requires fewer parameters, leading to faster training
  - Comparable or better performance than LSTM on many tasks
  - Easier to tune and implement
  - Suitable for real-time and low-resource applications

- **Cons**
  - May be less expressive than LSTM for very complex patterns
  - Lacks a separate memory cell, which may limit ability to retain long-term
    information
  - Slightly less studied and standardized than LSTM in some domains

- **Example**
  - GRUs are preferred in real-time speech recognition systems due to their
    efficiency

// TODO(TA): Add a graphviz pic

## Deep learning learning algorithms

* Techniques for Training Deep Neural Networks

- Deep networks are hard to train 
  - Due to vanishing/exploding gradients
  - Optimization can be slow and sensitive to hyperparameters

- **Momentum**
  - Modifies gradient descent to accelerate convergence
  - Updates use an exponentially decaying average of past gradients
  - Update rule: $v_t = \beta v_{t-1} + \eta \nabla \theta_t$,
    $\theta_{t+1} = \theta_t - v_t$
  - Helps overcome local minima and reduces oscillations in ravines

- **Adam (Adaptive Moment Estimation)**
  - Combines momentum with adaptive learning rates
  - Maintains estimates of both first moment (mean) and second moment
    (uncentered variance) of gradients
  - Update rule uses bias-corrected estimates:
    - $m_t = \beta_1 m_{t-1} + (1 - \beta_1)\nabla \theta_t$
    - $v_t = \beta_2 v_{t-1} + (1 - \beta_2)(\nabla \theta_t)^2$
    - $\theta_{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$
  - Works well in practice with little tuning

* Techniques for Training Deep Neural Networks

- **Batch Normalization**
  - Normalizes the input of each layer to have zero mean and unit variance
  - Computed over each mini-batch:
    $$\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
  - Introduces learnable parameters $\gamma$ and $\beta$ to scale and shift the
    normalized values
  - Benefits:
    - Reduces internal covariate shift
    - Enables higher learning rates
    - Acts as a regularizer, reducing need for dropout


* Regularization in neural networks
- (Soft) weight elimination: fewer weights $\implies$ smaller VC dimension, so
  we would like to remove some neurons (i.e., push weights towards 0)
- For any activation function (e.g., tanh()) a small weight means that we work
  in the linear regime, while a large weight leaves us in the binary regime
- Using a normal regularization
  $$
  \Omega(\vw) = \sum_{i,j,l} (w^{(l)}_{ij})^2
  $$
  we have the problem that a neuron in binary regime is penalized more than many
  neurons in linear regime
- So for neural networks we use a regularizer as:

  $$
  \Omega(\vw) = \lambda \sum_{i, j, l} \frac{(w^{(l)}_{ij})^2}{\beta^2 + (w^{(l)}_{ij})^2}
  $$

  so that the penalization is quadratic for small $\vw$ and then it saturates as
  function of the weight magnitude


## Deep learning for NLP

* NLP Tasks
- **Part-of-speech (POS) tagging**
  - Assign a POS (e.g., noun, verb, adjective) to each word in a sentence 
  - Data needs: labelled data

- **Co-reference resolution**
  - E.g., _"Mike told that John was sick, so I took him to the hospital"_
    - Who does _"him"_ refer to?
    - _"John"_ with high probability
  - The output is a distribution over possible antecedents
  - Data needs: labelled data

- **Sentiment analysis**
  - Classify a text as having positive or negative sentiment
    - E.g., "the movie was poorly written and acted" $\to$ negative
  - Data needs: nothing in theory

- **Machine translation**
  - Translate a sentence from a source language to a target language
    - E.g., from Spanish to English
  - Data needs: large corpus of source/target sentence pairs

- **Information extraction**
  - Automatically extracting structured information from unstructured text

- **Named entity recognition**
  - Identify and classify named entities (e.g., people, organizations, locations)
    in text

* Part-of-speech (POS) tagging
- **Example**
  - Input: "The dog barks loudly"
  - Output: [Determiner (DT), Noun (NN), Verb (VB), Adverb (RB)]

- **Techniques**
  - *Rule-based*: hand-crafted grammar rules
  - *Statistical*: models like Hidden Markov Models (HMMs)
  - *Machine Learning*: classifiers (e.g., SVM)
  - *Neural Networks*: LSTM, BiLSTM, or Transformers

- **Challenges**
  - Ambiguity: "can" can be a verb or a noun
  - Context-dependency: Tags depend on the surrounding words

* NLP using Rules-Based Systems
- Systems based on rules for parsing and semantic analysis have shown:
  - Success in many tasks
  - Performance limited by linguistic complexity
  - E.g., "cut" can be a verb or a noun
    - Present, past, infinite verb
    - Transitive or intransitive
    - The meaning depends on the context

- Idea:
  - Vast amount of text in machine-readable form $\to$ use data-driven ML
    approach 

* The bitter lesson
- _"General methods leveraging computation ultimately outperform
  human-knowledge-based systems"_, Sutton (2019)

- Key Observation
  - Human-designed heuristics and domain knowledge lose
  - General-purpose learning algorithms that scale with computation tend to win
  - E.g.,
    - Chess: hand-crafted rules outperformed by AlphaZero using self-play and
      deep reinforcement learning
    - Vision: feature-engineered models replaced by end-to-end trained
      convolutional networks

- The "bitter" part
  - Humans like to inject knowledge into systems
  - But machines learn better when allowed to discover patterns from data
    themselves

* The bitter lesson
- **Implications**
  - Invest in scalable methods over handcrafted knowledge
  - Embrace compute-heavy solutions that learn from data
  - Focus on architectures that can generalize across domains
  - Accept that performance gains come from scaling data and compute

- **Controversy**
  - Seen by some as dismissive of domain expertise and symbolic methods
  - Encourage a shift toward empirical and data-driven AI research

* Word Representation
- **Goal**: create representation of words for NLP tasks

- **Requirements**:
  - No need for manual feature engineering
  - Allow for generalization between related words
    - Syntactically: "colorless" and "ideal" are both adjectives
    - Semantically: "cat" and "kitten" are both felines
    - Topically: "sunny" and "rainy" are both weather terms
    - Sentiment: "awesome" and "cringeworthy" are opposite

- **Approaches**:
  1. Encode a word into an input vector (e.g., one-hot vector)
     - Cons: doesn't capture similarity between words
  2. Represent as a vector of n-gram counts
     - Phrases of $n$ words containing each word
     - Very large number of vectors
     - E.g., 100,000-word vocabulary $\to$ $100,000^5 = 10^{25}$ vectors
     - Very sparse since most counts are zero
  3. Learn word embeddings
     - Low-dimensional vector representing a word, learned from the data
     - E.g., 100 dimensions

* Word Embeddings: Emerging Properties
- Similar words have similar word embeddings
  - Words cluster based on their topics

- Difference between related words seems to have a meaning
  - E.g., `Greece` - `Athens` = country/capital relationship
  - E.g., negative, plural, superlative, past tense

- These properties are:
  - Not really enforced
  - Approximate
  - Emergent

* Word Embeddings: Pretrained vs Custom
- Word embedding representations are independent of the task
  - Can pretrain and reuse them
    - Word2vec
    - GloVe (global vectors)
    - FastText

- For some tasks better to learn word embeddings end-to-end with the task
  - E.g., POS tagging: the same word needs to be represented in different ways
    - E.g., "cut" can be verb or noun $\to$ multiple semantic embeddings

* Language Models
- Word embedding are a good representation for words in isolation
- Language consists of sequence of words where context of each word is important
  - E.g., in _"Michael told me that John was sick, so I took him to the
    hospital"_, "him" can refer to multiple people

- A **language model** is a probability distribution of sequences of words
  - Predict the next word in a text given all the previous words
    - Supervised learning
    - Enormous amount of data

- How to learn a language model?
  1. Feedforward neural network
  2. Recurrent neural network
  3. Transformer architecture

* Feedforward network for language models
- **Learning set-up**:
  - Large feedforward network
  - Fixed-size context window of $n$ words
  - Word embeddings

- **Problems:**
  1. Context might need to be very long
     - Not all the words are important
     - E.g., a sentence with 20 words requires a large number of parameters if
       each word is considered in any context
  2. Problem of asymmetry
     - Weights need to be learned for each word in each position
     - E.g., different weights for the word "cat" when it appears at the
       beginning versus the end of a sentence
   - Too many parameters to learn
   - Computationally expensive

* RNNs for language models
- RNNs process sequence of data, one piece of data at a time
  - Process language, one word at a time

- Architecture
  1. Each input word is encoded as word embedding vector
  2. A hidden layer gets the previous state and the new word
     - Allocate storage space for features of inputs useful for the task
  3. The output is a probability over output words

* RNNs for language models: pros and cons
- **Pros**:
  - Context too long:
    - The number of parameters is constant independently of the context (i.e.,
      n-grams)
  - Problem of asymmetry:
    - No problem of asymmetry since the weights are the same for every word
      position

- **Cons**:
  - Can be difficult to train
    - Solution: Use transformers
  - Solve the context problem, but only in theory
    - In theory, information can be passed along through steps
    - In practice, it is lost or distorted (similar to vanishing gradient problem,
      but over time)
    - Solution: Use LSTMs

* Sequence-to-Sequence Models
- Enable learning of mappings between sequences of differing lengths and
  structures
  - Used for transforming one sequence into another, e.g.,
    - Machine translation
    - Text summarization
    - Speech recognition

- Composed of two main components:
  - **Encoder**: Compress information from input sequence into a fixed-size
    context vector
  - **Decoder**: Generates output sequence from the context vector
  - Both encoder and decoder are typically RNNs (e.g., LSTM or GRU)

- Training is supervised with input-output sequence pairs

- Limitation:
  - Fixed-size context vector can become a bottleneck for long sequences
  - Attention mechanisms so decoder can access all encoder states

// TODO: Add pic

* Sequence-to-Sequence Attention
- **Intuition**
  - Instead of processing input sequentially (like RNNs), attention processes all
    simultaneously
  - Each token can "look at" others and decide which are most important
    - E.g., in translation, the word "bank" in "river bank" gets higher attention
      from nearby words like "river"
  - It's like "context-based summarization" of the source sentence into a
    fixed-dimensional representation

- **Motivation**
  - Enables models to focus on relevant parts of the input sequence
  - Mimics human cognitive attention by weighting important information

* Sequence-to-Sequence Attention
::: columns
:::: {.column width=65%}
- If the RNN model is
  $\vh_i = RNN(\vh_{i-1}, \vx_i)$
- The sequence-to-sequence attention is the concatenation of $\vx_i$
  and the context vector $\vc_i$
  $$\vh_i = RNN(\vh_{i-1}, [\vx_i, \vc_i])$$
- The raw attention score $r_{ij}$ is:
  $$r_{ij} = \vh_{i-1} \cdot \vs_j$$
  where
  - $\vh_{i-1}$ is the current target state 
  - $\vs_j$ is the $j$-th source word (i.e., the output of the source RNN vector
    for the word $j$)
::::
:::: {.column width=30%}

![](lectures_source/figures/Lesson11_Attention_1.png)
![](lectures_source/figures/Lesson11_Attention_2.png)
::::
:::

- The attention scores are normalized into probability using a softmax:
  $$a_{ij} = \frac{e^{r_{ij}}}{\sum_k e^{r_{ik}}}$$

- Finally the vectors $\vs_j$ are weighted and summed as:
  $$\vc_i = \sum_j a_{ij} \cdot \vs_j$$

// TODO: Create a schematic picture with graphviz

* Types of Attention
- **Self-Attention**: Each input attends to all inputs, including itself
  - Useful for capturing dependencies within a single sequence
  - E.g., in language models, each word can attend to all other words in a
    sentence
- **Cross-Attention**: One sequence attends to another
  - Facilitates the interaction between different sequences
  - E.g., in machine translation, the target language sequence attends to the
    source language sequence
- **Multi-Head Attention**
  - Applies attention multiple times in parallel (with different linear
    projections)
  - The results are concatenated together to form $\vc_i$
  - Concatenating is better than summing to keep important information
  - Captures different types of relationships at different subspaces
  - E.g., in a Transformer model, different heads might focus on different parts
    of a sentence, such as subject-verb agreement or noun-adjective
    relationships

* Attention: Vectorized Formula
- Compute a weighted sum of values ($V$) using weights derived from queries
  ($Q$) and keys ($K$)
  $$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
  where:
  - $Q$: matrix of query vectors (shape: $n_q \times d_k$)
  - $K$: matrix of key vectors (shape: $n_k \times d_k$)
  - $V$: matrix of value vectors (shape: $n_k \times d_v$)
  - $d_k$: dimensionality of the key vectors
  - $QK^T$: dot products between each query and all keys (shape: $n_q \times n_k$)

- **Steps**:
  1. **Dot Product**: Compute similarity between each query and all keys
     - $QK^T$ produces attention scores
  2. **Scaling**: Divide scores by $\sqrt{d_k}$
     - To stabilize gradients
  3. **Softmax**: Apply softmax to each row of the scaled scores
     - Converts scores into a probability distribution (attention weights)
  4. **Weighted Sum**: Multiply weights by $V$ to produce context vectors

- **Advantages**
  - Parallelizable computation
  - Better handling of long-range dependencies than RNNs

* Sequence-to-Sequence: Decoding
- Training a sequence-to-sequence model requires to:
  - Maximize the probability of each word in the target training sentence
  - Conditioned on the source and previous target words

- At inference time:
  - Given a source sentence
  - Generate the target word one at a time
  - Feed back the target word for the next time step

- Greedy decoding
  - Pick the next word that has the highest probability
  - Pros
    - Fast
  - Cons
    - We need to maximize the probability of the entire target sequence
    - Greedy decoding doesn't have a mechanism to correct a mistake
    - Many times the model needs to see what comes next

- Beam search:
  - Keep the top $k$ hypotheses at each stage
  - Choose the hypothesis with the best score

* Transformer Architecture
- Revolutionized sequence modeling through combining several ideas

- **Self-attention**
  - Eliminates recurrence by processing sequences in parallel
  - Model long-distance context without a sequential dependency
  - Multi-head attention to capture different aspects of relationships between
    tokens

- Transformer has many (6 or more) transformer layers
  - **Transformer layer**
    - Self-attention
    - Residual connection
    - Feedforward layer (with ReLU)
    - Residual connection

- **Positional embedding**: injects sequence order into token embeddings
  - Transformer architecture has no inherent way to capture token order in
    sequences
  - Add "positional embeddings" to input embeddings to provide information about
    the position of each token in the sequence

- **Pros**
  - Enables greater scalability and parallelization

- Dominates current state-of-the-art in NLP
  - E.g., BERT, GPT, T5, and others

// TODO(TA): Add a graphviz pic

* Pretraining
- For computer vision train using large collections of hand-labeled images (e.g., ImageNet)
- For text use un-labeled data
  - NLP tasks are not easy for labellers (e.g., POS)
  - Internet has large amount of text (100b words added every day)
    - Common crawl
    - Wikipedia
    - FAQs can be used (for question-answering)
    - Websites have multiple version (for translation)

- Pretrain using a large amount of text data
  - Fine tune the model with labeled data
  - It's an example of transfer learning

* Masked language models
- Word prediction in language models are made left-to-right
- Sometimes the context comes later in the context
  - E.g., "the river rose five feet"

- Train models using masking a word and predicting it
  - E.g., "the river _______ five feet"

- The sentence provides its own labels

/*
# Fundamentals of Deep Learning

* Neural Networks and Backpropagation
- Neural networks are composed of layers of interconnected nodes
- Each connection has an associated weight
- **Forward pass** computes output from input
- **Backward pass** updates weights to minimize error
- **Backpropagation** uses chain rule of calculus
- Training minimizes a **loss function**

* Loss Functions
- Measure the difference between predictions and true labels
- **Cross-Entropy Loss**:
  - Used for classification problems
  - Penalizes confident but wrong predictions
- **Mean Squared Error (MSE)**:
  - Used for regression problems
  - Measures average squared difference
- Choice of loss impacts model performance
- Example: use cross-entropy to classify cats vs dogs

* Optimization
- Optimization algorithms update model weights
- **Stochastic Gradient Descent (SGD)**:
  - Update weights using small random batches
- **Adam**:
  - Combines momentum and adaptive learning rates
- **RMSprop**:
  - Scales learning rates based on recent gradient magnitudes
- Adam is a popular default choice
- Good optimization accelerates convergence
- Example: using Adam for training an LSTM network

* Overfitting and Regularization
- **Overfitting** occurs when a model memorizes instead of generalizing
- Regularization methods:
  - **L2 Regularization** (Weight Decay): penalizes large weights
  - **Dropout**: randomly deactivates neurons during training
- Helps prevent overfitting
- Early stopping can also regularize training
- Balancing bias and variance is key
- Example: applying dropout in a fully connected network


# Training Deep Neural Networks
* Weight Initialization Strategies
- Proper initialization speeds up training
- Poor initialization leads to vanishing or exploding gradients
- Common strategies:
  - Xavier Initialization (for tanh activations)
  - He Initialization (for ReLU activations)
- Initialize biases to zero
- Helps networks converge faster and more reliably
- Example: using He initialization for a deep CNN

* Batch Normalization
- Normalizes activations across mini-batches
- Reduces internal covariate shift
- Stabilizes and speeds up training
- Acts as a form of regularization
- Can enable higher learning rates
- Often inserted after fully connected or convolutional layers
- Example: applying batch normalization after convolution

* Learning Rate Schedules and Warmup
- Adjust learning rate during training
- Common schedules:
  - Step decay
  - Cosine annealing
- **Warmup**:
  - Start with small learning rate and gradually increase
- Helps avoid instability at the start
- Critical for training deep models like Transformers
- Example: cosine annealing schedule with warmup for BERT training

* Early Stopping and Model Checkpointing
- **Early stopping**:
  - Halt training when validation loss stops improving
- **Model checkpointing**:
  - Save best-performing models during training
- Prevents overfitting and wasted computation
- Needs a validation set to monitor performance
- Standard practice in deep learning pipelines
- Example: stopping training after 5 epochs of no improvement

* Data Augmentation Techniques
- Create additional training data by transforming existing data
- Techniques:
  - Image: flips, rotations, color jittering
  - Text: synonym replacement, back-translation
  - Time series: window slicing, jittering
- Increases dataset size without new labels
- Helps generalize better and prevent overfitting
- Example: random cropping and flipping in image classification

# Interpretability and Explainability
* Feature Attribution Methods (Saliency Maps, SHAP)
- Assign importance scores to input features
- **Saliency Maps**:
  - Visualize gradient of output w.r.t. input
- **SHAP (SHapley Additive exPlanations)**:
  - Fairly distribute contribution among features
- Help identify what the model is focusing on
- Useful for trust, debugging, fairness analysis
- Example: heatmap showing important pixels for image classification

* Explainable AI (XAI) Techniques
- Techniques to make black-box models interpretable
- Approaches:
  - Post-hoc explanation (LIME, SHAP)
  - Intrinsic interpretability (decision trees)
- Helps users understand model decisions
- Critical for regulated domains (e.g., healthcare, finance)
- Builds trust and transparency
- Example: explaining a loan approval decision from a deep model

* Attention Mechanisms for Interpretability
- Highlight relevant parts of the input
- **Self-attention**:
  - Each element attends to all other elements
- Visualize attention weights to see focus areas
- Used heavily in Transformer architectures
- Example: attention heatmap in a translation model

* Layer-wise Relevance Propagation
- Decomposes model prediction into layer-wise contributions
- Tracks relevance backward through the network
- Shows how much each input contributes to the final output
- Provides pixel-level or feature-level explanations
- Popular in computer vision interpretability
- Example: understanding decision of a CNN on an X-ray image

# Deep Generative Models
* Variational Autoencoders (VAEs)
- Probabilistic models for generating data
- Encoder maps input to a distribution over latent space
- Decoder reconstructs input from latent variables
- Objective: maximize Evidence Lower Bound (ELBO)
- Introduces stochasticity via reparameterization trick
- Useful for unsupervised learning and anomaly detection
- Example: VAE generating synthetic handwritten digits

* Generative Adversarial Networks (GANs)
- Consist of two networks:
  - Generator tries to create realistic data
  - Discriminator distinguishes real from fake data
- Trained in a **minimax** adversarial game
- Objective: fool the discriminator
- Capable of producing highly realistic samples
- Example: GANs generating photorealistic faces (StyleGAN)

* Energy-Based Models
- Define probability via an energy function
- Lower energy implies higher probability
- Do not require explicit normalization
- Learning aims to assign low energy to real data
- Examples:
  - Restricted Boltzmann Machines (RBMs)
  - Contrastive Divergence for training
- Useful in unsupervised representation learning

* Flow-Based Generative Models (Normalizing Flows)
- Learn invertible transformations between data and latent space
- Allow exact likelihood computation
- Chains of simple bijective functions (flows)
- Training maximizes exact log-likelihood
- Examples:
  - RealNVP
  - Glow
- Useful for density estimation and sample generation

* Diffusion Models
- Generate data by reversing a noise process
- Forward process:
  - Gradually add noise to data
- Reverse process:
  - Denoise step-by-step using learned transitions
- Recent breakthroughs in image generation (e.g., DALL-E 2, Stable Diffusion)
- High sample quality and diversity
- Example: generating high-resolution synthetic images

# Bayesian Deep Learning
* Bayesian Neural Networks (BNNs)
- Place probability distributions over network weights
- Predictive distribution captures model uncertainty
- Typically intractable; use approximations
- Useful in safety-critical and low-data settings
- Higher computational cost than standard NNs
- Example: BNN estimating confidence in medical diagnosis

* Dropout as Approximate Bayesian Inference
- Dropout during inference approximates Bayesian uncertainty
- Interpreted as sampling from a distribution over networks
- **MC Dropout**:
  - Perform multiple stochastic forward passes
- Provides cheap uncertainty estimates
- No need to modify training procedure heavily
- Example: confidence intervals for deep regression outputs

* Deep Ensembles
- Train multiple models independently
- Combine predictions (averaging or voting)
- Capture epistemic uncertainty effectively
- Improve both accuracy and calibration
- More robust to adversarial examples and distribution shifts
- Example: ensemble of 5 ResNet models for image classification

* Posterior Approximation Techniques (MC Dropout, VI)
- **MC Dropout**:
  - Approximate posterior by dropout sampling
- **Variational Inference (VI)**:
  - Approximate posterior with a simpler distribution
- VI methods minimize KL divergence to true posterior
- Trade-off between accuracy and computational efficiency
- Important for Bayesian deep learning
- Example: using VI to train a Bayesian CNN

# Deep Probabilistic Models
* Probabilistic Graphical Models (PGMs)
- Represent dependencies among variables with graphs
- Nodes = random variables; edges = dependencies
- Two main types:
  - Bayesian Networks (directed)
  - Markov Networks (undirected)
- Enable efficient inference and learning
- Foundation for structured probabilistic modeling
- Example: graphical model for medical diagnosis

* Deep Latent Variable Models
- Combine deep learning and latent variable modeling
- Latent variables capture hidden structure
- Examples:
  - Variational Autoencoders (VAEs)
  - Deep Latent Gaussian Models
- Learning via variational inference
- Useful for generative modeling and semi-supervised learning
- Example: modeling topics in text documents

* Structured Variational Inference
- More expressive variational approximations
- Capture dependencies among latent variables
- Improves over mean-field assumptions
- Techniques:
  - Normalizing flows
  - Hierarchical variational models
- Enables more accurate posterior approximations
- Example: structured VI for complex Bayesian neural networks

* Neural Processes and Meta-Learning
- Neural Processes (NPs) model distributions over functions
- Combine ideas from GPs and deep learning
- Learn to predict new tasks from few examples
- Key idea:
  - Encode context set to latent representation
  - Decode latent to predict target set
- Useful in meta-learning and few-shot prediction
- Example: fast adaptation to new regression tasks

# Uncertainty Quantification
* Aleatoric vs Epistemic Uncertainty
- **Aleatoric uncertainty**:
  - Inherent noise in observations
  - Irreducible by collecting more data
- **Epistemic uncertainty**:
  - Uncertainty due to limited knowledge
  - Can be reduced with more data
- Important for risk-sensitive applications
- Example: distinguishing measurement noise vs model ignorance

* Calibration of Neural Networks
- A model is calibrated if its confidence matches actual accuracy
- Poor calibration leads to overconfident wrong predictions
- Techniques:
  - Temperature scaling
  - Platt scaling
- Important for safe deployment in critical systems
- Example: calibrating deep classifiers for medical diagnosis

* Conformal Prediction for Deep Models
- Produces prediction sets with guaranteed coverage
- Model-agnostic framework
- Guarantees valid uncertainty estimates under minimal assumptions
- Can adapt to regression and classification tasks
- Useful when strong reliability is required
- Example: predicting interval for house price with 90% confidence

* Predictive Uncertainty in Classification and Regression
- In classification:
  - Predict class probabilities
- In regression:
  - Predict intervals around point estimates
- Use uncertainty to abstain from decisions when unsure
- Important for active learning and decision-theoretic systems
- Example: model abstains if classification confidence < 0.7

# Probabilistic Programming and Inference
* Introduction to Probabilistic Programming Languages (PyMC, Pyro, Edward)
- PPLs express probabilistic models as programs
- Automate inference via variational methods or MCMC
- Popular libraries:
  - PyMC (Python-based, intuitive)
  - Pyro (scalable, deep learning integration)
  - Edward (built on TensorFlow)
- Enable rapid prototyping of complex models
- Example: building Bayesian regression in PyMC

* Variational Inference (VI)
- Approximate true posterior with simpler distribution
- Minimize KL divergence between true and approximate posterior
- Faster but biased compared to MCMC
- Often used in scalable probabilistic modeling
- Supports amortized inference via neural networks
- Example: VI for Bayesian neural network training

* Markov Chain Monte Carlo (MCMC) for Deep Models
- Sample from posterior distributions
- Guarantees asymptotic correctness
- Common algorithms:
  - Metropolis-Hastings
  - Hamiltonian Monte Carlo (HMC)
- Computationally expensive for deep networks
- Example: using HMC to sample weights of a small BNN

* Stochastic Variational Inference
- Scales variational inference to large datasets
- Uses mini-batches for optimization
- Combines VI with stochastic gradient methods
- Key for deep latent variable models like VAEs
- Example: stochastic VI training of deep generative models

# Modern Research Frontiers
* Diffusion Probabilistic Models (DDPMs, Stable Diffusion)
- Model data generation as a reverse diffusion process
- Gradually denoise from pure noise to structured data
- DDPMs achieve state-of-the-art in image synthesis
- Example: Stable Diffusion generating detailed artworks

* Score-Based Generative Modeling
- Train a model to estimate score (gradient of log-density)
- Use score to generate new samples
- Highly effective for generating complex high-dimensional data
- Connects to diffusion and Langevin dynamics
- Example: score-based models generating realistic audio samples

* Neural ODEs and Continuous Normalizing Flows
- **Neural ODEs**:
  - Model dynamics via ordinary differential equations
- **Continuous Normalizing Flows**:
  - Continuous transformation for exact density modeling
- Benefits:
  - Memory-efficient
  - Flexible modeling of dynamics
- Example: time-continuous modeling of irregularly-sampled data

* Self-Supervised Learning (SimCLR, BYOL)
- Learn useful representations without labels
- Techniques:
  - Contrastive learning (SimCLR)
  - Bootstrap learning (BYOL)
- Pretrain models on large datasets, fine-tune on tasks
- Key to scaling deep learning without costly annotations
- Example: self-supervised pretraining for image classification

* Causal Deep Learning (Counterfactual Deep Networks)
- Combine deep learning with causal reasoning
- Learn counterfactual outcomes from data
- Approaches:
  - Counterfactual data augmentation
  - Learning structural causal models with deep nets
- Critical for decision-making under interventions
- Example: predicting treatment effects using deep counterfactual models

# Bonus Topics
* Multimodal Deep Learning (Vision+Language Models)
- Fuse information from multiple modalities
- Applications:
  - Visual Question Answering (VQA)
  - Image captioning
- Techniques:
  - Late fusion, early fusion, joint embedding spaces
- Example: CLIP model aligning images and text embeddings

* Inverse Problems and Deep Learning
- Infer causes from observed effects
- Applications:
  - Medical imaging (e.g., MRI reconstruction)
  - Remote sensing
- Deep networks learn complex inverse mappings
- Example: reconstructing clear images from blurry observations

* Deep Learning for Scientific Discovery (e.g., Protein Folding, Physics)
- Deep learning accelerates scientific research
- Key examples:
  - AlphaFold for protein structure prediction
  - Neural networks solving PDEs
- Discover new insights from complex data
- Example: using neural networks for simulating physical systems
*/
