{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Home","text":""},{"location":"index.html#readme","title":"Readme","text":"<ul> <li>This file is the entrypoint of all the documentation and describes all the     documentation files in the <code>docs</code> directory</li> </ul>"},{"location":"index.html#how_to_organize_the_docs","title":"How to organize the docs","text":"<ul> <li> <p>Documentation can be organized in multiple ways:</p> <ul> <li>By software component</li> <li>By functionality (e.g., infra, backtesting)</li> <li>By team (e.g., trading ops)</li> </ul> </li> <li> <p>We have decided that</p> <ul> <li>For each software component there should be a corresponding documentation</li> <li>We have documentation for each functionality and team</li> </ul> </li> <li> <p>Processes</p> <ul> <li><code>onboarding</code></li> <li><code>general_background</code></li> <li><code>work_organization</code></li> <li><code>work_tools</code></li> <li><code>coding</code></li> <li>...</li> </ul> </li> <li> <p>Software components</p> <ul> <li><code>build</code></li> <li><code>kaizenflow</code></li> <li><code>datapull</code></li> <li><code>dataflow</code></li> <li><code>trade_execution</code></li> <li><code>infra</code></li> <li>...</li> </ul> </li> </ul>"},{"location":"index.html#dir_vs_no-dirs","title":"Dir vs no-dirs","text":"<ul> <li>Directories make it difficult to navigate the docs</li> <li>We use \u201cname spaces\u201d until we have enough objects to create a dir</li> </ul>"},{"location":"index.html#tracking_reviews_and_improvements","title":"Tracking reviews and improvements","text":"<ul> <li>Doc needs to be reviewed \"actively\", e.g., by making sure someone checks them     in the field</li> <li> <p>Somebody should verify that is \"executable\"</p> </li> <li> <p>There is a     Master Documentation Gdoc     that contains a list of tasks related to documentation, including what needs     to be reviewed</p> </li> <li> <p>For small action items we add a markdown TODO like we do for the code     <code>&lt;!-- TODO(gp): ... --&gt;</code></p> </li> <li> <p>To track the last revision we use a tag at the end of the document like:     <code>markdown     Last review: GP on 2024-04-20, ...</code></p> </li> </ul>"},{"location":"index.html#how_to_search_the_documentation","title":"How to search the documentation","text":"<ul> <li> <p>Be patient and assume that the documentation is there, but you can't find it     because you are not familiar with it and not because you think the     documentation is poorly done or not organized</p> </li> <li> <p>Look for files that contain words related to what you are looking for</p> <ul> <li>E.g., <code>ffind.py XYZ</code></li> </ul> </li> <li>Grep in the documentation looking for words related to what you are looking     for<ul> <li>E.g., <code>jackmd trading</code></li> </ul> </li> <li>Scan through the content of the references<ul> <li>E.g., <code>all.code_organization.reference.md</code></li> </ul> </li> <li>Grep for the name of a tool in the documentation</li> </ul>"},{"location":"index.html#file_description","title":"File description","text":"<ul> <li>Invariants:<ul> <li>Files are organized by directory (e.g., <code>docs</code>, <code>docs/work_tools</code>)</li> <li>Each file name uses the Diataxis naming convention</li> <li>Each file name should be linked to the corresponding file as always</li> <li>Files are organized in alphabetical order to make it easy to add more files     and see which file is missing</li> <li>Each file has a bullet lists summarizing its content using imperative mode</li> </ul> </li> </ul>"},{"location":"index.html#dir_structure","title":"Dir structure","text":"<ul> <li> <p>Prompt: For each directory write a short comment on its content, based on     the names of the files contained. Report the output in markdown, with a bullet     per directory, reflecting the structure of the files in the markdown and a     comment of fewer than 30 words for each directory     <code>bash     &gt; find docs -name \"*\\.md\" -d | sort</code></p> </li> <li> <p><code>docs/</code></p> <ul> <li>Contains top-level references and guides for documentation, workflows, and     software components.</li> </ul> </li> <li><code>docs/build_system/</code><ul> <li>Documents CI tools like Gitleaks, Semgrep, linters, and Pytest-Allure     integrations for the build process.</li> </ul> </li> <li><code>docs/code_guidelines/</code><ul> <li>Provides detailed how-to guides for writing clean, maintainable, and     review-ready code with AI productivity tips.</li> </ul> </li> <li><code>docs/documentation_meta/</code><ul> <li>Explains documentation frameworks, diagrams, and tooling for producing     technical and reproducible docs.</li> </ul> </li> <li><code>docs/general_background/</code><ul> <li>Offers foundational references like glossaries, abbreviations, and curated     reading materials.</li> </ul> </li> <li><code>docs/onboarding/</code><ul> <li>Comprehensive onboarding guides and checklists for developers, interns, and     contractors.</li> </ul> </li> <li><code>docs/tools/</code><ul> <li>Guides for general-purpose development and infrastructure tools, including     code hygiene, IDEs, and APIs.</li> <li><code>docs/tools/dev_system/</code><ul> <li>Focused on local development setup, environment configuration, dependency     management, and repo orchestration.</li> </ul> </li> <li><code>docs/tools/docker/</code><ul> <li>Docker usage guides for development workflows, including sibling     containers, DockerHub, and helper containers.</li> </ul> </li> <li><code>docs/tools/documentation_toolchain/</code><ul> <li>Covers the pipeline for rendering images, LaTeX integration, and notebook     asset extraction for docs.</li> </ul> </li> <li><code>docs/tools/git/</code><ul> <li>Git usage and automation guides, including hooks and helper integrations.</li> </ul> </li> <li><code>docs/tools/helpers/</code><ul> <li>Small helper modules and utilities, like caching, playback, dataframes, and     interactive grids.</li> </ul> </li> <li><code>docs/tools/jupyter_notebooks/</code><ul> <li>Jupyter-specific enhancements, including TOC creation, Jupytext usage, and     markdown publishing.</li> </ul> </li> <li><code>docs/tools/linter/</code><ul> <li>Development and customization of linters and link fixers for markdown     content.</li> </ul> </li> <li><code>docs/tools/notebooks/</code><ul> <li>Guides for managing and plotting data in Jupyter notebooks, including     pandas and publishing.</li> </ul> </li> <li><code>docs/tools/thin_environment/</code><ul> <li>Documentation of lightweight Python environments for GitHub and CI/CD     pipelines.</li> </ul> </li> <li><code>docs/tools/unit_test/</code><ul> <li>Unit testing practices: writing, structuring, and executing tests     effectively.</li> </ul> </li> </ul> </li> <li> <p><code>docs/work_organization/</code></p> <ul> <li>Team process documentation: roles, workflows, feedback, issue tracking, and     collaboration practices like Scrum and Kaizen.</li> </ul> </li> <li> <p>Review:</p> <ul> <li>GP, 2025-05-09</li> <li>GP, 2024-08-11</li> </ul> </li> </ul>"},{"location":"build_system/all.gitleaks_workflow.explanation.html","title":"All.gitleaks workflow.explanation","text":""},{"location":"build_system/all.gitleaks_workflow.explanation.html#gitleaks_integration_in_github_actions","title":"Gitleaks Integration in GitHub Actions","text":"<ul> <li>For more information on Gitleaks, refer to the following documentation:<ul> <li>/docs/tools/git/all.gitleaks.reference.md</li> <li>/docs/tools/git/all.gitleaks.how_to_guide.md</li> </ul> </li> </ul>"},{"location":"build_system/all.gitleaks_workflow.explanation.html#feature","title":"Feature","text":"<ul> <li>Automatic Scanning: Gitleaks runs automatically on every pull request to     the master branch and for every push to the master branch. This ensures that     new code is checked before merging</li> <li>Scheduled Scans: Gitleaks scans are scheduled to run once a day, ensuring     regular codebase checks even without new commits</li> <li>Workflow Dispatch: Allows for manual triggering of the Gitleaks scan,     providing flexibility for ad-hoc code analysis</li> </ul>"},{"location":"build_system/all.gitleaks_workflow.explanation.html#setup","title":"Setup","text":"<ul> <li>GitHub Action Workflow: The Gitleaks integration is set up as a part of     the GitHub Actions workflow in the <code>.github/workflows/gitleaks.yml</code> file in     each repo</li> <li>Gitleaks rules: The rules for Gitleaks used by the workflow are specified     in <code>.github/gitleaks-rules.toml</code></li> <li>Running Environment: The workflow runs on <code>ubuntu-latest</code> and uses GitHub     action <code>gitleaks/gitleaks-action@v2</code> available on the marketplace</li> </ul>"},{"location":"build_system/all.gitleaks_workflow.explanation.html#notification","title":"Notification","text":"<ul> <li>Slack: Sends a message to the <code>build-notifications</code> Slack channel when the     workflow fails (e.g., when leaks are detected)</li> </ul>"},{"location":"build_system/all.gitleaks_workflow.explanation.html#additional_resource","title":"Additional Resource","text":"<ul> <li>The official GitHub page for Gitleaks -     https://github.com/gitleaks/gitleaks</li> <li>For more information about how Gitleaks functions in GH Actions -     https://github.com/gitleaks/gitleaks-action/tree/master</li> <li>The custom ruleset was based on -     https://github.com/mazen160/secrets-patterns-db</li> </ul>"},{"location":"build_system/all.linter_gh_workflow.explanation.html","title":"All.linter gh workflow.explanation","text":""},{"location":"build_system/all.linter_gh_workflow.explanation.html#linter_gh_workflow","title":"Linter Gh Workflow","text":""},{"location":"build_system/all.linter_gh_workflow.explanation.html#linter_github_action_workflow_explanation","title":"Linter Github Action Workflow Explanation","text":""},{"location":"build_system/all.linter_gh_workflow.explanation.html#overview","title":"Overview","text":"<ul> <li>We want to use linter for all the new code that needs to be merged into the     <code>master</code> branch</li> <li>This is implemented by running the GitHub Actions workflow, <code>linter.yml</code>,     which is executed against the changed or added files in the branch</li> <li>If the linter detects changes in the new files, it indicates that the linter     did not run before.</li> <li>In this case, the workflow will fail, and will not allow the PR to be merged</li> </ul>"},{"location":"build_system/all.linter_gh_workflow.explanation.html#how_it_works","title":"How it works","text":""},{"location":"build_system/all.linter_gh_workflow.explanation.html#fetch_master_branch","title":"Fetch master branch","text":"<p>In order to compare the changed files in the PR with the latest master branch, fetch the latest master, e.g.,</p> <pre><code>invoke git_fetch_master\n</code></pre>"},{"location":"build_system/all.linter_gh_workflow.explanation.html#run_the_linter_and_check_the_linter_results","title":"Run the linter and check the linter results","text":"<ul> <li>Run the linter against the changed files in the PR branch</li> </ul> <pre><code>invoke lint --branch\n</code></pre> <ul> <li>Check if the git client is clean</li> </ul> <pre><code>git status\n</code></pre> <ul> <li>If the git client is not clean, abort the execution and the workflow will fail</li> <li>If the git client is clean, the workflow will exit successfully</li> </ul> <p>Invoke task for this action is:</p> <pre><code>invoke lint_check_if_it_was_run\n</code></pre>"},{"location":"build_system/all.pytest_allure.explanation.html","title":"All.pytest allure.explanation","text":""},{"location":"build_system/all.pytest_allure.explanation.html#pytest_allure_-_explanation","title":"Pytest Allure - Explanation","text":""},{"location":"build_system/all.pytest_allure.explanation.html#overview","title":"Overview","text":"<ul> <li>Allure Report boosts collaboration and project quality by providing clear,     detailed test reports that aid issue resolution for different team members</li> <li>The goal is to set up all our tests to generate and publish this report</li> <li>Here is the demo report     from allure</li> <li>The simplified flow for the Allure report is:</li> </ul> <pre><code>flowchart TD\n\nsubgraph A1[\" \"]\ndirection LR\n        A[Run tests and generate\\nthe Allure output] --&gt; B[(Backup the Allure output\\nto the AWS S3 bucket)]\n        B --&gt; C(Copy last history\\nfrom the S3 bucket\\nto the Allure output folder)\nend\nsubgraph B1[\" \"]\ndirection LR\n        D[Generate the HTML-report\\nfrom the Allure output] --&gt; E[(Publish the HTML-report\\nto the S3 HTML bucket)]\nend\nA1 ==&gt; B1 ==&gt; C1[View the report in the browser]\n</code></pre>"},{"location":"build_system/all.pytest_allure.explanation.html#core_features","title":"Core features","text":""},{"location":"build_system/all.pytest_allure.explanation.html#rich_and_interactive_reports","title":"Rich and Interactive Reports","text":"<ul> <li>Allure generates visually appealing and interactive HTML reports, making it     easy to analyze test results</li> <li>Reports include detailed information about test cases, steps, attachments, and     more</li> </ul>"},{"location":"build_system/all.pytest_allure.explanation.html#annotations_and_labels","title":"Annotations and Labels","text":"<ul> <li>Allure uses annotations and labels to provide additional information about     test methods, making it easier to understand and categorize test results</li> <li>Annotations are used to mark and describe test methods, and labels help in     categorizing and filtering tests</li> </ul>"},{"location":"build_system/all.pytest_allure.explanation.html#test_history_and_trends","title":"Test History and Trends","text":"<ul> <li>Allure maintains a history of test runs, allowing you to track changes in test     results over time</li> <li>Trends and statistics help identify patterns, improvements, or regressions in     the application's behavior</li> </ul>"},{"location":"build_system/all.pytest_allure.explanation.html#key_components","title":"Key Components","text":""},{"location":"build_system/all.pytest_allure.explanation.html#pytest_plugin_for_the_allure_output_generation","title":"Pytest plugin for the Allure output generation","text":"<p>In order to generate the Allure output, we need to install the <code>allure-pytest</code> plugin. For the time and efforts saving reasons, we will install it on-the-fly in the container where we will run the tests. This feature is introduced as the <code>allure_dir</code> parameter to the <code>run_&lt;type&gt;_tests</code> tasks. If specified the <code>allure-pytest</code> plugin will be installed and the results will be stored in the specified directory.</p>"},{"location":"build_system/all.pytest_allure.explanation.html#allure_reporting_tool","title":"Allure reporting tool","text":"<p>The CLI utility for generating Allure reports creates an HTML report from the Allure output. We'll install this utility using GitHub Actions workflow to generate the HTML report.</p>"},{"location":"build_system/all.pytest_allure.explanation.html#how_it_works","title":"How it works","text":"<ul> <li>Allure Report is composed of a framework(pytest) adapter and the allure     command-line utility</li> <li>Run the tests with pytest options <code>--alluredir=allure-results</code> to store the     results</li> <li>The generated files include:<ul> <li>Test result files     to describe execution of tests</li> <li>Container files     to describe test fixtures</li> </ul> </li> <li>If required, add additional files to the <code>allure-results</code> to display     additional info in the report</li> <li>Additional files includes:<ul> <li>Environment file     to store some global data for all the tests in the report</li> <li>History files     to enable trend graphs and other history-related features</li> <li>Categories file     to group test results into custom categories</li> </ul> </li> <li>Use <code>allure generate</code> to generate the test report into the specified     directory. For e.g.: <code>allure generate allure-report</code></li> </ul>"},{"location":"build_system/all.pytest_allure.explanation.html#historical_trends","title":"Historical Trends","text":"<p>A test report generated by Allure can not only display data about the latest test launch, but also help you compare it with the data from previous reports. To do so, Allure can keep a history or previous reports.</p> <p>In a tests report with the history included, you can:</p> <ul> <li>See what statuses did a test have previously (see Details panel \u2192 History tab)</li> <li>Find tests that changed status since last report (see Sorting and filtering \u2192     Filter tests by marks)</li> <li>See how specific values change over time (see Graphs)</li> </ul>"},{"location":"build_system/all.pytest_allure.how_to_guide.html","title":"All.pytest allure.how to guide","text":""},{"location":"build_system/all.pytest_allure.how_to_guide.html#pytest_allure","title":"Pytest Allure","text":""},{"location":"build_system/all.pytest_allure.how_to_guide.html#pytest_allure_how_to_guide","title":"Pytest Allure How to Guide","text":""},{"location":"build_system/all.pytest_allure.how_to_guide.html#how_to_run_the_flow_end-to-end_via_gh_actions","title":"How to run the flow end-to-end via GH actions","text":"<p>Considering that we run the tests on the <code>cmamp</code> repo with the fast <code>tests</code> group</p> <p>Important note: Unlike usual test run, we don't stop the execution on failure. For this we use the <code>continue-on-error: true</code> in the GitHub action step.</p> <p>Here is the link to the GitHub action file.</p>"},{"location":"build_system/all.pytest_allure.how_to_guide.html#how_to_generate_allure-pytest_results","title":"How to generate allure-pytest results","text":"<p>To save Allure results after a test run, append <code>--allure-dir</code> parameter to a <code>pytest</code> cmd, e.g.,</p> <pre><code>i run_fast_tests ... --allure-dir ./allure_results\n</code></pre> <p>where <code>allure-dir</code> is the directory where the Allure results will be stored.</p>"},{"location":"build_system/all.pytest_allure.how_to_guide.html#how_to_backup_the_allure_results","title":"How to backup the Allure results","text":"<p>To backup the Allure results, copy the <code>allure_results</code> directory to a AWS S3 bucket, e.g.,</p> <pre><code>aws s3 cp allure_results s3://cryptokaizen-unit-test/allure_test/cmamp/fast/results.20231120_102030 --recursive\n</code></pre> <p>where:</p> <ul> <li><code>allure_results</code> is the directory where the Allure results are stored</li> <li><code>20231120_102030</code> is the date and time with the mask <code>%Y%m%d_%H%M%S</code></li> <li><code>cmamp</code> is the name of the GitHub repo</li> <li><code>fast</code> is the name of the tests group</li> </ul>"},{"location":"build_system/all.pytest_allure.how_to_guide.html#how_to_generate_allure_html-report","title":"How to generate Allure HTML-report","text":"<ul> <li>Whenever Allure generates a test report in a specified directory i.e.     <code>allure-report</code> (refer to the     \"How it works\" section), it     concurrently produces data inside the history subdirectory</li> <li>The JSON files within the history subdirectory preserve all the necessary     information for Allure to use the data from this test report into the next     test report, see     History files</li> </ul> <p>To install the Allure CLI utility, refer to the official docs.</p> <p>In order to generate the HTML report, run the following command:</p> <pre><code>allure generate allure_results -o allure_report\n</code></pre> <p>where:</p> <ul> <li><code>allure_results</code> is the directory where the Allure results are stored</li> <li><code>allure_report</code> is the directory where the Allure HTML-report will be stored</li> </ul> <p>TODO(Vlad): Come up with a clean-up strategy for the S3 bucket.</p>"},{"location":"build_system/all.pytest_allure.how_to_guide.html#keep_the_history_of_test_runs","title":"Keep the history of test runs","text":"<ul> <li>To activate the features related to history, copy the history subdirectory     from the previous report into the latest test results directory before     generating the subsequent test report</li> <li>Here is an example of how to do it, assuming that your project is configured     to use <code>allure-results</code> and <code>allure-report</code> directories:<ul> <li>Make sure you have the previous report generated in the <code>allure-report</code>     directory</li> <li>Remove the <code>allure-results</code> directory</li> <li>Run the tests with the option <code>--allure-dir allure_results</code></li> <li>Copy the <code>allure-report/history</code> subdirectory to <code>allure-results/history</code></li> <li>Generate the new report</li> </ul> </li> </ul> <p>To copy the history subdirectory from the previous run to the <code>allure_results</code>:</p> <pre><code>aws s3 cp s3://cryptokaizen-html/allure_reports/cmamp/fast/report.20231120_102030/history allure_results/history --recursive\n</code></pre>"},{"location":"build_system/all.pytest_allure.how_to_guide.html#how_to_publish_the_allure-html_report","title":"How to publish the Allure-HTML report","text":"<p>To publish the Allure report, copy the <code>allure_report</code> directory to a AWS S3 bucket, e.g.,</p> <pre><code>aws s3 cp allure_report s3://cryptokaizen-html/allure_reports/cmamp/fast/report.20231120_102030 --recursive\n</code></pre> <p>where:</p> <ul> <li><code>allure_report</code> is the directory where the Allure HTML-report will be stored</li> <li><code>20231120_102030</code> is the date and time with the mask <code>%Y%m%d_%H%M%S</code></li> <li><code>cmamp</code> is the name of the GitHub repo</li> <li><code>fast</code> is the name of the tests group</li> </ul> <p>For e.g., to access the HTML-report open this link on a browser: http://172.30.2.44/allure_reports/cmamp/fast/report.20231120_102030</p>"},{"location":"build_system/all.semgrep_workflow.explanation.html","title":"All.semgrep workflow.explanation","text":""},{"location":"build_system/all.semgrep_workflow.explanation.html#semgrep_integration_in_github_actions","title":"Semgrep Integration in GitHub Actions","text":""},{"location":"build_system/all.semgrep_workflow.explanation.html#table_of_contents","title":"Table of Contents","text":""},{"location":"build_system/all.semgrep_workflow.explanation.html#overview","title":"Overview","text":"<p>This document provides an overview and guidance on the integration of Semgrep, a static analysis tool, into the GitHub Actions workflow. Semgrep is used for identifying issues and vulnerabilities in the codebase automatically during the development process.</p>"},{"location":"build_system/all.semgrep_workflow.explanation.html#features","title":"Features","text":"<ul> <li>Automatic Scanning: Semgrep runs automatically on every pull request to     the <code>master</code> branch and for every push to the <code>master</code> branch. This ensures     that new code is checked before merging.</li> <li>Scheduled Scans: Additionally, Semgrep scans are scheduled to run once a     day, ensuring regular codebase checks even without new commits.</li> <li>Workflow Dispatch: The integration allows for manual triggering of the     Semgrep scan, providing flexibility for ad-hoc code analysis.</li> </ul>"},{"location":"build_system/all.semgrep_workflow.explanation.html#setup","title":"Setup","text":"<ul> <li>GitHub Action Workflow: The Semgrep integration is set up as a part of the     GitHub Actions workflow in the <code>semgrep.yml</code>     file.</li> <li>Running Environment: The workflow runs on <code>ubuntu-latest</code> and uses the     <code>returntocorp/semgrep</code> container.</li> <li>Semgrep Rules: The current configuration uses the <code>p/secrets</code> rule pack     from Semgrep, focusing on detecting secrets and sensitive information     inadvertently committed to the repository.</li> </ul>"},{"location":"build_system/all.semgrep_workflow.explanation.html#exclusions","title":"Exclusions","text":""},{"location":"build_system/all.semgrep_workflow.explanation.html#semgrepignore_file","title":"<code>.semgrepignore</code> File","text":"<ul> <li>To optimize the scanning process, a <code>.semgrepignore</code> file     is placed in the root directory. This file functions similarly to     <code>.gitignore</code>, specifying files and paths that Semgrep should ignore during     scans. This is useful for excluding third-party libraries, or any other     non-relevant parts of the codebase from the scan.</li> </ul>"},{"location":"build_system/all.semgrep_workflow.explanation.html#ignoring_specific_lines","title":"Ignoring Specific Lines","text":"<ul> <li>To exclude a specific line of code from Semgrep analysis, append the comment     <code># nosemgrep</code> to the end of the line. This directive instructs Semgrep to     bypass that particular line, allowing developers to suppress false positives     or exclude non-relevant code snippets from analysis.</li> </ul>"},{"location":"build_system/all.semgrep_workflow.explanation.html#notifications","title":"Notifications","text":"<ul> <li>Failure Notifications: In case of scan failures on the <code>master</code> branch, a     Telegram notification is sent. This includes details such as the workflow     name, repository, branch, event type, and a link to the GitHub Actions run.</li> </ul>"},{"location":"build_system/all.semgrep_workflow.explanation.html#running_semgrep_locally","title":"Running Semgrep Locally","text":"<p>To run Semgrep locally for testing before pushing your changes:</p> <ol> <li>Install Semgrep on your local machine. Instructions can be found at    Semgrep Installation Guide.</li> <li>Run <code>semgrep --config=p/secrets</code> in your project directory to execute the    same rules as the CI/CD pipeline.</li> </ol>"},{"location":"build_system/all.semgrep_workflow.explanation.html#additional_resources","title":"Additional Resources","text":"<ul> <li>For more details on Semgrep rules and usage, visit the     Semgrep Official Documentation.</li> <li>To understand GitHub Actions configuration, refer to the     GitHub Actions Documentation.</li> </ul>"},{"location":"build_system/all.semgrep_workflow.explanation.html#conclusion","title":"Conclusion","text":"<p>The Semgrep integration into the GitHub Actions workflow provides an essential layer of code quality and security checks, aligning with the commitment to maintaining a robust and secure codebase.</p>"},{"location":"code_guidelines/all.code_design.how_to_guide.html","title":"All.code design.how to guide","text":""},{"location":"code_guidelines/all.code_design.how_to_guide.html#code_design","title":"Code Design","text":""},{"location":"code_guidelines/all.code_design.how_to_guide.html#design_philosophy","title":"Design Philosophy","text":""},{"location":"code_guidelines/all.code_design.how_to_guide.html#measure_seven_times_cut_once_russian_proverb","title":"Measure seven times, cut once (Russian proverb)","text":"<ul> <li> <p>Before doing any work, sit down and plan</p> <ul> <li>Describe somewhere in writing your high-level plan. Put it in a Google doc     to make it easier to collaborate and review.<ul> <li>What should the code do?</li> <li>What are the functionalities you want to implement?</li> <li>What are the functionalities you don't want to implement? (what are you     explicitly considering to be out-of-scope?)</li> <li>What is more important, what is less important? E.g., in terms of P0, P1,     P2</li> <li>What are the requirements/invariants?</li> <li>What are the semantics of the entities involved?</li> <li>What are the analyses, the comparisons, and the plots?</li> <li>What are the ideas (expressed without any code!)?</li> <li>ETA: Spend quality time thinking about it (e.g., 30 mins, 1 hr)</li> </ul> </li> <li>Review the plan<ul> <li>Look at the plan again with fresh eyes (e.g., go for a 5-min walk)</li> <li>Does the plan make sense?</li> <li>What can you remove?</li> <li>Can you make things simpler?</li> <li>What is not elegant?</li> <li>What entity is a special case of another?</li> <li>What is similar to what?</li> <li>ETA: Spend 30 mins thinking</li> </ul> </li> <li>Ask for someone to review the plan<ul> <li>Don't be ashamed of asking for advice</li> </ul> </li> <li>Implement a design<ul> <li>Transform the plan into high-level code, e.g.,<ul> <li>What are the objects / functions involved?</li> <li>What are the responsibilities of each class / function?</li> <li>What are the code invariants?</li> <li>What are the data structures?</li> </ul> </li> <li>Write the interfaces<ul> <li>Only the interfaces! Refrain from implementing the logic</li> <li>Comment the interfaces clearly</li> </ul> </li> <li>Think of how the objects / functions interact (who does what, what is the     data passed around)</li> <li>Sprinkle TODOs with ideas about potential problems, simpler approaches</li> <li>ETA: Spend 1/2 day, 1 day</li> </ul> </li> <li>Do a PR of the design</li> <li>Once we converge on the design:<ul> <li>Implement the functions</li> <li>Unit test</li> <li>PR</li> </ul> </li> </ul> </li> <li> <p>Remember:</p> <ul> <li>We want to do quick interactions: every day there is communication, update     and discussion</li> <li>Do not disappear for one week and come back with something that makes sense     only to you, or that you didn't get buy-in from others on</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#hacker_laws","title":"Hacker laws","text":"<ul> <li>A list of interesting \"laws\" (some are more rule of thumbs / heuristics)     related to computing:<ul> <li>hacker-laws</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#keep_it_simple","title":"Keep it simple","text":"<ul> <li>Follow the KISS principle.</li> <li>Pursue simple, elegant solutions. Some things are inherently complex, but even     complex systems can (and should) be broken down into simple pieces.</li> <li>Designs that are simple are easier to<ul> <li>Understand</li> <li>Modify</li> <li>Debug</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#tips_from_a_pro","title":"Tips from a pro","text":"<ul> <li>Adapted from     these slides     from a Stanford talk given by     Jeff Dean     (the Chuck Norris of SWE)</li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#designing_software_systems_is_tricky","title":"Designing software systems is tricky","text":"<ul> <li>Need to balance:<ul> <li>Simplicity [note that this comes first!]</li> <li>Scalability</li> <li>Performance</li> <li>Reliability</li> <li>Generality</li> <li>Features [note that this comes last!]</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#get_advice_early","title":"Get Advice Early!","text":"<ul> <li>Get advice<ul> <li>Before you write any code</li> <li>Before you write any lengthy design documents [notice the implicit     assumption that there is a design documented!]</li> </ul> </li> <li>Before writing a doc or code<ul> <li>Jot down some rough ideas (a few paragraphs)</li> <li>Chat about the design with colleagues</li> <li>Consider discussing multiple potential designs</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#interfaces","title":"Interfaces","text":"<ul> <li>Think carefully about interfaces in your system!<ul> <li>Imagine other hypothetical clients trying to use your interface</li> <li>Document precisely, but avoid constraining the implementation</li> <li>Get feedback on your interfaces before implementing!</li> <li>The best way to learn is to look at well-designed interfaces</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#architecture","title":"Architecture","text":""},{"location":"code_guidelines/all.code_design.how_to_guide.html#use_design_patterns","title":"Use design patterns","text":"<ul> <li>Design patterns are     idioms or recipes for solving problems that commonly appear in software     engineering across projects and even languages. The classical introduction to     design patterns is the so-called \"Gang of Four\"     book.</li> <li>A free Python-focused reference is available     here.</li> <li>Expanding your knowledge of design patterns is a worthwhile investment,     because design patterns<ul> <li>Capture elegant solutions that have been developed by many experienced     programmers over a long period of time</li> <li>Provide a framework and reference point for software architecture</li> <li>Are widely used and well-known and therefore quickly recognized by skilled     programmers</li> </ul> </li> <li>In other words, by using design patterns, you<ul> <li>Don't have to re-invent the wheel</li> <li>Simplify the high-level picture of your code</li> <li>Make it easier for other people to understand your code</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#functions","title":"Functions","text":""},{"location":"code_guidelines/all.code_design.how_to_guide.html#avoid_modifying_the_function_input","title":"Avoid modifying the function input","text":"<ul> <li> <p>If, for example, a function <code>f</code> accepts a dataframe <code>df</code> as its (sole)     argument, then, ideally, <code>f(df)</code> will not modify <code>df</code>. If modifications are     desired, then instead one can do:</p> <p><code>python def f(df): df = df.copy() ... return df</code></p> <p>in the function so that <code>f(df)</code> returns the desired new dataframe without modifying the dataframe that was passed in to the function.</p> </li> <li> <p>In some cases the memory costs associated with the copy are prohibitive, and     so modifying in-place is appropriate. If such is the case, state it explicitly     in the docstring.</p> </li> <li> <p>Functions that do not modify the input are especially convenient to have in     notebook settings. In particular, using them makes it easy to write blocks of     code in a notebook that will return the same results when re-executed out of     order.</p> </li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#prefer_pure_functions_by_default","title":"Prefer pure functions by default","text":"<ul> <li>Pure functions have two key     properties:<ul> <li>If the function arguments do not change, then the return value returned does     not change (in contrast to, e.g., functions that rely upon global state)</li> <li>Function evaluation does not have     side effects</li> </ul> </li> <li>Some nice properties enjoyed by pure functions are:<ul> <li>They are easy to understand and easy to test</li> <li>Using pure functions makes refactoring easier</li> <li>They allow chaining in an     elegant way</li> <li>They are often a natural choice for data manipulation and analysis</li> <li>They are convenient in notebooks</li> </ul> </li> <li>Though it is good to develop an appreciation for     functional programming,     and we like to adopt that style when appropriate, we recognize that it is not     pragmatic to dogmatically insist upon a functional style (especially in our     domain and when using Python).</li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#invariants","title":"Invariants","text":"<p>From ./oms/architecture.md</p> <p>Invariants and conventions</p> <ul> <li> <p>In this doc we use the new names for concepts and use \"aka\" to refer to the     old name, if needed</p> </li> <li> <p>We refer to:</p> </li> <li>The as-of-date for a query as <code>as_of_timestamp</code></li> <li>The actual time from <code>get_wall_clock_time()</code> as <code>wall_clock_timestamp</code></li> <li>Objects need to use <code>get_wall_clock_time()</code> to get the \"actual\" time</li> <li>We don't want to pass <code>wall_clock_timestamp</code> because this is dangerous</li> <li> <p>It is difficult to enforce that there is no future peeking when one object     tells another what time it is, since there is no way for the second object to     double check that the wall clock time is accurate</p> </li> <li> <p>We pass <code>wall_clock_timestamp</code> only when one \"action\" happens atomically but     it is split in multiple functions that need to all share this information.     This approach should be the exception to the rule of calling</p> </li> </ul> <p><code>get_wall_clock_time()</code></p> <ul> <li> <p>It's ok to ask for a view of the world as of <code>as_of_timestamp</code>, but then the     queried object needs to check that there is no future peeking by using     <code>get_wall_clock_time()</code></p> </li> <li> <p>Objects might need to get <code>event_loop</code></p> </li> <li> <p>TODO(gp): Clean it up so that we pass event loop all the times and event loop     has a reference to the global <code>get_wall_clock_time()</code></p> </li> <li> <p>The Optimizer only thinks in terms of dollar</p> </li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#our_approach_to_doing_things","title":"Our approach to doing things","text":""},{"location":"code_guidelines/all.code_design.how_to_guide.html#roles_and_responsibilities","title":"Roles and responsibilities","text":"<ul> <li> <p>How to communicate</p> <ul> <li>Slack for urgent stuff or interactive things (ideally on a small group chat)</li> <li>All the action happens in the GH issues<ul> <li>Update the Issues multiple times a day</li> </ul> </li> </ul> </li> <li> <p>RPs (responsible party), aka \"Tech leads\"</p> <ul> <li>Review / clean up the board once a week (before Mon meeting)</li> <li>Distribute / coordinate work in the teams</li> <li>Do first PR review</li> <li>Merge PRs (trivial or agreed upon)</li> <li>First line of defense for IT / dev issues<ul> <li>The best answer to any problem is a link to gdoc with the solution</li> </ul> </li> </ul> </li> <li> <p>Everybody to keep their Issues up to date in GH</p> <ul> <li>In the <code>In progress</code> and <code>Sprint Backlog</code></li> <li>Each issue has typically:<ul> <li>A single assignee</li> <li>A GH Project</li> <li>Maybe a label</li> </ul> </li> <li>Each Git branch should go with the corresponding Issue<ul> <li>E.g., <code>AmpTask2163_Implement_tiled_backtesting</code></li> <li><code>i git_issue_number</code>, <code>i git_create_branch -i XYZ</code></li> </ul> </li> <li>When you create a (draft) branch always create an associated PR<ul> <li>E.g., <code>i gh_create_pr</code></li> </ul> </li> <li>We like draft PRs to discuss architecture before unit testing</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_design.how_to_guide.html#good_practices","title":"Good practices","text":"<ul> <li> <p>Good Issue reports</p> <ul> <li>What are you trying to achieve</li> <li>What are you doing (cmd line, setup)</li> <li>What is the error</li> <li>Why do you think things should work differently?</li> </ul> </li> <li> <p>\"One PR a day keeps the doctor away\"</p> <ul> <li>\"... keeps investors at bay\"</li> <li>At least 1 PR a day</li> <li>Draft architectural PR when in doubt     </li> <li>Break work in smaller chunks (multiple PRs per bug)</li> </ul> </li> <li> <p>Go for a skateboard</p> <ul> <li>= something that works end-to-end     </li> </ul> </li> <li> <p>1 PR per day</p> </li> <li>Even if it's not complete (do a draft)</li> <li>Update the bugs (at least) every day</li> <li>Ask questions if you have any doubt</li> <li>Always sync</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html","title":"All.code like pragmatic programmer.how to guide","text":""},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#code_like_pragmatic_programmer","title":"Code Like Pragmatic Programmer","text":""},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#a_pragmatic_philosophy","title":"A pragmatic philosophy","text":""},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_1_care_about_your_craft","title":"PP_Tip 1: Care about your craft","text":"<ul> <li>Why spending your life developing software unless you care doing it well?</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_2_think_about_your_work","title":"PP_Tip 2: Think! About your work","text":"<ul> <li>Turn off the autopilot and take control</li> <li>Constantly critique and evaluate your work</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_3_provide_options_dont_make_lame_excuses","title":"PP_Tip 3: Provide options, don't make lame excuses","text":"<ul> <li> <p>Things go wrong:</p> <ul> <li>Deliverables are late</li> <li>Unforeseen technical problems come up</li> <li>Specs are misunderstood</li> <li>...</li> </ul> </li> <li> <p>Don't be afraid to admit ignorance or error</p> </li> <li>Before saying that something cannot be done, consider what the other people     are likely to say<ul> <li>\"Have you tried this?\"</li> <li>\"Did you consider that?\"</li> <li>...</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#broken_windows","title":"Broken windows","text":"<ul> <li>= bad designs, wrong decisions, poor code, rotten software</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_4_dont_live_with_broken_windows","title":"PP_Tip 4: Don't live with broken windows","text":"<ul> <li>A broken window left un-repaired instills a sense of abandonment</li> <li>Don't live with broken windows un-repaired<ul> <li>If there is no time, board it up</li> <li>Take action to show that you are on top of the situation</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_5_be_a_catalyst_for_change","title":"PP_Tip 5: Be a catalyst for change","text":"<ul> <li> <p>Sometimes you know what is right, but if you ask for permissions you will be     slowed down</p> <ul> <li>Request for feedback</li> <li>Committees, budgets, ...</li> <li>It's the so-called \"startup fatigue\"</li> </ul> </li> <li> <p>It's easier to ask forgiveness, than it is to get permission</p> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_6_remember_the_big_picture","title":"PP_Tip 6: Remember the big picture","text":"<ul> <li>Projects slowly and inexorably get totally out of hand</li> <li>Missing a deadline happens one day at a time</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_7_make_quality_a_requirement_issue","title":"PP_Tip 7: Make quality a requirement issue","text":"<ul> <li>One of the requirements from the user should be \"how good do you want the     software to be?\"</li> <li>Good software today is better than perfect software tomorrow</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_8_invest_regularly_in_your_knowledge_portfolio","title":"PP_Tip 8: Invest regularly in your knowledge portfolio","text":"<ul> <li> <p>Your knowledge and experience are your most important professional assets</p> <ul> <li>Unfortunately they are expiring assets</li> </ul> </li> <li> <p>It's like investing for retirement</p> <ol> <li>Invest as a habit</li> <li>Study every day</li> <li>Diversify</li> <li>The more different things you know, the more valuable you are</li> <li>Balance conservative and high-risk / high-reward investments</li> <li>Don't put all the technical eggs in one basket</li> <li>Review and rebalance periodically</li> </ol> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_9_critically_analyze_what_you_read_and_hear","title":"PP_Tip 9: Critically analyze what you read and hear","text":"<ul> <li>Beware of media hype</li> <li>Beware of zealots who insist that their dogma provides the only answer</li> <li>A best seller book is not necessarily a good book</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_10_its_both_what_you_say_and_the_way_you_say_it","title":"PP_Tip 10: It's both what you say and the way you say it","text":"<ul> <li>Plan what you want to say: write an outline</li> <li>Choose the right moment</li> <li>Choose a style; if in doubt, ask)</li> <li>Make it look good</li> <li>Involve your audience in early drafts</li> <li>Be a listener</li> <li>Get back to people</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_10_remember_the_wisdom_acrostic","title":"PP_Tip 10: Remember the WISDOM acrostic","text":"<ul> <li>What do you Want them to learn?</li> <li>What is their Interest in what you've got to say?</li> <li>How Sophisticated are they?</li> <li>How much Detail do they want?</li> <li>Whom do you want to Own the information?</li> <li>How can you Motivate them to listen?</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#a_pragmatic_approach","title":"A pragmatic approach","text":""},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_11_dry_-_dont_repeat_yourself","title":"PP_Tip 11: DRY - Don't repeat yourself","text":"<ul> <li> <p>Every piece of information must have a:</p> <ul> <li>Single</li> <li>Unambiguous</li> <li>Authoritative representation within a system</li> </ul> </li> <li> <p>Information is duplicated in multiple places -&gt; maintenance nightmare</p> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_11_programs_knowledge","title":"PP_Tip 11: Programs = knowledge","text":"<ul> <li>Programs are made of knowledge (e.g., requirements, specifications, code,     internal and external documentation)</li> <li>Knowledge is unstable<ul> <li>Over time one gets better understanding of requirements</li> <li>Requirements change</li> <li>The solution to a problem changes over time<ul> <li>E.g., tests show that an algorithm is not general or does not work</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_11_how_does_duplication_arise","title":"PP_Tip 11: How does duplication arise?","text":"<ul> <li> <p>There are 4 I's of duplication:</p> </li> <li> <p>Imposed</p> </li> <li>The environment seems to require duplication</li> <li>Inadvertent</li> <li>A developer does not realize that he/she is duplicating information</li> <li>Impatient</li> <li>A developer gets lazy and duplicates code because it is easy</li> <li>Inter-developer</li> <li>Multiple developers duplicate a piece of info</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_11_imposed_duplication","title":"PP_Tip 11: Imposed duplication","text":"<ul> <li> <p>Multiple representations of the same piece of info</p> <ul> <li>E.g., same info in two pieces of code written in different languages</li> <li>Solution: write a filter or code generator</li> </ul> </li> <li> <p>Internal documentation and code</p> <ul> <li>Solution: comments should not duplicate info present in the code</li> <li>Comments should be for why, code for how</li> </ul> </li> <li> <p>External documentation and code</p> <ul> <li>Solution: be careful, generate it automatically</li> </ul> </li> <li> <p>Language issues</p> <ul> <li>Some languages require duplication (e.g., C/C++ header vs cpp)</li> <li>Solution: at least warnings and errors are reported</li> <li>Do not duplicate comments in headers and code</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_11_inadvertent_duplication","title":"PP_Tip 11: Inadvertent duplication","text":"<ul> <li> <p>Un-normalized data structures</p> <ul> <li>E.g.,</li> <li>A <code>Truck</code> has a type, maker, a license plate, and a driver</li> <li>A <code>DeliveryRoute</code> has a route, a truck, and a driver</li> <li>Now if the driver of a truck is changed, this info needs to be changed in     two objects</li> <li>Solution: use a 3rd object to bind <code>DeliveryRoute</code>, <code>Truck</code>, <code>Driver</code></li> </ul> </li> <li> <p>Data elements that are mutually dependent for perf reason</p> <ul> <li>E.g., a line has <code>startPoint</code> and <code>endPoint</code>, and a field <code>length</code> is     redundant</li> <li>Solution: use accessors to read/write object attributes</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_11_impatient_duplication","title":"PP_Tip 11: Impatient duplication","text":"<ul> <li>The temptation is always to cut-and-paste code and then modify it<ul> <li>It increases the technical debt: shortcuts end up in long delays</li> <li>Solution: re-factor and then change it</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_11_inter-developer_duplication","title":"PP_Tip 11: Inter-developer duplication","text":"<ul> <li> <p>It is hard to detect</p> </li> <li> <p>Solutions:</p> <ul> <li>Encourage communication</li> <li>Do code reviews</li> <li>Project libraries: central place for utility and scripts (so one knows where     to look before writing a routine)</li> <li>Project librarian to supervise the reuse</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_12_make_it_easy_to_reuse","title":"PP_Tip 12: Make it easy to reuse","text":"<ul> <li>If something is not easy to find, use, reuse, people won't reuse</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#orthogonality","title":"Orthogonality","text":"<ul> <li>= independence, decoupling</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_13_eliminate_effects_between_unrelated_things","title":"PP_Tip 13: Eliminate effects between unrelated things","text":"<ul> <li>Systems are orthogonal when changes in one sub-system don't affect other     sub-systems<ul> <li>E.g., helicopter controls are not orthogonal</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_13_orthogonality_pros","title":"PP_Tip 13: Orthogonality: pros","text":"<ol> <li>Easier to change</li> <li>Changes are localized</li> <li>Changes are easier to be unit tested</li> <li>Gain productivity</li> <li>Less overlap means more useful function per unit of code</li> <li>Promote reuse</li> <li>Reduce risk</li> <li>Easier to change design</li> <li>Not being tied to a particular vendor / product / platform</li> </ol>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_13_how_to_maintain_orthogonality","title":"PP_Tip 13: How to maintain orthogonality","text":"<ol> <li>Avoid global state (e.g., singletons)</li> <li>Write shy code</li> <li>Modules / objects that don't reveal anything unnecessary</li> <li>Pass required context to modules, objects, functions</li> <li>Avoid coupling (e.g., law of Demeter)</li> <li>Always look for opportunities to refactor</li> <li>Reorganize</li> <li>Improve structure</li> <li>Increase orthogonality</li> </ol>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_14_there_are_no_final_decisions","title":"PP_Tip 14: There are no final decisions","text":"<ul> <li>Do not carve decisions into stone</li> <li>Think of them as being written on the sand</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_14_there_are_no_final_decisions_why","title":"PP_Tip 14: There are no final decisions: why","text":"<ol> <li>Requirements can change on us</li> <li>The first decision is not always the best one</li> <li>Change 3rd party components</li> </ol>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_15_use_tracer_bullets_to_find_the_target","title":"PP_Tip 15: Use tracer bullets to find the target","text":"<ul> <li>Ready, fire, aim!</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#analogy_between_shooting_bullets_in_the_dark_and_software_engineering","title":"Analogy between shooting bullets in the dark and software engineering","text":"<ul> <li> <p>How to shoot a bullet to a target in the dark?</p> <ol> <li>Open loop approach</li> <li>Compute all the interesting quantities (wind, angles, distance)</li> <li>Hope that the conditions don't change at all</li> <li>Closed loop approach</li> <li>Use tracer bullets</li> <li>They leave trace in the dark so one can see if they are hitting the    target</li> <li>If not, adjust the aim</li> <li>If they hit the target, other bullets will also hit the target</li> </ol> </li> <li> <p>Software engineering:</p> <ol> <li>Open loop approach</li> <li>Specify the system to death</li> <li>Closed loop approach</li> <li>Achieve end-to-end connection among components with minimal functionality</li> <li>Then adjust, re-aim, ... until you are on target</li> </ol> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_15_tracer_bullets_pros","title":"PP_Tip 15: Tracer bullets: pros","text":"<ul> <li>Users see something working early</li> <li>You have an integration platform, instead of big-bang integration</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_15_tracer_bullets_cons","title":"PP_Tip 15: Tracer bullets: cons","text":"<ul> <li>Tracer bullets don't always hit their target</li> <li>Still imagine the result using a waterfall approach</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_16_prototype_to_learn","title":"PP_Tip 16: Prototype to learn","text":"<ul> <li>The value of prototype lies not in the code produced, but in the lessons     learned</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_16_prototype_to_learn_cons","title":"PP_Tip 16: Prototype to learn: cons","text":"<ul> <li>Set the right expectations</li> <li>Make sure everyone understands that you are writing disposable code</li> <li>Otherwise management might insist on deploying the prototype or a cleaned up     version of it</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_16_tracer_code_vs_prototyping","title":"PP_Tip 16: Tracer code vs prototyping","text":"<ul> <li> <p>Prototype:</p> <ul> <li>When experimentation is done, the prototype is thrown away and it is     reimplemented using the lessons learned</li> <li>E.g., write prototype in Python and then code the production system in C++</li> </ul> </li> <li> <p>Tracer code:</p> <ul> <li>Build architectural template</li> <li>Components have minimal implementation</li> <li>Over time stubbed routines are completed</li> <li>The framework stays intact</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_17_program_close_to_the_problem_domain","title":"PP_Tip 17: Program close to the problem domain","text":"<ul> <li>Always try to write code using the vocabulary of the application domain</li> <li> <p>So you are free to concentrate on solving domain problems and ignore petty     implementation details</p> </li> <li> <p>Use mini-languages</p> <ul> <li>Other people can implement business logic</li> </ul> </li> <li>Code can issue domain specific errors</li> <li>Create metadata compiled or read-in by the main application</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_18_estimate_to_avoid_surprises","title":"PP_Tip 18: Estimate to avoid surprises","text":"<ul> <li> <p>All answers are estimates: some are more accurate than others</p> </li> <li> <p>Do we need a high accuracy or a ballpark figure?</p> <ul> <li>The unit of measurement of the estimate conveys a message about accuracy</li> <li>E.g., 130 working days vs 6 months</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_18_how_to_make_estimates","title":"PP_Tip 18: How to make estimates","text":"<ol> <li>Ask people that have done a similar project before</li> <li>Specify what's the scope of an estimate: \"under assumptions X and Y, the    estimate is Z\"</li> <li>Break the system in pieces, understand which parameters they take, assign a    value to each parameter</li> <li>Understand which are the critical parameters and try to estimate them    properly</li> <li>When estimates are wrong, understand why</li> </ol>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_19_iterate_the_schedule_with_the_code","title":"PP_Tip 19: Iterate the schedule with the code","text":"<ul> <li> <p>Management often wants a single estimate for the schedule before the project     starts</p> </li> <li> <p>In reality:</p> <ul> <li>Team</li> <li>Productivity</li> <li>Environment will determine the schedule</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_19_estimates_at_the_coffee_machine","title":"PP_Tip 19: Estimates at the coffee machine","text":"<ul> <li>Estimates given at the coffee machine will come back to haunt you</li> <li>When asked for an estimate, answer \"I'll get back to you\"</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#the_basic_tools","title":"The basic tools","text":""},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_20_keep_knowledge_in_plain_text","title":"PP_Tip 20: Keep knowledge in plain text","text":"<ul> <li> <p>The base material for a programmer is knowledge</p> </li> <li> <p>Knowledge is made of:</p> <ul> <li>Requirements</li> <li>Specifications</li> <li>Documentation</li> <li>Design</li> <li>Implementation</li> <li>Test</li> </ul> </li> <li> <p>The best format to store knowledge is plain text</p> </li> <li>Plain text can be manipulated manually and programmatically using virtually     any tool (e.g., source code control, editors, stand-alone filters)</li> <li>Human-readable self-describing data will outlive<ul> <li>All other forms of data; and</li> <li>The application that created it</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_21_use_the_power_of_command_shells","title":"PP_Tip 21: Use the power of command shells","text":"<ul> <li> <p>GUIs</p> <ul> <li>Are great</li> <li>Allow to do only what they were designed for</li> </ul> </li> <li> <p>In life one does not know what exactly will be needed</p> </li> <li>Shells allow to automate and combine tools in ways that one didn't intended or     planned for</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_22_use_a_single_editor_well","title":"PP_Tip 22: Use a single editor well","text":"<ul> <li>Better to know one editor very well than knowing many editors superficially</li> <li>Use the same editor for all editing tasks</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_23_always_use_source_code_control","title":"PP_Tip 23: Always use source code control","text":"<ul> <li>Source control is like an undo key, a time machine</li> <li>Use it always: even if you are a single person team, even if you are working     on a throw-away prototype</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_23_advantages_of_source_code_control","title":"PP_Tip 23: Advantages of source code control","text":"<ul> <li>It allows to answer questions like:<ul> <li>Who made changes to this line of code?</li> <li>What are the difference between this release and the previous one?</li> </ul> </li> <li>It allows to work on a new release, while fixing bugs in the previous (i.e.,     branches)</li> <li>It can connect to automatic repeatable builds and regressions</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_24_fix_the_problem_not_the_blame","title":"PP_Tip 24: Fix the problem, not the blame","text":"<ul> <li>No one writes perfect software so debugging will take up a major portion of     your day</li> <li>Attack debugging as a puzzle to be solved</li> <li>Avoid denial, finger pointing, lame excuses</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_25_dont_panic","title":"PP_Tip 25: Don't panic","text":"<ul> <li>Before you start debugging, adopt the right mindset:<ul> <li>Turn off defenses that protect your ego</li> <li>Tune out project pressure</li> <li>Get comfortable</li> </ul> </li> <li>Don't panic while you debug, even if you have your nervous boss or your client     breathing on your neck</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_25_how_to_debug","title":"PP_Tip 25: How to debug","text":"<ul> <li> <p>Don't waste time thinking \"this cannot happen\"</p> <ul> <li>Obviously it is happening</li> </ul> </li> <li> <p>Make the bug reproducible with a single command</p> <ul> <li>So you know you are fixing the right problem</li> <li>So you know when it is fixed</li> <li>You can easily add a unit test for that bug</li> </ul> </li> <li> <p>What could be causing the symptoms?</p> </li> <li>Check for warnings from the compiler<ul> <li>Always set all the compiler warnings so the compiler can find issues for you</li> </ul> </li> <li>Step through the code with a debugger</li> <li>Add tracing statements</li> <li>Corrupt variables?<ul> <li>Check their neighborhood variables, use <code>valgrind</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_25_rubber_ducking","title":"PP_Tip 25: Rubber ducking","text":"<ul> <li>= explain the issue step-by-step to someone else, even to a yellow rubber duck</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_25_why_rubber_ducking_works","title":"PP_Tip 25: Why rubber ducking works?","text":"<ul> <li>The simple act of explaining the issue often uncovers the problem</li> <li>You state things that you may take for granted</li> <li>Verbalizing your assumptions lets you gain new insights into the problem</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_26_select_is_not_broken","title":"PP_Tip 26: <code>select</code> is not broken","text":"<ul> <li>Do not assume that the library is broken</li> <li>Assume that you are calling the library in the wrong way</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_27_dont_assume_it_prove_it","title":"PP_Tip 27: Don't assume it: prove it","text":"<ul> <li>Don't assume that a piece of code works in any condition<ul> <li>Avoid statements like \"that piece of code has been used for years: it cannot     be wrong!\"</li> </ul> </li> <li> <p>Prove that the code works:</p> <ul> <li>In this context</li> <li>With this data</li> <li>With these boundary conditions</li> </ul> </li> <li> <p>After you find a surprising error, are there any other places in the code that     may be susceptible to the same bug?</p> </li> <li>Make sure that whatever happened, never happens again</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_28_learn_a_text_manipulation_language","title":"PP_Tip 28: Learn a text manipulation language","text":"<ul> <li>Spending 30 mins trying out a crazy idea is better than spending 5 hours</li> <li>With scripting languages (e.g., Python, perl) you can quickly prototype ideas     instead of using a production language (e.g., C, C++)</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_29_write_code_that_writes_code","title":"PP_Tip 29: Write code that writes code","text":"<ul> <li>Passive code generators</li> <li>They are run once and the origin of the code is forgotten<ul> <li>E.g., parameterized templates, boilerplate</li> </ul> </li> <li> <p>They can be 90\\% accurate and the rest is done by hand</p> </li> <li> <p>Active code generators</p> </li> <li>They convert a single representation of knowledge into all the forms that are     needed</li> <li>This is not duplication, it is the DRY principle in action</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pragmatic_paranoia","title":"Pragmatic paranoia","text":""},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_30_you_cannot_write_perfect_software","title":"PP_Tip 30: You cannot write perfect software","text":"<ul> <li>Accept it and celebrate it<ul> <li>Unless you accept it, you'll end up wasting time chasing an impossible dream</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#know_when_to_stop","title":"Know when to stop","text":"<ul> <li>A painter needs to know when to stop adding layers of paint</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#dont_trust_others","title":"Don't trust others","text":"<ul> <li>Code defensively</li> <li>Anticipate the unexpected</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#dont_trust_yourself","title":"Don't trust yourself","text":"<ul> <li>Code defensively against your own mistakes</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#preconditions","title":"Preconditions","text":"<ul> <li>= what must be true in order for the routine to be called</li> <li>It is caller's responsibility to pass good data</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#postconditions","title":"Postconditions","text":"<ul> <li>= what the routine is guaranteed to do</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#class_invariants","title":"Class invariants","text":"<ul> <li>= a class ensures that some conditions are always true<ul> <li>E.g., between invocations to public methods</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#contracts","title":"Contracts","text":"<ul> <li>If all routines' preconditions are met<ul> <li>=&gt; the routine guarantees that all postconditions and invariants will be     true when it completes</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_31_design_with_contracts","title":"PP_Tip 31: Design with contracts","text":"<ul> <li>Some languages support design by contract:<ul> <li>In the compiler (e.g., static assertion, type system)</li> <li>In the runtime systems (e.g., assertions)</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_32_crash_early","title":"PP_Tip 32: Crash early","text":"<ul> <li>Better to crash than to thrash (=corrupting the state of the system)</li> <li>When something unexpected happens throw a runtime exception<ul> <li>The exception, if not caught, will percolate up to the top level halting the     program</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_33_if_it_cannot_happen_use_assertions_to_ensure_that_it_wont","title":"PP_Tip 33: If it cannot happen, use assertions to ensure that it won't","text":"<ul> <li>Assertions check for things that should never happen<ul> <li>E.g., at the end of a sorting routine the data is not sorted</li> </ul> </li> <li>Don't use assertions in place of real error handling</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#leave_assertions_enabled","title":"Leave assertions enabled","text":"<ul> <li>Assertions should be left on even after the system is tested and shipped</li> <li>The assumption that testing will find all the bugs is wrong<ul> <li>Testing tests a miniscule percentage of possible real-world conditions</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_34_use_exceptions_for_exceptional_problems","title":"PP_Tip 34: Use exceptions for exceptional problems","text":"<ul> <li>Interleaving normal control flow code and error handling code leads to ugly     code<ul> <li>With exceptions one can split the code neatly into two parts</li> <li>Exceptions are like <code>goto</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#exceptions_are_for_unexpected_events","title":"Exceptions are for unexpected events","text":"<ul> <li>Use exceptions only for truly unexpected events</li> <li>The code should still run in normal conditions if one removes all the     exception handlers</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_35_finish_what_you_start","title":"PP_Tip 35: Finish what you start","text":"<ul> <li>Resources (e.g., memory, DB transactions, threads, files, timers) follow a     pattern:<ul> <li>Allocate</li> <li>Use</li> <li>Deallocate</li> </ul> </li> <li>Routine or objects that allocate resources are responsible for deallocating     them</li> <li>To avoid deadlocks always deallocate resources in the opposite order to that     in which you allocate them</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#bend_or_break","title":"Bend or break","text":""},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_36_minimize_coupling_between_modules","title":"PP_Tip 36: Minimize coupling between modules","text":"<ul> <li>We want to limit the interaction between modules<ul> <li>If one modules has to be replaced / is broken, the other modules can carry     on</li> </ul> </li> <li>Traversing relationships between objects can quickly lead to a combinatorial     explosion of dependencies</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_36_law_of_demeter_for_functions","title":"PP_Tip 36: Law of Demeter for functions","text":"<ul> <li> <p>Any method <code>O.m(A, B, C)</code> of an object <code>O</code> should call only methods belonging     to:</p> <ul> <li>Object <code>O</code> itself</li> <li>Objects through parameters passed to the method (e.g., <code>A</code>)</li> <li>Objects instantiated within <code>m</code></li> </ul> </li> <li> <p>A rule of thumb in OOP is to use a single dot, e.g., <code>a.m()</code> and avoid     multiple dots, e.g., <code>a.b.m()</code></p> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_36_intuition_of_the_law_of_demeter","title":"PP_Tip 36: Intuition of the Law of Demeter","text":"<ul> <li>Object <code>A</code> can call a method of <code>B</code></li> <li><code>A</code> cannot reach through <code>B</code> to access an object <code>C</code><ul> <li>Otherwise <code>A</code> knows about the internal structure of <code>B</code></li> </ul> </li> <li><code>B</code> needs to be changed to expose the interface of <code>C</code><ul> <li>Cons: lots of wrapper methods to forward requests to delegates</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_36_law_of_demeter_as_general_contractor","title":"PP_Tip 36: Law of Demeter as general contractor","text":"<ul> <li> <p>It's like using a general contractor</p> </li> <li> <p>Pros</p> <ul> <li>You ask a job to be done, but you don't deal with subcontractors directly</li> </ul> </li> <li>Cons<ul> <li>The client needs to go through the general contractor all the times</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_37_configure_dont_integrate","title":"PP_Tip 37: Configure, don't integrate","text":"<ul> <li> <p>Every time we change the code to accommodate a change in business logic we     risk to break the system or to introduce a new bug</p> </li> <li> <p>Make the system highly configurable</p> </li> <li>Always use metadata, i.e., data about data (e.g., choice of algorithm,     database, middleware, user-interface style, ...)</li> <li>Use <code>.ini</code> files</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_38_put_abstractions_in_code_details_in_metadata","title":"PP_Tip 38: Put abstractions in code, details in metadata","text":"<ul> <li>The goal is to think declaratively: specify what to do, not how to do it</li> <li>We want to configure and drive the application via metadata as much as     possible</li> <li>So we program for the general case and put the specifics outside the compiled     code</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#mechanisms_vs_policies","title":"Mechanisms vs policies","text":"<ul> <li>Mechanisms = primitives, what can be done</li> <li>Policies = how to put together primitives</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_38_advantages_of_splitting_mechanisms_and_policies","title":"PP_Tip 38: Advantages of splitting mechanisms and policies","text":"<ol> <li>Decouple components in the design, resulting in more flexible and adaptable    programs</li> <li>Customize the application without recompiling it</li> <li>Metadata can be expressed in a form closer to problem domain</li> <li>Anybody can change the behavior without understanding the code</li> </ol>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_38_put_business_logic_in_metadata","title":"PP_Tip 38: Put business logic in metadata","text":"<ul> <li>Business logic and rules are the parts that are most likely to change</li> <li>So we want to maintain them in a flexible format, e.g., metadata</li> <li>Metadata should be encoded in plain text</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_39_analyze_workflow_to_improve_concurrency","title":"PP_Tip 39: Analyze workflow to improve concurrency","text":"<ul> <li>Avoid temporal coupling</li> <li>Use activity diagrams to identify activities that could be performed in     parallel</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_40_temporal_coupling","title":"PP_Tip 40: Temporal coupling","text":"<ul> <li>Time is often ignored when designing a software architecture</li> <li>We tend to think in a linear sequential fashion: \"do this, then do that\"</li> <li> <p>This leads to temporal coupling, i.e., coupling in time</p> </li> <li> <p>When designing an architecture, we need to think about:</p> <ul> <li>Concurrency = things happening at the same time</li> <li>Ordering = <code>A</code> must occur before <code>B</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_40_uml_activity_diagram","title":"PP_Tip 40: UML activity diagram","text":"<ul> <li>Actions are represented by rounded boxes</li> <li>Arrows between actions mean \"temporal ordering\"</li> <li>Actions with no incoming arrows can be started at any time</li> <li>Synchronization points are represented by a thick bar:<ul> <li>Once all the actions leading to a barrier are done, one can proceed with the     arrows leaving the synchronization point</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#service","title":"Service","text":"<ul> <li>= independent, concurrent objects behind well-defined, consistent interfaces</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_40_design_using_services","title":"PP_Tip 40: Design using services","text":"<ul> <li>Using services allows to avoid temporal coupling</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_41_hungry_consumer_model","title":"PP_Tip 41: Hungry consumer model","text":"<ul> <li>There are multiple independent consumers and a centralized work queue</li> <li>Each consumer grabs a task from the work queue and processes it</li> <li>Load balancing: if a consumer task gets bogged down or it is slower, the     others can pick up the slack</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_41_example_of_3-tier_architecture_with_hungry_consumer_model","title":"PP_Tip 41: Example of 3-tier architecture with hungry consumer model","text":"<ul> <li>Goal</li> <li> <p>Read a request from multiple lines and process the transactions against the     database</p> </li> <li> <p>Constraints</p> </li> <li>DB operations are slow</li> <li>We need to keep accepting service requests even when waiting on DB</li> <li> <p>DB performance suffers with too many concurrent sessions</p> </li> <li> <p>Solution</p> </li> <li>3 tier, multi-processing distributed application</li> <li>Tier 1: input tasks monitor the input lines and add requests to application     queue<ul> <li>Application queue</li> </ul> </li> <li>Tier 2: read application queue, apply business logic, add transaction to DB     queue<ul> <li>DB queue</li> </ul> </li> <li> <p>Tier 3: read transactions from DB queue and apply it to DB</p> </li> <li> <p>Each component is an independent entity, running concurrently (on the same     machine or on multiple machines)</p> </li> <li>Actions are asynchronous: as soon as a request is handled by a process and put     on next queue, the process goes back to monitor inputs</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_41_always_design_for_concurrency","title":"PP_Tip 41: Always design for concurrency","text":"<ul> <li>Programming with threads imposes some design constraints</li> <li>Concurrency forces you to think things more carefully</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#examples_of_problems_with_concurrency","title":"Examples of problems with concurrency","text":"<ul> <li>Global or static variables must be protected from concurrent access<ul> <li>Do you really need a global variable?</li> </ul> </li> <li>Objects must always be in a valid state when called<ul> <li>Classes with separate constructor and initialization routines are     problematic</li> </ul> </li> <li>Interfaces should not keep state<ul> <li>Make services stateless</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#event","title":"Event","text":"<ul> <li>= special message that says \"something interesting just happened\"</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_42_objects_communicating_through_events","title":"PP_Tip 42: Objects communicating through events","text":"<ul> <li> <p>We know that we need to separate a program in modules / classes</p> <ul> <li>A module / class has a single, well-defined responsibility</li> </ul> </li> <li> <p>At run-time how do objects talk to each other?</p> </li> <li>We don't want objects to know too much about each other<ul> <li>E.g., how does object <code>A</code> know which objects to talk to?</li> </ul> </li> <li>By using events one can minimize coupling between objects: objects are     interested in events and not in other objects</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_42_all_events_through_a_single_routine_approach","title":"PP_Tip 42: All events through a single routine approach","text":"<ul> <li>One approach is to send all events to a single routine that dispatches them to     the objects</li> <li>This is bad!<ul> <li>A single routing needs to know all interactions among objects</li> <li>It's like a huge case statement</li> <li>It violates encapsulation, increases coupling</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_42_publish_subscribe_model","title":"PP_Tip 42: Publish / subscribe model","text":"<ul> <li>Objects should only receive events they want (do no spam objects!)<ul> <li>Subscribers register themselves with publisher objects for interesting     events</li> <li>Publisher calls all subscribers when a corresponding event occurs</li> </ul> </li> <li>Organization can be:<ul> <li>Peer-to-peer</li> <li>Software bus (a centralized object maintains DB of listeners and dispatches     messages)</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_42_sequence_diagram","title":"PP_Tip 42: Sequence diagram","text":"<ul> <li>It shows the flow of messages among several objects</li> <li>Objects are arranged in columns</li> <li>Each message is an arrow from sender's column to receiver's column</li> <li>Timing relationship between messages is captured by vertical ordering</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_42_push_pull_model_for_event_services","title":"PP_Tip 42: Push / pull model for event services","text":"<ul> <li>Push mode</li> <li>Event producers inform the event channel that event has occurred</li> <li> <p>Event channel distributes event to all client objects registered for that     event</p> </li> <li> <p>Pull mode</p> </li> <li>Event consumers poll the event channel periodically</li> <li>Event channel polls possible suppliers and report interesting events</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_42_separate_views_from_models","title":"PP_Tip 42: Separate views from models","text":"<ul> <li> <p>Model-View-Controller design pattern</p> </li> <li> <p>Model:</p> <ul> <li>It stores the data</li> <li>It has methods to change its state, to report changes (through events), and     to report data</li> <li>It is like a DB</li> </ul> </li> <li>View:<ul> <li>A way to interpret the Model</li> <li>It subscribes to changes in the Model</li> <li>It subscribes to events from the Controller to change representation</li> </ul> </li> <li> <p>Controller:</p> <ul> <li>It controls the View</li> <li>It provides new data to the Model</li> </ul> </li> <li> <p>One can even have a network of MVCs: e.g., a View for one Model can be the     Model for another View</p> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_42_examples_of_mvc","title":"PP_Tip 42: Examples of MVC","text":"<ul> <li>Almost any GUI</li> <li>Tree widget = a clickable, traversable tree</li> <li>Spreadsheet with a graph attached to represent data as bar charts, running     totals, ...; controllers allow to zoom in / out, enter new data, ...</li> <li>System reporting interesting information about a baseball game<ul> <li>Model is a DB with all possible information</li> <li>New data from the Controller: a pitcher is changed, a player strikes out, it     starts raining</li> <li>A View reports the score</li> <li>A View reports info about current batter</li> <li>A View looks for new world records</li> <li>A View posts information on the web</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_43_use_blackboards_to_coordinate_workflow","title":"PP_Tip 43: Use blackboards to coordinate workflow","text":"<ul> <li>A blackboard system lets decouple objects from each other completely</li> <li>There is even less coupling that publish / subscribe model</li> <li>Consumers and producers exchange data anonymously and asynchronously</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_43_example_of_blackboard_implementation","title":"PP_Tip 43: Example of blackboard implementation","text":"<ul> <li> <p>Blackboard is like a DB providing atomic and distributed storage of objects</p> </li> <li> <p>One can store any object, not just data</p> </li> <li>Objects can be retrieved by partial matching fields (templates and wildcards)     or by subtypes</li> <li>Operations can be:<ul> <li>Read = search and retrieve item from blackboard</li> <li>Write = put an item on the blackboard</li> <li>Take = read + remove from blackboard</li> <li>Notify = set up notification when an object matching a template is written</li> </ul> </li> <li>Advantage is a single and consistent interface to blackboard, instead of     different APIs for every transaction / interaction in the system</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_43_example_of_blackboard_application","title":"PP_Tip 43: Example of blackboard application","text":"<ul> <li> <p>Program that accepts and process loan applications</p> </li> <li> <p>Constrains</p> </li> <li>The laws are very complex</li> <li>There is no guarantee on the order in which data arrives</li> <li>Data is gathered by different people</li> <li>Certain data is dependent on other data</li> <li>Arrival of new data may raise request for more data and policies</li> <li> <p>As regulation change, the workflow must be re-organized</p> </li> <li> <p>Solutions</p> </li> <li>Encoding all the workflow is a nightmare</li> <li>A better solution is a blackboard + rules engine</li> <li>Any time new data arrives, a new rule is triggered</li> <li>Rules can output more data on the blackboard, triggering more rules and so on</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#while_you_are_coding","title":"While you are coding","text":""},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_44_dont_program_by_coincidence","title":"PP_Tip 44: Don't program by coincidence","text":"<ul> <li>Do not program by coincidence (= relying on luck and accidental successes)</li> <li>Program deliberately</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_44_how_to_program_by_coincidence","title":"PP_Tip 44: How to program by coincidence","text":"<ul> <li>You type some code, try it, and it seems to work</li> <li>Type more code and still works</li> <li>After few weeks the program stops to work</li> <li>You don't know why the code is failing, because you didn't know why it worked     in the first place</li> <li>The code seemed to work, given the (limited) testing did, but it was just a     coincidence</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_44_why_program_by_coincidence_seems_to_work","title":"PP_Tip 44: Why program by coincidence seems to work?","text":"<ul> <li>One ends up relying on undocumented boundary conditions and when the code is     fixed / changed, our code breaks</li> <li>One tries until something works, then it does not wonder why it works: \"it     works now, better leave it alone\"</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_44_how_to_program_deliberately","title":"PP_Tip 44: How to program deliberately","text":"<ul> <li>Rely only on reliable things</li> <li>Document your assumptions (e.g., design by contract)</li> <li>Add assertions to test assumptions</li> <li>Don't just test your code, test your assumptions as well (e.g., with small     unit tests)</li> <li>Don't let existing code (even your own code) dictate future code</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_45_estimate_the_order_of_your_algorithms","title":"PP_Tip 45: Estimate the order of your algorithms","text":"<ul> <li>We always want to estimate the resources (e.g., time, memory) required by     algorithms</li> <li>E.g., \"Would the algorithm scale up from 1k records to 1M records?\"</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#big-oh_notation","title":"Big-Oh notation","text":"<ul> <li>Big-Oh notation represents the worst-case time taken by an algorithm<ul> <li>Simple loops: $O(n)$</li> <li>Nested loops: $O(n^k)$</li> <li>Binary chop (e.g., binary search): $O(\\log(n))$</li> <li>Divide and conquer (split, recurse, combine): $O(n \\log(n))$</li> <li>Combinatoric: $O(2^n)$</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#estimating_big-oh","title":"Estimating Big-Oh","text":"<ul> <li>If you are not sure about Big Oh, vary input record size and plot the resource     needed (e.g., time, memory) against the input size</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#big-oh_in_the_real_world","title":"Big-Oh in the real world","text":"<ul> <li>It is possible that a $O(n^2)$ algorithm is faster than a $O(n \\log(n))$ for     small inputs</li> <li>Even if runtime looks linear, the machine might start trashing for lack of     memory and thus not scale linearly in the real world</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_46_test_your_estimate","title":"PP_Tip 46: Test your estimate","text":"<ul> <li>It's tricky to get accurate execution times</li> <li>Use code profilers to count the number of times different steps of your     algorithm get executed and plot against the input size</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#be_wary_of_premature_optimization","title":"Be wary of premature optimization","text":"<ul> <li>Make sure an algorithm is really the bottleneck before investing precious time     trying to improve it</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#refactoring","title":"Refactoring","text":"<ul> <li>= re-writing, re-working, re-architecting code</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_47_refactor_early_refactor_often","title":"PP_Tip 47: Refactor early, refactor often","text":"<ul> <li>If you cannot refactor immediately<ul> <li>Make space in the schedule</li> <li>File a bug</li> <li>Limit the spread of the virus</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#how_to_refactor","title":"How to refactor","text":"<ul> <li>Refactoring needs to be undertaken slowly, deliberately, and carefully</li> <li>Don't refactor and add functionality at the same time</li> <li>Make sure you have good tests before refactoring</li> <li>Take baby steps to avoid prolonged debugging</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_47_software_development_is_more_gardening_than_building","title":"PP_Tip 47: Software development is more gardening than building","text":"<ul> <li> <p>Unfortunately the most common metaphor for software development is building     construction: blueprints, build, release, maintenance, ...</p> </li> <li> <p>As a program evolves, it is necessary to rethink earlier decisions and rework     portions of the code</p> </li> <li>Software is more like gardening:<ul> <li>Start with a plan</li> <li>Some plants die</li> <li>Move plants to get in the sun / shade</li> <li>Pull weeds</li> <li>Monitor health of the plants</li> <li>Make adjustments</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_47_when_to_refactor","title":"PP_Tip 47: When to refactor?","text":"<ul> <li>It is time to refactor when you notice:<ul> <li>Duplication</li> <li>Non-orthogonal design</li> <li>Outdated knowledge</li> <li>Bad performance</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_47_management_and_refactoring","title":"PP_Tip 47: Management and refactoring","text":"<ul> <li>How do you explain to your boss that \"the code works, but it needs to be     refactored?\"</li> <li>Fail to refactor now and the investment to fix the problem later on will be     larger</li> <li>It's like accumulating debt: at some point it will need to be repaid, with     interests!</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_48_design_to_test","title":"PP_Tip 48: Design to test","text":"<ul> <li> <p>Chips are designed to be tested</p> <ul> <li>At the factory</li> <li>When they are installed</li> <li>When they are deployed in the field</li> </ul> </li> <li> <p>BIST (Built-In Self Test) features</p> </li> <li> <p>TAM (Test Access Mechanism) = test harness to provide stimuli and collect     responses</p> </li> <li> <p>We need to build testability into the software from the very beginning</p> </li> <li>Test each piece thoroughly (unit testing) before wiring them together     (integration testing)</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_48_testing_against_contract","title":"PP_Tip 48: Testing against contract","text":"<ul> <li> <p>Write test cases that ensure that a unit honors its contract</p> <ul> <li>This also checks whether the contract means what we think it means</li> </ul> </li> <li> <p>We need to check over a wide range of test cases and boundary conditions</p> </li> <li>There's no better way to fix errors than by avoiding them in the first place</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#test-driven_development","title":"Test-driven development","text":"<ul> <li>By building the tests before you implement the code, you try out the interface     before you commit to it</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_48_where_to_put_unit_tests","title":"PP_Tip 48: Where to put unit tests?","text":"<ul> <li>Unit tests should be somehow close to the code they test<ul> <li>E.g., in a parallel directory</li> </ul> </li> <li> <p>If something is not easy to find and use, it won't be used</p> </li> <li> <p>By making the test code readily accessible, one provides:</p> <ul> <li>Examples of how to use a module</li> <li>A means to validate any future changes to code</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_48_provide_a_test_harness","title":"PP_Tip 48: Provide a test harness","text":"<ul> <li>It's good to have a way to:<ul> <li>Select tests to run</li> <li>Pass arguments on the command line to control the testing</li> <li>Analyze the test output</li> <li>Automatically find all the unit tests (instead of having a list of tests, so     that we can honor the DRY principle)</li> </ul> </li> <li>Test harness can be built using OOP<ul> <li>E.g., objects provide setup and cleanup methods, standard form of failure     report, ...</li> </ul> </li> <li>There are standard test harness (e.g., <code>unittest</code> for Python, <code>cppunit</code> for     C++)</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_48_build_a_test_backdoor","title":"PP_Tip 48: Build a test backdoor","text":"<ul> <li>No piece of software is perfect and bugs show up in real world</li> <li>Have log files containing trace messages</li> <li>Log messages should be in a regular, consistent format</li> <li>An interesting technique is to have a built-in web server in the application<ul> <li>Pointing to a certain port of the machine one can see internal status, log     entries, a debug control panel</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_49_test_your_software_or_your_users_will","title":"PP_Tip 49: Test your software or your users will","text":"<ul> <li>All software you write will be tested<ul> <li>If not by you, then by the eventual users</li> </ul> </li> <li>It is better to test it thoroughly than being swamped in help desk calls</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_50_dont_use_wizard_code_you_dont_understand","title":"PP_Tip 50: Don't use wizard code you don't understand","text":"<ul> <li>Applications are getting harder and harder to write<ul> <li>User interfaces are becoming increasingly sophisticated</li> <li>The underlying applications are getting more complex</li> </ul> </li> <li> <p>One can use wizards to generate code to perform most functions, but one must     understand all of it (otherwise one is programming by coincidence)</p> </li> <li> <p>One could argue that we routinely rely on things we don't understand</p> <ul> <li>E.g., quantum mechanics in integrated circuits</li> <li>What we don't understand is behind a tidy interface</li> <li>Wizard code is interwoven with our application</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#before_the_project","title":"Before the project","text":""},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#requirement","title":"Requirement","text":"<ul> <li>= a statement about something that needs to be accomplished</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_51_dont_gather_requirements_dig_for_them","title":"PP_Tip 51: Don't gather requirements: dig for them","text":"<ul> <li>Gathering requirements implies that the requirements are already there</li> <li>In reality they are buried deep beneath layers of<ul> <li>Assumptions</li> <li>Misconceptions</li> <li>Politics</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_51_example_of_tricky_requirements","title":"PP_Tip 51: Example of tricky requirements","text":"<ul> <li>A requirements can sound like: \"Only an employee's supervisors and HR may view     an employee's records\"</li> <li>This requirement has a policy embedded in it<ul> <li>We don't want to hard-wire policies into requirements, since policies change     all the time</li> </ul> </li> <li>Requirements should be a general statement: \"An employee record may be viewed     only by a nominated group of people\"</li> <li>Give the policy as an example of what should be supported</li> <li>Policies should eventually go in the metadata of the application</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_52_work_with_a_user_to_think_like_a_user","title":"PP_Tip 52: Work with a user to think like a user","text":"<ul> <li>We need to discover the underlying reason why users do a thing, rather than     just the way they currently do it</li> <li>We want to solve a business problem, not just check off requirements</li> <li>You can ask the user to sit with him for a week while he does his job<ul> <li>See how the system will be really used, not how management intended to be     used</li> <li>Build trust and establish communication with your users</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#use_cases","title":"Use cases","text":"<ul> <li>= capture requirements through a particular use of the system</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_53_use_cases","title":"PP_Tip 53: Use cases","text":"<ul> <li>While sitting with the user, you see a few interesting scenarios that describe     what the application needs to do</li> <li>Write the scenarios in a document that everyone (developers, end users,     project sponsor, management) can discuss</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_53_abstractions_live_longer_than_details","title":"PP_Tip 53: Abstractions live longer than details","text":"<ul> <li>Good requirement docs should remain abstract</li> <li>They reflect the business needs, not architecture, not design, not user     interface, ...</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_53_feature-itis","title":"PP_Tip 53: Feature-itis","text":"<ul> <li>Aka feature bloat</li> <li>The issue is that by adding \"just one more feature\", the scope of the project     keeps growing</li> <li>One should track the number of bugs reported and fixed, the number of     additional features requested and who approved them</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#project_glossary","title":"Project glossary","text":"<ul> <li>= one place that defines all the specific terms and vocabulary used in a     project</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_54_use_a_project_glossary","title":"PP_Tip 54: Use a project glossary","text":"<ul> <li>It's hard to succeed if users and developers<ul> <li>Refer to the same thing with different names, or</li> <li>Refer to different things with the same name</li> </ul> </li> <li>Create and maintain a project glossary</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_55_dont_think_outside_the_box_find_the_box","title":"PP_Tip 55: Don't think outside the box: find the box","text":"<ul> <li>The secret to solve a puzzle is to identify the real (not imagined)     constraints and find a solution therein</li> <li>Some constraints are preconceived notions</li> <li> <p>E.g., \"the Gordian knot\"</p> </li> <li> <p>E.g., the Trojan horse:</p> <ul> <li>How do you get troops into a walled city?</li> <li>Would you have dismissed the idea of getting troops \"through the front     door\"?</li> </ul> </li> <li> <p>To solve a problem enumerate all the possible avenues</p> <ul> <li>Don't dismiss anything, then explain why a certain path cannot be taken. Can     you prove it?</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_55_impossible_problems","title":"PP_Tip 55: Impossible problems","text":"<ul> <li>You are stuck on a problem that seems \"impossible\"</li> <li>You are late on the schedule</li> <li>Step back and ask yourself:<ul> <li>Is there an easier way?</li> <li>What is that makes this problem so hard to solve?</li> <li>Does it have to be done this way?</li> <li>Does it have to be done at all?</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_56_listen_to_nagging_doubts_start_when_you_are_ready","title":"PP_Tip 56: Listen to nagging doubts: start when you are ready","text":"<ul> <li>When you experience some reluctance when faced with a task, take notice</li> <li>Sometimes your instinct is right on the spot, although you cannot put a finger     on it</li> <li>How can you tell that is not just procrastination?<ul> <li>You can start prototyping and verify if some basic premises were wrong</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#program_specification","title":"Program specification","text":"<ul> <li>= process of reducing requirements to the point where coding can start</li> <li>The goal is to remove major ambiguities</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_57_some_things_are_better_done_than_described","title":"PP_Tip 57: Some things are better done than described","text":"<ul> <li>Program specification is an agreement with the user</li> <li>It's important to stop increasing level of detail and start coding,     prototyping, tracer bullet, because:<ul> <li>It is naive to assume that a specification will ever capture every detail</li> <li>Once the system is running, users will ask for changes</li> <li>Natural language is not expressive enough to clarify everything</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_58_dont_be_a_slave_to_formal_methods","title":"PP_Tip 58: Don't be a slave to formal methods","text":"<ul> <li>Many methods have been developed to make programming more like engineering     (e.g., waterfall development, UML, ...)</li> <li>These formal methods use a combination of diagrams and supporting words</li> <li>The user typically does not understand them and cannot provide feedback</li> <li>It's better to give the users a prototype and let them play with it</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_59_expensive_tools_do_not_produce_better_designs","title":"PP_Tip 59: Expensive tools do not produce better designs","text":"<ul> <li>Never give in into a methodology just because it is the hot new fad</li> <li>Do not think about how much a tool costs when you look at its output</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pragmatic_projects","title":"Pragmatic projects","text":""},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_60_pragmatic_teams","title":"PP_Tip 60: Pragmatic teams","text":"<ul> <li> <p>Most of the pragmatic programming principles apply to teams, as much as they     apply to an individual</p> </li> <li> <p>No broken windows</p> </li> <li>Quality is a team issue</li> <li> <p>Team as a whole should not tolerate broken windows (= small imperfections      that no one fixes)</p> </li> <li> <p>Boiled frogs</p> </li> <li>People assume that someone is handling an issue or that someone must have      OK-ed a change request from the user</li> <li> <p>Fight this: everyone must actively monitor the environment for changes</p> </li> <li> <p>Communicate</p> </li> <li> <p>Great teams speak with one voice and are always prepared</p> </li> <li> <p>Don't repeat yourself (DRY)</p> </li> <li> <p>Some teams appoint a member as the project librarian, responsible for      checking on duplication, coordinating documentation, ...</p> </li> <li> <p>Orthogonality</p> </li> <li>Traditional team organization is based on the old-fashioned waterfall      method of software construction<ul> <li>Individuals are assigned roles based on their job function, e.g.,</li> <li>Business analysts</li> <li>Architects</li> <li>Designers</li> <li>Programmers</li> <li>Testers</li> </ul> </li> <li>Different activities of a project (analysis, design, coding, testing) can't      happen in isolation</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_60_organize_around_functionality_not_job_functions","title":"PP_Tip 60: Organize around functionality, not job functions","text":"<ul> <li> <p>Organize people in the same way one organizes code</p> <ul> <li>Design by contract</li> <li>Decoupling</li> <li>Orthogonality</li> </ul> </li> <li> <p>Split teams by functionality not by job function (e.g., architect, programmer,     tester)</p> <ul> <li>We look for cohesive, self-contained teams of people</li> </ul> </li> <li> <p>Let each team organize themselves internally</p> <ul> <li>Each team has agreed upon responsibilities and commitments to the other     teams</li> <li>Of course this approach works only with responsible developers and strong     project management</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_61_dont_use_manual_procedures","title":"PP_Tip 61: Don't use manual procedures","text":"<ul> <li> <p>We want to ensure consistency and repeatability in the project</p> <ul> <li>Manual procedures leave consistency up to chance</li> <li>Let the computer do the mundane jobs: it will do a better job than we do</li> </ul> </li> <li> <p>Compiling the project should be reliable and repeatable</p> </li> <li>We want to check out, build, test, ship with a single command</li> <li><code>make</code> and <code>cronjobs</code> are the solutions</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_62_test_early","title":"PP_Tip 62: Test early","text":"<ul> <li>Look for your bugs now, so you don't have to endure the shame of others     finding your bugs later</li> <li>Start testing as soon as you have code</li> <li>Code a little, test a little</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_62_test_often","title":"PP_Tip 62: Test often","text":"<ul> <li>The earlier a bug is found, the cheaper it is to remedy</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_62_test_automatically","title":"PP_Tip 62: Test automatically","text":"<ul> <li>Tests that run with every build are better than test plans that sit on a shelf</li> <li>A good project may well have more test code than production code</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_top_63_coding_aint_done_til_all_the_tests_run","title":"PP Top 63: Coding ain't done 'til all the tests run","text":"<ul> <li>Just because you finished coding, you cannot tell that the code is done</li> <li>You cannot claim that the code is done until it passes all the available tests</li> <li>Code is never really done</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_63_what_to_test","title":"PP_Tip 63: What to test","text":"<ul> <li>There are multiple types of tests:<ul> <li>Unit</li> <li>Integration</li> <li>Validation</li> <li>Performance</li> <li>Usability tests</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#unit_test","title":"Unit test","text":"<ul> <li>= exercise a module</li> <li>If the parts don't work by themselves, they won't work together</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#integration_test","title":"Integration test","text":"<ul> <li>= show that the major subsystems work well together</li> <li>Integration is the largest source of bugs in the system<ul> <li>Test that the entire system honors its contract</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#validation_and_verification_test","title":"Validation and verification test","text":"<ul> <li>= the users told you what they wanted, but is it what they really need?</li> <li>A bug-free system that answers the wrong question is not useful</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#resource_exhaustion_errors_and_recovery_test","title":"Resource exhaustion, errors, and recovery test","text":"<ul> <li>Resources are limited in the real world, e.g.:<ul> <li>Memory</li> <li>Disk space / bandwidth</li> <li>CPU</li> <li>Network bandwidth</li> <li>Video resolution</li> </ul> </li> <li>How will the system behave under real-world conditions?</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#performance_test","title":"Performance test","text":"<ul> <li>= testing under a given load (e.g., expected number of users, connections,     transactions per second)</li> <li>Does the system scale?</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#usability_test","title":"Usability test","text":"<ul> <li>= performed with real users, under real environmental conditions</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_63_how_to_test","title":"PP_Tip 63: How to test","text":"<ul> <li>Run regression tests for all types of tests</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#regression_testing","title":"Regression testing","text":"<ul> <li>= compare output of current test with previous known values</li> <li>It ensures that fixes for today's bugs don't break things that were working     yesterday</li> <li>All types of tests can be run as regression tests<ul> <li>Unit</li> <li>Integration</li> <li>Validation</li> <li>Performance</li> <li>Usability tests</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#test_data","title":"Test data","text":"<ul> <li>Test data is either real-world data or synthetic data<ul> <li>One needs to use both, since they expose different kind of bugs</li> </ul> </li> <li>Synthetic data<ul> <li>Stresses boundary conditions</li> <li>Can have certain statistical properties (e.g., data to sort is already     sorted or inversely sorted)</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#exercising_gui_systems","title":"Exercising GUI systems","text":"<ul> <li>Often specialized testing tools are required, e.g.,<ul> <li>Event capture / playback model</li> <li>A data processing application with GUI front end should be decoupled so that     one can test each component by itself</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#testing_the_tests","title":"Testing the tests","text":"<ul> <li>We cannot write perfect software</li> <li>We need to test the tests and the test infrastructure</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#testing_thoroughly","title":"Testing thoroughly","text":"<ul> <li>Use coverage analysis tools to keep track of which lines of the code have been     / not been executed</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_64_use_saboteurs_to_test_your_testing","title":"PP_Tip 64: Use saboteurs to test your testing","text":"<ul> <li>If the system is a security system, test the system by trying to break in</li> <li>After you have written a test to detect a particular bug, cause the bug     deliberately and make sure the tests catch it</li> <li>Write test for both positive and negative cases</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_65_test_state_coverage_not_code_coverage","title":"PP_Tip 65: Test state coverage, not code coverage","text":"<ul> <li>Knowing that you executed all code does not tell you if you tested all states</li> <li>This is a combinatorial problem</li> <li>You can tame the complexity thinking about:<ul> <li>Boundary conditions</li> <li>Structure of the code</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_65_when_to_test","title":"PP_Tip 65: When to test","text":"<ul> <li>As soon as any code exists, it must be tested</li> <li>Testing should be done automatically as often as we can (e.g., before code is     committed to the repository)</li> <li>Also results should be easy to interpret: ideally the outcome is binary <code>ok</code> /     <code>not_ok</code></li> <li>Expensive / special tests can be run less frequently, but on a regular basis</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_66_find_bugs_once","title":"PP_Tip 66: Find bugs once","text":"<ul> <li>Once a human tester finds a bug, a new test should be created to check for     that bug every time</li> <li>You don't want to keep chasing the same bugs that the automated tests could     find for you</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_67_treat_english_as_just_another_programming_language","title":"PP_Tip 67: Treat English as just another programming language","text":"<ul> <li>Embrace documentation as an integral part of software development</li> <li>Keep the documentation in the code itself as much as possible</li> <li>Treat code and documentation as two views of the same model</li> <li>Apply all the principles learned for coding (DRY principle, orthogonality,     ...) to English as well</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_68_internal_documentation","title":"PP_Tip 68: Internal documentation","text":"<ul> <li>Source code comments</li> <li>Design documents</li> <li>Test documents</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#external_documentation","title":"External documentation","text":"<ul> <li>= anything that is shipped or published to the outside world together with the     software product (e.g., user manuals)</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#documentation_vs_code","title":"Documentation vs code","text":"<ul> <li>Documentation and code are different views of the same underlying model</li> <li>If there is a discrepancy, the code is what matters</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_68_comments","title":"PP_Tip 68: Comments","text":"<ul> <li>Code should have comments, but too many comments are as bad as too few<ul> <li>Comments should discuss \\textit{why} something is done (e.g, engineering     trade-offs, why decisions were made)</li> <li>The code already shows \\textit{how} it is done</li> </ul> </li> <li>Modules, class, methods should have comments describing what is not obvious</li> <li>Javadoc notation is useful (<code>\\@param</code>, <code>\\@return</code>, ...) to extract information     from the code automatically</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#naming_concepts","title":"Naming concepts","text":"<ul> <li>Variable names should be meaningful</li> <li>Remember that you will be writing the code once, but reading it hundreds of     time: avoid write-only code</li> <li>Misleading names are worse than meaningless names</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_68_automatically_generated_documentation","title":"PP_Tip 68: Automatically generated documentation","text":"<ul> <li> <p>Also for documentation we want to use pragmatic principles</p> <ul> <li>DRY principle</li> <li>Orthogonality</li> <li>Model-view-controller</li> <li>Automation</li> </ul> </li> <li> <p>Printed material is out of date as soon as it is printed</p> <ul> <li>Publish documents on-line</li> </ul> </li> <li>There should be a single command to generate and publish the documents on-line</li> <li>Use a timestamp or review number for each page</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_69_gently_exceed_your_users_expectations","title":"PP_Tip 69: Gently exceed your users' expectations","text":"<ul> <li>Success of a project is measured by how well it meets the expectations of its     users</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#examples_of_difference_between_actual_and_expected_results","title":"Examples of difference between actual and expected results","text":"<ul> <li>A company announces record profits, and its share price drops 20\\%<ul> <li>It didn't meet analysts' expectations</li> </ul> </li> <li>A child opens an expensive present and bursts into tears<ul> <li>It wasn't the cheap doll that she wanted</li> </ul> </li> <li>A team works miracles to implement a complex application<ul> <li>The users don't like it because it does not have an help system</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_69_communicating_expectations","title":"PP_Tip 69: Communicating expectations","text":"<ul> <li>Users come to you with some vision of what they want</li> <li>It may be<ul> <li>Incomplete</li> <li>Inconsistent</li> <li>Impossible</li> </ul> </li> <li>They are invested in it: you cannot ignore this</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#manage_expectations","title":"Manage expectations","text":"<ul> <li>Work with your users so that they understand what you are delivering</li> <li>Never lose sight of the business problems your application is intended to     solve</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#go_the_extra_mile","title":"Go the extra mile","text":"<ul> <li>Surprise and delight your users</li> <li>E.g., balloon help, colorization, automatic installation, splash screen     customized for their organization, ...</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_70_sign_your_work","title":"PP_Tip 70: Sign your work","text":"<ul> <li>Craftsmen of earlier ages were proud to sign their work</li> <li>Your signature should come to be recognized as an indicator of quality</li> </ul>"},{"location":"code_guidelines/all.code_like_pragmatic_programmer.how_to_guide.html#pp_tip_70_code_ownership_vs_anonymity","title":"PP_Tip 70: Code ownership vs anonymity","text":"<ul> <li>Code ownership can cause cooperation problems: people become territorial</li> <li>Anonymity can enable sloppiness, laziness</li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html","title":"All.code review.how to guide","text":""},{"location":"code_guidelines/all.code_review.how_to_guide.html#code_review","title":"Code Review","text":""},{"location":"code_guidelines/all.code_review.how_to_guide.html#code_review_1","title":"Code review","text":""},{"location":"code_guidelines/all.code_review.how_to_guide.html#general_rules_about_code_review","title":"General rules about code review","text":""},{"location":"code_guidelines/all.code_review.how_to_guide.html#read_the_google_code_review_best_practices","title":"Read the Google code review best practices","text":"<ul> <li> <p>From the     developer\\'s perspective</p> </li> <li> <p>From the     reviewer\\'s perspective</p> </li> <li> <p>Where the Google guide says \"CL\", think \"PR\"</p> </li> <li> <p>Read it (several times, if you need to)</p> </li> <li> <p>Think about it</p> </li> <li> <p>Understand the rationale</p> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#code_review_workflows","title":"Code review workflows","text":""},{"location":"code_guidelines/all.code_review.how_to_guide.html#pull_request","title":"Pull request","text":"<ul> <li>Our usual review process is to work in a branch and create a pull request (PR)<ul> <li>See our Git doc for details</li> <li>The name of the pull request should be based on the name of the branch,     which in turn should reference the issue repo, number and title. The PR name     is generally generated automatically, e.g.,     <code>PTask2704 make exchange contracts get contracts applicable to series</code>.</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#from_the_code_author_point_of_view","title":"From the code author point of view","text":""},{"location":"code_guidelines/all.code_review.how_to_guide.html#why_we_review_code","title":"Why we review code","text":"<ul> <li>We spend time reviewing each other code so that we can:<ul> <li>Build a better product, by letting other people look for bugs</li> <li>Propagate knowledge of the code base through the team</li> <li>Learn from each other</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#pr_checklist","title":"PR checklist","text":"<ul> <li> <p>From     Google reviewer checklist:</p> </li> <li> <p>In asking (and doing) a code review, you should make sure that:</p> <ul> <li>The code is well-designed.</li> <li>The functionality is good for the users of the code.</li> <li>The code isn't more complex than it needs to be.</li> <li>The developer isn't implementing things they might need in the future but     don't know they need now.</li> <li>Code has appropriate unit tests.</li> <li>Tests are well-designed.</li> <li>The developer used clear names for everything.</li> <li>Comments are clear and useful, and mostly explain why instead of what.</li> <li>Code is appropriately documented.</li> <li>The code conforms to our style guides.</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#the_golden_rule_of_code_review","title":"The golden rule of code review","text":"<ul> <li> <p>Make life easy for the reviewers</p> <ul> <li>Aka \"Do not upset the reviewers, otherwise they won't let you merge your     code\"</li> </ul> </li> <li> <p>Remember that reviewing other people's code is hard and unrewarding work</p> <ul> <li>Do your best for not frustrating the reviewers</li> </ul> </li> <li> <p>If you are in doubt \"it's probably clear, although I am not 100% sure\", err on     giving more information and answer potential questions</p> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#be_clear_in_the_pr_request_about_what_you_want","title":"Be clear in the PR request about what you want","text":"<ul> <li> <p>Summarize what was done in the PR</p> <ul> <li>Refer to the GH task, but the task alone might not be sufficient</li> <li>A PR can implement only part of a complex task<ul> <li>Which part is it implementing?</li> <li>Why is it doing it in a certain way?</li> </ul> </li> </ul> </li> <li> <p>If the code is not ready for merge, but you want a \"pre-review\" convert PR to     a draft</p> <ul> <li>E.g., ask for an architectural review</li> <li>Draft PRs can not be merged</li> </ul> </li> <li> <p>Is it blocking?</p> <ul> <li>Do not abuse asking for a quick review</li> <li>All code is important and we do our best to review code quickly and     carefully</li> <li>If it\\'s blocking a ping on Slack is a good idea</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#do_not_mix_changes_and_refactoring_shuffling_code","title":"Do not mix changes and refactoring / shuffling code","text":"<ul> <li> <p>The job of the reviewers become frustrating when the author mixes:</p> <ul> <li>Refactoring / moving code; and</li> <li>Changes</li> </ul> </li> <li> <p>It is time consuming or impossible for a reviewer to understand what happened:</p> <ul> <li>What is exactly changed?</li> <li>What was moved where?</li> </ul> </li> <li> <p>In those cases reviewers have the right to ask the PR to be broken in pieces</p> </li> <li> <p>One approach for the PR author is to:</p> <ul> <li>Do a quick PR to move code around (e.g., refactoring) or purely cosmetic<ul> <li>You can ask the reviewer to take a quick look</li> </ul> </li> <li>Do the next PRs with the actual changes</li> </ul> </li> <li> <p>Another approach is to develop in a branch and break the code into PRs as the     code firms up</p> <ul> <li>In this case you need to be very organized and be fluent in using Git: both     qualities are expected of you</li> <li>E.g., develop in a branch (e.g., <code>gp_scratch</code>)</li> <li>Create a branch from it (e.g., <code>TaskXYZ_do_this_and_that</code>) or copy the files     from <code>gp_scratch</code> to <code>TaskXYZ_do_this_and_that</code></li> <li>Edit the files to make the PR self-consistent</li> <li>Do a PR for <code>TaskXYZ_do_this_and_that</code></li> <li>Keep working in <code>gp_scratch</code> while the review is moving forward</li> <li>Make changes to the <code>TaskXYZ_do_this_and_that</code> as requested</li> <li>Merge <code>TaskXYZ_do_this_and_that</code> to master</li> <li>Merge <code>master</code> back into <code>gp_scratch</code> and keep moving</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#double_check_before_sending_a_pr","title":"Double check before sending a PR","text":"<ul> <li>After creating a PR take a look at it to make sure things look good, e.g.,<ul> <li>Are there merge problems?</li> <li>Did you forget some file?</li> <li>Skim through the PR to make sure that people can understand what you changed</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#reviewing_other_peoples_code_is_usually_not_fun","title":"Reviewing other people's code is usually not fun","text":"<ul> <li> <p>Reviewing code is time-consuming and tedious</p> <ul> <li>So do everything you can to make the reviewer's job easier</li> <li>Don't cut corners</li> </ul> </li> <li> <p>If a reviewer is confused about something, other readers (including you in 1     year) likely would be too</p> <ul> <li>What is obvious to you as the author is often not obvious to readers</li> <li>Readability is paramount</li> <li>You should abhor write-only code</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#the_first_reviews_are_painful","title":"The first reviews are painful","text":"<ul> <li> <p>One needs to work on the same code over and over</p> <ul> <li>Just think about the fact that the reviewer is also reading (still crappy)     code over and over</li> </ul> </li> <li> <p>Unfortunately it is needed pain to get to the quality of code we need to make     progress as a team</p> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#apply_review_comments_everywhere","title":"Apply review comments everywhere","text":"<ul> <li> <p>Apply a review comment everywhere, not just where the reviewer pointed out the     issue</p> </li> <li> <p>E.g., reviewer says:</p> <ul> <li>\"Please replace <code>_LOG.warning(\"Hello %s\".format(name))</code> with     <code>_LOG.warning(\"Hello %s\", name)</code>\"</li> </ul> </li> <li>You are expected to do this replacement:<ul> <li>In the current review</li> <li>In all future code you write</li> <li>In old code, as you come across it in the course of your work<ul> <li>Of course don't start modifying the old code in this review, but open a     clean-up bug, if you need a reminder</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#look_at_the_code_top-to-bottom","title":"Look at the code top-to-bottom","text":"<ul> <li>E.g., if you do a search &amp; replace, make sure everything is fine</li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#answering_comments_after_a_review","title":"Answering comments after a review","text":"<ul> <li>It's better to answer comments in chunks so we don't get an email per comment<ul> <li>Use \"start a review\" (not in conversation)</li> </ul> </li> <li>If one of the comment is urgent (e.g., other comments depend on this) you can     send it as single comment</li> <li>When you answer a comment, mark it as resolved</li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#apply_changes_to_a_review_quickly","title":"Apply changes to a review quickly","text":"<ul> <li> <p>In the same way the reviewers are expected to review PRs within 24 hours, the     author of a PR is expected to apply the requested changes quickly, ideally in     few hours</p> <ul> <li>If it takes longer, then either the PR was too big or the quality of the PR     was too low</li> </ul> </li> <li> <p>If it takes too long to apply the changes:</p> <ul> <li>The reviewers (and the authors) might forget what is the context of the     requested changes</li> <li>It becomes more difficult (or even impossible) to merge, since the code base     is continuously changing</li> <li>It creates dependencies among your PRs</li> <li>Remember that you should not be adding more code to the same PR, but only     fix the problems and then open a PR with new code</li> <li>Other people that rely on your code are blocked</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#ask_for_another_review","title":"Ask for another review","text":"<ul> <li>Once you are done with resolving all the comments ask for another review</li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#workflow_of_a_review_in_terms_of_gh_labels","title":"Workflow of a review in terms of GH labels","text":"<ul> <li>The current meaning of the labels are:<ul> <li>See GitHub workflows     doc</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#link_pr_to_gh_issue","title":"Link PR to GH issue","text":"<ul> <li>Mention the corresponding issue in the PR description to ease the navigation     E.g., see an     example</li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#fix_later","title":"Fix later","text":"<ul> <li> <p>It's ok for an author to file a follow up Issue (e.g., with a clean up), by     pointing the new Issue to the comments to address, and move on with merge</p> </li> <li> <p>The Issue needs to be addressed immediately after</p> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#from_the_code_reviewer_point_of_view","title":"From the code reviewer point of view","text":""},{"location":"code_guidelines/all.code_review.how_to_guide.html#post-commit_review","title":"Post-commit review","text":"<ul> <li> <p>You can comment on a PR already merged</p> </li> <li> <p>You can comment on the relevant lines in a commit straight to <code>master</code> (this     is the exception)</p> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#code_walk-through","title":"Code walk-through","text":"<ul> <li> <p>It is best to create a branch with the files you want to review</p> <ul> <li>Add TODOs in the code (so that the PR will pick up those sections)</li> <li>File bugs for the more involved changes</li> </ul> </li> <li> <p>Try to get a top to bottom review of a component once every N weeks (N = 2, 3)</p> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#close_the_pr_and_delete_the_branch","title":"Close the PR and delete the branch","text":"<ul> <li> <p>When code is merged into master by one of the reviewers through the UI, one     can select the \"delete branch\" option (usually it is done automatically)</p> </li> <li> <p>Otherwise you can delete the branch using the procedure in     Git</p> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#give_priority_to_code_review","title":"Give priority to code review","text":"<ul> <li>We target to give feedback on a PR within 24hr so that the author is not     blocked for too long<ul> <li>Usually we respond in a few hours</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#multiple_reviewers_problem","title":"Multiple reviewers problem","text":"<ul> <li> <p>When there are multiple reviewers for the same PR there can be some problems</p> </li> <li> <p>Ok to keep moving fast and avoid blocking</p> <ul> <li>Block only if it is controversial</li> </ul> </li> <li> <p>Merge when we are confident that the other is ok with it</p> <ul> <li>The other can catch up with a post-commit review</li> <li>A good approach is to monitor recently merged PRs in GH to catch up</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#remember_small_steps_ahead","title":"Remember \"small steps ahead\"","text":"<ul> <li>Follow the Google approach of merging a PR that is a strict improvement</li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#nothing_is_too_small","title":"Nothing is too small","text":"<ul> <li> <p>Each reviewer reviews the code pointing out everything that can be a problem</p> </li> <li> <p>Problems are highlighted even if small or controversial</p> <ul> <li>Not all of those comments might not be implemented by the author</li> </ul> </li> <li> <p>Of course if different approaches are really equivalent but reviewers have     their own stylistic preference, this should not be pointed out, unless it's a     matter of consistency or the choice is left to the author</p> </li> </ul>"},{"location":"code_guidelines/all.code_review.how_to_guide.html#final_gh_comment","title":"Final GH comment","text":"<ul> <li> <p>Once you are done with the detailed review of the code, you need to</p> <ul> <li>Write a short comment</li> <li>Decide what is the next step for the PR, e.g.,<ul> <li>Comment<ul> <li>Submit general feedback without explicit approval</li> </ul> </li> <li>Approve<ul> <li>Submit feedback and approve merging these changes</li> </ul> </li> <li>Request changes<ul> <li>Submit feedback that must be addressed before merging</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>We use an integrator / developer manager workflow, initially with Paul and GP     testing and merging most of the PRs</p> </li> <li> <p>We use the 3 possible options in the following way:</p> <ul> <li>Comment<ul> <li>When reviewers want the changes to be applied and will then look at the     resulting changes to decide the next steps</li> <li>In practice this means \"make the changes and then we'll discuss more\"</li> <li>E.g., this is of course the right choice for a pre-PR</li> </ul> </li> <li>Approve<ul> <li>No more changes: time to merge!</li> <li>Often it is accompanied by the comment \"LGTM\" (= \"Looks Good To Me\")</li> </ul> </li> <li>Request changes<ul> <li>This typically means \"if you address the comments we can merge\"</li> <li>In practice this is more or less equivalent to \"Comment\"</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html","title":"All.coding style.how to guide","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#coding_style","title":"Coding Style","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#kaizenflow_-_python_style_guide","title":"KaizenFlow - Python Style Guide","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#meta","title":"Meta","text":"<ul> <li>What we call \"rules\" are actually just a convention</li> <li>The \"rules\"<ul> <li>Are optimized for the common case</li> <li>Are not the absolute best way of doing something in all possible cases</li> <li>Can become cumbersome or weird to follow for some corner cases</li> </ul> </li> <li>We prefer simple rather than optimal rules so that they can be applied in most     of the cases, without thinking or going to check the documentation</li> <li>The rules are striving to achieve consistency and robustness<ul> <li>We prefer to care about consistency rather than arguing about which approach     is better in each case</li> <li>E.g., see the futile \"tab vs space\" flame-war from the 90s</li> </ul> </li> <li>The rules are optimized for the average developer / data scientist and not for     power users</li> <li>The rules try to minimize the maintenance burden<ul> <li>We want to minimize the propagation of a change</li> </ul> </li> <li>Rules are not fixed in stone<ul> <li>Rules evolve based on what we discuss through the reviews</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#disclaimer","title":"Disclaimer","text":"<ul> <li>This document was forked from     Google Python Style Guide,     therefore, the numbering of chapters sets off where the Style Guide ends. Make     sure to familiarize yourself with it before proceeding to the rest of the doc,     since it is the basis of our team's code style.</li> <li>Another important source is     The Pragmatic Programmer     by David Thomas and Andrew Hunt. While not Python-specific, it provides an     invaluable set of general principles by which any person working with code     (software developer, DevOps or data scientist) should abide. Read it on long     commutes, during lunch, and treat yourself to a physical copy on Christmas.     The book is summarized     here, but     do not deprive yourself of the engaging manner in which Thomas &amp; Hunt     elaborate on these points -- on top of it all, it is a very, very enjoyable     read.</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#references","title":"References","text":"<ul> <li>Coding<ul> <li>Google Python Style Guide (GPSG)</li> <li>Code convention from PEP8</li> </ul> </li> <li>Documentation<ul> <li>Docstring convention from PEP257</li> <li>Google documentation best practices</li> </ul> </li> <li>Commenting style<ul> <li>Sphinx</li> </ul> </li> <li>Sphinx tutorial</li> <li>Design<ul> <li>Google philosophical stuff</li> <li>Unix rules (although a bit cryptic sometimes)</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#high-level_principles","title":"High-Level Principles","text":"<ul> <li>In this paragraph we summarize the high-level principles that we follow for     designing and implementing code and research. We should be careful in adding     principles here. Ideally principles should be non-overlapping and generating     all the other lower level principles we follow (like a basis for a vector     space)</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#follow_the_dry_principle","title":"Follow the DRY principle","text":"<ul> <li>DRY =     Don't repeat yourself</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#the_writer_is_the_reader","title":"The writer is the reader","text":"<ul> <li>Make code easy to read even if it is more difficult to write</li> <li>Code is written 1x and read 100x</li> <li>Remember that even if things are perfectly clear now to the person that wrote     the code, in a couple of months the code will look foreign to whoever wrote     the code.</li> <li>So make your future-self's life easier by following the conventions and erring     on the side of documenting for the reader.</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#encapsulate_what_changes","title":"Encapsulate what changes","text":"<ul> <li>Separate what changes from what stays the same</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#least_surprise_principle","title":"Least surprise principle","text":"<ul> <li>Try to make sure that the reader is not surprised</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#pay_the_technical_debt","title":"Pay the technical debt","text":"<ul> <li>Any unpaid debt is guaranteed to bite you when you don't expect it</li> <li>Still some debt is inevitable: try to find the right trade-off</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#end-to-end_first","title":"End-to-end first","text":"<ul> <li>Always focus on implementing things end-to-end, then improve each block</li> <li>Remember the analogy of building the car through the skateboard, the bike,     etc.<ul> <li>Compare this approach to building wheels, chassis, with a big-bang     integration at the end</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#unit_test_everything","title":"Unit test everything","text":"<ul> <li>Code that matters needs to be unit tested</li> <li>Code that doesn't matter should not be checked in the repo</li> <li>The logical implication is: all code checked in the repo should be unit tested</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#dont_get_attached_to_code","title":"Don't get attached to code","text":"<ul> <li>It's ok to delete, discard, retire code that is not useful any more</li> <li>Don't take it personally when people suggest changes or simplification</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#always_plan_before_writing_code","title":"Always plan before writing code","text":"<ul> <li>File a GitHub issue</li> <li>Think about what to do and how to do it</li> <li>Ask for help or for a review</li> <li>The best code is the one that we avoid to write through a clever mental     kung-fu move</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#think_hard_about_naming","title":"Think hard about naming","text":"<ul> <li>Finding a name for a code object, notebook, is extremely difficult but very     important to build a mental map</li> <li>Spend the needed time on it</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#look_for_inconsistencies","title":"Look for inconsistencies","text":"<ul> <li>Stop for a second after you have, before sending out:<ul> <li>Implemented code or a notebook</li> <li>Written documentation</li> <li>Written an e-mail</li> <li>...</li> </ul> </li> <li>Reset your mind and look at everything with fresh eyes like if it was the     first time you saw it<ul> <li>Does everything make sense to someone that sees this for the first time?</li> <li>Can (and should) it be improved?</li> <li>Do you see inconsistencies, potential issues?</li> </ul> </li> <li>It will take less and less time to become good at this</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#no_ugly_hacks","title":"No ugly hacks","text":"<ul> <li>We don't tolerate \"ugly hacks\", i.e., hacks that require lots of work to be     undone (much more than the effort to do it right in the first place)<ul> <li>Especially an ugly design hack, e.g., a Singleton, or some unnecessary     dependency between distant pieces of code</li> <li>Ugly hacks spreads everywhere in the code base</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#our_coding_suggestions","title":"Our coding suggestions","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#being_careful_with_naming","title":"Being careful with naming","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#follow_the_conventions","title":"Follow the conventions","text":"<ul> <li>Name executable files (scripts) and library functions using verbs (e.g.,     <code>download.py</code>, <code>download_data()</code>)</li> <li>Name classes and (non-executable) files using nouns (e.g., <code>Downloader()</code>,     <code>downloader.py</code>)</li> <li>For decorators we don't use a verb as we do for normal functions, but rather     an adjective or a past tense verb, e.g.,     <code>python def timed(f):         \"\"\"         Add a timer decorator around a specified function.         \"\"\"         ...</code></li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#follow_spelling_rules","title":"Follow spelling rules","text":"<ul> <li>We spell commands in lower-case, and programs with initial upper case:<ul> <li>\"Git\" (as program), \"git\" (as the command)</li> </ul> </li> <li>We distinguish \"research\" (not \"default\", \"base\") vs \"production\"</li> <li>We use different names for indicating the same concept, e.g., <code>dir</code>, <code>path</code>,     <code>folder</code><ul> <li>Preferred term is <code>dir</code></li> </ul> </li> <li>Name of columns<ul> <li>The name of columns should be <code>..._col</code> and not <code>..._col_name</code> or <code>_column</code></li> </ul> </li> <li>Timestamp<ul> <li>We spell <code>timestamp</code>, we do not abbreviate it as <code>ts</code></li> <li>We prefer timestamp to <code>datetime</code><ul> <li>E.g., <code>start_timestamp</code> instead of <code>start_datetime</code></li> </ul> </li> </ul> </li> <li>Abbreviations<ul> <li>JSON, CSV, DB, etc., are abbreviations and thus should be capitalized in     comments and docstrings, and treated as abbreviations in code when it     doesn't conflict with other rules<ul> <li>E.g., <code>convert_to_CSV</code>, but <code>csv_file_name</code> as a variable name that is not     global</li> </ul> </li> <li>Profit-and-loss: PnL instead of pnl or PNL</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#search_good_names_avoid_bad_names","title":"Search good names, avoid bad names","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#general_naming_rules","title":"General naming rules","text":"<ul> <li>Naming things properly is one of the most difficult task of a programmer /     data scientist<ul> <li>The name needs to be (possibly) short and memorable<ul> <li>However, don't be afraid to use long names, if needed, e.g.,     <code>process_text_with_full_pipeline_twitter_v1</code></li> <li>Clarity is more important than number of bytes used</li> </ul> </li> <li>The name should capture what the object represents, without reference to     things that can change or to details that are not important</li> <li>The name should refer to what objects do (i.e., mechanisms), rather than how     we use them (i.e., policies)</li> <li>The name needs to be non-controversial: people need to be able to map the     name in their mental model</li> <li>The name needs to sound good in English<ul> <li>Bad: <code>AdapterSequential</code> sounds bad</li> <li>Good: <code>SequentialAdapter</code> sounds good</li> </ul> </li> </ul> </li> <li>Some examples of how NOT to do naming:<ul> <li><code>raw_df</code> is a terrible name<ul> <li>\"raw\" with respect to what?</li> <li>Cooked?</li> <li>Read-After-Write race condition?</li> </ul> </li> <li><code>person_dict</code> is bad<ul> <li>What if we switch from a dictionary to an object?<ul> <li>Then we need to change the name everywhere!</li> </ul> </li> <li>The name should capture what the data structure represents (its semantics)     and not how it is implemented</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#do_not_be_stingy","title":"Do not be stingy","text":"<ul> <li>Why calling an object <code>TimeSeriesMinStudy</code> instead of <code>TimeSeriesMinuteStudy</code>?<ul> <li>Saving 3 letters is not worth</li> <li>The reader might interpret <code>Min</code> as <code>Minimal</code> (or <code>Miniature</code>, <code>Minnie</code>,     <code>Minotaur</code>)</li> </ul> </li> <li>If you don't like to type, we suggest you get a better keyboard, e.g.,     this</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#do_not_abbreviate_just_to_save_characters","title":"Do not abbreviate just to save characters","text":"<ul> <li>Abbreviations just to save space are rarely beneficial to the reader. E.g.,<ul> <li>Fwd (forward)</li> <li>Bwd (backward)</li> <li>Act (actual)</li> <li>Exp (expected)</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#when_to_use_abbreviations","title":"When to use abbreviations","text":"<ul> <li>We could relax this rule for short lived functions and variables in order to     save some visual noise.</li> <li>Sometimes an abbreviation is so short and common that it's ok to leave it     E.g.,<ul> <li>Df (dataframe)</li> <li>Srs (series)</li> <li>Idx (index)</li> <li>Id (identifier)</li> <li>Val (value)</li> <li>Var (variable)</li> <li>Args (arguments)</li> <li>Kwargs (keyword arguments)</li> <li>Col (column)</li> <li>Vol (volatility) while volume is always spelled out</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#avoid_code_stutter","title":"Avoid code stutter","text":"<ul> <li>An example of code stutter: you want to add a function that returns <code>git</code> root     path in a module <code>git</code></li> <li>Bad<ul> <li> <p>Name is <code>get_git_root_path()</code></p> <p>```python import helpers.hgit as hgit</p> </li> </ul> </li> </ul> <p>... hgit.get_git_root_path()         ```     - You see that the module is already specifying we are talking about Git</p> <ul> <li>Good<ul> <li> <p>Name is <code>get_root_path()</code></p> <p>```python import helpers.hgit as hgit</p> </li> </ul> </li> </ul> <p>... hgit.get_root_path()         ```     - This is not only aesthetic reason but a bit related to a weak form of DRY</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#comments_and_docstrings","title":"Comments and docstrings","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#general_conventions","title":"General conventions","text":"<ul> <li>Code needs to be properly commented</li> <li>We follow Python standard PEP 257     for commenting<ul> <li>PEP 257 standardizes what comments should express and how they should do it     (e.g., use triple quotes for commenting a function), but does not specify     what markup syntax should be used to describe comments</li> </ul> </li> <li>Different conventions have been developed for documenting interfaces<ul> <li>ReST</li> <li>Google (which is cross-language, e.g., C++, Python, ...)</li> <li>Epytext</li> <li>Numpydoc</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#descriptive_vs_imperative_style","title":"Descriptive vs imperative style","text":"<ul> <li>We decided to use imperative style for our comments and docstrings<ul> <li>Pylint and other Python QA tools favor an imperative style</li> <li>From PEP 257 <code>The docstring is a phrase ending in a period. It prescribes the function or     method's effect as a command (\"Do this\", \"Return that\"), not as a description;     e.g. don't write \"Returns the pathname ...\".</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#docstrings_style","title":"Docstrings style","text":"<ul> <li>We follow ReST (aka re-Structured Text) style for docstrings which is:<ul> <li>The most widely supported in the Python community</li> <li>Supported by all doc generation tools (e.g., epydoc, sphinx)</li> <li>Default in Pycharm</li> <li>Default in <code>pyment</code></li> <li>Supported by <code>pydocstyle</code> (which does not support Google style as explained     here)</li> </ul> </li> <li> <p>Example of a function definition with ReST styled docstring:</p> <p>```python def my_function(param1: str) -&gt; str:     \"\"\"     A one-line description of what the function does.</p> <pre><code>A longer description (possibly on multiple lines) with a more detailed\nexplanation of what the function does, trying to not be redundant with\nthe parameter / return description below. The focus is on the interface\nand what the user should know to use the function and not how the\nfunction is implemented.\n\n:param param1: this is the first param\n:return: this is a description of what is returned\n\"\"\"\n</code></pre> <p><code>`` - We use lowercase after</code>:param XYZ: ...<code>/</code>:return:` unless the first word is     a proper noun - First docstring line must be a single line - A full ReST docstring styling also requires to specify params and return     types, however type hinting makes it redundant so you should use only type     hinting</p> </li> <li> <p>Put docstrings in triple quotation marks</p> <ul> <li>Bad <code>python Generate \"random returns\".</code></li> <li>Good <code>python \"\"\" Generate \"random returns\". \"\"\"</code></li> </ul> </li> <li>Sometimes functions are small enough so we just use a 1-liner docstring     without detailed params and return descriptions. Just do not put text and     docstring brackets in one line<ul> <li>Bad <code>python \"\"\"This is not our approach.\"\"\"</code></li> <li>Good <code>python \"\"\" This is our approach. \"\"\"</code></li> </ul> </li> <li>Follow this example for indentation of param/return descriptions:<ul> <li>Good     ```python \"\"\" :param param1: a very very long param description that     continues into a second line :param param2: a param with two possible values<ul> <li>first value description</li> <li>second value description that is very long and     continues into a second line \"\"\" ```</li> </ul> </li> </ul> </li> <li>More examples of and discussions on Python docstrings</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#doctrings_content","title":"Doctrings content","text":"<ul> <li> <p>The docstring should focus on the \"what\" (the goal of the code) and not the     \"how\" (the implementation)</p> </li> <li> <p>The docstring should not be a line-by-line \"translation\" of code into text</p> <ul> <li>People can look at the code if they want to know the details of the     implementation</li> <li>We don't want to create a maintainability burden where the docstring should     change with every update in the implementation</li> </ul> </li> <li> <p>We encourage adding examples (e.g., of input and output) to the docstring to     improve clarity</p> </li> <li> <p>Type of variables should not be mentioned in <code>:param:</code>/<code>:return:</code> descriptions</p> <ul> <li>E.g., do not do <code>:return: a list of messages</code>, do <code>:return: messages</code></li> <li>Types should already be evident (from the type hinting)</li> <li>We don't want to have to change all the docstrings if we change the     implementation (e.g., turn a list into a set)</li> </ul> </li> <li> <p>Default values of parameters should not be mentioned in <code>:param:</code> descriptions     for the same reasons as above</p> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#comments_style","title":"Comments style","text":"<ul> <li>Comments follow the same style of docstrings, e.g., imperative style, start     with a capital letter, with period <code>.</code> at the end<ul> <li>Bad     ```python</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#this_comment_is_not_imperative_not_capitalized_and_has_no_period_at_the_end","title":"this comment is not imperative, not capitalized and has no period at the end","text":"<pre><code>    ```\n- _Good_\n    ```python\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#make_comments_imperative_start_with_a_capital_letter_and_end_with_a_period","title":"Make comments imperative, start with a capital letter and end with a period.","text":"<pre><code>    ```\n</code></pre> <ul> <li>Always place comments above the lines that they are referring to. Avoid     writing comments on the same line as code since they require extra maintenance     (e.g., when the line becomes too long)<ul> <li>Bad <code>python print(\"hello world\")      # Introduce yourself.</code></li> <li>Good     ```python</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#introduce_yourself","title":"Introduce yourself.","text":"<p>print(\"hello world\")         <code>- The only exception is commenting `if-elif-else` statements: we comment them     underneath the each statement in order to explain the code that belongs to the     each statement particularly     - _Bad_</code>python</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#set_remapping_based_on_the_run_type","title":"Set remapping based on the run type.","text":"<p>if is_prod:         ... else:         ...         <code>- _Good_</code>python</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#_1","title":"All.coding style.how to guide","text":"<p>if is_prod:         # Set remapping for database data used in production.         ... else:         # Set remapping for file system data used in simulation.         ...         <code>``         - If you want to separate an</code>if` statement from a bunch of code preceding             it, you can leave an empty comment before it</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#replace_empty_lines_in_code_with_comments","title":"Replace empty lines in code with comments","text":"<ul> <li> <p>The problem with empty lines is that they are visually confusing since one     empty line is used also to separate functions. For this reason we suggest     using a comment</p> </li> <li> <p>If you feel that you need an empty line in the code, it probably means that a     specific chunk of code is a logical piece of code performing a cohesive     function</p> <p>```python ... end_y = end_dt.year</p> </li> </ul> <p>paths = list() ...     ```</p> <ul> <li>Instead of putting an empty line, you should put a comment describing at high     level what the code does.     ```python ... end_y = end_dt.year</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#generate_a_list_of_file_paths_for_parquet_dataset","title":"Generate a list of file paths for Parquet dataset.","text":"<p>paths = list() ...     <code>- A less optimal solution is to add an empty comment, but this is not great:</code>python ... end_y = end_dt.year</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#_2","title":"All.coding style.how to guide","text":"<p>paths = list() ...     ```</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#comment_chunks_of_code","title":"Comment chunks of code","text":"<ul> <li> <p>Avoid wall-of-code, by commenting chunks of code that perform a cohesive work</p> </li> <li> <p>Bad</p> <p><code>python system_log_dir = rsiprrec.get_system_log_dir(mode) prod_dir = os.path.join(target_dir, system_log_dir) hio.create_dir(prod_dir, incremental=True) config_dict = {     \"dag_runner_config\": {             \"wake_up_timestamp\": \"2023-11-13 08:09:00-05:00\",             \"rt_timeout_in_secs_or_time\": 86400,             \"bar_duration_in_secs\": 300,     }, } config = cconfig.Config.from_dict(config_dict) config_tag = \"system_config.output\" config.save_to_file(prod_dir, config_tag)</code></p> </li> <li> <p>The code above has obviously various chunks that can be described with     comments     <code># Create a dir to store the System Config.     ...     # Create an example Config.     ...     # Save the Config in the target dir.</code></p> </li> <li> <p>Good</p> <p>```python</p> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#create_a_dir_to_store_the_system_config","title":"Create a dir to store the System Config.","text":"<p>system_log_dir = rsiprrec.get_system_log_dir(mode) prod_dir = os.path.join(target_dir, system_log_dir) hio.create_dir(prod_dir, incremental=True)</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#create_an_example_config","title":"Create an example Config.","text":"<p>config_dict = {     \"dag_runner_config\": {             \"wake_up_timestamp\": \"2023-11-13 08:09:00-05:00\",             \"rt_timeout_in_secs_or_time\": 86400,             \"bar_duration_in_secs\": 300,     }, } config = cconfig.Config.from_dict(config_dict) config_tag = \"system_config.output\"</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#save_the_config_in_the_target_dir","title":"Save the Config in the target dir.","text":"<p>config.save_to_file(prod_dir, config_tag) ```</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#referring_to_an_object_in_code_comments","title":"Referring to an object in code comments","text":"<ul> <li>In general, avoid this whenever possible</li> <li>Code object names (e.g., function, class, params) are often subject to change,     so we need to take care of them everywhere. It is very hard to track all of     them in comments so replace the names with their actual meaning<ul> <li>Bad     ```python</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#generate_a_list_of_file_paths_for_parquetdataset","title":"Generate a list of file paths for <code>ParquetDataset</code>.","text":"<pre><code>    ```\n- _Good_\n    ```python\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#generate_a_list_of_file_paths_for_parquet_dataset_1","title":"Generate a list of file paths for Parquet dataset.","text":"<pre><code>    ```\n</code></pre> <ul> <li>However, sometimes it is necessary. In this case refer to objects in the code     using Markdown. This is useful for distinguishing the object code from the     real-life object<ul> <li>Bad     ```python</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#the_dataframe_df_tmp_is_used_for","title":"The dataframe df_tmp is used for ...","text":"<pre><code>    ```\n- _Good_\n    ```python\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#the_dataframe_df_tmp_is_used_for_1","title":"The dataframe <code>df_tmp</code> is used for ...","text":"<pre><code>    ```\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#referring_to_type_of_vars_in_code_comments","title":"Referring to type of vars in code comments","text":"<ul> <li>Similarly, and for the same reason, avoid referring to the type of a variable     in the comments<ul> <li>Bad     ```python</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#filter_the_list_of_llm_prompts","title":"Filter the list of LLM prompts.","text":"<pre><code>    ```\n- _Good_\n    ```python\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#filter_the_llm_prompts","title":"Filter the LLM prompts.","text":"<pre><code>    ```\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#avoid_distracting_comments","title":"Avoid distracting comments","text":"<ul> <li>Use comments to explain the high level logic / goal of a piece of code and not     the implementation details (\"what\" and not \"how\")<ul> <li>Bad     ```python</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#format_the_output_by_displaying_only_the_beginning_and_the_end_segments","title":"Format the output by displaying only the beginning and the end segments.","text":"<pre><code>    ```\n- _Good_\n    ```python\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#format_the_output","title":"Format the output.","text":"<pre><code>    ```\n</code></pre> <ul> <li>Do not comment things that are obvious<ul> <li>Bad     ```python</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#print_results","title":"Print results.","text":"<p>_LOG.info(\"Results are %s\", ...)         ```</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#commenting_out_code","title":"Commenting out code","text":"<ul> <li>When we comment out code, we should explain why it is no longer relevant<ul> <li>Bad     ```python is_alive = pd.Series(True, index=metadata.index)</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#is_alive_kgutilsannotate_alivemetadata_selfalive_cutoff","title":"is_alive = kgutils.annotate_alive(metadata, self.alive_cutoff)","text":"<pre><code>    ```\n- _Good_\n    ```python\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#todo_as_discussed_in_ptask5047_for_now_we_set_all_timeseries_to_be_alive","title":"TODO(*): As discussed in PTask5047 for now we set all timeseries to be alive.","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#is_alive_kgutilsannotate_alivemetadata_selfalive_cutoff_1","title":"is_alive = kgutils.annotate_alive(metadata, self.alive_cutoff)","text":"<p>is_alive = pd.Series(True, index=metadata.index)         ```</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#use_type_hints","title":"Use type hints","text":"<ul> <li>We expect new code to use type hints whenever possible<ul> <li>See PEP 484</li> <li>Type hints cheat sheet</li> </ul> </li> <li>At some point we will start adding type hints to old code</li> <li>We use <code>mypy</code> to check for type mistakes and to enforce type hints as part of     the Linter flow</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#interval_notation","title":"Interval notation","text":"<ul> <li>Intervals are represented with <code>[a, b), (a, b], (a, b), [a, b]</code></li> <li>We don't use the other style <code>[a, b[</code></li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#if_you_find_a_bug_or_obsolete_docstringtodo_in_the_code","title":"If you find a bug or obsolete docstring/TODO in the code","text":"<ul> <li>The process is:<ul> <li>Do a <code>git blame</code> to find who wrote the code</li> <li>If it's an easy bug, you can fix it and ask for a review from the author</li> <li>You can comment on a PR (if there is one)</li> <li>You can file a bug on Github with<ul> <li>Clear info on the problem</li> <li>How to reproduce it, ideally a unit test</li> <li>Stacktrace</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#linter","title":"Linter","text":"<ul> <li>Linter is in charge of reformatting the code according to our conventions and     reporting potential problems</li> <li>You can find instructions on how to run Linter     here</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#remove_linter_messages","title":"Remove Linter messages","text":"<ul> <li>When Linter reports a problem:<ul> <li>We assume that Linter messages are correct, until it is proven wrong</li> <li>We try to understand what is the rationale for Linter's complaints</li> <li>We then change the code to follow Linter's suggestions and remove the lint</li> </ul> </li> <li>If you think a message is too pedantic, please file a bug with the example and     as a team we will consider whether to exclude that message from our list of     Linter suggestions</li> <li>If you think the message is a false positive, then try to change the code to     make Linter happy<ul> <li>E.g., if the code depends on some run-time behavior that Linter can't infer,     then you should question whether that behavior is really needed</li> <li>A human reader would probably be as confused as Linter is</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#when_to_disable_linter_messages","title":"When to disable Linter messages","text":"<ul> <li>If you really believe you should override Linter in this particular case, then     use something like:     ```python</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#pylint_disablesome-messageanother-one","title":"pylint: disable=some-message,another-one","text":"<pre><code>```\n- You then need to explain in a comment why you are overriding Linter.\n- Don't use Linter code numbers, but the\n    [symbolic name](https://github.com/josherickson/pylint-symbolic-names)\n    whenever possible:\n    - _Bad_\n        ```python\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#pylint_disablew0611","title":"pylint: disable=W0611","text":"<p>import config.logging_settings             <code>- _Good_</code>python</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#pylint_disableunused-import","title":"pylint: disable=unused-import","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#this_is_needed_when_evaluating_code_at_run-time_that_depends_from","title":"This is needed when evaluating code at run-time that depends from","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#this_import","title":"this import.","text":"<p>import config.logging_settings             ```</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#prefer_non-inlined_linter_comments","title":"Prefer non-inlined Linter comments","text":"<ul> <li>As for the general comments, we prefer to make Linter comments non-inlined</li> <li>However, sometimes there is no other choice than an inlined comment to get     Linter to understand which line we are referring to, so in rare cases it is     OK:<ul> <li>Bad but ok if needed     <code>python import config.logging_settings  # pylint: disable=unused-import</code></li> <li>Good     ```python</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#pylint_disableline-too-long","title":"pylint: disable=line-too-long","text":"<pre><code>expected_df_as_str = \"\"\"# df=\n                                                        asset_id   last_price            start_datetime              timestamp_db\nend_datetime\n2000-01-01 09:31:00-05:00      1000   999.874540 2000-01-01 09:30:00-05:00 2000-01-01 09:31:00-05:00\n2000-01-01 09:32:00-05:00      1000  1000.325254 2000-01-01 09:31:00-05:00 2000-01-01 09:32:00-05:00\n2000-01-01 09:33:00-05:00      1000  1000.557248 2000-01-01 09:32:00-05:00 2000-01-01 09:33:00-05:00\"\"\"\n# pylint: enable=line-too-long\n    ```\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#dont_mix_real_changes_with_linter_changes","title":"Don't mix real changes with Linter changes","text":"<ul> <li>We don't commit changes that modify the code together with Linter     reformatting, unless linting is applied to the changes we just made<ul> <li>The reason for not mixing real and Linter changes is that for a PR or to     just read the code it is difficult to understand what really changed vs what     was just a cosmetic modification</li> </ul> </li> <li>If you are worried Linter might change your code in a way you don't like,     e.g.,<ul> <li>Screwing up some formatting you care about for some reason, or</li> <li>Suggesting changes that you are worried might introduce bugs you can commit     your code and then do a \"lint commit\" with a message \"CMTaskXYZ: Lint\"</li> <li>In this way you have a backup state that you can rollback to, if you want</li> </ul> </li> <li>If you run Linter and see that it is reformatting / modifying pieces of code     you didn't change, it means that our teammate forgot to lint their code<ul> <li><code>git blame</code> can figure out the culprit</li> <li>You can send him / her a ping to remind her to lint, so you don't have to     clean after him / her</li> <li>In this case, the suggested approach is:<ul> <li>Commit your change to a branch / stash</li> <li>Run Linter by itself on the files that need to be cleaned, without any     change</li> <li>Run the unit tests to make sure nothing is breaking</li> <li>You can fix lints or just do formatting: it's up to you</li> <li>You can make this change directly on <code>master</code> or do a PR if you want to be     extra sure: your call</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#logging","title":"Logging","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#always_use_logging_instead_of_prints","title":"Always use logging instead of prints","text":"<ul> <li>Always use <code>logging</code> and never <code>print()</code> to monitor the execution</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#our_logging_idiom","title":"Our logging idiom","text":"<ul> <li> <p>In order to use our logging framework (e.g., <code>-v</code> from command lines, and much     more) use:</p> <p>```python import helpers.hdbg as hdbg</p> </li> </ul> <p>_LOG = logging.getLogger(name)</p> <p>hdbg.init_logger(verbosity=logging.DEBUG)</p> <p>_LOG.debug(\"I am a debug function about %s\", a)     ```</p> <ul> <li>In this way one can decide how much debug info is needed (see Unix rule of     silence)<ul> <li>E.g., when there is a bug one can run with <code>-v DEBUG</code> and see what's     happening right before the bug</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#logging_level","title":"Logging level","text":"<ul> <li>Use <code>_LOG.warning</code> for messages to the final user related to something     unexpected where the code is making a decision that might be controversial<ul> <li>E.g., processing a dir that is supposed to contain only <code>.csv</code> files the     code finds a non-<code>.csv</code> file and decides to skip it, instead of breaking</li> </ul> </li> <li>Use <code>_LOG.info</code> to communicate to the final user, e.g.,<ul> <li>When the script is started</li> <li>Where the script is saving its results</li> <li>A progress bar indicating the amount of work completed</li> </ul> </li> <li>Use <code>_LOG.debug</code> to communicate information related to the internal behavior     of code<ul> <li>Do not pollute the output with information a regular user does not care     about</li> </ul> </li> <li>Make sure the script prints when the work is terminated, e.g., \"DONE\" or     \"Results written to ...\"<ul> <li>This is useful to indicate that the script did not die in the middle:     sometimes this happens silently and it is reported only from the OS return     code</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#how_to_pick_the_level_for_a_logging_statement","title":"How to pick the level for a logging statement","text":"<ul> <li>If all the debug info was printed at <code>INFO</code> level, the output will be too slow     by default</li> <li>So we separate what needs to be always printed (i.e., <code>INFO</code>) and what is     needed only if there is a problem to debug (i.e., <code>DEBUG</code>)<ul> <li>Only who writes the code should decide what is <code>DEBUG</code>, since they know what     is needed to debug</li> <li>In fact many loggers use multiple levels of debugging level depending of how     much detailed debugging info are needed</li> </ul> </li> <li><code>logging</code> has ways to enable logging on a per module basis<ul> <li>So in prod mode you need to know which part you want to debug, since     printing everything at <code>INFO</code> level is not possible</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#use_positional_args_when_logging","title":"Use positional args when logging","text":"<ul> <li>Bad <code>python _LOG.debug(\"cmd=%s %s %s\" % (cmd1, cmd2, cmd3)) _LOG.debug(\"cmd=%s %s %s\".format(cmd1, cmd2, cmd3)) _LOG.debug(\"cmd={cmd1} {cmd2} {cmd3}\")</code></li> <li>Good <code>python _LOG.debug(\"cmd=%s %s %s\", cmd1, cmd2, cmd3)</code></li> <li>All the statements are equivalent from the functional point of view</li> <li>The reason is that in the second case the string is not built unless the     logging is actually performed, which limits time overhead from logging</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#exceptions_dont_allow_positional_args","title":"Exceptions don't allow positional args","text":"<ul> <li>For some reason people tend to believe that using the <code>logging</code> / <code>dassert</code>     approach of positional param to exceptions<ul> <li>Bad (use positional args)     <code>python raise ValueError(\"Invalid server_name='%s'\", server_name)</code></li> <li>Good (use string interpolation)     <code>python raise ValueError(\"Invalid server_name='%s'\" % server_name)</code></li> <li>Best (use string format)     <code>python raise ValueError(f\"Invalid server_name='{server_name}'\")</code></li> </ul> </li> <li>The constructor of an exception accepts a string</li> <li>Using the string f-format is best since<ul> <li>It's more readable</li> <li>There is little time overhead since if you get to the exception probably the     code is going to terminate, and it's not in a hot loop</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#report_warnings","title":"Report warnings","text":"<ul> <li>If there is a something that is suspicious but you don't feel like it's     worthwhile to assert, report a warning with:     <code>python _LOG.warning(...)</code></li> <li>If you know that if there is a warning then there are going to be many many     warnings<ul> <li>Print the first warning</li> <li>Send the rest to <code>warnings.log</code></li> <li>At the end of the run, reports \"there are warnings in warnings.log\"</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#assertions","title":"Assertions","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#validate_values_before_an_assignment","title":"Validate values before an assignment","text":"<ul> <li>We consider this as an extension of a pre-condition (\"only assign values that     are correct\") rather than a postcondition</li> <li>Often is more compact since it doesn't have reference to <code>self</code><ul> <li>Bad <code>python self._tau = tau hdbg.dassert_lte(self._tau, 0)</code></li> <li>Good <code>python hdbg.dassert_lte(tau, 0) self._tau = tau</code></li> <li> <p>Exceptions</p> <p>When we handle a default assignment, it's more natural to implement a post-condition:</p> <p><code>python col_rename_func = col_rename_func or (lambda x: x) hdbg.dassert_isinstance(col_rename_func, collections.Callable)</code></p> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#encode_the_assumptions_using_assertions","title":"Encode the assumptions using assertions","text":"<ul> <li>If your code makes an assumption don't just write a comment, but implement an     assertion so the code can't be executed if the assertion is not verified     (instead of failing silently)     <code>python hdbg.dassert_lt(start_date, end_date)</code></li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#use_positional_args_when_asserting","title":"Use positional args when asserting","text":"<ul> <li><code>dassert_*</code> is modeled after logging so for the same reasons one should use     positional args</li> <li>Bad <code>python hdbg.dassert_eq(a, 1, \"No info for %s\" % method)</code>     Good     <code>python hdbg.dassert_eq(a, 1, \"No info for %s\", method)</code></li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#report_as_much_information_as_possible_in_an_assertion","title":"Report as much information as possible in an assertion","text":"<ul> <li>When using a <code>dassert_*</code> you want to give to the user as much information as     possible to fix the problem<ul> <li>E.g., if you get an assertion after 8 hours of computation you don't want to     have to add some logging and run for 8 hours to just know what happened</li> </ul> </li> <li>A <code>dassert_*</code> typically prints as much info as possible, but it can't report     information that is not visible to it:<ul> <li>Bad <code>python hdbg.dassert(string.startswith(\"hello\"))</code><ul> <li>You don't know what is value of <code>string</code> is</li> </ul> </li> <li>Good <code>python hdbg.dassert(string.startswith(\"hello\"), \"string='%s'\", string)</code><ul> <li>Note that often is useful to add <code>'</code> (single quotation mark) to fight     pesky spaces that make the value unclear, or to make the error as readable     as possible</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#imports","title":"Imports","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#dont_use_evil_import","title":"Don't use evil <code>import *</code>","text":"<ul> <li>Do not use in notebooks or code the evil <code>import *</code><ul> <li>Bad <code>python from helpers.hsql import *</code></li> <li>Good <code>python import helpers.hsql as hsql</code></li> </ul> </li> <li>The <code>from ... import *</code>:<ul> <li>Pollutes the namespace with the symbols and spreads over everywhere, making     it painful to clean up</li> <li>Obscures where each function is coming from, removing the context that comes     with the namespace</li> <li>Is evil in many other ways</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#cleaning_up_the_evil_import","title":"Cleaning up the evil <code>import *</code>","text":"<ul> <li>To clean up the mess you can:<ul> <li>For notebooks<ul> <li>Find &amp; replace (e.g., using jupytext and Pycharm)</li> <li>Change the import and run one cell at the time</li> </ul> </li> <li>For code<ul> <li>Change the import and use Linter on file to find all the problematic spots</li> </ul> </li> </ul> </li> <li>One of the few spots where the evil <code>import *</code> is ok is in the <code>__init__.py</code>     to tweak the path of symbols exported by a library<ul> <li>This is an advanced topic and you should rarely use it</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#avoid_from_import","title":"Avoid <code>from ... import ...</code>","text":"<ul> <li>Import should always start from <code>import</code>:     <code>python import library as short_name import library.sublibrary as short_name</code></li> <li>This rule applies to imports of third party libraries and our library</li> <li>Because of this rule we have to always specify a short import of a parent lib     before every code object that does not belong to the file:<ul> <li>Bad <code>python from helpers.hsql import get_connection, get_connection_from_env_vars, \\     DBConnection, wait_connection_from_db, execute_insert_query</code></li> <li>Good <code>python import helpers.hsql as hsql ... ... hsql.get_connection()</code></li> </ul> </li> <li>The problem with the <code>from ... import ...</code> is that it:<ul> <li>Creates lots of maintenance effort<ul> <li>E.g., anytime you want a new function you need to update the import     statement</li> </ul> </li> <li>Creates potential collisions of the same name<ul> <li>E.g., lots of modules have a <code>read_data()</code> function</li> </ul> </li> <li>Impairs debugging<ul> <li>Importing directly in the namespace loses information about the module</li> <li>E.g.,<code>read_documents()</code> is not clear: what documents?</li> <li><code>np.read_documents()</code> at least gives information of which packages is it     coming from and enables us to track it down to the code</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#exceptions_to_the_import_style","title":"Exceptions to the import style","text":"<ul> <li>We try to minimize the exceptions to this rule to avoid to keep this rule     simple, rather than discussing about</li> <li>The current agreed upon exceptions are:<ul> <li>For <code>typing</code> it is ok to do:     <code>python from typing import Iterable, List</code>     in order to avoid typing everywhere, since we want to use type hints as much     as possible</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#always_import_with_a_full_path_from_the_root_of_the_repo_submodule","title":"Always import with a full path from the root of the repo / submodule","text":"<ul> <li>Bad <code>python import exchange_class</code></li> <li>Good <code>python import datapull.ccxt.data.extract.exchange_class</code><ul> <li>In this way your code can run without depending upon your current dir</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#baptizing_module_import","title":"Baptizing module import","text":"<ul> <li> <p>Each module that can be imported should have a docstring at the very beginning     (before any code) describing how it should be imported</p> <p>```python \"\"\" Import as:</p> </li> </ul> <p>import datapull.ccxt.data.client.ccxt_clients as imvcdccccl \"\"\"     ```</p> <ul> <li>The import abbreviations are called 'short imports' and usually consist of 7-9     first letters of all the words that comprise path to file</li> <li>DO NOT simply give a random short import name<ul> <li>Run Linter to generate short import for a file automatically</li> </ul> </li> <li>For some most files we specify short imorts by hand so thay may contain less     symbols, e.g., <code>hdbg</code></li> <li>The goal is to have always the same imports so it's easy to move code around,     without collisions</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#examples_of_imports","title":"Examples of imports","text":"<ul> <li>Example 1<ul> <li>Bad <code>python from datapull.ccxt.data.client import ccxt_clients as ccxtcl</code></li> <li>Good <code>python import datapull.ccxt.data.client.ccxt_clients as imvcdccccl</code></li> </ul> </li> <li>Example 2<ul> <li>Bad <code>python from edgar.shared import headers_extractor as he</code></li> <li>Good <code>python import edgar.shared.headers_extractor as eshheext</code></li> </ul> </li> <li>Example 3<ul> <li>Bad <code>python from helpers import hdbg</code></li> <li>Good <code>python import helpers.hdbg as hdbg</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#scripts","title":"Scripts","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#use_python_and_not_bash_for_scripting","title":"Use Python and not bash for scripting","text":"<ul> <li>We prefer to use Python instead of bash scripts with very few exceptions<ul> <li>E.g., scripts that need to modify the environment by setting env vars, like     <code>setenv.sh</code></li> </ul> </li> <li>The problem with bash scripts is that it's too easy to put together a sequence     of commands to automate a workflow</li> <li>Quickly things always become more complicated than what you thought, e.g.,<ul> <li>You might want to interrupt if one command in the script fails</li> <li>You want to use command line options</li> <li>You want to use logging to see what's going on inside the script</li> <li>You want to do a loop with a regex check inside</li> </ul> </li> <li>Thus you need to use the more complex features of bash scripting and bash     scripting is absolutely horrible, much worse than perl (e.g., just think of     <code>if [ ... ]</code> vs <code>if [[ ... ]]</code>)</li> <li>Our approach is to make simple to create scripts in Python that are equivalent     to sequencing shell commands, so that can evolve in complex scripts</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#template_for_a_script","title":"Template for a script","text":"<ul> <li>The ingredients are:<ul> <li><code>/dev_scripts_helpers/coding_tools/script_template.py</code>:     a template to write simple scripts you can copy and modify it</li> <li><code>/helpers/hsystem.py</code>: a set of utilities that make     simple to run shell commands (e.g., capturing their output, breaking on     error or not, tee-ing to file, logging, ...)</li> <li><code>helpers</code> has lots of useful libraries</li> </ul> </li> <li>The official reference for a script is     <code>/dev_scripts_helpers/coding_tools/script_template.py</code><ul> <li>You can copy this file and change it</li> <li>A simple example is:     <code>/dev_scripts_helpers/git/gup.py</code></li> <li>A complex example is:     <code>/dev_scripts_helpers/system_tools/replace_text.py</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#some_useful_patterns","title":"Some useful patterns","text":"<ul> <li>Some useful patterns / idioms that are supported by the framework are:<ul> <li>Incremental mode: you skip an action if its outcome is already present     (e.g., skipping creating a dir, if it already exists and it contains all the     results)</li> <li>Non-incremental mode: clean and execute everything from scratch</li> <li>Dry-run mode: the commands are written to screen instead of being executed</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#use_scripts_and_not_notebooks_for_long-running_jobs","title":"Use scripts and not notebooks for long-running jobs","text":"<ul> <li>We prefer to use scripts to execute code that might take long time (e.g.,     hours) to run, instead of notebooks</li> <li>Pros of script<ul> <li>All the parameters are completely specified by a command line</li> <li>Reproducible and re-runnable</li> </ul> </li> <li>Cons of notebooks<ul> <li>Tend to crash / hang for long jobs</li> <li>Not easy to understand if the notebook is doing progress</li> <li>Not easy to get debug output</li> </ul> </li> <li>Notebooks are designed for interactive computing / debugging and not batch     jobs<ul> <li>You can experiment with notebooks, move the code into a library, and wrap it     in a script</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#follow_the_same_structure","title":"Follow the same structure","text":"<ul> <li> <p>All Python scripts that are meant to be executed directly should:</p> <ol> <li>Be marked as executable files with:  ``` <p>chmod +x foo_bar.py  ```</p> </li> <li>Have the Python code should start with the standard Unix shebang notation:  ```python</li> </ol> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#usrbinenv_python","title":"!/usr/bin/env python","text":"<p><code>`` - This line tells the shell to use the</code>python` defined in the environment - In this way you can execute directly without prepending with \"python\"</p> <ol> <li>Have a:  <code>python if __name__ == \"__main__\":     ...</code></li> <li>Ideally use <code>argparse</code> to have a minimum of customization</li> </ol>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#use_clear_names_for_the_scripts","title":"Use clear names for the scripts","text":"<ul> <li>In general scripts (like functions) should have a name like \"action_verb\".<ul> <li>Bad<ul> <li>Examples of bad script names are <code>timestamp_extractor.py</code> and     <code>timestamp_extractor_v2.py</code><ul> <li>Which timestamp data set are we talking about?</li> <li>What type of timestamps are we extracting?</li> <li>What is the difference about these two scripts?</li> </ul> </li> </ul> </li> </ul> </li> <li>We need to give names to scripts that help people understand what they do and     the context in which they operate</li> <li>We can add a reference to the task that originated the work (to give more     context)<ul> <li>Good<ul> <li>E.g., for a script generating a dataset there should be an (umbrella) bug     for this dataset, that we refer in the bug name, e.g.,     <code>TaskXYZ_edgar_timestamp_dataset_extractor.py</code></li> </ul> </li> </ul> </li> <li>Also where the script is located should give some clue of what is related to</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#functions","title":"Functions","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#avoid_using_non-exclusive_bool_arguments","title":"Avoid using non-exclusive <code>bool</code> arguments","text":"<ul> <li>While a simple <code>True</code>/<code>False</code> switch may suffice for today's needs, very often     more flexibility is eventually needed</li> <li>If more flexibility is needed for a <code>bool</code> argument, you are faced with the     choice:<ul> <li>Adding another parameter (then parameter combinations grow exponentially and     may not all make sense)</li> <li>Changing the parameter type to something else</li> </ul> </li> <li>Either way, you have to change the function interface</li> <li>To maintain flexibility from the start, opt for a <code>str</code> parameter \"mode\",     which is allowed to take a small well-defined set of values.</li> <li>If an implicit default is desirable, consider making the default value of the     parameter <code>None</code>. This is only a good route if the default operation is     non-controversial / intuitively obvious.</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#try_to_make_functions_work_on_multiple_types","title":"Try to make functions work on multiple types","text":"<ul> <li>We encourage implementing functions that can work on multiple related types:<ul> <li>Bad: implement <code>demean_series()</code>, <code>demean_dataframe()</code></li> <li>Good: implement a function <code>demean(obj)</code> that can work with <code>pd.Series</code>     and <code>pd.DataFrame</code><ul> <li>One convention is to call <code>obj</code> the variable whose type is not known until     run-time</li> </ul> </li> </ul> </li> <li>In this way we take full advantage of duck typing to achieve something similar     to C++ function overloading (actually even more expressive)</li> <li>Try to return the same type of the input, if possible<ul> <li>E.g., the function called on a <code>pd.Series</code> returns a <code>pd.Series</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#avoid_hard-wired_column_name_dependencies","title":"Avoid hard-wired column name dependencies","text":"<ul> <li>When working with dataframes, we often want need handle certain columns     differently, or perform an operation on a strict subset of columns</li> <li>In these cases, it is tempting to assume that the special columns will have     specific names, e.g., <code>\"datetime\"</code></li> <li>The problem is that column names are<ul> <li>Rarely obvious (e.g., compare <code>\"datetime\"</code> vs <code>\"timestamp\"</code> vs <code>\"Datetime\"</code>)</li> <li>Tied to specific use cases<ul> <li>The function you are writing may be written for a specific use case today,     but what if it is more general</li> <li>If someone wants to reuse your function in a different setting where     different column names make sense, why should they have to conform to your     specific use case's needs?</li> </ul> </li> <li>May overwrite existing column names<ul> <li>For example, you may decided to call a column <code>\"output\"</code>, but what if the     dataframe already has a column with that name?</li> </ul> </li> </ul> </li> <li>To get around this, allow the caller to communicate to the function the names     of any special columns<ul> <li>Good <code>python def func(datetime_col: str):     ...</code></li> </ul> </li> <li>Make sure that you require column names only if they are actually used by the     function</li> <li>If you must use hard-write column names internally or for some application,     define the column name in the library file as a global variable, like     <code>python DATETIME_COL = \"datetime\"</code><ul> <li>Users of the library can now access the column name through imports</li> <li>This prevents hidden column name dependencies from spreading like a virus     throughout the codebase</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#single_exit_point_from_a_function","title":"Single exit point from a function","text":"<ul> <li>Consider the following Bad function     <code>python def _get_zero_element(list_: List):             if not list_:                     return None             else:                     return list_[0]</code><ul> <li>Linter message is     <code>im.kibot/utils.py:394: [R1705(no-else-return), ExpiryContractMapper.extract_contract_expiry] Unnecessary \"else\" after \"return\"     [pylint]</code></li> </ul> </li> <li>Try to have a single exit point from a function, since this guarantees that     the return value is always the same</li> <li>In general returning different data structures from the same function (e.g., a     list in one case and a float in another) is indication of bad design<ul> <li>There are exceptions like a function that works on different types (e.g.,     accepts a dataframe or a series and then returns a dataframe or a series,     but the input and output is the same)</li> <li>Returning different types (e.g., float and string) is also bad</li> <li>Returning a type or <code>None</code> is typically ok</li> </ul> </li> <li>Try to return values that are consistent so that the client doesn't have to     switch statement, using <code>isinstance(...)</code><ul> <li>E.g., return a <code>float</code> and if the value can't be computed return <code>np.nan</code>     (instead of <code>None</code>) so that the client can use the return value in a uniform     way</li> </ul> </li> <li>Function examples with single exit point     <code>python def _get_zero_element(list_: List):         if not list_:                 ret = np.nan         else:                 ret = list_[0]         return ret</code>     or     <code>python def _get_zero_element(list_: List):         ret = np.nan if not list_ else list_[0]         return ret</code></li> <li>However in rare cases it is OK to have functions like:     <code>python def ...(...):         # Handle simple cases.         ...         if ...:                 return         # lots of code         ...         return</code></li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#order_of_function_parameters","title":"Order of function parameters","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#problem","title":"Problem","text":"<ul> <li>We want to have a standard, simple, and logical order for specifying the     arguments of a function</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#decision","title":"Decision","text":"<ul> <li>The preferred order is:<ul> <li>Input parameters</li> <li>Output parameters</li> <li>In-out parameters</li> <li>Default parameters</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#consistency_of_ordering_of_function_parameters","title":"Consistency of ordering of function parameters","text":"<ul> <li>Try to:<ul> <li>Keep related variables close to each other</li> <li>Keep the order of parameters similar across functions that have similar     interface</li> </ul> </li> <li>Enforcing these rules is based on best effort</li> <li>Pycharm is helpful when changing order of parameters</li> <li>Use Linter to check consistency of types between function definition and     invocation</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#style_for_default_parameter","title":"Style for default parameter","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#problem_1","title":"Problem","text":"<ul> <li>How to assign default parameters in a function to make them clear and     distinguishable?</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#decision_1","title":"Decision","text":"<ul> <li>We make all the default parameters keyword-only<ul> <li>This means that we should always specify default parameters using a keyword</li> <li>When building a function, always put default parameters after <code>*</code></li> </ul> </li> <li>It's ok to use a default parameter in the interface as long as it is a Python     scalar (which is immutable by definition)<ul> <li>Good <code>python def function( value: int = 5, *, dir_name: str = \"hello_world\", ):</code></li> </ul> </li> <li>You should not use list, maps, objects, etc. as the default value but pass     <code>None</code> and then initialize the default param inside the function<ul> <li>Bad <code>python def function( *, obj: Object = Object(), list_: List[int] = [], ):</code></li> <li>Good <code>python def function( *, obj: Optional[Object] = None, list_: Optional[List[int]] = None, ): if obj is None:     obj = Object() if list_ is None:     list_ = []</code></li> </ul> </li> <li>We use a <code>None</code> default value when a function needs to be wrapped and the     default parameter needs to be propagated<ul> <li> <p>Good</p> <p>```python def function1(     ...,     *,     dir_name: Optional[str] = None, ):     dir_name = dir_name or \"/very_long_path\"</p> </li> </ul> </li> </ul> <p>def function2(     ...,     *,     dir_name: Optional[str] = None, ):     function1(..., dir_name=dir_name)         ```</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#rationale","title":"Rationale","text":"<ul> <li>Pros of the Good vs Bad style<ul> <li>When you wrap multiple functions, each function needs to propagate the     default parameters, which:<ul> <li>Violates DRY; and</li> <li>Adds maintenance burden (if you change the innermost default parameter,     you need to change all of them!)</li> <li>With the proposed approach, all the functions use <code>None</code>, until the     innermost function resolves the parameters to the default values</li> </ul> </li> <li>The interface is cleaner</li> <li>Implementation details are hidden (e.g., why should the caller know what is     the default path?)</li> <li>Mutable parameters can not be passed through (see     here))</li> </ul> </li> <li>Cons:<ul> <li>One needs to add <code>Optional</code> to the type hint</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#calling_functions_with_default_parameters","title":"Calling functions with default parameters","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#problem_2","title":"Problem","text":"<ul> <li>You have a function     <code>python def func(     task_name : str,     dataset_dir : str,     *,     clobber : bool = clobber, ): ...</code></li> <li>How should it be invoked?</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#decision_2","title":"Decision","text":"<ul> <li>We prefer to<ul> <li>Assign directly the positional parameters</li> <li>Bind explicitly the parameters with a default value using their name</li> <li>Do not put actual parameter values to the function call but specify them     right before</li> <li>Bad <code>python func(\"some_task_name\", \"/dir/subdir\", clobber=False)</code></li> <li>Good <code>python task_name = \"some_task_name\" dataset_dir = \"/dir/subdir\" clobber = False func(task_name, dataset_dir, clobber=clobber)</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#rationale_1","title":"Rationale","text":"<ul> <li>Pros of Good vs Bad style<ul> <li>If a new parameter with a default value is added to the function <code>func</code>     before <code>clobber</code>:<ul> <li>The Good idiom doesn't need to be changed</li> <li>All instances of the Bad idiom need to be updated<ul> <li>The Bad idiom might keep working but with silent failures</li> <li>Of course <code>mypy</code> and <code>Pycharm</code> might point this out</li> </ul> </li> </ul> </li> <li>The Good style highlights which default parameters are being overwritten,     by using the name of the parameter<ul> <li>Overwriting a default parameter is an exceptional situation that should be     explicitly commented</li> </ul> </li> </ul> </li> <li>Cons:<ul> <li>None</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#dont_repeat_non-default_parameters","title":"Don't repeat non-default parameters","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#problem_3","title":"Problem","text":"<ul> <li>Given a function with the following interface:     <code>python def mult_and_sum(multiplier_1, multiplier_2, sum_):         return multiplier_1 * multiplier_2 + sum_</code>     how to invoke it?</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#decision_3","title":"Decision","text":"<ul> <li>Positional arguments are not default, so not keyword-only for consistency<ul> <li>Bad <code>python a = 1 b = 2 c = 3 mult_and_sum(multiplier_1=a,          multiplier_2=b,          sum_=c)</code></li> <li>Good <code>python a = 1 b = 2 c = 3 mult_and_sum(a, b, c)</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#rationale_2","title":"Rationale","text":"<ul> <li>Pros of Good vs Bad<ul> <li>Non-default parameters in Python require all the successive parameters to be     name-assigned<ul> <li>This causes maintenance burden</li> </ul> </li> <li>The Bad approach is in contrast with our rule for the default parameters<ul> <li>We want to highlight which parameters are overriding the default</li> </ul> </li> <li>The Bad approach in practice requires all positional parameters to be     assigned explicitly causing:<ul> <li>Repetition in violation of DRY (e.g., you need to repeat the same     parameter everywhere); and</li> <li>Maintainance burden (e.g., if you change the name of a function parameter     you need to change all the invocations)</li> </ul> </li> <li>The Bad style is a convention used in no language (e.g., C, C++, Java)<ul> <li>All languages allow binding by parameter position</li> <li>Only some languages allow binding by parameter name</li> </ul> </li> <li>The Bad makes the code very wide, creating problems with our 80 columns     rule</li> </ul> </li> <li>Cons of Good vs Bad<ul> <li>One could argue that the Bad form is clearer<ul> <li>IMO the problem is in the names of the variables, which are uninformative,     e.g., a better naming achieves the same goal of clarity     <code>python mul1 = 1 mul2 = 2 sum_ = 3 mult_and_sum(mul1, mul2, sum_)</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#writing_clear_beautiful_code","title":"Writing clear beautiful code","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#keep_related_code_close","title":"Keep related code close","text":"<ul> <li>E.g., keep code that computes data close to the code that uses it.</li> <li>This holds also for notebooks: do not compute all the data structure and then     analyze them.</li> <li>It's better to keep the section that \"reads data\" close to the section that     \"processes it\". In this way it's easier to see \"blocks\" of code that are     dependent from each other, and run only a cluster of cells.</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#order_functions_in_topological_order","title":"Order functions in topological order","text":"<ul> <li>Order functions / classes in topological order so that the ones at the top of     the files are the \"innermost\" and the ones at the end of the files are the     \"outermost\"<ul> <li>In this way, reading the code top to bottom one should not find a forward     reference that requires skipping back and forth</li> </ul> </li> <li>Linter reorders functions and classes in the topological order so make sure     you run it after adding new ones</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#distinguish_public_and_private_functions","title":"Distinguish public and private functions","text":"<ul> <li>The public functions <code>foo_bar()</code> (not starting with <code>_</code>) are the ones that     make up the interface of a module and that are called from other modules and     from notebooks</li> <li>Use private functions like <code>_foo_bar()</code> when a function is a helper of another     private or public function</li> <li>Also follow the \"keep related code close\" close by keeping the private     functions close to the functions (private or public) that are using them</li> <li>Some references:<ul> <li>StackOverflow</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#keep_public_functions_organized_in_a_logical_order","title":"Keep public functions organized in a logical order","text":"<ul> <li>Keep the public functions in an order related to the use representing the     typical flow of use, e.g.,<ul> <li>Common functions, used by all other functions</li> <li>Read data</li> <li>Process data</li> <li>Save data</li> </ul> </li> <li>You can use banners to separate layers of the code. Use the banner long 80     cols (e.g., I have a vim macro to create banners that always look the same)     and be consistent with empty lines before / empty and so on.</li> <li> <p>The banner is a way of saying \"all these functions belong together\".</p> <p>```python</p> </li> </ul> <p>def _helper1_to_func1(): ... def _helper2_to_func1(): ... def func1_read_data1():         _helper1_to_func1()         ...         _helper2_to_func2()</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#_3","title":"All.coding style.how to guide","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#read_data","title":"Read data.","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#_4","title":"All.coding style.how to guide","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#_5","title":"All.coding style.how to guide","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#process_data","title":"Process data.","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#_6","title":"All.coding style.how to guide","text":"<p>...</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#_7","title":"All.coding style.how to guide","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#save_data","title":"Save data.","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#_8","title":"All.coding style.how to guide","text":"<p>...     ```</p> <ul> <li>Ideally each section of code should use only sections above, and be used by     sections below (aka \"Unix layer approach\").</li> <li>If you find yourself using too many banners this is in indication that code     might need to be split into different classes or files<ul> <li>Although we don't have agreed upon rules, it might be ok to have large files     as long as they are well organized. E.g., in <code>pandas</code> code base, all the     code for DataFrame is in a single file long many thousands of lines (!), but     it is nicely separated in sections that make easy to navigate the code</li> <li>Too many files can become problematic, since one needs to start jumping     across many files: in other words it is possible to organize the code too     much (e.g. what if each function is in a single module?)</li> <li>Let's try to find the right balance.</li> </ul> </li> <li>It might be a good idea to use classes to split the code, but also OOP can     have a dark side<ul> <li>E.g., using OOP only to reorganize the code instead of introducing     \"concepts\"</li> <li>IMO the worst issue is that they don't play super-well with Jupyter     autoreload</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#do_not_make_tiny_wrappers","title":"Do not make tiny wrappers","text":"<ul> <li>Examples of horrible functions:<ul> <li>How many characters did we really save? If typing is a problem, learn to     touch type.     <code>python def is_exists(path: str) -&gt; None:     return os.path.exists(path)</code>     or     <code>python def make_dirs(path: str) -&gt; List[str]:     os.makedirs(path)</code></li> <li>This one can be simply replaced by <code>os.path.dirname</code> <code>python def folder_name(f_name: str) -&gt; str:     if f_name[-1] != \"/\":             return f_name + \"/\"     return f_name</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#regex","title":"Regex","text":"<ul> <li>The rule of thumb is to compile a regex expression, e.g.,     <code>python backslash_regex = re.compile(r\"\\\\\")</code>     only if it's called more than once, otherwise the overhead of compilation and     creating another var is not justified</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#do_not_introduce_another_concept_unless_really_needed","title":"Do not introduce another \"concept\" unless really needed","text":"<ul> <li>We want to introduce degrees of freedom and indirection only when we think     this can be useful to make the code easy to maintain, read, and expand.</li> <li>If we add degrees of freedom everywhere just because we think that at some     point in the future this might be useful, then there is very little advantage     and large overhead.</li> <li>Introducing a new variable, function, class introduces a new concept that one     needs to keep in mind. People that read the code, needs to go back and forth     in the code to see what each concept means.</li> <li>Think about the trade-offs and be consistent.</li> <li>Example 1     <code>python def fancy_print(txt):         print \"fancy: \", txt</code><ul> <li>Then people that change the code need to be aware that there is a function     that prints in a special way. The only reason to add this shallow wrapper is     that, in the future, we believe we want to change all these calls in the     code.</li> </ul> </li> <li>Example 2     <code>python SNAPSHOT_ID = \"SnapshotId\"</code><ul> <li>Another example is parametrizing a value used in a single function.</li> <li>If multiple functions need to use the same value, then this practice can be     a good idea. If there is a single function using this, one should at least     keep it local to the function.</li> <li>Still note that introducing a new concept can also create confusion. What if     we need to change the code to:     <code>python SNAPSHOT_ID = \"TigerId\"</code>     then the variable and its value are in contrast.</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#return_none_or_keep_one_type","title":"Return <code>None</code> or keep one type","text":"<ul> <li>Functions that return different types can make things complicated downstream,     since the callers need to be aware of all of it and handle different cases.     This also complicates the docstring, since one needs to explicitly explain     what the special values mean, all the types and so on.</li> <li>In general returning multiple types is an indication that there is a problem.</li> <li>Of course this is a trade-off between flexibility and making the code robust     and easy to understand, e.g.,</li> <li>In the following example it is better to either return <code>None</code> (to clarify that     something special happened) or an empty dataframe <code>pd.DataFrame(None)</code> to     allow the caller code being indifferent to what is returned.<ul> <li>Bad <code>python if \"Tags\" not in df.columns:     df[\"Name\"] = np.nan else:     df[\"Name\"] = df[\"Tags\"].apply(extract_name)</code></li> <li>Good <code>python if \"Tags\" not in df.columns:     df[\"Name\"] = None else:     df[\"Name\"] = df[\"Tags\"].apply(extract_name)</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#avoid_wall-of-text_functions","title":"Avoid wall-of-text functions","text":"<ul> <li>Bad <code>python def get_timestamp_data(raw_df: pd.DataFrame) -&gt; pd.DataFrame:         timestamp_df = get_raw_timestamp(raw_df)         documents_series = raw_df.progress_apply(he.extract_documents_v2, axis=1)         documents_df = pd.concat(documents_series.values.tolist())         documents_df.set_index(api.cfg.INDEX_COLUMN, drop=True, inplace=True)         documents_df = documents_df[                 documents_df[api.cfg.DOCUMENTS_DOC_TYPE_COL] != \"\"]         types = documents_df.groupby(                 api.cfg.DOCUMENTS_IDX_COL)[api.cfg.DOCUMENTS_DOC_TYPE_COL].unique()         timestamp_df[api.cfg.TIMESTAMP_DOC_TYPES_COL] = types         is_xbrl_series = raw_df[api.cfg.DATA_COLUMN].apply(he.check_xbrl)         timestamp_df[api.cfg.TIMESTAMP_DOC_ISXBRL_COL] = is_xbrl_series         timestamp_df[api.cfg.TIMESTAMP_DOC_EX99_COL] = timestamp_df[                 api.cfg.TIMESTAMP_DOC_TYPES_COL].apply(                         lambda x: any(['ex-99' in t.lower() for t in x]))         timestamp_df = timestamp_df.rename(columns=api.cfg.TIMESTAMP_COLUMN_RENAMES)         return timestamp_df</code><ul> <li>This function is correct but it has few problems (e.g., lack of a docstring,     lots of unclear concepts, abuse of constants).</li> </ul> </li> <li> <p>Good</p> <p>```python def get_timestamp_data(raw_df: pd.DataFrame) -&gt; pd.DataFrame:     \"\"\"     Get data containing timestamp information.</p> <pre><code>:param raw_df: input non-processed data\n:return: timestamp data\n\"\"\"\n# Get data containing raw timestamp information.\ntimestamp_df = get_raw_timestamp(raw_df)\n# Extract the documents with data type information.\ndocuments_series = raw_df.progress_apply(he.extract_documents_v2, axis=1)\ndocuments_df = pd.concat(documents_series.values.tolist())\ndocuments_df.set_index(api.cfg.INDEX_COLUMN, drop=True, inplace=True)\ndocuments_df = documents_df[\n        documents_df[api.cfg.DOCUMENTS_DOC_TYPE_COL] != \"\"\n]\ntypes = documents_df.groupby(\n        api.cfg.DOCUMENTS_IDX_COL\n)[api.cfg.DOCUMENTS_DOC_TYPE_COL].unique()\n# Set columns about types of information contained.\ntimestamp_df[api.cfg.TIMESTAMP_DOC_TYPES_COL] = types\nis_xbrl_series = raw_df[api.cfg.DATA_COLUMN].apply(he.check_xbrl)\ntimestamp_df[api.cfg.TIMESTAMP_DOC_ISXBRL_COL] = is_xbrl_series\ntimestamp_df[api.cfg.TIMESTAMP_DOC_EX99_COL] = timestamp_df[\n        api.cfg.TIMESTAMP_DOC_TYPES_COL\n].apply(lambda x: any([\"ex-99\" in t.lower() for t in x]))\n# Rename columns to canonical representation.\ntimestamp_df = timestamp_df.rename(columns=api.cfg.TIMESTAMP_COLUMN_RENAMES)\nreturn timestamp_df\n</code></pre> <p><code>`` - You should at least split the functions in chunks using</code>#` or even better     comment what each chunk of code does.</p> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#writing_robust_code","title":"Writing robust code","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#dont_let_your_functions_catch_the_default-itis","title":"Don't let your functions catch the default-itis","text":"<ul> <li>Default-itis is a disease of a function that manifests itself by getting too     many default parameters.</li> <li>Default params should be used only for parameters that 99% of the time are     constant.</li> <li>In general we require the caller to be clear and specify all the params.</li> <li>Functions catch defaultitis when the programmer is lazy and wants to change     the behavior of a function without changing all the callers and unit tests.     Resist this urge! <code>grep</code> is friend. Pycharm does this refactoring     automatically.</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#explicitly_bind_default_parameters","title":"Explicitly bind default parameters","text":"<ul> <li>It's best to explicitly bind functions with the default params so that if the     function signature changes, your functions doesn't confuse a default param was     a positional one.<ul> <li>Bad <code>python hdbg.dassert(     args.form or args.form_list,     \"You must specify one of the parameters: --form or --form_list\", )</code></li> <li>Good <code>python hdbg.dassert(     args.form or args.form_list,     msg=\"You must specify one of the parameters: --form or --form_list\", )</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#dont_hardwire_params_in_a_function_call","title":"Don't hardwire params in a function call","text":"<ul> <li>Bad <code>python esa_df = universe.get_esa_universe_mapped(False, True)</code><ul> <li>It is difficult to read and understand without looking for the invoked     function (aka write-only code) and it's brittle since a change in the     function params goes unnoticed.</li> </ul> </li> <li>Good <code>python gvkey = False cik = True esa_df = universe.get_esa_universe_mapped(gvkey, cik)</code><ul> <li>It's better to be explicit (as usual)</li> <li>This solution is robust since it will work as long as gvkey and cik are the     only needed params, which is as much as we can require from the called     function.</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#make_if-elif-else_complete","title":"Make <code>if-elif-else</code> complete","text":"<ul> <li>In general all the <code>if-elif-else</code> statements should to be complete, so that     the code is robust.</li> <li>Bad <code>python hdbg.dassert_in(         frequency,         [\"D\", \"T\"]         \"Only daily ('D') and minutely ('T') frequencies are supported.\", ) if frequency == \"T\":         ... if frequency == \"D\":         ...</code></li> <li>Good <code>python if frequency == \"T\":         ... elif frequency == \"D\":         ... else:         raise ValueError(\"The %s frequency is not supported\" % frequency)</code><ul> <li>This code is robust and correct</li> <li>Still the <code>if-elif-else</code> is enough and the assertion is not needed<ul> <li>DRY here wins: you don't want to have to keep two pieces of code in sync</li> <li>The last line is a catch-all that makes sure even if we modify the     previous</li> </ul> </li> <li>It makes sense to check early only when you want to fail before doing more     work</li> <li>E.g., sanity checking the parameters of a long running function, so that it     doesn't run for 1 hr and then crash because the name of the file is     incorrect</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#add_todos_when_needed","title":"Add TODOs when needed","text":"<ul> <li>When there is something that you know you should have done, but you didn't     have time to do, add a TODO, possibly using your github name e.g.,     ```python</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#todogp","title":"TODO(gp): ...","text":"<pre><code>```\n- In this way it's easy to grep for your TODOs, which becomes complicated when\n    using different names.\n</code></pre> <ul> <li>Be clear on the meaning of TODO<ul> <li>A <code>TODO(Batman): clean this up</code> can be interpreted as<ol> <li>\"Batman suggested to clean this up\"</li> <li>\"Batman should clean this up\"</li> <li>\"Batman has the most context to explain this problem or fix it\"</li> </ol> </li> <li>On the one hand, <code>git blame</code> will report who created the TODO, so the first     meaning is redundant.</li> <li>On the other hand, since we follow a shared ownership of the code, the     second meaning should be quite infrequent. In fact the code has mostly     <code>TODO(*)</code> todos, where <code>*</code> relates to all the team members</li> <li>Given pros and cons, the proposal is to use the first meaning.</li> <li>This is also what Google style guide suggests     here</li> </ul> </li> <li>If the TODO is associated with a Github issue, you can simply put the issue     number and description inside the TODO, e.g.,     ```python</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#todogrisha_handle_missing_tiles_cmtask_1775","title":"TODO(Grisha): \"Handle missing tiles\" CmTask #1775.","text":"<pre><code>```\n</code></pre> <ul> <li>You can create a TODO for somebody else, or you can create a Upsource comment     / review or Github bug, depending on how important the issue is</li> <li>If the TODO is general, e.g., anybody can fix it, then you can avoid to put a     name. This should not be abused since it creates a culture when people don't     take responsibility for their mistakes.</li> <li>You can use P1, P2 to indicate if the issue is critical or not. E.g., P0 is     the default for saying this is important, P1 is more of a \"nice to have\".     ```python</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#todosergey_p1_this_can_be_implemented_in_pandas_using_a_range_generation","title":"TODO(Sergey): P1 This can be implemented in pandas using a range generation.","text":"<pre><code>```\n</code></pre>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#common_python_mistakes","title":"Common Python mistakes","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#vs_is","title":"<code>==</code> vs <code>is</code>","text":"<ul> <li><code>is</code> checks whether two variables point to the same object (aka reference     equality), while <code>==</code> checks if the two pointed objects are equivalent (value     equality).</li> <li>For checking against types like <code>None</code> we want to use <code>is</code>, <code>is not</code><ul> <li>Bad <code>python if var == None:</code></li> <li>Good <code>python if var is None:</code></li> </ul> </li> <li>For checking against values we want to use <code>==</code><ul> <li>Bad <code>python if unit is \"minute\":</code></li> <li>Good <code>python if unit == \"minute\":</code></li> </ul> </li> <li>For more info checks     here</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#type_vs_isinstance","title":"<code>type()</code> vs <code>isinstance()</code>","text":"<ul> <li><code>type(obj) == list</code> is worse since we want to test for reference equality (the     type of object is a list) and not the type of obj is equivalent to a list.</li> <li><code>isinstance</code> caters for inheritance (an instance of a derived class is an     instance of a base class, too), while checking for equality of type does not     (it demands identity of types and rejects instances of subtypes, AKA     subclasses).<ul> <li>Bad <code>python if type(obj) is list:</code></li> <li>Good <code>python if isinstance(obj, list):</code></li> </ul> </li> <li>For more info check     here</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#use_helpers","title":"Use <code>helpers</code>","text":"<ul> <li>The <code>helpers</code> dir contains libraries with a lot of useful code</li> <li>If you find yourself in need of a general-purpose/basic functionality, chances     are, it already exists in one of the <code>helpers</code> libraries. E.g.:<ul> <li>For anything related to filesystem operations, see     <code>/helpers/hio.py</code></li> <li>For constructing assertions, see <code>/helpers/hdbg.py</code></li> <li>For handling printing, see <code>/helpers/hprint.py</code></li> <li>For working with <code>pandas</code> dataframes, see     <code>/helpers/hpandas.py</code></li> <li>Etc etc</li> </ul> </li> <li>Before implementing something, always check first if it is already available     in <code>helpers</code></li> <li>Do not reinvent the wheel</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#unit_tests","title":"Unit tests","text":"<ul> <li>Provide minimal end-to-end unit testing</li> <li>Usually we are happy with<ul> <li>Lightly testing the tricky functions</li> <li>Some end-to-end test to make sure the code is working</li> </ul> </li> <li>Use your common sense<ul> <li>E.g., no reason to test code that will be used only once</li> </ul> </li> <li>To run unit tests in a single file     <code>&gt; pytest datetime_utils_test.py -x -s</code></li> <li>For more information on our testing conventions and guidelines, see     <code>/docs/coding/all.write_unit_tests.how_to_guide.md</code></li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#refactoring","title":"Refactoring","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#when_moving_refactoring_code","title":"When moving / refactoring code","text":"<ul> <li>If you move files, refactor code, move functions around make sure that:<ul> <li>Code and notebook work (e.g., imports and caller of the functions)</li> <li>Documentation is updated (this is difficult, so best effort is enough)</li> </ul> </li> <li> <p>If the change you made is renaming a function:</p> <ul> <li>Check all calls made to that function, but if it is in notebooks, update     ONLY in Master notebooks (name starts from <code>Master_...</code>)</li> <li>Double check that changes are done both in <code>.ipynb</code> and <code>.py</code> versions</li> <li>If you make changes directly in jupyter version, they will be automatically     pulled to its Python copy</li> <li>If you make changes in Python copy of the notebook, you'd have to pull     changes to related jupyter notebook with a script:     <code>&gt; jupytext --to ipynb {notebook name}.py</code></li> </ul> </li> <li> <p>For code find all the places that have been modified     <code>&gt; grep -r \"create_dataframe\" *     edgar/form_4/notebooks/Task252_EDG4_Coverage_of_our_universe_from_Forms4.ipynb:    \"documents, transactions = edu.create_dataframes(\\n\",     edgar/form_4/notebooks/Task313_EDG4_Understand_Form_4_amendments.ipynb:    \"documents, transactions = edu.create_dataframes(\\n\",     edgar/form_4/notebooks/Task193_EDG4_Compare_form4_against_Whale_Wisdom_and_TR.ipynb:    \"documents, transactions, owners, footnotes = edu.create_dataframes(\\n\",</code></p> </li> <li>Or if you use mighty Pycharm, Ctrl + Mouse Left Click (Shows you all places     where this function or variable was used) and try to fix them, at least to     give your best shot at making things work<ul> <li>You can edit the notebooks directly without opening, or open and fix it.</li> </ul> </li> <li>But remember, you must know how to do it without fancy IDE like Pycharm.</li> <li>If it's important code:<ul> <li>Run unit tests</li> <li>Run notebooks</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#write_script_for_renamings","title":"Write script for renamings","text":"<ul> <li>When you need to rename any code object that is being used in many files, use     <code>/dev_scripts_helpers/system_tools/replace_text.py</code>     to write a script that will implement your task<ul> <li>Read the script docstring for detailed information about how to use it</li> </ul> </li> <li>You DO NOT use <code>replace_text.py</code> directly. Instead, create an executable <code>.sh</code>     script that uses <code>replace_text.py</code><ul> <li>Look for examples at <code>dev_scripts_helpers/cleanup_scripts</code></li> <li>Commit the created script to the mentioned folder so then your team members     can use it to implement renaming in other libs</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#architectural_and_design_pattern","title":"Architectural and design pattern","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#research_quality_vs_production_quality","title":"Research quality vs production quality","text":"<ul> <li>Code belonging to top level libraries (e.g., <code>//amp/core</code>, <code>//amp/helpers</code>)     and production (e.g., <code>//.../db</code>, <code>vendors</code>) needs to meet high quality     standards, e.g.,<ul> <li>Well commented</li> <li>Following our style guide</li> <li>Thoroughly reviewed</li> <li>Good design</li> <li>Comprehensive unit tests</li> </ul> </li> <li>Research code in notebook and Python can follow slightly looser standards,     e.g.,<ul> <li>Sprinkled with some TODOs</li> <li>Not perfectly general</li> </ul> </li> <li>The reason is that:<ul> <li>Research code is still evolving and we want to keep the structure flexible</li> <li>We don't want to invest the time in making it perfect if the research     doesn't pan out</li> </ul> </li> <li>Note that research code still needs to be:<ul> <li>Understandable / usable by not authors</li> <li>Well commented</li> <li>Follow the style guide</li> <li>Somehow unit tested</li> </ul> </li> <li>We should be able to raise the quality of a piece of research code to     production quality when that research goes into production</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#always_separate_what_changes_from_what_stays_the_same","title":"Always separate what changes from what stays the same","text":"<ul> <li>In both main code and unit test it's not a good idea to repeat the same code</li> <li>Bad<ul> <li>Copy-paste-modify</li> </ul> </li> <li>Good<ul> <li>Refactor the common part in a function and then change the parameters used     to call the function</li> </ul> </li> <li>Example:<ul> <li>What code is clearer to you, VersionA or VersionB?</li> <li>Can you spot the difference between the 2 pieces of code?<ul> <li> <p>Version A</p> <p>```python stopwords = nlp_ut.read_stopwords_json(_STOPWORDS_PATH) texts = [\"a\", \"an\", \"the\"] stop_words = nlp_ut.get_stopwords(         categories=[\"articles\"], stopwords=stopwords ) actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words) expected_result = [] self.assertEqual(actual_result, expected_result)</p> </li> </ul> </li> </ul> </li> </ul> <p>... texts = [\"do\", \"does\", \"is\", \"am\", \"are\", \"be\", \"been\", \"'s\", \"'m\", \"'re\"] stop_words = nlp_ut.get_stopwords(         categories=[\"auxiliary_verbs\"], stopwords=stopwords, ) actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words) expected_result = [] self.assertEqual(actual_result, expected_result)             ```         - Version B</p> <pre><code>        ```python\n</code></pre> <p>def _helper(texts, categories, expected_result):         stopwords = nlp_ut.read_stopwords_json(_STOPWORDS_PATH)         stop_words = nlp_ut.get_stopwords(                 categories=categories, stopwords=stopwords         )         actual_result = nlp_ut.remove_tokens(texts, stop_words=stop_words)         expected_result = []         self.assertEqual(actual_result, expected_result)</p> <p>texts = [\"a\", \"an\", \"the\"] categories = [\"articles\"] expected_result = [] _helper(texts, categories, expected_result) ...</p> <p>texts = [\"do\", \"does\", \"is\", \"am\", \"are\", \"be\", \"been\", \"'s\", \"'m\", \"'re\"] categories = [\"auxiliary_verbs\"] expected_result = [] helper(texts, categories, expected_result)             ```         - Yes, Version A is _Bad and Version B is Good</p>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#organize_scripts_as_pipelines","title":"Organize scripts as pipelines","text":"<ul> <li>One can organize complex computations in stages of a pipeline<ul> <li>E.g., to parse EDGAR forms<ul> <li>Download -&gt; (raw data) -&gt; header parser -&gt; (pq data) -&gt; XBLR / XML / XLS     parser -&gt; (pq data) -&gt; custom transformation</li> </ul> </li> </ul> </li> <li>One should be able to run the entire pipeline or just a piece<ul> <li>E.g., one can run the header parser from the raw data, save the result to     file, then read this file back, and run the XBLR parser</li> </ul> </li> <li>Ideally one would always prefer to run the pipeline from scratch, but     sometimes the stages are too expensive to compute over and over, so using     chunks of the pipeline is better</li> <li>This can also mixed with the \"incremental mode\", so that if one stage has     already been run and the intermediate data has been generated, that stage is     skipped<ul> <li>Each stage can save files in a <code>tmp_dir/stage_name</code></li> </ul> </li> <li>The code should be organized to allow these different modes of operations, but     there is not always need to be super exhaustive in terms of command line     options<ul> <li>E.g., I implement the various chunks of the pipeline in a library,     separating functions that read / save data after a stage and then assemble     the pieces into a throw-away script where I hardwire the file names and so     on</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#make_filename_unique","title":"Make filename unique","text":"<ul> <li>Problem<ul> <li>We have a lot of structure / boilerplate in our project around RH     hypotheses.<ul> <li>E.g., there are corresponding files for all the RH like:<ul> <li><code>RHxyz/configs.py</code></li> <li><code>RHxyz/pipeline.py</code></li> </ul> </li> </ul> </li> <li>It is not clear if it's better to make filenames completely unique by     repeating the <code>RH</code>, e.g., <code>RH1E_configs.py</code>, or let the directories     disambiguate.</li> <li>Note that we are not referring to other common files like <code>utils.py</code>, which     are made unique by their position in the file system and by the automatic     shortening of the imports.</li> </ul> </li> <li>Decision<ul> <li>Invoking the principle of 'explicit is better than implicit', the proposal     is to repeat the prefix.<ul> <li>Bad: <code>RH1E/configs.py</code></li> <li>Good: <code>RH1E/RH1E_configs.py</code></li> </ul> </li> </ul> </li> <li>Rationale<ul> <li>Pros of the repetition (e.g., <code>RH1E/RH1E_configs.py</code>):<ul> <li>The filename is unique so there is no dependency on where you are</li> <li>Since pytest requires all files to be unique, we need to repeat the prefix     for the test names and the rule is \"always make the names of the files     unique\"</li> <li>We are going to have lots of these files and we want to minimize the risk     of making mistakes</li> </ul> </li> <li>Cons of the repetition:<ul> <li>Stuttering</li> <li>What happens if there are multiple nested dirs? Do we repeat all the     prefixes?<ul> <li>This seems to be an infrequent case</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#incremental_behavior","title":"Incremental behavior","text":"<ul> <li>Often we need to run the same code over and over<ul> <li>E.g., because the code fails on an unexpected point and then we need to     re-run from the beginning</li> </ul> </li> <li>We use options like:     <code>--incremental     --force     --start_date     --end_date     --output_file</code></li> <li>Check existence output file before start function (or a thread when using     parallelism) which handle data of the corresponding period<ul> <li>If <code>--incremental</code> is set and output file already exists then skip the     computation and report<ul> <li><code>_LOG.info(\"Skipping processing file %s as requested\", ...)</code></li> </ul> </li> <li>If <code>--incremental</code> is not set<ul> <li>If output file exists then we issue a <code>_LOG.warning</code> and abort the process</li> <li>If output file exists and param <code>--force</code>, then report a <code>_LOG.warning</code>     and rewrite output file</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#run_end-to-end","title":"Run end-to-end","text":"<ul> <li>Try to run things end-to-end (and from scratch) so we can catch these     unexpected issues and code defensively<ul> <li>E.g., we found out that TR data is malformed sometimes and only running     end-to-end we can catch all the weird cases</li> <li>This also helps with scalability issues, since if takes 1 hr for 1 month of     data and we have 10 years of data is going to take 120 hours (=5 days) to     run on the entire data set</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#think_about_scalability","title":"Think about scalability","text":"<ul> <li>Do experiments to try to understand if a code solution can scale to the     dimension of the data we have to deal with<ul> <li>E.g., inserting data by doing SQL inserts of single rows are not scalable     for pushing 100GB of data</li> </ul> </li> <li>Remember that typically we need to run the same scripts multiple times (e.g.,     for debug and / or production)</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#use_command_line_for_reproducibility","title":"Use command line for reproducibility","text":"<ul> <li>Try to pass params through command line options when possible<ul> <li>In this way a command line contains all the set-up to run an experiment</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#structure_the_code_in_terms_of_filters","title":"Structure the code in terms of filters","text":"<ul> <li>Focus on build a set of \"filters\" split into different functions, rather than     a monolithic flow</li> <li>Organize the code in terms of a sequence of transformations that can be run in     sequence, e.g.,<ol> <li>Create SQL tables</li> <li>Convert json data to csv</li> <li>Normalize tables</li> <li>Load csv files into SQL</li> <li>Sanity check the SQL (e.g., mismatching TR codes, missing dates)</li> <li>Patch up SQL (e.g., inserting missing TR codes and reporting them to us so  we can check with TR)</li> </ol> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#code_style_for_different_languages","title":"Code style for different languages","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#sql","title":"SQL","text":"<ul> <li>You can use the package     https://github.com/andialbrecht/sqlparse     to format SQL queries</li> <li>There is also an on-line version of the same formatter at     https://sqlformat.org</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#conventions_addendum","title":"Conventions (Addendum)","text":""},{"location":"code_guidelines/all.coding_style.how_to_guide.html#be_patient","title":"Be patient","text":"<ul> <li>For some reason talking about conventions makes people defensive and     uncomfortable, sometimes.</li> <li>Conventions are not a matter of being right or wrong, but to consider pros and     cons of different approaches, and make the decision only once instead of     discussing the same problem every time. In this way we can focus on achieving     the Ultimate Goal.</li> <li>If you are unsure or indifferent to a choice, be flexible and let other     persons that seem to be less flexible decide.</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#goal","title":"Goal","text":"<ul> <li>The goal of the conventions is to simplify our job by removing ambiguity</li> <li>There is no right or wrong: that's why it's a convention and not a law of     nature<ul> <li>On the flip-side, if there is a right and wrong, then what we are discussing     probably shouldn't be considered as a convention</li> </ul> </li> <li>We don't want to spend time discussing inconsequential points</li> <li>We don't want reviewers to pick lints instead of focusing on architectural     issues and potential bugs</li> <li>Remove cognitive burden of being distracted by \"this is an annoying lint\" (or     at least perceived lint)</li> <li>Once a convention is stable, we would like to automate enforcing it by Linter<ul> <li>Ideally Linter should fix our mistakes so we don't even have to think about     them, and reviewers don't have to be distracted with pointing out the lints</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#keep_the_rules_simple","title":"Keep the rules simple","text":"<ul> <li>E.g., assume that we accepted the following rules:<ul> <li>Git is capitalized if it refers to the tool and it's not capitalized when it     refers to the command (this is what Git documentation suggests)</li> <li>Python is written capitalized (this is what Python documentation suggests)</li> <li><code>pandas</code> is written lowercase, unless it is a beginning of the line in which     case it's capitalized, but it's better to try to avoid to start a sentence     with it (this is what <code>pandas</code> + English convention seems to suggest)</li> <li>Any other library could suggest a different convention based on the     preference of its author, who tries to finally force people to follow his /     her convention ...)</li> </ul> </li> <li>All these rules require mental energy to be followed and readers will spend     time checking that these rules are enforced, rather than focusing on bugs and     architecture.</li> <li>In this case we want to leverage the ambiguity of \"it's unclear what is the     correct approach\" by simplifying the rule<ul> <li>E.g., every name of tools or library is always capitalized</li> <li>This is simple to remember and automatically enforce</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#allow_turning_off_the_automatic_tools","title":"Allow turning off the automatic tools","text":"<ul> <li>We understand that tools can't always understand the context and the     subtleties of human thoughts, and therefore they yield inevitably to false     positives.</li> <li>Then we always want to permit disabling the automatic checks / fixes e.g., by     using directives in comments or special syntax (e.g., anything in a <code>...</code> or     <code>...</code> block should be left untouched)</li> <li>It can be tricky determining when an exception is really needed and when     overriding the tool becomes a slippery slope for ignoring the rules.</li> <li>Patience and flexibility is advised here.</li> </ul>"},{"location":"code_guidelines/all.coding_style.how_to_guide.html#make_the_spell-checker_happy","title":"Make the spell-checker happy","text":"<ul> <li>The spell-checker is not always right: false positives are often very annoying</li> <li>We prefer to find a way to make the spell-checker happy rather than argue that     the spell-checker is wrong and ignore it</li> <li>The risk with overriding the spell-checker (and any other tool) is that the     decision is not binary anymore correct / not-correct and can't be automated     and requires mental energy to see if the flagged error is real or not.<ul> <li>E.g., <code>insample</code> is flagged as erroneous, so we convert it into <code>in-sample</code>.</li> </ul> </li> <li>The solution for the obvious cases of missing a word (e.g., a technical word)     is to add words to the vocabulary. This still needs to be done by everyone,     until we find a way to centralize the vocabulary.<ul> <li>E.g., untradable is a valid English word, but Pycharm's spell-checker     doesn't recognize it.</li> </ul> </li> <li>TODO(*): Should we add it to the dictionary or write it as \"un-tradable\"?</li> <li>Still we don't want to override the spell-checker when an alternative     lower-cost solution is available. E.g.,<ul> <li><code>in-sample</code> instead of <code>insample</code></li> <li><code>out-of-sample</code> instead of <code>oos</code></li> </ul> </li> <li>We decided that <code>hyper-parameter</code> can be written without hyphen:     <code>hyperparameter</code></li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html","title":"All.coding style guidelines.reference","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#all_style_guide","title":"All Style Guide","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#summary","title":"Summary","text":"<ul> <li>This document contains all the rules we enforce for code and documentation     through the <code>linter</code> and <code>ai_review.py</code></li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#general","title":"General","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#spelling","title":"Spelling","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm","title":"LLM","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter","title":"Linter","text":"<ul> <li>Spell commands in lower case and programs with the first letter in upper case<ul> <li>E.g., <code>git</code> as a command, <code>Git</code> as a program</li> <li>E.g., capitalize the first letter of <code>Python</code></li> </ul> </li> <li>Spell <code>Linter</code> with the first letter in upper case and do not use an article<ul> <li>E.g., <code>Linter</code> instead of <code>the Linter</code></li> </ul> </li> <li>Capitalize <code>JSON</code>, <code>CSV</code>, <code>DB</code> and other abbreviations</li> <li>Represent intervals with <code>[a, b), (a, b], (a, b), [a, b]</code>, not <code>[a, b[</code></li> <li>Write <code>hyperparameter</code> without a hyphen</li> <li>Use <code>Python</code> for scripting and automation tasks</li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#python","title":"Python","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#naming","title":"Naming","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_1","title":"LLM","text":"<ul> <li>Name functions using verbs and verbs/actions<ul> <li>Good: <code>download_data()</code>, <code>process_input()</code>, <code>calculate_sum()</code></li> <li>Good: Python internal functions as <code>__repr__</code>, <code>__init__</code> are valid</li> <li>Good: Functions names like <code>to_dict()</code>, <code>_parse()</code>, <code>_main()</code> are valid</li> </ul> </li> <li>Name classes using nouns<ul> <li>Good: <code>Downloader()</code>, <code>DataProcessor()</code>, <code>User()</code></li> <li>Bad: <code>DownloadStuff()</code>, <code>ProcessData()</code>, <code>UserActions()</code></li> </ul> </li> <li>Name decorators with an adjective or a past tense verb<ul> <li>Good: <code>timed</code>, <code>cached</code>, <code>logged</code></li> <li>Bad: <code>time</code>, <code>cache</code>, <code>log</code></li> </ul> </li> <li>Variable and function names should not reference implementation details, and     things that can change or details that are not important<ul> <li>E.g., the name of a variable should not include its type<ul> <li>Good: <code>embeddings</code></li> <li>Bad: <code>embeddings_list</code></li> <li>Good: <code>data</code></li> <li>Bad: <code>data_dict</code></li> </ul> </li> </ul> </li> <li>Abbreviations in the names of variables and functions should be avoided<ul> <li>Exceptions are the following<ul> <li><code>df</code> for dataframe</li> <li><code>srs</code> for series</li> <li><code>idx</code> for index</li> <li><code>id</code> for identifier</li> <li><code>val</code> for value</li> <li><code>var</code> for variable</li> <li><code>args</code> for arguments and <code>kwargs</code> for keyword arguments</li> <li><code>col</code> for columns and <code>row</code> for rows</li> </ul> </li> </ul> </li> <li>Do not repeat in a function name what is already included in the library name     avoiding \"code stutter\"<ul> <li>E.g., if using a library named <code>math</code>, avoid naming a function     <code>math_calculate()</code><ul> <li>Good: <code>calculate()</code></li> <li>Bad: <code>math_calculate()</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_1","title":"Linter","text":"<ul> <li>Name executable Python scripts using verbs and actions<ul> <li>E.g., <code>download.py</code> and not <code>downloader.py</code></li> </ul> </li> <li>Name non-executable files using nouns<ul> <li>E.g., <code>downloader.py</code></li> </ul> </li> <li>Use <code>dir</code> and not <code>directory</code> or <code>folder</code><ul> <li>E.g., <code>dir_path</code></li> </ul> </li> <li>Use <code>file_name</code> and not <code>filename</code><ul> <li>E.g., <code>file_name</code> for storing the name of a file</li> </ul> </li> <li>Use <code>dir_name</code> and not <code>dirname</code><ul> <li>E.g., <code>dir_name</code> for storing the name of a directory</li> </ul> </li> <li>Use <code>timestamp</code> and not <code>ts</code> or <code>datetime</code><ul> <li>E.g., <code>event_timestamp</code></li> </ul> </li> <li>To refer to the name of a column, use <code>..._col</code> and not <code>..._col_name</code> or     <code>..._column</code><ul> <li>E.g., <code>age_col</code> for a column storing age values</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#docstrings","title":"Docstrings","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_2","title":"LLM","text":"<ul> <li>All functions and methods must have a docstring<ul> <li>Good:     <code>def add(a, b):         \"\"\"         Add two numbers and return the result         \"\"\"         return a + b</code></li> <li>Bad:     <code>def add(a, b):         return a + b</code></li> </ul> </li> <li>The docstring must describe the goal of the function, the interface, and what     the user needs to know to use the function<ul> <li>Good: \"Calculate the sum of two numbers and return the result.\"</li> <li>Good     ```     def get_repository_settings(             repo: github.Repository.Repository,     ) -&gt; Dict[str, Any]:             \"\"\"             Get the current settings of the repository.<pre><code>    :param repo: GitHub repository object\n    :return: dictionary containing repository settings\n    \"\"\"\n</code></pre> <p><code>- When the output is a tuple indent like:</code> :param mode: format of the output:         - <code>list</code>: indents headers to create a nested list         - <code>headers</code>: uses Markdown header syntax (e.g., '#', '##', '###'`) <code>- The docstring must use imperative form, whenever possible     - Good: \"Calculate the sum of two numbers and return the result.\"     - Bad: \"Calculates the sum of two numbers and returns the result.\" - The docstring should not describe implementation details that can be changed     - Good: \"Sort the list of integers in ascending order.\"     - Bad: \"Use the quicksort algorithm to sort the list of integers in ascending order.\" - Follow this example for indentation of parameter descriptions:     - Good</code>python :param param1: a very very long param description that continues into a second line :param param2: a param with two possible values - first value description - second value description that is very long and     continues into a second line <code>- Adding examples (e.g., of input and output) to the docstring is encouraged     - Good</code></p> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#example_usage","title":"Example usage:","text":"<p>result = add_numbers(3, 5)</p>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#the_result_is_8","title":"The result is 8.","text":"<p><code>- References to variables, file paths, functions, classes, etc. should be     wrapped in backticks     - Good: \"The `add_numbers()` function takes two arguments `a` and `b`.\"     - Bad: \"The add_numbers() function takes two arguments a and b.\" - References to values should be wrapped in single ticks     - Good '//' with '# ' - Multi-line representations of data structures (e.g., an output example) should     be wrapped in triple backticks     - Good</code> { \"name\": \"John\", \"age\": 30, \"city\": \"New York\" } ```</p>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_2","title":"Linter","text":"<ul> <li>The first docstring line is followed by a blank line and then, optionally, by     a longer description (possibly on multiple lines) with a more detailed     explanation of what the function does</li> <li>The more detailed description is followed by a blank line and then the param     and return description section in reST style<ul> <li>Use lowercase after <code>:param XYZ: ...</code> / <code>:return:</code> unless the description     starts with a proper noun</li> </ul> </li> <li>Do not mention default values of parameters in parameter descriptions</li> <li>Docstrings should be wrapped in triple quotation marks (<code>\"\"\"</code>)<ul> <li>The opening and closing triple quotation marks should be located on their     own separate lines</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#comments","title":"Comments","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_3","title":"LLM","text":"<ul> <li>Add a comment for every logically distinct chunk of code, spanning 4-5 lines</li> <li> <p>Use comments to separate chunks of code instead of blank lines</p> <ul> <li>Good:     <code>function1()     # Then do something else.     function2()</code></li> <li> <p>Bad:     ```     function1()</p> <p>function2() ```</p> </li> </ul> </li> <li> <p>Do not use inline comments; every comment should be on its own separate line,     before the line it refers to</p> <ul> <li>Good:     <code># Grant access to admin panel access_admin_panel().     if user.is_admin():</code></li> <li>Bad:     <code>if user.is_admin(): # Check if the user is an admin access_admin_panel().</code></li> <li>In <code>if-elif-else</code> statements, the comments are placed underneath each     statement in order to explain the code that belongs to each statement in     particular<ul> <li>Good:     <code>if ...:         # Do this     else:         # Do that</code></li> </ul> </li> </ul> </li> <li>Avoid referring to the type of a variable in the comments<ul> <li>Keeps comments focused on functionality rather than implementation specifics</li> <li>Good: \"Store the user's age for validation.\"</li> <li>Bad: \"Store the user's age as an integer for validation.\"</li> </ul> </li> <li>Do not include implementation details in comments<ul> <li>Describe \"what\" and \"why\" the code does something and not \"how\" the code     does it</li> <li>Ensures comments remain relevant even if the implementation changes</li> </ul> </li> <li>If some code is commented out in a PR, a comment should be added to explain     the reason why<ul> <li>Provides context for future reference and helps other developers understand     the decision</li> <li>E.g., \"This section is commented out due to a known bug that needs fixing\"     or \"Temporarily disabled for performance testing\"</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_3","title":"Linter","text":"<ul> <li>Avoid empty comments and line inside the code when possible</li> <li>Every comment should start with a capital letter</li> <li>Every comment should start with a verb in the imperative form</li> <li>Every comment should end with a period</li> <li>Comments with TODOs should have the format of <code># TODO(username): ...</code></li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#code_implementation","title":"Code Implementation","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_4","title":"LLM","text":"<ul> <li>Encode the assumptions made in the code using assertions and report as much     information as possible in an assertion to make it easy to debug the output<ul> <li>Good:     <code>hdbg.dassert_lt(start_date, end_date,         msg=\"start_date needs to be before end_date\")</code></li> <li>Ensure that assertions provide detailed information for debugging</li> <li>Use assertions to validate input parameters and preconditions</li> </ul> </li> <li> <p>Do not use f-strings in <code>hdbg.dassert()</code>, but use traditional string     formatting methods in assertions</p> <ul> <li>Good:     <code>hdbg.dassert_eq(len(list1), len(list2), \"Lists must be of equal length: %d vs %d\" % (len(list1), len(list2)))</code></li> </ul> </li> <li> <p>Add type hints only to the function definitions, if they are missing.</p> <ul> <li>Good:     <code>def process_data(data, threshold=0.5):             results = []             for item in data:                     if item &gt; threshold:                             results.append(item)             return results</code></li> <li>Bad:     <code>def process_data(data: List[float], threshold: float = 0.5) -&gt; List[float]:             results: List[float] = []             for item in data:                     if item &gt; threshold:                             results.append(item)             return results</code></li> </ul> </li> <li> <p>Avoid complex assignments into if-then-else statements.</p> <ul> <li>Good:     <code>capitalized_parts = []     for w in parts:             if is_first_or_last or w.lower() not in small_words:                     w_out = w.capitalize()             else:                     w_out = w.lower()     capitalized_parts.append(w_out)</code></li> <li> <p>Bad:     <code>capitalized_parts = [             w.capitalize() if is_first_or_last or w.lower() not in small_words else w.lower()             for w in parts     ]</code></p> <p>to:     - Good: <code>if i == 0:         is_first_or_last = True elif i == len(tokens) - 1:         is_first_or_last = True elif i &gt; 0 and not re.search(r'\\w', tokens[i - 1]):         is_first_or_last = True elif i &lt; len(tokens) - 1 and not re.search(r'\\w', tokens[i + 1]):         is_first_or_last = True else:         is_first_or_last = False</code>     - Bad: <code>is_first_or_last = (i == 0 or i == len(tokens) - 1 or                                 (i &gt; 0 and not re.search(r'\\w', tokens[i - 1])) or                                 (i &lt; len(tokens) - 1 and not re.search(r'\\w', tokens[i + 1])))</code></p> </li> </ul> </li> <li> <p>Provide clear and informative error messages in exceptions using f-strings</p> <ul> <li>Good: <code>raise ValueError(f\"Invalid server_name='{server_name}'\")</code></li> <li>Good: <code>raise TypeError(f\"Expected type int, but got {type(var).__name__}\")</code></li> </ul> </li> <li>Use complete <code>if-elif-else</code> statements instead of a sequence of <code>if</code>     statements<ul> <li>Ensure logical flow and clarity in conditional statements</li> <li>Good:     ```python if condition1:</li> </ul> </li> <li>Compile a regex expression only if it's called more than once<ul> <li>Optimize performance by compiling regex expressions that are reused</li> <li>E.g.,     <code>import re     pattern = re.compile(r'\\d+')     if pattern.match(string):         # Do something.</code></li> </ul> </li> <li>Use <code>if var is None</code> to check if <code>var</code> is <code>None</code> instead of <code>if not var</code><ul> <li>Good: <code>if my_variable is None:</code></li> <li>Bad: <code>if not my_variable:</code></li> </ul> </li> <li>Use <code>isinstance()</code> instead of <code>type()</code> to check the type of an object<ul> <li>Good: <code>if isinstance(obj, str):</code></li> <li>Bad: <code>if type(obj) == str:</code></li> </ul> </li> <li>Do not use <code>from ... import ...</code>, unless it is the <code>typing</code> package, e.g.,     <code>from typing import Iterable, List</code><ul> <li>Good: <code>from typing import Dict, Tuple</code></li> <li>Bad: <code>from os import path</code></li> </ul> </li> <li>Always import with a full path from the root of the repo / submodule<ul> <li>Good: <code>import myproject.module.submodule</code></li> <li>Bad: <code>from submodule import my_function</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#execute_block_for_condition1","title":"Execute block for condition1.","text":"<p>... elif condition2:</p>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#execute_block_for_condition2","title":"Execute block for condition2.","text":"<p>... else:</p>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#execute_block_if_none_of_the_above_conditions_are_met_or_raise_an","title":"Execute block if none of the above conditions are met or raise an","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#exception","title":"exception.","text":"<pre><code>```\n</code></pre>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_4","title":"Linter","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#code_design","title":"Code Design","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_5","title":"LLM","text":"<ul> <li>Follow DRY principle (Don't Repeat Yourself):<ul> <li>Factor out common code in a separate function/method</li> <li>Do not copy-and-paste parameter descriptions; instead, write them in only     one function and put a reference to it in the other functions where the same     parameters are used<ul> <li>E.g., \"See <code>func_name()</code> for the param description\"</li> </ul> </li> <li>Avoid redundancy in code logic and comments</li> </ul> </li> <li>Keep public functions in an order representing the typical flow of use:<ul> <li>Common functions, used by all other functions<ul> <li>E.g., utility functions like <code>log_message()</code>, <code>validate_input()</code></li> </ul> </li> <li>Read data<ul> <li>Good: <code>read_csv()</code>, <code>load_json()</code></li> </ul> </li> <li>Process data<ul> <li>Good: <code>clean_data()</code>, <code>transform_data()</code></li> </ul> </li> <li>Save data<ul> <li>Good: <code>write_csv()</code>, <code>export_json()</code></li> </ul> </li> </ul> </li> <li>Ensure that function names are descriptive and convey their purpose</li> <li>Use comments to explain complex logic or calculations</li> <li>Implement error handling to manage exceptions and edge cases</li> <li>Use inheritance or composition to reuse code in object-oriented programming</li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_5","title":"Linter","text":"<ul> <li>Order functions / classes in a topological order so that the ones at the top     of the files are the \"innermost\" and the ones at the end of the files are the     \"outermost\"</li> <li>Use banners to separate large sections of code, e.g.:     ```python</li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#_1","title":"All.coding style guidelines.reference","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#read_data","title":"Read data.","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#_2","title":"All.coding style guidelines.reference","text":"<pre><code>```\n- The text inside the banner should start with a capital letter and end with a\n    period\n</code></pre>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#imports","title":"Imports","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_6","title":"LLM","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_6","title":"Linter","text":"<ul> <li>All imports should be located at the top of the file</li> <li>Each module that can be imported should have a docstring at the very beginning     describing how it should be imported<ul> <li>Linter adds it automatically</li> </ul> </li> <li>No import cycles should be introduced by the changes in the PR</li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#type_annotations","title":"Type Annotations","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_7","title":"LLM","text":"<ul> <li>For type hints use use <code>List</code>, <code>Dict</code>, and <code>Tuple</code> to provide more explicit     type information and help with static type checking<ul> <li>E.g., <code>List[int]</code> instead of <code>list</code></li> <li>E.g., <code>List[str]</code> instead of <code>list</code></li> <li>Use <code>Dict</code> instead of <code>dict</code><ul> <li>E.g., <code>Dict[str, int]</code> instead of <code>dict</code></li> <li>E.g., <code>Dict[int, List[str]]</code> instead of <code>dict</code></li> </ul> </li> <li>Use <code>Tuple</code> instead of <code>tuple</code><ul> <li>E.g., <code>Tuple[int, str]</code> instead of <code>tuple</code></li> <li>E.g., <code>Tuple[str, List[int]]</code> instead of <code>tuple</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_7","title":"Linter","text":"<ul> <li>All functions and methods, including constructors, must have type annotations     for all the parameters and returned structures<ul> <li>Use <code>-&gt; None</code> if a function doesn't return anything</li> <li>The only exception are invoke tasks, i.e. functions with the <code>@task</code>     decorator, they shouldn't have type annotations</li> </ul> </li> <li>Type annotation <code>Any</code> should be avoided, if possible</li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#functions","title":"Functions","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_8","title":"LLM","text":"<ul> <li>Avoid pure functions without side effects, i.e., for the same input arguments,     the returned value should not change (in contrast to functions that rely upon     external state)</li> <li>Functions should not modify the function inputs<ul> <li>E.g., if a function <code>f()</code> accepts a dataframe <code>df</code> as its argument, then     <code>f()</code> will not modify <code>df</code> but make a copy and work on it</li> <li>This ensures that the original data remains unchanged and can be reused</li> </ul> </li> <li>To maintain clarity and consistency in function definitions, use the following     order of parameters in a function declaration:<ul> <li>Input parameters</li> <li>Output parameters</li> <li>In-out parameters</li> <li>Default parameters</li> </ul> </li> <li>Default parameters should be used sparingly and only for parameters that 99%     of the time are constant</li> <li>All the default parameters should be keyword-only<ul> <li>They should be separated from the other parameters by <code>*</code><ul> <li>Good: <code>def example_function(param1: str, *, default_param1: int = 10)</code></li> <li>Bad: <code>def example_function(param1: str, default_param1 : int =10)</code></li> </ul> </li> <li>This ensures that default parameters are always explicitly specified by     name, improving readability</li> </ul> </li> <li>Do not use mutable objects (such as lists, maps, objects) as default value for     functions; instead, pass <code>None</code> and then initialize the default parameter     inside the function<ul> <li>E.g., instead of using a list as a default parameter, use <code>None</code> and     initialize the list inside the function:</li> <li>Good:     <code>def add_item(item: str, *, items: Optional[List[str]]) -&gt; List[str]:         if items is None:             items = []         items.append(item)         return items</code></li> </ul> </li> <li>Do not use a boolean parameter as a switch controlling some function behavior;     instead, use a string parameter <code>mode</code>, which is allowed to take a small     well-defined set of values<ul> <li>Good: <code>def process_data(mode: str = 'fast'):</code> where <code>mode</code> can be <code>'fast'</code>,     <code>'slow'</code>, etc</li> </ul> </li> <li>For functions dealing with dataframes, avoid hard-wired column name     dependencies; instead, allow the caller to pass the column name to the     function as a parameter<ul> <li>E.g., <code>def calculate_average(df: pd.DataFrame, column_name: str):</code></li> </ul> </li> <li>Do not put computations of the output together in a <code>return</code> statement,     instead, compute the output first, assign it to a variable, and then return     this variable<ul> <li>Good     <code>result = compute_value()     return result</code></li> <li>Bad     <code>return compute_value()</code></li> </ul> </li> <li>A function should have a single exit point, i.e., one single line with     <code>return</code><ul> <li>Good:     <code>python def calculate_total(price, tax):     total = price + (price * tax)     return total</code></li> <li>Bad:     <code>python def calculate_total(price, tax):     if price &gt; 0:             return price + (price * tax)     else:             return 0</code></li> </ul> </li> <li>A function should ideally return objects of only one type (or <code>None</code>)</li> <li>When calling a function, assign all the input parameter values to variables on     separate lines and then pass these variables to the function<ul> <li>Good:     <code>param1 = 10     param2 = 11     result = my_function(param1, param2)</code></li> <li>Bad:     <code>result = my_function(10, 11)</code></li> </ul> </li> <li>Explicitly bind default parameters, i.e., specify the parameter name when     calling a function, and do not bind non-default parameters<ul> <li>Good: <code>func(10, 20, param3=30)</code></li> <li>Bad: <code>func(10, 20, 30)</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_8","title":"Linter","text":"<ul> <li>Make a function private (e.g., <code>_foo_bar()</code>) when it is a helper of another     private or public function</li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#scripts","title":"Scripts","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_9","title":"LLM","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_9","title":"Linter","text":"<ul> <li>Use Python and not bash for scripting</li> <li>All Python scripts that are meant to be executed directly should:<ul> <li>Be marked as executable files with <code>&gt; chmod +x foo_bar.py</code></li> <li>Have the standard Unix shebang notation at the top: <code>#!/usr/bin/env python</code></li> <li>Use the following idiom at the bottom:     <code>python if __name__ == \"__main__\":     ...</code></li> <li>Use <code>argparse</code> for argument parsing</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#logging","title":"Logging","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_10","title":"LLM","text":"<ul> <li>Use logging <code>_LOG.debug()</code> and not <code>print()</code> for tracing execution<ul> <li>Good: <code>_LOG.debug(\"value=%s\", value)</code></li> <li>Bad: <code>print(\"value=%s\", value)</code></li> </ul> </li> <li>Use positional args in logging and not inline formatting<ul> <li>Good: <code>_LOG.debug(\"cmd=%s\", cmd1)</code></li> <li>Bad: <code>_LOG.debug(f\"cmd={cmd1}\")</code></li> </ul> </li> <li> <p>Use the following idiom to configure logging:</p> <p>```python import helpers.hdbg as hdbg</p> </li> </ul> <p>_LOG = logging.getLogger(name) ...</p> <p>hdbg.init_logger(verbosity=logging.DEBUG)     ```</p>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_10","title":"Linter","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#misc","title":"Misc","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_11","title":"LLM","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_11","title":"Linter","text":"<ul> <li>If a PR includes renaming a file, variable, parameter, function, class, etc.,     then all the instances and references to it throughout the codebase should be     updated</li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#unit_tests","title":"Unit Tests","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#rules","title":"Rules","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_12","title":"LLM","text":"<ul> <li>A test class should test only one function or class to help understanding test     failures</li> <li>A test method should only test a single case to ensures clarity and precision     in testing<ul> <li>E.g., \"for these inputs the function responds with this output\"</li> </ul> </li> <li>Adhere to the following conventions for naming:<ul> <li>Class <code>TestFooBar</code> tests the class <code>FooBar</code> and its methods<ul> <li><code>TestFooBar.test_method_a</code>, <code>TestFooBar.test_method_b</code> test the methods     <code>FooBar.method_a</code> and <code>FooBar.method_b</code></li> </ul> </li> <li>Class <code>Test_foo_bar</code> tests the function <code>foo_bar()</code><ul> <li>E.g., <code>Test_foo_bar.test_valid_input</code>, <code>Test_foo_bar.test_invalid_input</code>     for different cases / inputs</li> </ul> </li> <li><code>Test_foo_bar.test1</code>, <code>Test_foo_bar.test2</code> for different cases / inputs</li> </ul> </li> <li>A unit test should be independent of all the other unit tests<ul> <li>Ensures that tests do not affect each other and can be run in isolation</li> </ul> </li> <li>If there is a lot of common code across individual test methods, it should be     factored out in a helper method within the test class<ul> <li>Reduces redundancy and improves maintainability of the test code</li> <li>E.g., a <code>setUp</code> method to initialize common test data or configurations</li> </ul> </li> <li>If some code needs to be repeated at the beginning / end of each test method,     it should be moved to <code>set_up_test()</code> / <code>tear_down_test()</code> methods and the     following idiom should be added to the test class:     <code>python @pytest.fixture(autouse=True) def setup_teardown_test(self):         # Run before each test.         self.set_up_test()         yield         # Run after each test.         self.tear_down_test()</code></li> <li>Each test method should have a docstring describing briefly what case is being     tested<ul> <li>E.g., \"Tests the addition of two positive integers.\"</li> <li>E.g., \"Verifies that an exception is raised when dividing by zero.\"</li> </ul> </li> <li>Test methods should have type hint annotations<ul> <li>E.g., <code>def test_addition(self) -&gt; None:</code></li> </ul> </li> <li>Do not create temporary files for tests (e.g., with <code>tempfile</code>) but use     <code>hunittest.TestCase.get_scratch_space()</code> instead</li> <li>If the input to the test is a large piece of code/text, it should be moved to     a separate file in the <code>input</code> dir corresponding to the test<ul> <li>E.g., <code>outcomes/&lt;TestClassName.test_method_name&gt;/input</code> and read through the     function <code>self.get_input_dir()</code> of <code>TestCase</code></li> <li>This approach allows for easy updates and modifications to test inputs     without altering the test code itself</li> </ul> </li> <li>Do not use pickle files for test inputs<ul> <li>Use JSON, YAML, CSV files for test inputs as they are more secure and     human-readable</li> </ul> </li> <li>In every test method separate logically distinct code chunks to prepare the     inputs, run the tests, and check the outputs using comments like below:<ul> <li>E.g.,     <code># Prepare inputs.     input_data = [1, 2, 3]     # Run test.     result = my_function(input_data)     # Check outputs.     self.assert_equal(result, expected_output)</code></li> </ul> </li> <li>Do not use <code>hdbg.dassert</code> in testing but use <code>self.assert*()</code> methods</li> <li>Prefer <code>self.assert_equal()</code> instead of <code>self.assertEqual()</code><ul> <li>Always use actual and then expected value</li> <li>E.g., <code>self.assert_equal(actual, expected)</code></li> </ul> </li> <li>Use strings to compare actual and expected outputs instead of data structures<ul> <li>E.g., use <code>self.assert_equal(str(actual_list), str(expected_list))</code></li> </ul> </li> <li>Use <code>self.check_string()</code> to compare the actual output to a golden output in     the <code>outcomes</code> dir, when the output is large or needs to be modified easily<ul> <li>E.g., <code>self.check_string(actual_output)</code></li> </ul> </li> <li>When testing for an assertion, check that you are getting the exact exception     that is expected     <code># Make sure function raises an error.     with self.assertRaises(AssertionError) as cm:             config_list.configs = configs     act = str(cm.exception)     self.check_string(act, fuzzy_match=True)</code></li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_12","title":"Linter","text":"<ul> <li>Unit tests should be placed in a <code>test_*.py</code> file in the <code>test</code> directory,     close to the library / code it tests<ul> <li>Test file <code>test_file_name.py</code> testing the library <code>file_name.py</code></li> </ul> </li> <li>Every test class should inherit from <code>hunitest.TestCase</code></li> <li>We use <code>pytest</code> as test harness so do not add the following idiom in the     testing file     <code>python if __name__ == \"__main__\":         unittest.main()</code></li> <li>If a unit test is renamed or removed in a PR, the corresponding files in the     <code>outcomes</code> dir should also be renamed or removed</li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#notebooks","title":"Notebooks","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#general_1","title":"General","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_13","title":"LLM","text":"<ul> <li>The code in the notebook should adhere to the same style and formatting     guidelines as the code in libraries and scripts</li> <li>Common or general-purpose code should be factored out in functions and moved     from the notebook to a Python library, which would then be imported in the     notebook<ul> <li>E.g., create a <code>utils.py</code> file for helper functions</li> </ul> </li> <li>Notebook cells should be idempotent, i.e., able to be executed multiple times     without changing their output value<ul> <li>Avoid side effects such as modifying global variables or external states</li> <li>Ensure that cell execution order does not affect the results</li> </ul> </li> <li>If the data is transformed, display a few lines to show the outcome<ul> <li>E.g., <code>df.head(3)</code> to preview the first three rows of a DataFrame</li> </ul> </li> <li>If any data is discarded/filtered, display the percentage of the rows dropped<ul> <li>E.g.,     <code>print(f\"Percentage of rows dropped: {dropped_rows / total_rows * 100:.2f}%\")</code></li> <li>Provides insight into data cleaning and filtering processes</li> </ul> </li> <li>Progress bars should be added where applicable<ul> <li>Use libraries like <code>tqdm</code> to show progress in loops or data processing tasks</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_13","title":"Linter","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#plotting","title":"Plotting","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_14","title":"LLM","text":"<ul> <li>Each plot should have a descriptive title to understand the context of the     plot at a glance<ul> <li>E.g., \"Monthly Sales Data for 2023\" instead of just \"Sales Data\"</li> </ul> </li> <li>Each plot should have axes labels<ul> <li>E.g., label the x-axis as \"Months\" and the y-axis as \"Revenue in USD\"</li> </ul> </li> <li>If there are several multiple data series on the same plot, it should have a     legend</li> <li>In a plotting function, <code>plt.show()</code> should not be added at the end<ul> <li>This allows for further customization or saving of the plot before     displaying</li> <li>E.g., users might want to save the plot using <code>plt.savefig('plot.png')</code>     before showing it</li> </ul> </li> <li>In a plotting function, the <code>ax</code> parameter should be exposed to allow users to     customize the plot further<ul> <li>E.g., users can modify the axes limits or add additional annotations</li> </ul> </li> <li>If a function plots multiple plots, they should be generally plotted in a     single figure<ul> <li>E.g., use <code>plt.subplots()</code> to create a grid of plots within a single figure</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_14","title":"Linter","text":"<ul> <li>The name of a notebook should generally be the same as the branch name, unless     it's a Master notebook</li> <li>All notebooks should have a table of contents<ul> <li>Linter automatically adds and updates the table of contents</li> </ul> </li> <li>At the top of the notebook there should be a Markdown cell <code># Description</code>,     followed by a Markdown cell with an explanation of the notebook's goal, what     it does, etc.</li> <li>Immediately below the description, there should be a Markdown cell     <code># Imports</code>, followed by a code cell importing all the needed libraries<ul> <li>It should include autoreload modules to keep the local code updated in real     time:     <code>python %load_ext autoreload %autoreload 2</code></li> <li>All the imports should be located in a single cell</li> </ul> </li> <li>Below the cell with the imports, there should be a code cell that configures     the logging and notebook style, and reports execution info:     ```python</li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#configure_logger","title":"Configure logger.","text":"<p>hdbg.init_logger(verbosity=logging.INFO) _LOG = logging.getLogger(name)</p>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#print_system_signature","title":"Print system signature.","text":"<p>_LOG.info(\"%s\", henv.get_system_signature()[0])</p>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#configure_the_notebook_style","title":"Configure the notebook style.","text":"<p>hprint.config_notebook()     ``` - The rest of the notebook should be clearly organized using Markdown cells with     headings of different levels - There should be no errors in the executed notebook - Ideally, there should be no warnings in the executed notebook</p>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#jupytext","title":"Jupytext","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_15","title":"LLM","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_15","title":"Linter","text":"<ul> <li>Each notebook must have an accompanying Python file, linked via <code>jupytext</code>,     which contains a synchronized copy of the notebook's code</li> <li>The notebook and its paired Python file should share the same name, differing     only in their file extensions</li> <li>Ensure that the code in the notebook and its paired Python file remains     synchronized at all times</li> <li>If you update or delete the notebook, you must also update or delete its     paired Python file, and vice versa</li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#markdown","title":"Markdown","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#general_2","title":"General","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_16","title":"LLM","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_16","title":"Linter","text":"<ul> <li> <p>Names of documentation files should follow the format     <code>docs/{component}/{audience}.{topic}.{diataxis_tag}.md</code> to help in organizing     and categorizing documentation files effectively where:</p> <ul> <li>The <code>{component}</code> part specifies the part of the project the documentation     is related to</li> <li>The <code>{audience}</code> part indicates who the documentation is intended for</li> <li>The <code>{topic}</code> part describes the subject matter of the documentation</li> <li>The <code>{diataxis_tag}</code> part categorizes the documentation according to the     Di\u00e1taxis framework (e.g., explanation, tutorial)</li> <li>E.g., <code>docs/documentation_meta/all.diataxis.explanation.md</code></li> </ul> </li> <li> <p>All Markdown files should have a table of contents</p> <ul> <li>The linter automatically adds and updates the table of contents</li> </ul> </li> <li> <p>There should be one and only one level 1 heading (with one <code>#</code>) in a Markdown</p> <ul> <li>The level 1 heading:<ul> <li>Should clearly convey the primary topic or purpose of the document</li> <li>Serves as the main title of the document</li> <li>Should be the first line and located above the table of contents</li> </ul> </li> </ul> </li> <li> <p>Wrap file paths, names of variables, functions, and classes in backticks</p> <ul> <li>E.g., <code>file_path</code>, <code>variable_name</code>, <code>function_name()</code>, <code>ClassName</code></li> </ul> </li> <li>Commands should be prepended by <code>&gt;</code><ul> <li>Example     <code>&gt; notes_to_pdf.py \\         --input lectures_source/Lesson5-Theory_Statistical_learning.txt \\         --output Lesson5.pdf \\         --type slides \\         --toc_type navigation \\         --debug_on_error \\         --skip_action cleanup_after</code></li> </ul> </li> <li>Commands should be prepended by <code>docker&gt;</code> if they need to be run inside Docker</li> <li>Avoid using screenshots whenever possible and instead copy-and-paste text with     the right highlighting<ul> <li>E.g., instead of a screenshot of a terminal command, provide the command     text: <code>&gt; ls -la</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#headers","title":"Headers","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_17","title":"LLM","text":"<ul> <li>Do not use bold or italics in headings</li> <li>Use headers so that it's easy to refer to something by link</li> <li>Do not make the chunk of text in a header too small since we don't want to     have too many headers<ul> <li>E.g., there should be at least 5-10 lines in each header</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_17","title":"Linter","text":"<ul> <li>Headings are capitalized as a title<ul> <li>E.g., <code>Data schema</code> instead of <code>Data Schema</code></li> <li>The linter automatically formats them</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#text","title":"Text","text":""},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#llm_18","title":"LLM","text":"<ul> <li> <p>We use bullet point lists</p> <ul> <li>For the items, <code>-</code> should be used instead of <code>*</code> or circles</li> <li>Items in bullet point lists should not end with a period</li> </ul> </li> <li> <p>Boldface and italics should be used sparingly throughout the text</p> </li> <li> <p>Structure the text so that bullet points of higher level correspond to     \"nesting\" in the concept</p> </li> <li> <p>Examples should go in a sub-bullet</p> <ul> <li>Good     ```<ul> <li>We typically increment the revision, likely a minor one<ul> <li>E.g., from <code>v0.3</code> to <code>v0.3.1</code> ```</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Use \"you\" and not \"we\" or \"one\"</p> <ul> <li>Let's just be direct: no need to be passive-aggressive</li> </ul> </li> <li> <p>Text should be reflowed to the maximum of 80 columns per line</p> <ul> <li>The linter performs this operation automatically</li> </ul> </li> <li> <p>Use active voice most of the time and use passive voice sparingly</p> <ul> <li>Good: \"The user updates the file\"</li> <li>Bad: \"The file is updated by the user\"</li> </ul> </li> <li> <p>Be efficient</p> <ul> <li>Do not explain things in a repetitive way</li> <li>Rewrite long-winded AI-generated texts in a concise way</li> <li>E.g.,<ul> <li>Good: \"Update the software by following these steps\"</li> <li>Bad: \"The process of updating the software can be done by following these     steps\"</li> </ul> </li> </ul> </li> <li> <p>When describing a tool the format should be the following:</p> <ul> <li>A description of what the tool does</li> <li>A list of examples of invocations of a tool with:<ul> <li>A comment on the command line</li> <li>The command line</li> <li>Its output if possible</li> </ul> </li> <li>A copy-paste version of the tool interface running <code>-h</code></li> </ul> </li> <li> <p>When nesting code blocks under list items in Markdown, we align the code block     to the previous block without empty lines     ````text</p> <ul> <li>Clone the super-repo locally     <code>bash     &gt; git clone --recursive git@github.com:causify-ai/{repo_name}.git ~/src/{repo_name}{index}</code></li> <li>Line2 ````</li> <li>The rendering stage makes sure that the output is correctly indented and     spaced</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.coding_style_guidelines.reference.html#linter_18","title":"Linter","text":"<ul> <li>Code blocks should always be accompanied by language markers<ul> <li>E.g., <code>bash</code>, <code>python</code>, <code>text</code>, <code>markdown</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html","title":"All.reading other people code.how to guide","text":""},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#reading_other_people_code","title":"Reading Other People Code","text":""},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#reading_other_people_code_1","title":"Reading other people code","text":"<ul> <li> <p>People don't like reading other people's code</p> <ul> <li>Still reading existing code needs to be done</li> <li>Nobody can code in a vacuum</li> </ul> </li> <li> <p>When done with the right attitude, reading code can be enjoyable, and you can     actually learn and improve as a coder</p> <ul> <li>E.g., writers read and study other writers' book to improve</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#what_not_to_do","title":"What not to do","text":""},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#rewrite_coding","title":"Rewrite coding","text":"<ul> <li> <p>You think \"This code is a complete ugly mess. It needs to be rewritten\"</p> <ul> <li>The answer is: ABSOLUTELY NO!</li> </ul> </li> <li> <p>The best case of a code rewrite is to have:</p> <ul> <li>The same code (in reality is likely that new bugs entered the system)</li> <li>With a different complexity that you only now understand</li> <li>After a long time and effort</li> </ul> </li> <li> <p>In other terms, there is no reason to believe that you are going to do a     better job than others did</p> </li> </ul>"},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#incremental_renovation","title":"Incremental renovation","text":"<ul> <li> <p>The first thing that programmers want to do is to bulldoze the place flat and     build something great</p> </li> <li> <p>Nobody is excited about incremental renovation:</p> <ul> <li>Improving</li> <li>Refactoring</li> <li>Cleaning out</li> <li>Adding unit tests</li> </ul> </li> <li>In reality, 99.9% of work is incremental</li> </ul>"},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#its_harder_to_read_code_than_to_write_it","title":"It's harder to read code than to write it","text":"<ul> <li>For this reason code reuse is hard</li> <li>For this reason, everybody on the team has the same function to do the same     thing</li> <li>It's easier and more fun to write new code than figuring out how the old code     works</li> </ul>"},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#respect_old_code","title":"Respect old code!","text":"<ul> <li> <p>When you think \"the old code is a mess\", you are probably wrong</p> </li> <li> <p>The idea that new code is better than old code is absurd</p> <ul> <li>Old code has been used</li> <li>Old code has been tested</li> <li>Lots of bugs in old code have been found and fixed</li> </ul> </li> <li> <p>When the code looks a mess is because it handles many corner cases you didn't     even think about, you didn't even know were possible</p> <ul> <li>Each of that bug took a long time to be discovered and fixed it</li> </ul> </li> <li> <p>When you throw away code and start from scratch, you are throwing away all the     knowledge, all the bug fixes, all the hard thinking</p> </li> </ul>"},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#what_makes_code_a_mess","title":"What makes code a mess?","text":"<ul> <li> <p>What makes the code a \"mess\" (at least according to your expert opinion as     world-class coder):</p> </li> <li> <p>Architectural problems</p> </li> <li> <p>E.g.,</p> <ul> <li>Code is not split into pieces in a way that makes sense</li> <li>Interfaces that are too broad and brittle</li> </ul> </li> <li> <p>These problems can be easily fixed!</p> </li> <li> <p>Inefficiency</p> </li> <li> <p>Profile and find what is the problem and fix that</p> </li> <li> <p>Ugly</p> </li> <li>E.g., horrible variable and function names</li> <li> <p>It takes 5 minutes of search-and-replace</p> </li> <li> <p>All these problems can be easily fixed in 100x less time than rewriting</p> </li> </ul>"},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#what_to_do","title":"What to do","text":""},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#get_into_the_right_attitude","title":"Get into the right attitude","text":"<ol> <li>Assume that whoever wrote the code knew what he/she was doing</li> <li>If that's not the case, he/she would have already been fired from the team</li> <li> <p>Therefore he/she is as competent than you</p> </li> <li> <p>Sorry for the bad news, but no, you are not the best coder on planet Earth</p> </li> <li> <p>So be humble</p> </li> <li> <p>\"Why is the code so complicated? I would have done XYZ and make it much    simpler?\"</p> </li> <li>There is no reason to believe that you can write the code in a simpler way</li> <li>The complexity is almost always needed to solve the complex problem we have</li> </ol>"},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#reading_other_people_code_is_painful","title":"Reading other people code is painful","text":"<ul> <li> <p>The problem is that code reflects the thought process of the person who wrote     the code</p> <ul> <li>The goal of the style guide, code writing guide, Linter is precisely to push     us to write code consistently so that it's less painful to read</li> </ul> </li> <li> <p>When you write code first hand, you think about it, and you build your mental     model</p> <ul> <li>You see the problems, understand them, and then appreciate why a complex     solution is needed</li> </ul> </li> <li> <p>Maybe some constraints and specs were not adequately documented</p> </li> <li>Maybe there were multiple iterations and compromise between different     solutions</li> <li>Maybe several people were involved</li> <li>Maybe a hack solution needed to be added to ship and get the \\$1m from the     customers</li> </ul>"},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#suggestions_on_how_to_read_code","title":"Suggestions on how to read code","text":"<ul> <li> <p>Use <code>git blame</code> to understand who wrote the code and over what period of time</p> <ul> <li>Knowing the author can help you ask him/her questions directly</li> </ul> </li> <li> <p>Use the same approach as for a code review</p> </li> <li> <p>Budget some time, like a few hours and stick to the process for that amount of     time</p> <ul> <li>Otherwise, after 5 mins you are like \"Argh! I can't do this!\" and you give     up</li> </ul> </li> <li> <p>Read the specs</p> <ul> <li>What is the code supposed to do?</li> <li>What are the edge cases to handle?</li> <li>How is it integrated into the rest of the code base?</li> </ul> </li> <li> <p>Skim through the entire code, top to bottom without reading line-by-line</p> <ul> <li>What's the goal?</li> <li>What are the specs?</li> <li>What are the functions?</li> <li>What are the interfaces?</li> <li>What is the structure?</li> </ul> </li> <li> <p>Read and execute the unit tests, if present</p> </li> <li> <p>Run the code with debug output <code>-v DEBUG</code> on simple examples to see what it     does</p> </li> <li> <p>Use PyCharm to navigate the code and jump around</p> </li> <li> <p>Add comments, when missing, to functions, to chunks of code, to each file</p> <ul> <li>Watch out for confusing comments</li> <li>Sometimes a comment can be out of date</li> </ul> </li> <li> <p>Add TODOs in the code for yourself</p> </li> <li> <p>Remember the coding conventions:</p> <ul> <li>Global variables are capital letters</li> <li>Functions starting with <code>_</code> are just for internals</li> <li>We have all the conventions to convey information about the thought process     of who wrote the code</li> </ul> </li> <li> <p>Take down notes about the code</p> <ul> <li>Write down the questions about what you don't understand<ul> <li>Maybe you will find the answer later (feel free to congratulate yourself!)</li> <li>Send an email to the author with the questions</li> </ul> </li> </ul> </li> <li> <p>Approach reading code as an active form</p> <ul> <li>Start writing unit tests for each piece that is not unit tested</li> <li>Step through the code with PyCharm</li> </ul> </li> <li> <p>Factor out the code</p> <ul> <li>Make sure you have plenty of unit tests before touching the code</li> </ul> </li> <li> <p>Expect to find garbage</p> </li> <li> <p>Don't feel bad when you get lost</p> <ul> <li>Reading code is not linear, like reading a book</li> <li>Instead, it's about building a mental model of how the code is structured,     how it works and why it's done in a certain way</li> </ul> </li> <li> <p>The more code you read, the more comfortable you will become</p> </li> </ul>"},{"location":"code_guidelines/all.reading_other_people_code.how_to_guide.html#refs","title":"Refs","text":"<ul> <li>How to Read Code (Eight Things to Remember)</li> <li>Things you should never do</li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html","title":"All.submit code for review.how to guide","text":""},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#first_review_process","title":"First Review Process","text":"<p>We understand that receiving feedback on your code can be a difficult process, but it is an important part of our development workflow. Here we have gathered some helpful tips and resources to guide you through your first review.</p>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#read_python_style_guide","title":"Read Python Style Guide","text":"<ul> <li>Before submitting your code for review, we highly recommend that you read our     Python Style Guide,     which outlines the major conventions and best practices for writing Python     code.</li> <li>Adhering to these standards will help ensure that your code is easy to read,     maintain, and understand for other members of the team.</li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#run_linter","title":"Run Linter","text":"<ul> <li>Linter is a tool that checks (and tries to fix automatically) your code for     syntax errors, style violations, and other issues.</li> <li>Run it on all the changed files to automatically catch any code issues before     filing any PR or before requesting a review!</li> <li>Run Linter with <code>invoke</code> command (which is abbreviated as <code>i</code>) and pass all     the files you need to lint in quotation marks after the <code>--files</code> option,     separated by a space</li> <li>The command should be run from the root of the repo you are developing in</li> <li>The file paths should be relative to the repo root<ul> <li>Command example: ```bash <p>i lint --files=\"docs/coding/all.str_to_df.how_to_guide.md linters/utils.py\" ```</p> </li> <li>Output example:     <code>bash     docs/coding/all.str_to_df.how_to_guide.md: 'docs/coding/all.str_to_df.how_to_guide.md' is not referenced in README.md [check_md_reference]     docs/coding/all.str_to_df.how_to_guide.md:79: 'figs/str_to_df/image1.png' does not follow the format 'figs/all.str_to_df.how_to_guide.md/XYZ' [fix_md_links]     linters/utils.py:294: [R0916(too-many-boolean-expressions), get_dirs_with_missing_init] Too many boolean expressions in if statement (6/5) [pylint]</code></li> <li><code>i lint</code> has options for many workflows. E.g., you can automatically lint     all the files that you touched in your PR with <code>--branch</code>, the files in the     last commit with <code>--last-commit</code>. You can look at all the options with:     <code>bash     &gt; i lint --help</code></li> </ul> </li> <li>Fix the lints<ul> <li>No need to obsessively fix all of them - just crucial and obvious ones</li> </ul> </li> <li>Running Linter on a file also applies some formatting fixes to it     automatically and stages the updated file for commit</li> <li>If Linter introduces extensive changes in a PR, causing difficulty in reading     the diff, a new pull request should be created exclusively for the Linter     changes, based on the branch of the original PR.</li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#compare_your_code_to_example_code","title":"Compare your code to example code","text":"<ul> <li>To get an idea of what well-formatted and well-organized code looks like, we     suggest taking a look at some examples of code that adheres to our standards.</li> <li>We try to maintain universal approaches to all the parts of the code, so when     looking at a code example, check for:<ul> <li>Code style</li> <li>Docstrings and comments</li> <li>Type hints</li> <li>Dir structure</li> </ul> </li> <li>Here are some links to example code:<ul> <li>Classes, functions and scripts:<ul> <li><code>/import_check/show_imports.py</code></li> <li><code>/linters/amp_fix_md_links.py</code></li> <li><code>/helpers/lib_tasks_lint.py</code></li> </ul> </li> <li>Unit tests:<ul> <li><code>/import_check/test/test_show_imports.py</code></li> <li><code>/linters/test/test_amp_fix_md_links.py</code></li> <li><code>/helpers/test/test_lib_tasks_lint.py</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#edit_codetext_written_by_ai","title":"Edit code/text written by AI","text":"<ul> <li>We do not mind if the AI writes the first version of the code or text for you</li> <li>We do mind if you simply copy-and-paste its output and submit if for our     review as-is</li> <li>Take whatever is produced by AI as the first draft: go over it, revise it,     make sure it's readable and relevant, adjust it to fit our specific     conventions</li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#save_reviewer_time","title":"Save Reviewer time","text":"<ul> <li>Follow the instructions given in     <code>PR workflows</code></li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#make_sure_checks_are_green","title":"Make sure checks are green","text":"<ul> <li>Before requesting review, make sure that all the checks performed by GitHub     Actions pass<ul> <li>The checks and their status can be seen at the bottom of the PR page or in     the \"Checks\" tab</li> </ul> </li> <li>Clicking on a failed check will take you to the extended report with the full     failure stacktrace</li> <li>If the Linter check fails with \"The Git client is not clean\", then run Linter     on the files reported by the check (it will automatically update them with     formatting fixes), commit and push them</li> <li>In rare cases it's okay to submit a PR for review with some of the checks     failing:<ul> <li>If the tests are already broken in master</li> <li>If a genuine bug in Linter causes the check to fail or requires reverting     the automatic fixes<ul> <li>Check if this bug is already tracked in an issue, and if not, open a new     issue to report it</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#review_your_own_pr","title":"Review your own PR","text":"<ul> <li>Before requesting review, take a final look at the \"Files changed\" tab</li> <li>Make sure there are<ul> <li>No changes you haven't intended</li> <li>No leftovers from debugging</li> <li>No files added by mistake (such as tmp files)</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#assign_reviewers","title":"Assign Reviewers","text":"<ul> <li>Make sure to select a Reviewer in a corresponding GitHub field so he/she gets     notified<ul> <li></li> <li>Junior contributors should assign Team Leaders (e.g., Grisha, DanY, Samarth,     ...) to review their PR<ul> <li>Team Leaders will assign integrators (GP &amp; Paul) themselves after all     their comments have been addressed</li> </ul> </li> <li>Ping the assigned Reviewer in the issue if nothing happens in 24 hours</li> <li>If you want to keep someone notified about changes in the PR but do not want     to make him/her a Reviewer, type <code>FYI @github_name</code> in the comment section</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#mention_the_issue","title":"Mention the issue","text":"<ul> <li>Mention the corresponding issue in the PR description to ease navigation<ul> <li>E.g., see an     example<ul> <li></li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#resolve_conversations","title":"Resolve conversations","text":"<ul> <li>When you've addressed a comment from a Reviewer, press <code>Resolve conversation</code>     button so the Reviewer knows that you actually took care of it<ul> <li></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#merge_master_into_your_branch","title":"Merge master into your branch","text":"<ul> <li>Before any PR review request do <code>i git_merge_master</code> in order to keep the code     updated<ul> <li>Resolve conflicts if there are any</li> <li>Do not forget to push it since this action is a commit itself</li> </ul> </li> <li>Actually, a useful practice is to merge master to your branch every time you     get back to work on it<ul> <li>This way you make sure that your branch is always using relevant code and     avoid huge merge conflicts</li> </ul> </li> <li>You can also easily merge master into your branch by clicking on \"Update     branch\" button on the PR page     </li> <li>NEVER press <code>Squash and merge</code> button yourself<ul> <li>You need to merge master branch into your branch - not vice verca!</li> <li>This is a strictly Team Leaders' and Integrators' responsibility</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#ask_for_reviews","title":"Ask for reviews","text":"<ul> <li>When you've addressed all the comments and need another round of review:<ul> <li>Press the circling arrows sign next to the Reviewer for the ping<ul> <li></li> </ul> </li> <li>Remove <code>PR_for_authors</code> and add the <code>PR_for_reviewers</code> label (see     here for     the description of available labels)<ul> <li></li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#do_not_use_screenshots","title":"Do not use screenshots","text":"<ul> <li>Stack traces and logs are much more convenient to use for debugging</li> <li>Screenshots are often too small to capture both input and return logs while     consuming a lot of basically useless memory</li> <li>Reviewers and collaborators cannot copy from the screenshot, which means that     if they want to reproduce the error, they need to manually type the code shown     in the screenshot, which is very inconvenient and error-prone</li> <li>The exceptions are plots and non-code information</li> <li>Examples:<ul> <li> <p>Bad</p> <p>     - Good</p> <p>Input:</p> <p><code>python type_ = \"supply\" supply_curve1 = ddcrsede.get_supply_demand_discrete_curve( type_, supply_orders_df1 ) supply_curve1</code></p> <p>Error:</p> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#text","title":"```text","text":"<p>NameError                                 Traceback (most recent call last) Cell In [5], line 2             1 type_ = \"supply\" ----&gt; 2 supply_curve1 = ddcrsede.get_supply_demand_discrete_curve(             3     type_, supply_orders_df1             4 )             5 supply_curve1</p> <p>NameError: name 'ddcrsede' is not defined ```</p>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#report_bugs_correctly","title":"Report bugs correctly","text":"<ul> <li>Whenever you face any errors, put as much information about the problem as     possible, e.g.,:<ul> <li>What you are trying to achieve</li> <li>Command line you ran, e.g.,     <code>bash     &gt; i lint -f defi/tulip/test/test_dao_cross_sol.py</code></li> <li>Copy-paste the error and the stack trace from the cmd line, no     screenshots, e.g.,     <code>text     Traceback (most recent call last):         File \"/venv/bin/invoke\", line 8, in &lt;module&gt;             sys.exit(program.run())         File \"/venv/lib/python3.8/site-packages/invoke/program.py\", line 373, in run             self.parse_collection()     ValueError: One and only one set-up config should be true:</code></li> <li>The log of the run<ul> <li>Maybe the same run using <code>-v DEBUG</code> to get more info on the problem</li> </ul> </li> <li>What the problem is</li> <li>Why the outcome is different from what you expected</li> <li>E.g. on how to report any issues<ul> <li>https://github.com/kaizen-ai/kaizenflow/issues/370#issue-1782574355</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#stick_to_smaller_prs","title":"Stick to smaller PRs","text":"<ul> <li>It's better to push frequently and ask for feedback early to avoid large     refactoring</li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#talk_through_code_and_not_github","title":"Talk through code and not GitHub","text":"<ul> <li>PR authors should, as a rule, talk to reviewers not through GitHub but through     code<ul> <li>E.g., if there is something you want to explain to the reviewers, you should     not comment on your own PR, but instead add comments in the code itself, or     improve the code so that it doesn't need explanation</li> <li>Everything on GitHub is lost once the PR is closed, so all knowledge needs     to go inside the code or the documentation</li> </ul> </li> <li>Of course it's ok to respond to questions on GitHub</li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#look_at_examples_of_first_reviews","title":"Look at examples of first reviews","text":"<ul> <li>It can be helpful to review some examples of previous first reviews to get an     idea of what common issues are and how to address them.</li> <li>Here are some links to a few \"painful\" first reviews:<ul> <li>Adding unit tests:<ul> <li>https://github.com/kaizen-ai/kaizenflow/pull/166</li> <li>https://github.com/kaizen-ai/kaizenflow/pull/186</li> </ul> </li> <li>Writing scripts:<ul> <li>https://github.com/kaizen-ai/kaizenflow/pull/267</li> <li>https://github.com/kaizen-ai/kaizenflow/pull/276</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.submit_code_for_review.how_to_guide.html#checklist","title":"Checklist","text":"<p>Post this checklist in a PR and check the boxes when the PR meets the requirements</p> <ul> <li>[ ] The branch is named following the format             <code>RelatedIssueTag_Normalized_issue_title</code><ul> <li>E.g., <code>HelpersTask123_Provide_branch_name_example</code>, if the branch is for     working on the issue #123 in the <code>helpers</code> repo with the title \"Provide     branch name example\"</li> </ul> </li> <li>[ ] Commit messages are short and informative<ul> <li>Ideally, they follow the format     <code>RelatedIssueTag: High-level commit description</code></li> <li>E.g., <code>HelpersTask123: Add example</code></li> <li>They do not mention the name of the file that has been changed by the commit</li> </ul> </li> <li>[ ] The title of the PR matches the name of the branch</li> <li>[ ] The starting post of the PR briefly describes the content of the PR on a             high level</li> <li>[ ] The issue related to the PR is mentioned in the starting post of the PR</li> <li>[ ] The PR is not linked to any issues under the <code>Development</code> section</li> <li>[ ] At least one reviewer is assigned under <code>Reviewers</code></li> <li>[ ] The PR author is listed under <code>Assignees</code></li> <li>[ ] All the checks performed by GitHub Actions pass<ul> <li>If they do not and it can't be fixed, explain why in a comment</li> <li>E.g., \"The fast tests failure is also present in <code>master</code> and is unrelated     to this PR\"</li> </ul> </li> <li>[ ] The branch is up to date with the master branch</li> <li>[ ] There are no conflicts with the master branch</li> <li>[ ] There are no files checked in by mistake (such as tmp and log files)</li> <li>[ ] All checked in files are checked and formatted by Linter in the latest             commit</li> <li>[ ] No files larger than 500 KB are checked in</li> <li>[ ] Screenshots are not used in PR posts to describe the situation or report             an error (if needed, copy-and-paste is used instead)</li> <li>[ ] Label <code>PR_for_reviewers</code> is present if a review is requested</li> <li>[ ] Fixes addressing a review comment are applied everywhere, not just where             the reviewer pointed out the issue</li> <li>[ ] After addressing review comments, all corresponding conversations are             marked as <code>resolved</code></li> <li>[ ] After all review comments are resolved, <code>re-request review</code> button is used             to request another round of review</li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html","title":"All.type hints.how to guide","text":""},{"location":"code_guidelines/all.type_hints.how_to_guide.html#type_hints","title":"Type Hints","text":""},{"location":"code_guidelines/all.type_hints.how_to_guide.html#type_hints_1","title":"Type hints","text":""},{"location":"code_guidelines/all.type_hints.how_to_guide.html#why_we_use_type_hints","title":"Why we use type hints","text":"<ul> <li>We use Python 3 type hints to:<ul> <li>Improve documentation</li> <li>Allow mypy to perform static checking of the code, looking for bugs</li> <li>Enforce the type checks at run-time, through automatic assertions (not     implemented yet)</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#what_to_annotate_with_type_hints","title":"What to annotate with type hints","text":"<ul> <li>We expect all new library code (i.e., that is not in a notebook) to have type     annotations</li> <li>We annotate the function signature</li> <li>We don't annotate the variables inside a function unless <code>mypy</code> reports that     it can't infer the type</li> <li>We strive to get no errors / warnings from Linter, including <code>mypy</code></li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#conventions","title":"Conventions","text":""},{"location":"code_guidelines/all.type_hints.how_to_guide.html#empty_return","title":"Empty return","text":"<ul> <li>Return <code>-&gt; None</code> if your function doesn't return<ul> <li>Pros:<ul> <li><code>mypy</code> checks functions only when there is at least an annotation: so     using <code>-&gt; None</code> enables mypy to do type checking</li> <li>It reminds us that we need to use type hints</li> </ul> </li> <li>Cons:<ul> <li><code>None</code> is the default value and so it might seem redundant</li> </ul> </li> </ul> </li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#invoke_tasks","title":"Invoke tasks","text":"<ul> <li> <p>For some reason <code>invoke</code> does not like type hints, so we</p> <ul> <li>Omit type hints for <code>invoke</code> tasks, i.e. functions with the <code>@task</code>     decorator</li> <li>Put <code># type: ignore</code> so that <code>mypy</code> does not complain</li> </ul> </li> <li> <p>Example:     <code>python @task def run_qa_tests( # type: ignore     ctx,     stage=\"dev\",     version=\"\", ):</code></p> </li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#annotation_for_kwargs","title":"Annotation for <code>kwargs</code>","text":"<ul> <li>We use <code>kwargs: Any</code> and not <code>kwargs: Dict[str, Any]</code></li> <li><code>*</code> always binds to a <code>Tuple</code>, and <code>**</code> always binds to a <code>Dict[str, Any]</code>.     Because of this restriction, type hints only need you to define the types of     the contained arguments. The type checker automatically adds the     <code>Tuple[_, ...]</code> and <code>Dict[str, _]</code> container types.</li> <li>Reference article</li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#any","title":"<code>Any</code>","text":"<ul> <li><code>Any</code> type hint = no type hint</li> <li>We try to avoid it everywhere when possible</li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#nparray_and_npndarray","title":"<code>np.array</code> and <code>np.ndarray</code>","text":"<ul> <li>If you get something like the following lint:     <code>bash     dataflow/core/nodes/sklearn_models.py:537:[amp_mypy] error: Function \"numpy.core.multiarray.array\" is not valid as a type [valid-type]</code></li> <li>Then the problem is probably that a parameter that the lint is related to has     been typed as <code>np.array</code> while it should be typed as <code>np.ndarray</code>:     <code>python `x_vals: np.array` -&gt; `x_vals: np.ndarray`</code></li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#handling_the_annoying_incompatible_types_in_assignment","title":"Handling the annoying <code>Incompatible types in assignment</code>","text":"<ul> <li><code>mypy</code> assigns a single type to each variable for its entire scope</li> <li>The problem is in common idioms where we use the same variable to store     different representations of the same data     ```python output : str = ... output = output.split(\"\\n\") ...</li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#process_output","title":"Process output.","text":"<p>... output = \"\\n\".join(output)     <code>- Unfortunately the proper solution is to use different variables</code>python output : str = ... output_as_array = output.split(\"\\n\") ...</p>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#process_output_1","title":"Process output.","text":"<p>... output = \"\\n\".join(output_as_array)     <code>- Another case could be:</code>python from typing import Optional def test_func(arg: bool): ... var: Optional[bool] = ... dbg.dassert_is_not(var, None) test_func(arg=var)     <code>- Sometimes `mypy` doesn't pick up the `None` check, and warns that the function     expects a `bool` rather than an `Optional[bool]`. In that case, the solution     is to explicitly use `typing.cast` on the argument when passing it in, note     that `typing.cast` has no runtime effects and is purely for type checking. - Here're the relevant [docs](https://mypy.readthedocs.io/en/stable/casts.html) - So the solution would be:</code>python from typing import cast ... ... test_func(arg=cast(bool, var))     ```</p>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#handling_the_annoying_none_has_no_attribute","title":"Handling the annoying <code>\"None\" has no attribute</code>","text":"<ul> <li>In some model classes <code>self._model</code> parameter is being assigned to <code>None</code> in     ctor and being set after calling <code>set_fit_state</code> method</li> <li>The problem is that statically it's not possible to understand that someone     will call <code>set_fit_state</code> before using <code>self._model</code>, so when a model's method     is applied:     <code>python self._model = self._model.fit(...)</code>     the following lint appears:     <code>bash     dataflow/core/nodes/sklearn_models.py:155:[amp_mypy] error: \"None\" has no attribute \"fit\"</code></li> <li>A solution is to<ul> <li>Type hint when assigning the model parameter in ctor:     <code>python self._model: Optional[sklearn.base.BaseEstimator] = None</code></li> <li>Cast a type to the model parameter after asserting that it is not <code>None</code>:     <code>python hdbg.dassert_is_not(self._model, None) self._model = cast(sklearn.base.BaseEstimator, self._model)</code></li> </ul> </li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#disabling_mypy_errors","title":"Disabling <code>mypy</code> errors","text":"<ul> <li>If <code>mypy</code> reports an error and you don't understand why, please ping one of     the Python experts asking for help</li> <li>If you are sure that you understand why <code>mypy</code> reports and error and that you     can override it, you disable this <code>error</code></li> <li>When you want to disable an error reported by <code>mypy</code>:<ul> <li>Add a comment reporting the <code>mypy</code> error</li> <li>Explain why this is not a problem</li> <li>Add <code># type: ignore</code> with two spaces as usual for the inline comment</li> <li>Example     ```python</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#mypy_cannot_find_module_named_pyannotate_runtime","title":"mypy: Cannot find module named 'pyannotate_runtime'","text":""},{"location":"code_guidelines/all.type_hints.how_to_guide.html#pyannotate_is_not_always_installed","title":"pyannotate is not always installed","text":"<p>from pyannotate_runtime import collect_types # type: ignore         ```</p>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#what_to_do_when_you_dont_know_what_to_do","title":"What to do when you don't know what to do","text":"<ul> <li>Go to the     <code>mypy</code> official cheat sheet</li> <li>Use <code>reveal_type</code><ul> <li>To find out what type <code>mypy</code> infers for an expression anywhere in your     program, wrap it in <code>reveal_type()</code></li> <li><code>mypy</code> will print an error message with the type; remove it again before     running the code</li> <li>See     the official <code>mypy</code> documentation</li> </ul> </li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#library_without_types","title":"Library without types","text":"<ul> <li><code>mypy</code> is unhappy when a library doesn't have types</li> <li>Lots of libraries are starting to add type hints now that Python 2 has been     deprecated     <code>bash     *.py:14: error: No library stub file for module 'sklearn.model_selection' [mypy]</code></li> <li>You can go in <code>mypy.ini</code> and add the library (following the alphabetical     order) to the list</li> <li>Note that you need to ensure that different copies of <code>mypy.ini</code> in different     sub projects are equal     <code>bash     &gt; vimdiff mypy.ini amp/mypy.ini     or     &gt; cp mypy.ini amp/mypy.ini</code></li> </ul>"},{"location":"code_guidelines/all.type_hints.how_to_guide.html#inferring_types_using_unit_tests","title":"Inferring types using unit tests","text":"<ul> <li>Sometimes it is possible to infer types directly from unit tests. We have used     this flow to annotate the code when we switched to Python3 and it worked fine     although there were various mistakes. We still prefer to annotate by hand     based on what the code is intended to do, rather than automatically infer it     from how the code behaves.<ul> <li>Install <code>pyannotate</code> <code>bash     &gt; pip install pyannotate</code></li> <li>To enable collecting type hints run     <code>bash     &gt; export PYANNOTATE=True</code></li> <li>Run <code>pytest</code>, e.g., on a subset of unit tests:</li> <li>Run <code>pytest</code>, e.g., on a subset of unit tests like <code>helpers</code>:     <code>bash     &gt; pytest helpers</code></li> <li>A file <code>type_info.json</code> is generated</li> <li>Annotate the code with the inferred types:     <code>bash     &gt; pyannotate -w --type-info type_info.json . --py3</code></li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html","title":"All.architecture diagrams.explanation","text":""},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#architecture_diagrams","title":"Architecture Diagrams","text":""},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#architecture_diagrams_1","title":"Architecture Diagrams","text":""},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#summary","title":"Summary","text":"<ul> <li>We use C4 as a way to describe graphically software architecture together with     some conventions</li> <li>Both Mermaid and PlantUML support C4 diagrams</li> <li>Mermaid is preferred since it can be rendered natively by GitHub</li> <li>PlantUML can be rendered through some of our scripts in regular markdown</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#brief_introduction_to_c4","title":"Brief introduction to C4","text":"<ul> <li> <p>A detailed description of C4 is https://C4model.com</p> </li> <li> <p>C4 stands for \"context, container, component, code\" (the 4 Cs)</p> </li> <li> <p>C4 model helps developers describe software architecture</p> <ul> <li>It maps code at various level of detail</li> <li>It is useful for both software architects and developers</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#different_levels_of_detail","title":"Different levels of detail","text":"<ul> <li>The 4 levels of detail are:<ol> <li>(System) Context system</li> <li>How the system fits in the world</li> <li>Container</li> <li>High-level technical blocks</li> <li>Component</li> <li>Show the components inside a container (i.e., a high-level block)</li> <li>Code</li> <li>Show how components are implemented</li> <li>Represented in terms of UML class diagrams</li> </ol> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#system_context_level_1","title":"(System) Context (Level 1)","text":"<ul> <li> <p>A system context describes something that delivers value to its users</p> <ul> <li>Typically a system is owned by a single software development team</li> </ul> </li> <li> <p>System context diagram shows the big picture of how the software system     interacts with users and other systems in the IT environment</p> </li> <li> <p>The focus is not on:</p> <ul> <li>Technologies</li> <li>Protocols</li> <li>Low-level details</li> </ul> </li> <li> <p>Audience:</p> <ul> <li>Both technical and non-technical people</li> <li>Both inside and outside the software development team</li> </ul> </li> <li> <p>A system system is made up of one or more containers</p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#container_level_2","title":"Container (Level 2)","text":"<ul> <li> <p>A container represents an application</p> <ul> <li>E.g.,<ul> <li>Server-side web application (e.g., Tomcat running Java EE web application,     Ruby on Rails application)</li> <li>Client-side web application (e.g., JavaScript running in a web browser,     e.g., using Angular)</li> <li>Client-side desktop application (e.g., an macOS application)</li> <li>Mobile app (e.g., an iOS or Android app)</li> <li>Server-side console application</li> <li>Server-less function (e.g., AWS Lambda)</li> <li>Database (e.g., MySQL, MongoDB)</li> <li>Content-store (e.g., AWS S3)</li> <li>File-system (e.g., a local filesystem)</li> <li>Shell script</li> </ul> </li> </ul> </li> <li> <p>A container runs some code and stores some data</p> <ul> <li>Typically each container runs in its own process space</li> <li>Containers communicate through inter-process communication</li> </ul> </li> <li> <p>A container diagram shows the high-level shape of the software architecture     and how responsibilities are distributed across it</p> </li> <li> <p>A container is the sum of components</p> <ul> <li>All components inside a container execute together</li> <li>Components can't be deployed as separate units</li> </ul> </li> <li> <p>Audience:</p> <ul> <li>Technical people</li> <li>Inside and outside of the software development team</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#component_level_3","title":"Component (level 3)","text":"<ul> <li> <p>Component is a group of related functionality encapsulated behind a     well-defined interface</p> <ul> <li>E.g., collection of classes behind an interface</li> </ul> </li> <li> <p>A component diagram decomposes each container to identify major structural     building blocks and interactions</p> </li> <li> <p>Audience</p> <ul> <li>Software architects and developers</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#code_level_4","title":"Code (level 4)","text":"<ul> <li> <p>Code is the implementation of the software system</p> <ul> <li>Each component can represented in terms of UML class diagrams, entity     relationship diagrams, etc.</li> <li>This diagram should be generated automatically from code</li> </ul> </li> <li> <p>Audience</p> <ul> <li>Software architects and developers</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#our_conventions_for_c4_diagrams","title":"Our conventions for C4 diagrams","text":""},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#mapping_c4_and_code_structure","title":"Mapping C4 and code structure","text":"<ul> <li>To simplify, we map the 4 levels of C4 in the code structure</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#system_context_level_2","title":"(System) Context (Level 1)","text":"<ul> <li>= big picture of how the system interacts with users and other systems</li> <li>Mapped onto a code repository</li> <li>E.g.,<ul> <li><code>//...</code> is a system providing data and analytics for commodity</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#container_level_3","title":"Container (Level 2)","text":"<ul> <li>= high-level software architecture and how responsibilities are split in the     system</li> <li>Mapped onto the first level of directories in a repo</li> <li>E.g., in <code>//...</code><ul> <li><code>automl</code>: application for automatic machine learning for commodity analysis</li> <li><code>edgar</code>: application to handle EDGAR data</li> <li><code>etl3</code>: back-end db for time series with real-time and point-in-time     semantics</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#component_level_4","title":"Component (Level 3)","text":"<ul> <li>= a group of related functionality encapsulated behind a well-defined     interface (e.g., collection of classes behind an interface)</li> <li>Mapped onto the second level of directory</li> <li>E.g., in <code>//.../edgar</code><ul> <li><code>api</code>: real-time system storing the data from EDGAR</li> <li><code>company_commodity_mapping</code>: data pipeline to process mapping between     commodities and companies</li> <li><code>form8</code>: data pipeline processing form 8</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#component_level_5","title":"Component (Level 4)","text":"<ul> <li>= OOP classes</li> <li>Typically we organize multiple related classes in files</li> <li>E.g., in <code>//.../edgar/form8</code><ul> <li><code>analyze_results.py</code>: classes and functions to analyze results from the data     pipeline</li> <li><code>extract_tables.py</code>: class <code>TableExtractor</code> extracting tables from Form 8</li> <li><code>filter_tables.py</code>: class <code>TableFilterer</code></li> <li><code>match_targets.py</code></li> <li><code>normalize_table.py</code></li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#use_classes","title":"Use classes!","text":"<ul> <li> <p>In order to be able to describe the system with C4 it is best to use classes     to separate responsibilities and package code</p> </li> <li> <p>Using classes has the following advantages:</p> <ul> <li>Organizes the code in cohesive parts</li> <li>Makes clear what is a public interface vs a private interface (e.g.,     helpers)</li> <li>Highlights responsibility (e.g., builder, annotation, processor, analyzer)</li> <li>Simplifies the interface of functions by sharing state in the object</li> </ul> </li> <li> <p>Note that classes still allow our favorite functional style of programming</p> <ul> <li>E.g., pandas is implemented with classes and it allows functional style</li> <li>The difference is going from:     <code>python f(ton of arguments)</code>     to     <code>python o(some argument).f(other arguments)</code></li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#generating_class_diagram","title":"Generating class diagram","text":"<ul> <li>To generate a class diagram (level 4 of C4), you can run     <code>bash     &gt; dev_scripts/create_class_diagram.sh</code></li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#mermaid","title":"Mermaid","text":""},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#class_diagram","title":"Class diagram","text":"<ul> <li>See https://mermaid.js.org/syntax/classDiagram.html</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#support_for_c4","title":"Support for C4","text":"<ul> <li>Mermaid supports most features of C4<ul> <li>See https://mermaid.js.org/syntax/c4.html</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#render_on-line","title":"Render on-line","text":"<ul> <li>See https://mermaid.live/edit</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#plantuml","title":"PlantUML","text":"<ul> <li> <p>Unified Modeling Language (UML) is a modeling language for software     engineering to provide a standard way to visualize design of a system</p> </li> <li> <p>We use mainly Class Diagrams</p> <ul> <li>For information on some class diagram convention see     https://en.wikipedia.org/wiki/Class_diagram</li> </ul> </li> <li> <p>You can refer to the PDF guide at http://plantuml.com/guide for an extensive     description of what PlantUML can do</p> <ul> <li>We are mainly interested in the \"Class diagram\" section</li> </ul> </li> <li> <p>The website https://structurizr.com has lots of information on using tools for     C4 and lots of examples</p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#plantuml_is_markdown","title":"PlantUML is Markdown","text":"<ul> <li> <p>We use PlantUML for rendering diagrams in our documentation</p> <ul> <li>For interactive use you can rely on online tools like:<ul> <li>Online editors:<ul> <li>planttext</li> <li>liveuml</li> <li>PlantUML Web Server</li> </ul> </li> <li>PyCharm plugin (create and edit <code>.puml</code> file locally):<ul> <li>PlantUML integration</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>We create <code>README.md</code> and <code>architecture.md</code> markdown files to document     software. <code>README.md</code> is for general content, <code>architecture.md</code> is for code     architecture description. You can embed the diagrams in <code>architecture.md</code> file     in a correspondent folder.</p> </li> <li> <p>To render PlantUML in our markdown files instead of <code>@startuml</code> you need to     use the tag:     <code>txt     ```plantuml     ...     ```</code></p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#our_conventions","title":"Our conventions","text":"<ul> <li> <p>Names</p> <ul> <li> <p>Each name in mappings should be exactly the same (maybe without some invalid     chars, like <code>.</code>) to not create a cognitive burden to the reader. It's better     to optimize for readability rather than by the number of chars. E.g.,</p> <p><code>plantuml [build_configs.py] as build_configs_py [TableExtractor] as TableExtractor</code>     - We keep components / classes in alphabetical order, so we can find them quickly in the code</p> </li> </ul> </li> <li> <p>Notes</p> <ul> <li> <p>Put notes describing some components / classes inside the blocks they refer     to. E.g.,</p> <p><code>plantuml node mapping as map { [CIK&lt;-&gt;Ticker] as ctmap note top of ctmap: My useful note. [CIK&lt;-&gt;GVKEY] as cgmap }</code>     - We use conventions for notes as for the code comments: - Start a note with a capital and end with <code>.</code>. In this way, it may be even     easier to visually distinguish notes from arrow labels. - Put notes straight after their related component definition, so a note     will look like a comment in the code</p> </li> </ul> </li> <li> <p>Arcs</p> <ul> <li> <p>The direction of the arcs represents the direction of the action. E.g.,</p> <p><code>plantuml apple --&gt; ground : falls to</code>     - We use the third person for describing actions</p> </li> </ul> </li> <li> <p>We use comments as headers to organize the <code>architecture.md</code>. Note that the     comments in <code>plantuml</code> are introduced with <code>'</code>. Some frequently used headers     are:</p> <ul> <li><code>' Components</code></li> <li><code>' Databases</code></li> <li><code>' Containers</code></li> <li><code>' Edge labels</code></li> <li><code>' Notes</code></li> </ul> </li> <li> <p>An example of acceptable C4 diagram plantuml snippet:</p> <p>```plantuml     ' Components     component [Edgar API] as Edgar_API     note top of Edgar_API : System storing the real-time\\nand historical data from EDGAR.     component [Headers dataset] as Headers_dataset</p> <pre><code>' Databases\ndatabase \"Compustat DB\" as Compustat_DB\nnote top of Compustat_DB : Third-party database\\nwith financial data.\n\n' Containers\nnode Form8 as form8 {\n</code></pre> <p>[analyze_results.py] as analyze_results_py  note left of analyze_results_py: Computes matching statistics.  [build_configs.py] as build_configs_py  [edgar_utils.py] as edgar_utils_py  [run_pipeline.py] as run_pipeline_py  [TableExtractor]  note right of TableExtractor: Extracts forms tables.  [TableNormalizer]  note right of TableNormalizer: Normalizes extracted tables.  [TableFilterer]  note right of TableFilterer: Takes only financial tables\\nfrom normalized tables.  [TargetMatcher]  note right of TargetMatcher: Matches financial values in tables.     }     node mapping as mapping {  [CIK&lt;-&gt;Ticker] as CIK_Ticker  [CIK&lt;-&gt;GVKEY] as CIK_GVKEY     }     node universe as universe{  [S&amp;P400]  [S&amp;P500]  [S&amp;P600]  [S&amp;P1500]     }     note left of universe: Universe of companies\\n as Tickers/GVKEYs.</p> <pre><code>' Edge labels\nEdgar_API --&gt; edgar_utils_py: provides filings payloads to\nCompustat_DB --&gt; run_pipeline_py: provides target\\nvalues to match on to\nbuild_configs_py --&gt; run_pipeline_py: provides pipeline\\nparameters to\nedgar_utils_py --&gt; TableExtractor: provides universe filings to\nanalyze_results_py --&gt; run_pipeline_py: provides functions\\nto run the matching in to\nmapping --&gt; edgar_utils_py: provides mapping to construct\\n universe as CIKs to\nHeaders_dataset --&gt; analyze_results_py: provides filing\\ndates to\nTableExtractor --&gt; TableNormalizer: provides tables to be normalized to\nTableFilterer --&gt; run_pipeline_py: provides forms\\n values to be matched to\nTargetMatcher --&gt; analyze_results_py: matches values in\nTableNormalizer --&gt; TableFilterer: provides tables to be filtered to\nuniverse --&gt; mapping: provides universe of companies to\n</code></pre> <p>```</p> <p>You can find the correspondent <code>architecture.md</code> file here.</p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#plotting_online","title":"Plotting online","text":"<ul> <li>Plantuml on-line</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#uml_-_unified_modeling_language","title":"UML - Unified Modeling Language","text":"<ul> <li> <p>The Unified Modeling Language (UML) serves as a versatile visual modeling     language designed to offer a standard way to visualize the design of a system</p> </li> <li> <p>UML employs a standardized notation for various diagram types, broadly     categorized into three primary groups:</p> <ul> <li>Structure diagrams: These diagrams depict the static elements within the     system, highlighting essential components necessary in the modeled system.     As they focus on system structure, they are extensively utilized in     documenting software architecture</li> <li>Behavior diagrams: These diagrams portray the dynamic aspects of the     system, emphasizing the actions and processes required within the modeled     system. Given their role in illustrating system behavior, they are     extensively used to describe the functionality of software systems</li> <li>Interaction diagrams: A subset of behavior diagrams, these emphasize the     flow of data and control among the components within the modeled system</li> </ul> <p></p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#uml_class_diagrams","title":"UML Class Diagrams","text":"<ul> <li>The UML Class Diagram is a graphical notation used to construct and visualize     object-oriented systems</li> <li>A class diagram in the Unified Modeling Language (UML) is a type of static     structure diagram that describes the structure of a system by showing the     system's:<ul> <li>Classes</li> <li>Attributes</li> <li>Methods</li> <li>Relationships among objects</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#class_notation","title":"Class Notation","text":"<ul> <li>A class represents a concept which encapsulates state (attributes) and     behavior (methods)</li> <li>The class name is the only mandatory information</li> <li> <p>The class diagram consists of</p> <ul> <li>Class Name:<ul> <li>The name of the class appears in the first partition</li> <li>The first letter is capitalized</li> </ul> </li> <li>Class Attributes:<ul> <li>Attributes are shown in the second partition</li> <li>The attribute type is shown after the colon</li> <li>The first letter is lowercase</li> <li>Attributes map onto member variables (data members) in code</li> </ul> </li> <li>Class Methods:<ul> <li>Methods are shown in the third partition. They are services the class     provides</li> <li>The return type of a method is shown after the colon at the end of the     method signature</li> <li>The return type of method parameters are shown after the colon following     the parameter name</li> <li>The first letter is lowercase</li> <li>Methods map onto class methods in code</li> </ul> </li> <li>Class Visibility:<ul> <li>The <code>+, -, #</code> symbols before an attribute and operation name in a class     denote the visibility of the attribute and operation</li> <li><code>+</code> denotes public attributes or methods</li> <li><code>-</code> denotes private attributes or methods</li> <li><code>#</code> denotes protected attributes or methods</li> </ul> </li> </ul> </li> <li> <p>An example of a Class Diagram is below</p> <p>```mermaid classDiagram</p> <p>class BankAccount{     #owner: str     +balance: int     +deposit(amount: float) bool     +withdrawal(amount: float) int     -checkBalance(account_number: int) float } ```</p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#class_relationships","title":"Class Relationships","text":"<ul> <li>Classes can engage in multiple relationships with other classes</li> <li>Relationships in UML class diagrams can be defined in several distinct types</li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#inheritancegeneralization","title":"Inheritance/Generalization","text":"<ul> <li>It indicates that:<ul> <li>One of the two related classes (the subclass) is considered to be a     specialized form of the other (the superclass) and</li> <li>The superclass is considered a generalization of the subclass</li> </ul> </li> <li>Each instance of the subclass is also an indirect instance of the superclass</li> <li>Represents an \"is-a\" relationship</li> <li> <p>An abstract class name is shown in italics</p> </li> <li> <p>Example: in the given diagram, class <code>Animal</code> is the superclass and class     <code>Duck</code> and <code>Fish</code> are its subclass</p> <p>```mermaid classDiagram</p> <p>Animal &lt;|-- Duck Animal &lt;|-- Fish Animal : +int age Animal: +isMammal() class Duck{     +String beakColor     +swim() } class Fish{     -int sizeInFeet     -canEat() } ```</p> </li> </ul>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#association","title":"Association","text":"<ul> <li>Associations are relationships between classes in a UML Class Diagram</li> <li>They are represented by a solid line between classes</li> <li> <p>Simple Association</p> <ul> <li>A structural link between two peer classes</li> <li>There is an association between Class1 and Class2</li> </ul> <p>```mermaid classDiagram</p> <p>Class1 --&gt; Class2 ```</p> </li> <li> <p>Aggregation</p> <ul> <li>A special type of association which represents a \"part of\" relationship</li> <li>Variant of the \"has-a\" association relationship</li> <li>Many instances (denoted by the diamond) of <code>Duck</code> can be associated with     <code>Pond</code></li> <li><code>Duck</code> is part of <code>Pond</code></li> <li>Objects of <code>Pond</code> and <code>Duck</code> have separate lifetimes</li> <li>In the given e.g., a Pond has zero or more Ducks, and a Duck has at most one     Pond (at a time). Duck can exist separately from a Pond</li> </ul> <p>```mermaid classDiagram</p> <p>Pond o-- Duck ```</p> </li> <li> <p>Composition</p> <ul> <li>A special type of aggregation where parts are destroyed when the whole is     destroyed</li> <li>Objects of <code>Class2</code> live and die with <code>Class1</code></li> <li><code>Class2</code> cannot stand by itself</li> <li>In the given e.g., a <code>Car</code> has exactly one <code>Carburetor</code>, and a <code>Carburetor</code></li> <li>Is a part of one <code>Car</code>. <code>Carburetor</code>s cannot exist as separate parts,     detached from a specific <code>Car</code></li> </ul> <p>```mermaid classDiagram</p> <p>Car *-- Carburetor ```</p> </li> <li> <p>Dependency</p> <ul> <li>An object of one class might use an object of another class in the code of a     method. If the object is not stored in any field, then this is modeled as a     dependency relationship</li> <li>Exists between two classes if changes to the definition of one may cause     changes to the other (but not the other way around)</li> <li><code>Class1</code> depends on <code>Class2</code></li> </ul> <p>```mermaid classDiagram</p> <p>Class1 ..&gt; Class2 ```</p> </li> <li> <p>Realization</p> <ul> <li>It is a relationship between the blueprint class and the object containing     its respective implementation level details</li> <li>This object is said to realize the blueprint class</li> <li>For e.g., the <code>Owner</code> interface might specify methods for acquiring property     and disposing of property. The <code>Person</code> and <code>Corporation</code> classes need to     implement these methods, possibly in very different ways</li> </ul> <p>```mermaid classDiagram</p> <p>class Owner{     &lt;&gt;     +accquire(property: str) bool     +dispose(property: str) bool } <p>class Person{     -real     -tangible     +accquire(property: str) bool     +dispose(property: str) bool }</p> <p>class Corporation{     -current     -fixed     -longterm     +accquire(property: str) bool     +dispose(property: str) bool }</p> <p>Person ..|&gt; Owner Corporation ..|&gt; Owner ```</p>"},{"location":"documentation_meta/all.architecture_diagrams.explanation.html#complete_example","title":"Complete Example","text":"<pre><code>\nclassDiagram\n        class Animal {\n                - String name\n                - int age\n                + Animal(name: String, age: int)\n                + setName(name: String): void\n                + setAge(age: int): void\n                + getName(): String\n                + getAge(): int\n                + makeSound(): void\n        }\n\n        class Dog {\n                - String breed\n                + Dog(name: String, age: int, breed: String)\n                + setBreed(breed: String): void\n                + getBreed(): String\n                + makeSound(): void\n        }\n\n        class Cat {\n                - boolean isLazy\n                + Cat(name: String, age: int, isLazy: boolean)\n                + setIsLazy(isLazy: boolean): void\n                + getIsLazy(): boolean\n                + makeSound(): void\n        }\n\n        class Car {\n                - String manufacturer\n                - String model\n                + Car(manufacturer: String, model: String)\n                + setManufacturer(manufacturer: String): void\n                + setModel(model: String): void\n                + getManufacturer(): String\n                + getModel(): String\n                + startEngine(): void\n        }\n\n        class Person {\n                - String name\n                - int age\n                - List&lt;Car&gt; ownedCars\n                + Person(name: String, age: int)\n                + setName(name: String): void\n                + setAge(age: int): void\n                + getName(): String\n                + getAge(): int\n                + addCar(car: Car): void\n                + removeCar(car: Car): void\n                + getOwnedCars(): List&lt;Car&gt;\n        }\n\n        Animal &lt;|-- Dog\n        Animal &lt;|-- Cat\n        Person \"1\" *-- \"*\" Car : owns\n\n        class SoundMaker {\n                &lt;&lt;interface&gt;&gt;\n                + makeSound(): void\n        }\n\n        SoundMaker &lt;|-- Animal\n        SoundMaker &lt;|-- Dog\n        SoundMaker &lt;|-- Cat\n\n        class Zoo {\n                - List&lt;Animal&gt; animals\n                + Zoo()\n                + addAnimal(animal: Animal): void\n                + removeAnimal(animal: Animal): void\n                + getAnimals(): List&lt;Animal&gt;\n        }\n\n        Zoo \"1\" *-- \"*\" Animal : contains\n</code></pre>"},{"location":"documentation_meta/all.diataxis.explanation.html","title":"All.diataxis.explanation","text":""},{"location":"documentation_meta/all.diataxis.explanation.html#diataxis","title":"Diataxis","text":""},{"location":"documentation_meta/all.diataxis.explanation.html#diataxis_a_framework_to_write_documentation","title":"Diataxis: a framework to write documentation","text":"<p>For more information look https://diataxis.fr/</p> <p>There are 4 modes of documentation</p> <ul> <li>Tutorial</li> <li>How-to guide</li> <li>Reference</li> <li>Explanation</li> </ul> <p></p>"},{"location":"documentation_meta/all.diataxis.explanation.html#tutorial","title":"Tutorial","text":"<ul> <li>Learning oriented</li> <li>Is a playground for users to learn something about the product by completing a     set of steps and achieving a meaningful result</li> <li>Helps the user/client achieve more understanding of the product</li> <li>Assumes the user does not have prior knowledge of the features used in the     tutorial</li> <li>Food analogy: teaching a child how to cook starting from cutting carrots,     celery, and onions for lasagna \"battuto\"</li> <li>File suffix: <code>.tutorial.md</code></li> <li>Example:     datapull/ck.create_airflow_dag.tutorial.md</li> </ul>"},{"location":"documentation_meta/all.diataxis.explanation.html#how-to_guide","title":"How-to guide","text":"<ul> <li>Goal oriented</li> <li>Is a guide to complete a real-world task</li> <li>Assumes the user has some knowledge about the background of used technology</li> <li>Food analogy: a recipe for cooking lasagna</li> <li>File suffix: <code>.how_to_guide.md</code></li> <li>Example:     work_tools/all.pycharm.how_to_guide.md</li> </ul>"},{"location":"documentation_meta/all.diataxis.explanation.html#reference","title":"Reference","text":"<ul> <li>Information oriented</li> <li>Provide a technical description of a component/piece of infra. The emphasis is     on completeness and accuracy</li> <li>Is difficult to keep everything up to date, so mostly rely on code and     docstring</li> <li>Food analogy: a reference encyclopaedia article about ingredients used in     lasagna (e.g., tomatoes, basil)</li> <li>File suffix: <code>.reference.md</code></li> <li>Example:     datapull/ck.ccxt_exchange_timestamp_interpretation.reference.md</li> </ul>"},{"location":"documentation_meta/all.diataxis.explanation.html#explanation","title":"Explanation","text":"<ul> <li>Understanding oriented</li> <li>Is used in our documentation to explain design decisions and choices,     architecture of components, how components interacted<ul> <li>E.g., high level broker explanation (what kind of behavior the broker class     encapsulates)</li> </ul> </li> <li>Does not provide specifications or instruction</li> <li>Food analogy: an article on culinary social history</li> <li>File suffix: <code>.explanation.md</code></li> <li>Example:     documentation_meta/all.architecture_diagrams.explanation.md</li> </ul>"},{"location":"documentation_meta/all.gdocs.how_to_guide.html","title":"All.gdocs.how to guide","text":""},{"location":"documentation_meta/all.gdocs.how_to_guide.html#gdoc_-_markdown_conversion","title":"Gdoc &lt;-&gt; Markdown conversion","text":""},{"location":"documentation_meta/all.gdocs.how_to_guide.html#converting_gdocs_to_markdown","title":"Converting Gdocs to Markdown","text":"<ul> <li>Use bullet lists to organize the whole Markdown for consistency with other     docs. See     all.coding_style.how_to_guide.md     or any other published Markdown format as reference</li> <li>Add missing \"```\" around code blocks. These could be missing in the original     Google doc. Also adjust code block indentations if needed</li> <li> <p>The generated markdown may convert http links as <code>html</code> <code>&lt;span&gt;</code> objects. This     hinders the readability of the <code>md</code> file. In this case, manually convert to a     standard <code>http://</code> link: -     <code>[&lt;span class=\"underline\"&gt;https://www.sorrentum.org/&lt;/span&gt;](https://www.sorrentum.org/)</code>     -&gt; <code>https://www.sorrentum.org/</code></p> </li> <li> <p>Remove empty lines manually</p> <p><code>markdown :'&lt;,'&gt;! perl -ne 'print if /\\S/'</code></p> </li> <li> <p>Run Linter</p> <ul> <li>Usage:     <code>bash     &gt; i lint --files=\"docs/documentation_meta/all.writing_docs.how_to_guide.md\"</code></li> <li>What Linter will do:<ul> <li>Build TOC automatically</li> <li>Adjust the indentation to improve the Markdown's format (but the     precondition is that you have properly adjusted the indentation levels).</li> <li>Remove extra empty lines under headings.</li> <li>Adjust text layout.</li> </ul> </li> <li>Do not mix manual edits and Linter runs</li> <li>If Linter messes up the text<ul> <li>File bugs in <code>helpers</code> with examples what Linter does incorrectly</li> </ul> </li> </ul> </li> <li>Last steps<ul> <li>Compare the generated markdown file with the original Gdoc from top to     bottom to ensure accurate rendering.</li> <li>Review the markdown file on GitHub to make sure it looks good, as it may     slightly differ from the preview in your local markdown editor</li> </ul> </li> <li>When a gdoc becomes obsolete, delete it</li> </ul>"},{"location":"documentation_meta/all.gdocs.how_to_guide.html#other_approaches","title":"Other approaches","text":"<ul> <li>Best for a large document</li> <li>Approach 1 - Chrome Docs to Markdown extension:<ul> <li>Use the Docs to Markdown     extension<ul> <li>Install     the extension     from the G Suite marketplace</li> <li>User guide     for the extension</li> </ul> </li> <li>One needs to accept/reject all suggestions in a gdoc as the extension works     poorly when a document is edited in the suggestion mode</li> </ul> </li> <li> <p>Approach 2 - Online converter:</p> <ul> <li>Google-docs-to-markdown/</li> </ul> </li> <li> <p>Also need to go through     Cleaning up converted markdown</p> </li> <li>You might need to remove artifacts manually</li> </ul>"},{"location":"documentation_meta/all.gdocs.how_to_guide.html#converting_markdown_to_gdoc","title":"Converting Markdown to Gdoc","text":"<ul> <li>Approach 1:<ul> <li>Run     <code>bash     &gt; pandoc MyFile.md -f markdown -t odt -s -o MyFile.odt</code></li> <li>Download the     template     in odt format</li> <li>Run     <code>bash     &gt; pandoc code_organization.md -f markdown -t odt -s -o code_org.odt --reference-doc /Users/saggese/Downloads/Gdoc\\ -\\ Template.odt</code></li> <li>Open it with TextEdit, copy-paste to Gdoc</li> </ul> </li> <li>Approach 2:<ul> <li>Instead of copy-paste the markdown into Gdocs, you can copy the rendered     markdown in a Gdoc<ul> <li>Gdocs does a good job of maintaining the formatting, levels of the     headers, the links, and so on</li> </ul> </li> </ul> </li> <li>Approach 3:<ul> <li>https://markdownlivepreview.com/</li> <li>TODO(gp): Check if the roundtrip works</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html","title":"All.google technical writing.how to guide","text":""},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#google_technical_writing","title":"Google Technical Writing","text":"<p>// From https://developers.google.com/tech-writing/one/</p>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#googles_technical_writing_part_1","title":"Google's technical writing: Part 1","text":""},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#define_new_or_unfamiliar","title":"Define new or unfamiliar","text":"<ul> <li>If your document introduces a term, define the term</li> <li>If the term already exists, link to a good existing explanation</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#use_terms_consistently","title":"Use terms consistently","text":"<ul> <li>Don't change the name of something while talking about it<ul> <li>E.g., <code>Protocol Buffers</code> vs <code>protobufs</code></li> </ul> </li> <li>You can do something like:     <code>Protocol Buffers (or protobufs for short)</code></li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#use_acronyms_properly","title":"Use acronyms properly","text":"<ul> <li>On the initial use of an unfamiliar acronym spell out the full term</li> <li>E.g., <code>Telekinetic Tactile Network (TTN) ...</code></li> <li>Acronyms take attention to be expanded in their full form</li> <li>Sometimes acronyms develop their own identity (e.g., HTML)</li> <li>An acronym should be significantly shorter than the full term</li> <li>Don't define acronyms that will be used only a few times</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#use_strong_verbs","title":"Use strong verbs","text":"<ul> <li>Choose precise, strong, and specific verbs<ul> <li>Weak verbs are \"be\", \"occur\", \"happen\"</li> </ul> </li> </ul> <p>Good</p> <pre><code>Dividing by zero raises the exception.\n</code></pre> <p>Bad</p> <pre><code>The exception occurs when dividing by zero.\n</code></pre>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#use_short_sentences","title":"Use short sentences","text":"<ul> <li>Each sentence should convey a single idea, thought, concept<ul> <li>Break long sentences into single-idea sentences</li> </ul> </li> <li>Convert long sentences into bulleted list<ul> <li>E.g., \"and\", \"or\" suggest to refactor into a bulleted list</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#remove_fillers","title":"Remove fillers","text":"<p>Good</p> <pre><code>This design document describes Project Frambus.\n</code></pre> <p>Bad</p> <pre><code>This design document provides a detailed description of Project Frambus.\n</code></pre>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#focus_each_paragraph_on_a_single_topic","title":"Focus each paragraph on a single topic","text":"<ul> <li>A paragraph is an independent unit of logic</li> <li>Ruthlessly delete sentence that doesn't relate to the current topic</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#avoid_wall-of-text","title":"Avoid wall-of-text","text":"<ul> <li>Readers often ignore long paragraphs</li> <li>Paragraphs should contain 3 to 5 sentences</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#answer_what_why_and_how","title":"Answer what, why, and how","text":"<ul> <li> <p>Good paragraphs answer the following questions</p> <ul> <li>What: what are you trying to tell your reader?</li> <li>Why: why is it important for the reader to know this?</li> <li>How: how should the reader use this knowledge</li> </ul> </li> <li> <p>E.g.,</p> <ul> <li>[What]: The <code>garp()</code> function returns the delta between a dataset's mean and     median.</li> <li>[Why]: Many people believe unquestioningly that a mean always holds the     truth. However, a mean is easily influenced by a few very large or very     small data points.</li> <li>[How]: Call <code>garp()</code> to help determine whether a few very large or very     small data points are influencing the mean too much. A relatively small     <code>garp()</code> value suggests that the mean is more meaningful than when the     <code>garp()</code> value is relatively high.</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#know_your_audience","title":"Know your audience","text":"<ul> <li>Your document needs to provide information that your audience needs but     doesn't already have<ul> <li>Define your audience<ul> <li>E.g., software engineers vs program managers</li> <li>E.g., graduate students vs first-year undergraduate students</li> </ul> </li> <li>Determine what your audience needs to learn<ul> <li>E.g.,     ```verbatim     After reading the documentation, the audience will know how to do the     following tasks<ul> <li>Use ...</li> <li>Do ... ... ```</li> </ul> </li> </ul> </li> <li>Fit documentation to your audience<ul> <li>Avoid the \"curse of knowledge\": experts forget that novices don't know     what you already know</li> </ul> </li> </ul> </li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#state_documents_scope","title":"State document's scope","text":"<ul> <li>A good document begins by defining its scope and its non-scope, e.g.,     <code>This document describes the design of Project Frambus, but not the related     technology Froobus.</code></li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#summarize_the_key_points_at_the_start","title":"Summarize the key points at the start","text":"<ul> <li>Ensure that the start of your document answers your readers' essential     questions</li> <li>The first page of a document determines if the readers makes it to page two</li> </ul> <p>// From https://developers.google.com/tech-writing/two</p>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#googles_technical_writing_part_2","title":"Google's technical writing: Part 2","text":""},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#adopt_a_style_guide","title":"Adopt a style guide","text":"<ul> <li>Many companies and large open source projects adopt a style guide for     documentation<ul> <li>E.g., https://developers.google.com/style</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#think_like_your_audience","title":"Think like your audience","text":"<ul> <li>Step back and try to read your draft from the point of view of your audience</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#come_back_to_it_later","title":"Come back to it later","text":"<ul> <li>After you write your first (or second or third) draft, set it aside</li> <li>Come back later and read it with fresh eyes to find things you can improve</li> </ul>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#organizing_large_docs","title":"Organizing large docs","text":"<ul> <li>You can organize a collection of information into<ul> <li>A longer standalone document; or</li> <li>Set of shorter interconnected documents (e.g., website, wiki)<ul> <li>Pros: easy to find information searching in the single back</li> </ul> </li> </ul> </li> </ul> <p>// TODO</p>"},{"location":"documentation_meta/all.google_technical_writing.how_to_guide.html#resources","title":"Resources","text":"<ul> <li>[https://developers.google.com/tech-writing/overview]</li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html","title":"All.markdown tools.explanation","text":""},{"location":"documentation_meta/all.markdown_tools.explanation.html#_1","title":"All.markdown tools.explanation","text":"<ul> <li>We use the following tools for documentation and blogging</li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#mkdocs","title":"MkDocs","text":"<ul> <li>MkDocs: Static site generator for project     documentation</li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#jupyter_book","title":"Jupyter Book","text":"<ul> <li>Jupyter Book: Useful for books with code and     notebooks</li> <li>It's based on CommonMark (a Markdown standard) but adds syntax for scientific     and technical publishing</li> <li>It's designed to map closely to reStructuredText (reST) features so that     Markdown can be used in the Sphinx ecosystem</li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#pandoc","title":"Pandoc","text":"<ul> <li> <p>Pandoc: Combine Markdown into PDF, ePub, or other     formats</p> <ul> <li>Aims for maximum portability: you write in Pandoc Markdown and convert to     HTML, PDF, LaTeX, DOCX, and more</li> <li>Supports multiple dialects: Pandoc Markdown, Markdown Extra, GitHub-Flavored     Markdown (GFM)</li> </ul> </li> <li> <p>https://executablebooks.org/en/latest/gallery </p> </li> <li>https://bvanderlei.github.io/jupyter-guide-to-linear-algebra/intro.html </li> <li>https://github.com/executablebooks/cookiecutter-jupyter-book</li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#material_for_mkdocs","title":"Material for MkDocs","text":"<ul> <li> <p>Material for MkDocs is a     popular theme and extension for MkDocs, which is a static site generator for     project documentation written in Markdown</p> <ul> <li>MkDocs = turns Markdown files into a static documentation website</li> <li>Material for MkDocs = makes that website look beautiful and adds powerful features.</li> </ul> </li> <li> <p>Inspired by Google's Material Design guidelines, i.e., clean, modern, responsive</p> </li> <li> <p>Lots of color palettes, light/dark modes, and logo customization</p> </li> <li> <p>Advanced navigation:</p> <ul> <li>Auto-generated sidebar and table of contents</li> <li>Breadcrumbs and sticky navigation</li> <li>Tabs, sections, version dropdowns, etc</li> </ul> </li> <li> <p>Markdown extensions:</p> <ul> <li>Syntax highlighting for code blocks</li> <li>Admonitions (!!! note blocks) and callouts</li> <li>Built-in support for math (MathJax), Mermaid diagrams, and more</li> <li>Works great with the MyST plugin, too!</li> </ul> </li> <li> <p>Search:</p> <ul> <li>Instant full-text search across your docs</li> <li>Works offline with lunr.js</li> </ul> </li> <li> <p>Plugins &amp; Integrations:</p> <ul> <li>Works well with MkDocs plugins for blogging, versioning, PDF export, etc</li> <li>Excellent support for custom JS/CSS</li> <li>Good support for GitHub Pages deployment</li> </ul> </li> <li> <p>Great for docs-as-code:         Used by open-source projects, scientific documentation, knowledge bases, internal wikis, and more</p> </li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#other_tools","title":"Other tools","text":""},{"location":"documentation_meta/all.markdown_tools.explanation.html#mdbooks","title":"mdBooks","text":"<ul> <li>mdBook: Inspired by GitBook,     ideal for technical books.</li> <li>Rust book example </li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#sphinx","title":"Sphinx","text":"<ul> <li>Purpose<ul> <li>Designed for Python documentation</li> <li>Supports general technical docs</li> </ul> </li> <li>Input Format<ul> <li>Primarily reStructuredText (<code>.rst</code>)</li> <li>Supports Markdown with plugins (<code>myst-parser</code>)</li> </ul> </li> <li>Features<ul> <li>Rich cross-referencing</li> <li>Automatic API docs (<code>autodoc</code>)</li> <li>Extensions for LaTeX, PDF output</li> <li>Theming system (ReadTheDocs theme popular)</li> </ul> </li> <li>Output Formats<ul> <li>HTML</li> <li>PDF (via LaTeX)</li> <li>EPUB</li> </ul> </li> <li>Complexity<ul> <li>Steeper learning curve</li> <li>More configuration options</li> </ul> </li> <li>Ecosystem<ul> <li>Large extension ecosystem</li> <li>Widely used in Python community</li> </ul> </li> </ul> Feature Sphinx MkDocs Input Format reStructuredText, MD Markdown Output Formats HTML, PDF, EPUB HTML Extensions Large ecosystem Smaller, plugins available Learning Curve Steeper Easier Theming Custom, RTD theme Material theme popular Best For API docs, Python libs Project websites, wikis"},{"location":"documentation_meta/all.markdown_tools.explanation.html#markdown_dialects","title":"Markdown Dialects","text":"<ul> <li>Markdown: Lightweight markup language for plain-text formatting<ul> <li>Dialects: Variants that extend or tweak core Markdown features</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#commonmark","title":"CommonMark","text":"<ul> <li>Standardized version of original Markdown</li> <li>Clear spec to reduce inconsistencies</li> <li>Many dialects are based on CommonMark</li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#github_flavored_markdown_gfm","title":"GitHub Flavored Markdown (GFM)","text":"<ul> <li>Based on CommonMark with extensions for GitHub.<ul> <li>Syntax highlighting for fenced code blocks.</li> <li>Tables:</li> <li>Task Lists:     ```<ul> <li>[ ] Task</li> <li>[x] Done ```</li> </ul> </li> <li>Strikethrough: <code>~~text~~</code></li> <li>Autolinks for URLs.</li> </ul> </li> <li>Widely used in README.md, issues, PRs.</li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#pandoc_markdown","title":"Pandoc Markdown","text":"<ul> <li>Very flexible, used by Pandoc converter</li> <li>Supports multiple extensions:<ul> <li>Footnotes:</li> <li>Citations</li> <li>Tables, definition lists, math.</li> <li>Customizable with <code>+extension</code> flags.</li> </ul> </li> <li> <p>Used for converting Markdown to LaTeX, PDF, HTML, DOCX.</p> </li> <li> <p>It supports many extra features not in standard Markdown: citations, footnotes,     tables, definition lists, math (via LaTeX), fenced code blocks with attributes,     etc.</p> </li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#myst_markdown","title":"MyST Markdown","text":"<ul> <li>MyST = \"MarkedlY Structured Text\"</li> <li> <p>Extensions:</p> <ul> <li>Directive blocks (like reST) using {% directive %} blocks     <code>markdown</code>{note}     This is a note directive.     <code></code></li> <li>Roles (inline extensions) using {role} syntax     <code>markdown     {ref}`section-name`</code></li> <li>Better support for equations, citations, figures, and complex structures.</li> <li>Equations: <code>$E=mc^2$</code> or <code>$$...$$</code></li> <li>Citations &amp; bibliography integration</li> <li>YAML metadata and configuration.</li> </ul> </li> <li> <p>Built for scientific/technical docs (Jupyter, Sphinx)</p> </li> <li> <p>Bridges Markdown with reStructuredText features</p> </li> <li> <p>MkDocs doesn't natively support the full MyST spec, but you can add it with     the mkdocs-myst-plugin</p> <ul> <li>TODO</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#preprocessor_for_markdown_dialects","title":"Preprocessor for Markdown Dialects","text":"<ul> <li>To render Markdown we want to perform some transformations</li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#lint_for_markdown_dialects","title":"Lint for Markdown dialects","text":""},{"location":"documentation_meta/all.markdown_tools.explanation.html#render_to_pdf","title":"Render to PDF","text":"<ul> <li> <p>How to generate a PDF from jupyterbook and from mkdocs</p> </li> <li> <p>Render mkdocs as PDFs</p> <ul> <li>Plug-in mkdocs-with-pdf</li> </ul> </li> <li> <p>Use pandoc</p> </li> </ul>"},{"location":"documentation_meta/all.markdown_tools.explanation.html#how_to_do_a_blog_with_mkdocs","title":"How to do a blog with mkdocs","text":""},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html","title":"All.mkdocs deployment.how to guide","text":""},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#summary","title":"Summary","text":"<ul> <li>Describe the tools under <code>//helpers/docs/mkdocs</code></li> </ul>"},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#generate_and_deploy_the_documentation","title":"Generate and deploy the documentation","text":""},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#mkdocs","title":"mkdocs","text":"<ul> <li>We choose MkDocs with     Material theme      to publish the documentation<ul> <li><code>MkDocs</code> has native support for markdown files in contrast with <code>ReadTheDocs</code></li> </ul> </li> <li>The entrypoint for the documentation home page is     <code>/docs/README.md</code></li> </ul>"},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#scripts","title":"Scripts","text":"<pre><code>&gt; ls -1 docs/mkdocs/docs/\nfix_markdown.sh\nfix_markdown2.sh\npreprocess_mkdocs.py\nrender_local.sh\nset_mkdocs.sh\n</code></pre> <ul> <li><code>fix_markdown.sh</code>, <code>fix_markdown2.sh</code>: fix some small char issues in markdown</li> <li><code>preprocess_mkdocs.py</code>: pre-process markdown files from an input directory so    that they can be rendered by <code>mkdocs</code></li> <li><code>render_local.sh</code>: </li> <li><code>set_mkdocs.sh</code>:</li> </ul>"},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#layout_of_a_publishable_dir","title":"Layout of a publishable dir","text":"<ul> <li>TODO(gp): Finish this and make sure the layout is always the same for all the     publishing stuff     <code>&gt; tree blog --dirsfirst -n -F --charset unicode     blog/     |-- docs/     |   |-- assets/     |   |   |-- favicon.ico     |   |   `-- logo.png     |   |-- posts/     |   |   |-- blog1.md     |   |   |-- blog2.md     |   |   `-- blog3.md     |   |-- styles/     |   |   `-- styles.css     |   `-- index.md     `-- mkdocs.yml</code></li> </ul>"},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#to_lint_the_markdown","title":"To lint the markdown","text":"<ul> <li> <p>Run the markdown:     <code>bash     &gt; lint_txt.py -i $FILE --use_dockerized_prettier --use_dockerized_markdown_toc</code></p> </li> <li> <p>To use <code>prettier</code> directly:     <code>bash     &gt; prettier --write --print-width 80 --prose-wrap always $FILE</code></p> </li> </ul>"},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#generate_the_mkdocs_dir","title":"Generate the <code>mkdocs</code> dir","text":"<ul> <li> <p>Set the dir to render:     <code>bash     &gt; export SRC_DIR=docs     &gt; export DST_DIR=dev_scripts_helpers/documentation/mkdocs/tmp.mkdocs</code></p> </li> <li> <p>To render the docs in the tutorials:     <code>bash     &gt; cd //tutorials1     &gt; export SRC_DIR=notes.startup_admin_guide     &gt; export DST_DIR=tmp.mkdocs</code></p> </li> <li> <p>Create the documentation for <code>mkdocs</code> from the <code>docs</code> directory     <code>bash     &gt; preprocess_mkdocs.py --input $SRC_DIR --output_dir $DST_DIR</code></p> </li> <li>This script:<ul> <li>Copies all the files from <code>docs</code> to <code>tmp.mkdocs</code> so that we can modify the     files in <code>tmp.mkdocs</code></li> <li>Process each of the markdown files in place in <code>tmp.mkdocs</code>, performing     several transformations, e.g.,</li> <li>Remove the table of content stored between </li> <li>Render ```python by dedenting so that it is aligned</li> <li>Replace 2 spaces indentation with 4 spaces since this is what <code>mkdocs</code> needs</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#debug","title":"Debug","text":"<ul> <li> <p>After running <code>preprocess_mkdocs.py</code> you can see how the markdown are     transformed with:     <code>bash     &gt; diff -r --brief docs tmp.mkdocs     &gt; diff_to_vimdiff.py --dir1 docs --dir2 tmp.mkdocs</code></p> </li> <li> <p>To serve the HTML locally:     <code>bash     &gt; (cd $DST_DIR; mkdocs serve --dev-addr localhost:8001)</code></p> </li> <li> <p>Go to http://localhost:8001</p> </li> </ul>"},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#publish_the_documentation","title":"Publish the documentation","text":"<ul> <li> <p><code>mkdocs</code> will generate HTML code from the <code>tmp.mkdocs</code> dirs</p> </li> <li> <p>You need to install <code>mkdocs</code> in a virtual env     <code>bash     &gt; set_mkdocs.sh</code></p> </li> <li> <p>Then you can activate the environment with:     <code>bash     &gt; source mkdocs.venv/bin/activate</code></p> <ul> <li>TODO(gp): Convert this into a dockerized executable</li> </ul> </li> <li> <p>Publish      <code>&gt; (cd $DST_DIR; mkdocs gh-deploy)</code></p> </li> <li> <p>GitHub renders the documentation at https://causify-ai.github.io/helpers/</p> </li> </ul>"},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#docu","title":"Docu","text":"<p>export SRC_DIR=notes.startup_admin_guide preprocess_mkdocs.py --input $SRC_DIR --output_dir tmp.mkdocs | tee log.txt</p> <p>(cd ~/src/tutorials1/tmp.mkdocs; mkdocs serve --dev-addr localhost:8001)</p> <p>saggese@gpmac.local venv:(mkdocs) branch:'master' ~/src/tutorials1</p> <p>(cd ~/src/tutorials1/tmp.mkdocs; mkdocs serve --dev-addr localhost:8001)</p>"},{"location":"documentation_meta/all.mkdocs_deployment.how_to_guide.html#document_how_the_stuff_is_deployed","title":"Document how the stuff is deployed","text":"<ul> <li> <p>TODO(gp):</p> </li> <li> <p>How to publish the blog</p> <ul> <li>Where is it?</li> </ul> </li> <li> <p>How to publish <code>docs</code> from <code>//helpers</code></p> </li> <li> <p>How to publish doc for <code>notes.</code> in `//tutorials/</p> <ul> <li>notes.programming_with_ai/</li> <li>notes.startup_admin_guide/</li> </ul> </li> <li> <p>How to publish documentation with GH actions</p> </li> <li> <p>How to publish from gpsaggese GitHub?</p> </li> </ul>"},{"location":"documentation_meta/all.plotting_in_latex.how_to_guide.html","title":"All.plotting in latex.how to guide","text":"<p>For plotting a certain classes of drawings (e.g., diagrams, graph) one should use frameworks like dot, mermaid, plantuml</p> <p>For technical drawing there are several solutions, as described below</p>"},{"location":"documentation_meta/all.plotting_in_latex.how_to_guide.html#tikz","title":"Tikz","text":"<ul> <li>Language for producing vector graphics from a textual description</li> <li> <p>Several drawing programs can export figures as Tikz format (e.g., Inkspace,     matplotlib, gnuplot)</p> </li> <li> <p>Refs</p> <ul> <li>Https://en.wikipedia.org/wiki/PGF/TikZ</li> <li>Https://tikz.net/</li> <li>Https://tikz.org/</li> <li>Examples<ul> <li>Https://tikz.dev/</li> <li>Https://texample.net/tikz/examples/</li> <li>Https://tex.stackexchange.com/questions/175969/block-diagrams-using-tikz</li> </ul> </li> <li>Web application<ul> <li>Https://tikzmaker.com/editor</li> </ul> </li> <li>Local editor<ul> <li>Https://tikzit.github.io/</li> </ul> </li> <li>Misc<ul> <li>Https://tex.stackexchange.com/?tags=tikz-pgf</li> </ul> </li> </ul> </li> </ul>"},{"location":"documentation_meta/all.plotting_in_latex.how_to_guide.html#pgfplots","title":"Pgfplots","text":"<ul> <li>Plots functions directly in Tex/Latex</li> <li> <p>Based on TikZ</p> </li> <li> <p>Refs</p> <ul> <li>Https://ctan.math.washington.edu/tex-archive/graphics/pgf/contrib/pgfplots/doc/pgfplots.pdf</li> <li>Https://www.overleaf.com/learn/latex/Pgfplots_package</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.plotting_in_latex.how_to_guide.html#asymptote","title":"Asymptote","text":"<ul> <li>A descriptive vector graphics language</li> <li>Provide a coordinate-based framework for technical drawing</li> <li> <p>It has a Python frontend</p> </li> <li> <p>Refs</p> <ul> <li>Https://en.wikipedia.org/wiki/Asymptote_(vector_graphics_language)</li> <li>Https://asymptote.sourceforge.io/</li> <li>Gallery<ul> <li>Https://asymptote.sourceforge.io/gallery/</li> </ul> </li> <li>Asymptote web application<ul> <li>Http://asymptote.ualberta.ca/</li> </ul> </li> </ul> </li> </ul>"},{"location":"documentation_meta/all.plotting_in_latex.how_to_guide.html#plotting_in_markdown","title":"Plotting in markdown","text":""},{"location":"documentation_meta/all.plotting_in_latex.how_to_guide.html#how_to_draw_in_markdown","title":"How to draw in markdown","text":"<p>We would like to use the same plots for both Latex and Markdown documents</p> <p>We can use pandoc</p> <p>TODO(gp): Consider extending ./dev_scripts_helpers/documentation/render_images.py to render also complex Latex, tikz</p> <p>https://tex.stackexchange.com/questions/586285/pandoc-markdown-drawing-circuit-diagrams-using-circuitikz</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html","title":"All.publish docs blogs.how to guide","text":""},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#summary","title":"Summary","text":"<p>This document covers how to publish documents, books, and blogs across different repos (e.g., <code>//helpers</code>, <code>//csfy</code>, <code>//tutorials</code>, and <code>//umd_classes</code>.</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#publishing_docs_blogs_with_mkdocs","title":"Publishing Docs &amp; Blogs with Mkdocs","text":""},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#concepts_at_a_glance","title":"Concepts at a Glance","text":"<ul> <li>The content to publish is organized under directories like<ul> <li><code>docs/</code></li> <li><code>blog/</code></li> <li><code>mkdocs/</code></li> </ul> </li> <li><code>preprocess.py</code> preprocesses Markdown into <code>tmp.mkdocs/</code> using our helper     script (cleans ToCs, normalizes code fences, etc.).</li> <li><code>mkdocs.yml</code> points <code>docs_dir: tmp.mkdocs</code> so builds use the preprocessed     files.</li> <li>Assets &amp; CSS are symlinked (preferred) or copied from canonical files so all     sites look the same.</li> <li>We publish with either:<ul> <li>GitHub Pages (<code>gh-pages</code> branch) for websites, or</li> <li>S3 (for some blogs), or</li> <li>GitHub Actions that build and push to Pages.</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#directory_structure_invariants","title":"Directory Structure (Invariants)","text":"<ul> <li> <p>Every site we publish must follow the same dir / file layout:     <code>pgsql     &lt;site-root&gt;/         mkdocs.yml                # site config (docs_dir points to tmp.mkdocs)         docs/                     # raw content (source)             index.md             &lt;other .md files and folders&gt;             assets/                 # logo.png, favicon.ico (symlinks or real files)             styles/                 # styles.css (symlink or real)         tmp.mkdocs/               # generated by preprocess (DO NOT COMMIT)</code></p> </li> <li> <p>Invariants</p> <ul> <li>All documents reuse the same <code>mkdocs.yml</code> template (only <code>site_name</code> etc.     change).</li> <li>All blogs reuse the same <code>mkdocs.yml</code> (plus the blog plugin).</li> <li>Only two GitHub workflows: one for documents, one for blogs (same logic;     small variations).</li> <li>Assets/CSS shared across repos should be symlinked to a single canonical     copy so there\u2019s only one source of truth.</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#canonical_assets_css","title":"Canonical Assets &amp; CSS","text":"<ul> <li> <p>We keep the \"master look &amp; feel\" here (examples):     <code>bash     helpers_root/docs/mkdocs/assets/logo.png     helpers_root/docs/mkdocs/assets/favicon.ico     helpers_root/docs/mkdocs/styles/styles.css</code></p> </li> <li> <p>In a site (e.g., <code>data605_book</code> present in tutorials repo), symlink them rather     than copying the entire file:     ```bash     &gt; mkdir -p mkdocs/docs/assets mkdocs/docs/styles</p> </li> <li> <p>IMPORTANT: Our preprocess script dereferences symlinks (uses <code>cp -rL</code>), so the     output in <code>tmp.mkdocs/</code> contains real files that MkDocs can copy during build.</p> </li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#find_helpers_path_submodule_or_sibling","title":"Find Helpers Path (Submodule or Sibling).","text":"<p>ROOT=$(git rev-parse --show-toplevel) if [ -d \"$ROOT/helpers_root\" ]; then HEL=\"$ROOT/helpers_root\"     elif [ -d \"$ROOT/helpers\" ]; then HEL=\"$ROOT/helpers\"     else echo \"No helpers{,_root} found\"; exit 1; fi</p> <p>ln -snf \"$HEL/docs/mkdocs/assets/logo.png\" mkdocs/docs/assets/logo.png ln -snf \"$HEL/docs/mkdocs/assets/favicon.ico\" mkdocs/docs/assets/favicon.ico ln -snf \"$HEL/docs/mkdocs/styles/styles.css\" mkdocs/docs/styles/styles.css ```</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#mkdocsyml_templates","title":"<code>mkdocs.yml</code> Templates","text":"<ul> <li>Template at - /docs/mkdocs/mkdocs.yml</li> <li>Following is an exammple template that might update as we improve     <code>yml     site_name: Your Site Name     theme:         name: material         palette:             primary: black             accent: cyan         logo: assets/logo.png         favicon: assets/favicon.ico         features:             - content.action.edit     docs_dir: tmp.mkdocs     use_directory_urls: false     plugins:         - search     markdown_extensions:         - toc:                 permalink: \"#\"                 baselevel: 2                 separator: \"_\"     extra_css:         - styles/styles.css</code></li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#blog_template","title":"Blog Template","text":"<ul> <li>Template at      Blogs Template</li> <li>Following is an example template that might update as we improve     <code>yml     site_name: Your Blog     theme:         name: material         palette:             primary: black             accent: cyan         logo: assets/logo.png         favicon: assets/favicon.ico         features:             - navigation.indexes     use_directory_urls: false     nav:         - Blog:                 - index.md     plugins:         - search         - blog:                 enabled: true                 blog_dir: .                 blog_toc: true                 post_date_format: full                 post_url_date_format: yyyy/MM/dd                 post_excerpt_separator: &lt;!-- more --&gt;                 pagination: false                 pagination_per_page: 1                 pagination_keep_content: true     markdown_extensions:         - toc:                 permalink: \"#\"                 baselevel: 2                 separator: \"_\"         - pymdownx.superfences:                 custom_fences:                     - name: mermaid                         class: mermaid                         format: !!python/name:pymdownx.superfences.fence_code_format     extra_css:         - styles/styles.css</code></li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#preprocess_preview_locally","title":"Preprocess \u2192 Preview Locally","text":"<ul> <li> <p>From the site directory:     ```bash     # Preprocess: Copy Docs -&gt; Tmp.Mkdocs (Resolving Symlinks) &amp; Transform Markdown.     &gt; python ../helpers_root/docs/mkdocs/preprocess_mkdocs.py \\         --input_dir mkdocs/docs \\         --output_dir tmp.mkdocs \\         -v INFO</p> </li> <li> <p>If you see 404 at /:</p> <ul> <li>Ensure there is an <code>index.md</code> in raw content (e.g., <code>mkdocs/docs/index.md</code>).</li> <li>Ensure <code>docs_dir: tmp.mkdocs</code> in <code>mkdocs.yml</code>.</li> <li>Confirm <code>tmp.mkdocs/index.md</code> exists after preprocess.</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#activate_env","title":"Activate Env.","text":"<p>source \"$HOME/src/venv/mkdocs/bin/activate\"</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#serve_locally","title":"Serve Locally.","text":"<p>mkdocs serve -f mkdocs.yml -a 127.0.0.1:8000</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#visit_http1270018000","title":"Visit Http://127.0.0.1:8000/","text":"<p>```</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#publish_to_github_pages_manual_one-off","title":"Publish to Github Pages (Manual, One-Off)","text":"<ul> <li> <p>Use this when testing or before your Actions are in place.     ```bash     # Build Into a Staging Folder Inside the Repo.     mkdocs build -f mkdocs.yml --site-dir ../_site/book.data605</p> </li> <li> <p>GitHub Pages settings (one-time):</p> <ul> <li>Repo \u2192 Settings \u2192 Pages \u2192 Build and deployment:</li> <li>Source: Deploy from a branch</li> <li>Branch: gh-pages and Folder: / (root)</li> <li>Give it ~1 minute, then open:<ul> <li>Https://.github.io///"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#prepare_gh-pages_worktree_in_repo_root","title":"Prepare Gh-Pages Worktree in Repo Root.","text":"<p>cd .. git fetch origin</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#clean_out_any_old_worktree","title":"Clean Out Any Old Worktree.","text":"<p>git worktree remove --force .ghpages-pub 2&gt;/dev/null || true rm -rf .ghpages-pub git worktree prune</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#create_fresh_worktree","title":"Create Fresh Worktree.","text":"<p>git worktree add -B gh-pages .ghpages-pub origin/gh-pages || \\ git worktree add -B gh-pages .ghpages-pub</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#sync_only_your_book_folder_exclude_overly_large_files_if_hooks_block_them","title":"Sync Only Your Book Folder (Exclude Overly Large Files If Hooks Block Them).","text":"<p>rsync -a --delete \\     --exclude '*.map' \\     --exclude 'assets/javascripts/lunr/wordcut.js' \\     _site/book.data605/ .ghpages-pub/book.data605/</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#ensure_pages_doesnt_run_jekyll_on_our_output","title":"Ensure Pages Doesn'T Run Jekyll on Our Output.","text":"<p>touch .ghpages-pub/.nojekyll</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#commit_push","title":"Commit &amp; Push.","text":"<p>cd .ghpages-pub git add -A git commit -m \"Publish site (manual)\" git push origin gh-pages ```</p>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#add_github_action_to_update_push_site_when_changes_in_docs","title":"Add Github Action to Update / Push Site When Changes in Docs","text":"<ul> <li> <p>We publish automatically via a GitHub Actions workflow</p> </li> <li> <p>Once set up:</p> <ul> <li>Trigger: Any commit that touches your content paths (e.g.,     notes.programming_with_ai/, notes.startup_admin_guide/, or shared     assets/styles) will trigger the workflow.</li> <li>Build: The workflow:<ul> <li>Checks out the repo</li> <li>Installs MkDocs + plugins</li> <li>Runs our preprocessor to generate <code>tmp.mkdocs/</code> (dereferencing symlinks)</li> <li>Runs <code>mkdocs build</code> to produce the static site in <code>_site/...</code>.</li> </ul> </li> <li>Deploy: The workflow uploads the <code>_site/</code> artifact and deploys it to GitHub     Pages (<code>gh-pages</code>), updating the live site automatically.</li> <li>Optional schedule: You can also enable a nightly cron to rebuild even if there     were no commits (useful if you have external includes or shared assets).</li> <li>No secrets needed: Pages deploy uses GitHub\u2019s built-in token. Ensure the     workflow has permissions: <code>contents: write</code>, <code>pages: write</code>, and     <code>id-token: write</code>.</li> </ul> </li> <li> <p>One-time setup</p> <ul> <li>Enable Pages: Repo \u2192 Settings \u2192 Pages \u2192 Deploy from a branch \u2192 Branch:     gh-pages, Folder: / (root).</li> <li>Add the workflow: Copy our template from     <code>//.github/workflows/publish_mkdocs.yml</code> (see template link below).</li> <li>Adjust paths: In the workflow, list the folders that should trigger a rebuild     under on.push.paths (e.g., your book/blog directories and shared     assets/styles).</li> <li>Verify mkdocs.yml: Each site uses docs_dir: tmp.mkdocs and references assets/     &amp; styles/ (which are symlinked in the raw docs/ tree).</li> <li>Commit to main: Push a change under a watched path and confirm the workflow     publishes.</li> <li>Template: See TEMPLATE: <code>.github/workflows/publish_mkdocs.yml</code> in this repo     for the exact workflow</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#per-repo_examples","title":"Per-Repo Examples","text":""},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#tutorials","title":"<code>//tutorials</code>","text":"<ul> <li><code>notes.programming_with_ai/</code></li> <li> <p><code>notes.startup_admin_guide/</code></p> </li> <li> <p>Each has:     <code>pgsql     mkdocs.yml     docs/         index.md         assets/ (symlinks)         styles/ (symlink styles.css)     tmp.mkdocs/ (generated)</code></p> </li> <li> <p>Published to:</p> <ul> <li></li> <li> ```</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#umd_classes","title":"<code>//umd_classes</code>","text":"<ul> <li><code>data605_book/</code></li> <li> <p><code>msml610_book/</code></p> </li> <li> <p>Published to:</p> <ul> <li></li> <li></li> </ul> </li> <li> <p>Root page can link to each: <code>index.html</code> in <code>gh-pages</code> with simple <code>&lt;a&gt;</code> links.</p> </li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#redirects_cleanup","title":"Redirects &amp; Cleanup","text":"<ul> <li>To retire an old path, add an index.html that meta-redirects:     <code>html     &lt;meta http-equiv=\"refresh\" content=\"0; url=/&lt;repo&gt;/&lt;new-path&gt;/\" /&gt;</code></li> <li>To disable a Pages site entirely: <code>Settings</code> \u2192 <code>Pages</code> \u2192 <code>Source</code>: <code>Disabled</code>.</li> <li>To remove old content: delete the folder from the <code>gh-pages</code> worktree, commit,     push.</li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>404 at <code>/</code> while serving locally: - <code>tmp.mkdocs/index.md</code> missing \u2192 rerun     preprocess or create <code>mkdocs/docs/index.md</code>. - <code>docs_dir</code> misconfigured \u2192     ensure <code>docs_dir: tmp.mkdocs</code>. -Assets 404 (logo/favicon/css): - Ensure     symlinks exist and point to real files (<code>ls -lL</code> to follow links). - Rerun     preprocess (it resolves symlinks into real files under <code>tmp.mkdocs/</code>).</li> <li>Preprocess fails with cyclic links:<ul> <li>Do not symlink mkdocs/docs back into itself. Keep <code>mkdocs/docs</code> pointing to     real content; only individual files under it may be symlinks to canonical     assets.</li> </ul> </li> <li>Git hooks block large files:<ul> <li>Exclude <code>*.map</code> and large vendor files when rsyncing to the <code>gh-pages</code>     worktree.</li> </ul> </li> <li>Pages shows \"There isn\u2019t a GitHub Pages site here.\"<ul> <li>Enable <code>Pages</code>: <code>Settings</code> \u2192 <code>Pages</code> \u2192 <code>Deploy</code> from a branch \u2192     <code>gh-pages / root</code>.</li> <li>Push to <code>gh-pages</code> and wait ~1 minute; hard refresh.</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.publish_docs_blogs.how_to_guide.html#quick_checklists","title":"Quick Checklists","text":"<ul> <li>New document/book<ul> <li>Create <code>mkdocs.yml</code> from the document template.</li> <li>Create <code>docs/</code> with <code>index.md</code>.</li> <li>Symlink <code>docs/assets/{logo.png,favicon.ico}</code> &amp; <code>docs/styles/styles.css</code> to     canonical copies (if needed).</li> <li>Preprocess \u2192 <code>tmp.mkdocs/</code>.</li> <li>Preview: <code>mkdocs serve -f mkdocs.yml -a 127.0.0.1:8000</code>.</li> <li>Publish via Actions or manual worktree.</li> </ul> </li> <li>New blog<ul> <li>Create <code>mkdocs.yml</code> from the blog template.</li> <li>Place posts under <code>blog/docs/</code> (or the configured <code>blog_dir</code>).</li> <li>Symlink <code>assets/styles</code> (if using the same images/ style files).</li> <li>Build/preview/publish (to S3 or Pages).</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html","title":"All.writing docs.how to guide","text":""},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#writing_docs","title":"Writing Docs","text":""},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#summary","title":"Summary","text":"<ul> <li> <p>This document describes how to write markdown for:</p> <ul> <li>Internal documentation</li> <li>Blog entries</li> <li>Tutorials</li> </ul> </li> <li> <p>This document is geared towards humans, while the style guide      [<code>//helpers/docs/code_guidelines/all.coding_style_guidelines.reference.md</code>]     is for non-human users (e.g., <code>linter</code> and <code>ai_review.py</code>)</p> </li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#meta","title":"Meta","text":"<ul> <li>Each of the suggestions below should be a level 3 heading so that it's easy to     point to it with a link</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#conventions","title":"Conventions","text":""},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#layout_rules","title":"Layout Rules","text":"<ul> <li> <p>Visual Structure</p> <ul> <li>Use clear headings</li> <li>Use nested bullets for hierarchy<ul> <li>1 idea per bullet</li> </ul> </li> <li>Separate sections logically</li> </ul> </li> <li> <p>Clarity</p> <ul> <li>Use simple language</li> <li>Define terms immediately</li> <li>Avoid ambiguous phrasing</li> </ul> </li> <li> <p>Headings</p> <ul> <li>Use <code>#</code> for main topics</li> <li>Use <code>##</code> for subtopics</li> <li>Keep headings short and descriptive</li> </ul> </li> <li> <p>Bullets</p> <ul> <li><code>-</code> for main bullets</li> <li>Indent for sub-bullets</li> <li>Each bullet = 1 idea</li> </ul> </li> <li> <p>Diagrams</p> <ul> <li>Use diagrams instead of wordy explanations</li> <li>Use <code>mermaid</code>, <code>graphviz</code>, or <code>tikz</code> fenced code blocks</li> <li>Prefer flowcharts, sequence diagrams, or graphs</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#make_no_assumptions_on_the_users_knowledge","title":"Make no assumptions on the user's knowledge","text":"<ul> <li>Nothing is obvious to somebody who doesn't know</li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#hold_users_hand","title":"Hold user's hand","text":"<ul> <li>Add ways to verify if a described process worked<ul> <li>E.g., \"do this and that, if this and that is correct should see this\"</li> </ul> </li> <li>Have a trouble-shooting procedure<ul> <li>One approach is to always start from scratch</li> </ul> </li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#always_use_linter","title":"Always use Linter","text":"<ul> <li> <p>Most cosmetic suggestions are handled by our Linter</p> <ul> <li>Run it after changes</li> <li>Use <code>i lint --files=\"your_file_name\"</code></li> </ul> </li> <li> <p>Avoid mixing manual edits and Linter runs</p> <ul> <li>Run Linter and commit changes separately</li> </ul> </li> <li> <p>Add a table of contents</p> <ul> <li>Markdown doesn't auto-generate TOC</li> <li>Run Linter to build TOC and place it at the top</li> </ul> </li> <li> <p>If Linter errors occur, file an issue with examples</p> </li> </ul>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#format_of_each_doc","title":"Format of each doc","text":"<ul> <li>There should be only one level 1 header with the title of the document<ul> <li>The title is used by <code>mkdocs</code></li> </ul> </li> <li> <p>There should be a summary with a short summary in bullets of the document</p> </li> <li> <p>Good:     ```text</p>"},{"location":"documentation_meta/all.writing_docs.how_to_guide.html#_1","title":"&lt;a class=\"headerlink\" href=\"#_1\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h2&gt; &lt;h3 id=\"summary_1\"&gt;Summary&lt;a class=\"headerlink\" href=\"#summary_1\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;This document contains ...&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=\"resources\"&gt;Resources&lt;a class=\"headerlink\" href=\"#resources\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h3&gt; &lt;h3 id=\"last_review\"&gt;Last review&lt;a class=\"headerlink\" href=\"#last_review\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;GP on 2024-04-20&lt;/li&gt; &lt;li&gt;Paul on 2024-03-10 ```&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"use_80_columns_formatting_for_markdown_files\"&gt;Use 80 columns formatting for markdown files&lt;a class=\"headerlink\" href=\"#use_80_columns_formatting_for_markdown_files\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;The &lt;code&gt;linter&lt;/code&gt; takes care of reflowing the text&lt;/li&gt; &lt;li&gt;&lt;code&gt;vim&lt;/code&gt; has a &lt;code&gt;:gq&lt;/code&gt; command to reflow the comments&lt;/li&gt; &lt;li&gt;There are plugins for PyCharm and VisualStudio&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"use_good_vs_bad\"&gt;Use good vs bad&lt;a class=\"headerlink\" href=\"#use_good_vs_bad\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Make examples of \"good\" ways of doing something and contrast them with \"bad\"     ways using the following format, e.g.,&lt;/p&gt; &lt;p&gt;&lt;code&gt;` - Good:&lt;/code&gt;markdown     ...     ```&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Bad:     &lt;code&gt;markdown     ...&lt;/code&gt; ````&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"use_an_empty_line_after_heading\"&gt;Use an empty line after heading&lt;a class=\"headerlink\" href=\"#use_an_empty_line_after_heading\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Leave an empty line after a heading to make it more visible, e.g.,&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Good ```markdown&lt;/li&gt; &lt;/ul&gt; &lt;h2 id=\"very_important_title\"&gt;Very important title&lt;a class=\"headerlink\" href=\"#very_important_title\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Less important text ```&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Bad ```markdown&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2 id=\"coming_through_ive_big_important_things_to_do\"&gt;Coming through! I've big important things to do!&lt;a class=\"headerlink\" href=\"#coming_through_ive_big_important_things_to_do\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h2&gt; &lt;ul&gt; &lt;li&gt;... and his big important wheels got STUCK! ```&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Our Linter automatically takes care of this&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"bullet_lists\"&gt;Bullet lists&lt;a class=\"headerlink\" href=\"#bullet_lists\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Use bullet lists since they represent the thought process, force people to     focus on short sentences (instead of rambling wall-of-text), and relation     between sentences&lt;/li&gt; &lt;li&gt;E.g.,     ```markdown&lt;ul&gt; &lt;li&gt;This is thought #1&lt;ul&gt; &lt;li&gt;This is related to thought #1&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;This is thought #2&lt;ul&gt; &lt;li&gt;Well, that was cool!&lt;/li&gt; &lt;li&gt;But this is even better ```&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Use &lt;code&gt;-&lt;/code&gt; instead of &lt;code&gt;*&lt;/code&gt; or circles&lt;/li&gt; &lt;li&gt;Linter automatically enforces this&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"use_the_right_syntax_highlighting\"&gt;Use the right syntax highlighting&lt;a class=\"headerlink\" href=\"#use_the_right_syntax_highlighting\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;When using a block of code use the write syntax highlighting&lt;ul&gt; &lt;li&gt;Code (```python)&lt;/li&gt; &lt;li&gt;Dirs (e.g.,&lt;code&gt;/home/users&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Command lines (e.g., &lt;code&gt;&gt; git push&lt;/code&gt; or &lt;code&gt;docker&gt; pytest&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;Bash     &lt;code&gt;bash     &gt; git push&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Python     &lt;code&gt;python if __name__ == \"__main__\":     predict_the_future()     print(\"done!\")&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Markdown     &lt;code&gt;markdown     ...&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Nothing     &lt;code&gt;verbatim     ....&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"indent_code_style\"&gt;Indent &lt;code&gt;code&lt;/code&gt; style&lt;a class=\"headerlink\" href=\"#indent_code_style\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;GitHub / Pandoc seems to render incorrectly a code block unless it's indented     over the previous line&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"embed_screenshots_only_when_strictly_necessary\"&gt;Embed screenshots only when strictly necessary&lt;a class=\"headerlink\" href=\"#embed_screenshots_only_when_strictly_necessary\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Avoid to use screenshots whenever possible and use copy-paste of text with the     right highlighting&lt;/li&gt; &lt;li&gt;Sometimes you need to use screenshots (e.g., plots, website interface)&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"improve_your_written_english\"&gt;Improve your written English&lt;a class=\"headerlink\" href=\"#improve_your_written_english\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Use English spell-checker&lt;/li&gt; &lt;li&gt;Type somewhere where you can use several choices:&lt;ul&gt; &lt;li&gt;&lt;a href=\"https://www.grammarly.com/\"&gt;Grammarly&lt;/a&gt;&lt;/li&gt; &lt;li&gt;ChatGPT&lt;/li&gt; &lt;li&gt;&lt;a href=\"https://www.languagetool.org/\"&gt;LanguageTool&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;This is super-useful to improve your English since you see the error and the     correction&lt;ul&gt; &lt;li&gt;Otherwise you will keep making the same mistakes forever&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"make_sure_your_markdown_looks_good\"&gt;Make sure your markdown looks good&lt;a class=\"headerlink\" href=\"#make_sure_your_markdown_looks_good\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Compare your markdown with others already published&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;You can:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Check in the code in a branch and use GitHub to render it&lt;/li&gt; &lt;li&gt;Use IDEs to edit, which also renders it side-by-side&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"capitalize_headings\"&gt;Capitalize headings&lt;a class=\"headerlink\" href=\"#capitalize_headings\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Headings titles should be like &lt;code&gt;Data Schema&lt;/code&gt; not &lt;code&gt;Data schema&lt;/code&gt;&lt;ul&gt; &lt;li&gt;This is automatically enforced by the &lt;code&gt;linter&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"update_the_last_review_tag\"&gt;Update the &lt;code&gt;Last review&lt;/code&gt; tag&lt;a class=\"headerlink\" href=\"#update_the_last_review_tag\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;When you read/refresh a file update the last line of the text     &lt;code&gt;verbatim     ## Last review         - GP on 2024-04-20         - Paul on 2024-03-10&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"comment_the_code_structure\"&gt;Comment the code structure&lt;a class=\"headerlink\" href=\"#comment_the_code_structure\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;When you want to describe and comment the code structure do something like     this     &lt;code&gt;&gt; tree.sh -p data_schema     data_schema/     |-- dataset_schema_versions/     |   `-- dataset_schema_v3.json         Description of the current schema     |-- test/     |   |-- __init__.py     |   `-- test_dataset_schema_utils.py     |-- __init__.py     |-- changelog.txt         Changelog for dataset schema updates     |-- dataset_schema_utils.py         Utilities to parse schema     `-- validate_dataset_signature.py*         Script to test a schema&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"convention_for_file_names\"&gt;Convention for file names&lt;a class=\"headerlink\" href=\"#convention_for_file_names\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Each file name should have a format like     &lt;code&gt;docs/{component}/{audience}.{topic}.{diataxis_tag}.md&lt;/code&gt;&lt;ul&gt; &lt;li&gt;E.g., &lt;code&gt;docs/documentation_meta/all.diataxis.explanation.md&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;Where&lt;ul&gt; &lt;li&gt;&lt;code&gt;component&lt;/code&gt; is one of the software components (e.g., &lt;code&gt;datapull&lt;/code&gt;, &lt;code&gt;dataflow&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;&lt;code&gt;audience&lt;/code&gt; is the target audience (e.g., &lt;code&gt;all&lt;/code&gt;, &lt;code&gt;ck&lt;/code&gt;)&lt;/li&gt; &lt;li&gt;&lt;code&gt;topic&lt;/code&gt; is the topic of the file&lt;/li&gt; &lt;li&gt;The topic of a \"how to guide\" should have a verb-object format&lt;ul&gt; &lt;li&gt;E.g., &lt;code&gt;docs/oms/broker/ck.generate_broker_test_data.how_to_guide.md&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;li&gt;The topic of a \"reference\" is often just a name&lt;ul&gt; &lt;li&gt;E.g., &lt;code&gt;docs/oms/broker/ck.binance_terms.reference.md&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;p&gt;// From https://opensource.com/article/20/3/documentation&lt;/p&gt; &lt;h4 id=\"use_active_voice\"&gt;Use active voice&lt;a class=\"headerlink\" href=\"#use_active_voice\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Use active voice most of the time and use passive voice sparingly&lt;/li&gt; &lt;li&gt;Active voice is shorter than passive voice&lt;/li&gt; &lt;li&gt; &lt;p&gt;Readers convert passive voice to active voice&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Good: \"You can change these configuration by ...\"&lt;/p&gt; &lt;/li&gt; &lt;li&gt;Bad: \"These configurations can be changed by ...\"&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"format_for_easy_reading\"&gt;Format for easy reading&lt;a class=\"headerlink\" href=\"#format_for_easy_reading\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Use headings, bullet points, and links to break up information into chunks     instead of long explanatory paragraphs&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"keep_it_visual\"&gt;Keep it visual&lt;a class=\"headerlink\" href=\"#keep_it_visual\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Use tables and diagrams, together with text, whenever possible&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"be_efficient\"&gt;Be efficient&lt;a class=\"headerlink\" href=\"#be_efficient\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Nobody wants to read meandering paragraphs in documentation&lt;/li&gt; &lt;li&gt;Engineers want to get technical information as efficiently as possible&lt;/li&gt; &lt;li&gt;Do not add \"fluff\"&lt;/li&gt; &lt;li&gt;Do not explain things in a repetitive way&lt;/li&gt; &lt;li&gt;Focus on how we do, why we do, rather than writing AI-generated essays&lt;/li&gt; &lt;/ul&gt; &lt;h4 id=\"do_not_add_redundancy\"&gt;Do not add redundancy&lt;a class=\"headerlink\" href=\"#do_not_add_redundancy\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h4&gt; &lt;ul&gt; &lt;li&gt;Always point to documentation on the web instead of summarizing it&lt;/li&gt; &lt;li&gt;If you want to summarize some doc (e.g., so that people don't have to read too     much), add it to a different document instead of mixing with our documentation&lt;/li&gt; &lt;/ul&gt; &lt;h3 id=\"resources_1\"&gt;Resources&lt;a class=\"headerlink\" href=\"#resources_1\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;&lt;a href=\"https://opensource.com/article/20/3/documentation\"&gt;https://opensource.com/article/20/3/documentation&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet\"&gt;Markdown cheatsheet&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"https://github.com/google/styleguide/blob/gh-pages/docguide/style.md\"&gt;Google guide to Markdown&lt;/a&gt;&lt;ul&gt; &lt;li&gt;TODO(gp): Make sure it's compatible with our Linter&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h3 id=\"last_review_1\"&gt;Last review&lt;a class=\"headerlink\" href=\"#last_review_1\" title=\"Permanent link\"&gt;#&lt;/a&gt;&lt;/h3&gt; &lt;ul&gt; &lt;li&gt;GP on 2025-07-15&lt;/li&gt; &lt;/ul&gt;","text":""},{"location":"general_background/all.common_abbreviations.reference.html","title":"All.common abbreviations.reference","text":""},{"location":"general_background/all.common_abbreviations.reference.html#common_abbreviations","title":"Common abbreviations","text":"<ul> <li>-&gt; room, -&gt; = \"let's go to the conference room\" (typically the one pinned in     the chat)</li> <li>AFAIK = as far as I know</li> <li>AFK, AFTK = away from (the) keyboard</li> <li>BM = Build-meister</li> <li>BRB = be right back</li> <li>CWR = call when ready</li> <li>GH = GitHub</li> <li>IMO = in my opinion</li> <li>KG = knowledge graph</li> <li>KG-OG = KG Original Gangsta</li> <li>KG-fication = The process of turning something in a KG</li> <li>KOTH = King Of The Hill (the best model so far)</li> <li>OOO, OOTO = Out Of The Office</li> <li>PR = Pull Request (we prefer this to MR = merge request)</li> <li>PTAL = please take a look</li> <li>SO = stack overflow</li> <li>Skateboard = getting something working end-to-end even with hacks (as long as     reversible)</li> <li>TBH = to be honest</li> <li>TTYL = talk to you later</li> <li>WFH = working from home</li> <li>WIP = work in progress</li> <li>Windows = the worst OS ever made</li> <li>Np = no problem</li> <li>Sg = sounds good</li> <li>Vim = the best editor ever made</li> </ul>"},{"location":"general_background/all.glossary.reference.html","title":"All.glossary.reference","text":""},{"location":"general_background/all.glossary.reference.html#glossary","title":"Glossary","text":""},{"location":"general_background/all.glossary.reference.html#meta","title":"Meta","text":"<ul> <li>Keep the terms in alphabetical order</li> <li>People can add terms that they are not clear and others can add definitions</li> <li>Always use \"suggestion\" mode for adding new terms or new definitions</li> </ul>"},{"location":"general_background/all.glossary.reference.html#definitions","title":"Definitions","text":"<ul> <li> <p>Asset</p> <ul> <li>A financial instrument with an associated price that changes over time</li> <li>Aka: symbol, name, ticker</li> <li>E.g., bitcoin, ethereum, Apple stock (US equity), orange futures</li> </ul> </li> <li> <p>FM (Financial Instrument)</p> <ul> <li>Financial instruments are monetary contracts between parties</li> </ul> </li> <li> <p>GH (GitHub)</p> </li> <li> <p>HLD (High Level Design)</p> <ul> <li>Is a general system design and includes the description of the System     architecture and design</li> </ul> </li> <li> <p>IM (Instrument Master)</p> <ul> <li>A software component that associates symbolic names to assets and their     prices</li> </ul> </li> <li> <p>Integrator</p> <ul> <li>Someone on the team that is in charge of merging code to the main line of     development</li> <li>Aka: master</li> </ul> </li> <li> <p>OHLCV bar</p> <ul> <li>An open-high-low-close chart (also OHLC) is a type of chart     typically used to illustrate movements in the price of a financial     instrument over time</li> </ul> </li> <li> <p>OMS (Order Management System)</p> <ul> <li>A software component in charge of placing and monitoring trading orders to     market or broker</li> </ul> </li> <li> <p>PR (Pull Request)</p> <ul> <li>Request to merge code in GitHub</li> </ul> </li> <li> <p>RP (Responsible Party)</p> <ul> <li>Someone on the team that helps following our process</li> <li>Aka: tech lead</li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html","title":"All.literature review.reference","text":""},{"location":"general_background/all.literature_review.reference.html#literature_review","title":"Literature Review","text":""},{"location":"general_background/all.literature_review.reference.html#meta","title":"Meta","text":"<ul> <li>Year - Title</li> <li>Paper authors:</li> <li>Link to the paper (ideally on gdrive)</li> <li>Review author / date</li> <li>Score in [0, 5], where:<ul> <li>5/5: Must-read</li> <li>4/5: Some interesting ideas we can reuse</li> <li>3/5: Pretty much what one would have done as first experiment</li> <li>2/5: ...</li> <li>1/5: Horrible: same bullet-proof logic as in a politician speech</li> </ul> </li> <li>Summary:<ul> <li>At most 5-10 bullet points explaining what the paper tries to accomplish</li> <li>Describe the data used, setup, model formulation, ...</li> <li>Good references</li> </ul> </li> <li>Praises:<ul> <li>At most 5 bullet points</li> <li>Focus on what is different, interesting, and not on the obvious</li> </ul> </li> <li>Critiques:<ul> <li>At most 5 bullet points</li> <li>Explain what is not solid in the analysis, suggestions on how to improve</li> </ul> </li> <li>Next steps:<ul> <li>What next steps should we take, if any, e.g.,<ul> <li>Read the bibliography</li> <li>Try experiments</li> </ul> </li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html#to_cut_and_paste","title":"To cut and paste","text":"<pre><code>\n### Year - Title\n- Paper authors:\n- [Link]()\n- Review author / date:\n- Score:\n- Summary:\n- Praises:\n- Critiques:\n- Next steps:\n</code></pre>"},{"location":"general_background/all.literature_review.reference.html#news_for_commodity_prediction","title":"News for commodity prediction","text":""},{"location":"general_background/all.literature_review.reference.html#2015_-_the_role_of_news_in_commodity_markets","title":"2015 - The role of news in commodity markets","text":"<ul> <li>Paper authors: Borovkova</li> <li>Link</li> <li>Review author / date: GP, 2019-11-22</li> <li>Score: 4/5</li> <li>Summary:<ul> <li>Dataset: prepackaged Thomson-Reuters sentiment (TRNA)</li> <li>Studies the effect of sentiment on commodities through event studies</li> <li>Forecast prices and volatility</li> </ul> </li> <li>Praises:<ul> <li>Decent statistics about the data set</li> <li>States that one needs to understand if the sentiment is attached to demand     and supply<ul> <li>Not sure if TR actually does that</li> </ul> </li> <li>Confirms our point about \"momentum-related news\" (i.e., news about the fact     that the price is going up)</li> <li>Confirms periodicity we are aware of</li> <li>Interesting local level model to extract the hidden sentiment<ul> <li>Very similar to what we thought to do (including the idea of using Kalman     smoother)</li> </ul> </li> </ul> </li> <li>Critiques:<ul> <li>Nothing really</li> </ul> </li> <li>Next steps:<ul> <li>Understand if TR considers sentiment distinguishing supply or demand<ul> <li>We should do this (not sure how PR does that)</li> </ul> </li> <li>Remove carefully momentum-related news</li> <li>Remove or count carefully repeated news (maybe use a measure of similarity     between articles)</li> <li>How to deliver \"event study\" models to customers? Should we \"unroll the     model\" for them providing a stream of predictions?</li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html#social_sentiment","title":"Social sentiment","text":""},{"location":"general_background/all.literature_review.reference.html#2015_predicting_global_economic_activity_with_media_analytics","title":"2015, Predicting global economic activity with media analytics","text":"<ul> <li>Paper authors: Peterson et al.</li> <li>Link: In <code>Tech/papers</code></li> <li>Review author / date: GP, 2019/12/08</li> <li>Score: 2/5</li> <li>Summary:<ul> <li>Predict PMI indices (which are related to the</li> </ul> </li> <li>Praises:<ul> <li>Interesting approach for going beyond polarity in sentiment considering</li> </ul> </li> <li>Critiques:<ul> <li>No seasonal component</li> <li>Usual problems with methodology OOS</li> </ul> </li> <li>Next steps:<ul> <li>Consider the TRMI \"indices\" (optimism, fear, joy, trust, violence)</li> <li>Consider the difference in professional news vs social news sentiment<ul> <li>What does it mean if there are large statistically significant difference?</li> </ul> </li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html#2018_-_twitter_investor_sentiment_and_capital_markets_what_do_we_know","title":"2018 - Twitter, Investor Sentiment and Capital Markets, what do we know?","text":"<ul> <li>Paper authors:</li> <li>Review author: GP, 2019-08-21</li> <li>Link:</li> <li>Score: 3 / 5</li> <li>Summary:<ul> <li>Good survey of the literature about social sentiment used for finance</li> <li>Most authors report predictivity of social sentiment for:</li> <li>Different metrics (returns, risk, trading volume)</li> <li>Assets (US stocks, exchange rates, commodities)</li> <li>Events (IPO, earnings)</li> </ul> </li> <li>Next steps:<ul> <li>Read all the bibliography and reproduce some of the results</li> </ul> </li> <li>TODO: Update this to new template</li> </ul>"},{"location":"general_background/all.literature_review.reference.html#time_series","title":"Time series","text":""},{"location":"general_background/all.literature_review.reference.html#on-line_learning_of_linear_dynamical_systems_exponential_forgetting_in_kalman_filters","title":"On-Line Learning of Linear Dynamical Systems: Exponential Forgetting in Kalman Filters","text":"<ul> <li>Paper authors: Mark Kozdoba, Jakub Marecek, Tigran Tchrakian, and Shie Mannor</li> <li>Review author: Paul, 2019-12-02</li> <li>arXiv,     AAAI</li> <li>Score: 4/5</li> <li>Summary:<ul> <li>Interesting insight into how to approximate a non-convex optimization     problem with an approximate convex one</li> <li>Shows that for observable Linear Dynamical Systems with non-degenerate     noise, the dependence of the Kalman filter on the past decays exponentially</li> <li>For this class of systems, predictions may be modeled as autoregressions. In     practice, not many terms are needed for a \"good\" approximation.</li> <li>The algorithm is on-line</li> <li>Comparison to the Kalman filter is formalized with regret bounds</li> <li>IBM / Technion research</li> <li>The setting is one where we are learning the best fixed but unknown     autoregression coefficients (rather than one where we are interested in     truly dynamic updates)<ul> <li>The learning rate decays like $1 / \\sqrt{t}$, and so under some mild     constraints on the time series being modeled, the autoregression     coefficients converge</li> <li>The linear dynamical system setup considered is one where the state     transition matrix and the observation direction are time-independent</li> </ul> </li> </ul> </li> <li>Praises:<ul> <li>References standard big works in the time series literature, like West and     Harrison (1997) and Hamilton (1994)</li> <li>Introduces a relatively simple online technique that competes well with the     more complex Kalman filter</li> </ul> </li> <li>Critiques:<ul> <li>Bounds / constants aren't quantitative</li> </ul> </li> <li>Next steps:<ul> <li>Look at the code accompanying the paper:     https://github.com/jmarecek/OnlineLDS</li> <li>Implement and compare to, e.g., z-scoring (a particularly simple case of     Kalman filtering)</li> <li>If we have a long history, it may be better to perform a single     autoregression over the whole history<ul> <li>This suggests</li> </ul> </li> <li>What if we keep the learning rate fixed over time?<ul> <li>This would effectively allow for \"drifting\" dynamics</li> <li>The proofs of the results of the paper would no longer apply</li> <li>It isn't obvious how the learning rate ought to be chosen</li> </ul> </li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html#predictive_state_smoothing_press_scalable_non-parametric_regression_for_high-dimensional_data_with_variable_selection","title":"Predictive State Smoothing (PRESS): Scalable non-parametric regression for high-dimensional data with variable selection","text":"<ul> <li>Paper author: Georg M. Goerg</li> <li>Review author: Paul, 2019-12-03</li> <li>Link</li> <li>Score: 4/5</li> <li>Summary:<ul> <li>A kernel smoother, but unlike traditional ones, it<ul> <li>Allows non-local (with respect to the x-var space) pooling</li> <li>Is scalable (e.g., computationally efficient)</li> </ul> </li> <li>PRESS is a generative, probabilistic model</li> <li>States are interpretable</li> <li>Compatible with deep neural networks (though experiments referenced in the     paper suggest depth doesn't help, e.g., a wide net with one softmax is     enough)</li> <li>Competitive with SVMs, Random Forests, and DNN</li> <li> <p>Predictive state representations are statistically and computationally     &gt; efficient for obtaining optimal forecasts of non-linear dynamical systems     &gt; (Shalizi and Crutchfield, 2001). Examples include time series forecasting     &gt; via epsilon-machines (Shalizi and Shalizi, 2004)...</p> </li> </ul> </li> <li>Praises:<ul> <li>Combines some clever insights</li> <li>References a TensorFlow implementation and suggests that implementing in     various frameworks is straightforward</li> </ul> </li> <li>Critiques:<ul> <li>No pointers to actual implementations</li> <li>Time series applications are referenced in Section 2, but many relevant (to     our work) practical ts-specific points are not developed in the paper</li> </ul> </li> <li>Next steps:<ul> <li>See if someone has already implemented PRESS publicly</li> <li>If no implementation is available, scope out how much work a minimal     pandas-compatible implementation would require</li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html#2019_high-dimensional_multivariate_forecasting_with_low-rank_gaussian_copula_processes","title":"2019, High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes","text":"<ul> <li>Paper authors: David Salinas, Michael Bohlke-Schneider, Laurent Callot,     Roberto Medico, Jan Gasthaus</li> <li>Review author: Paul, 2019-12-28</li> <li>arXiv</li> <li>Score: 4/5</li> <li>Summary:<ul> <li>Learns covariance structure and model together</li> <li>Handles series with time-varying, high-dimensional covariance structure</li> <li>Simultaneously handles series at different scales (in terms of the range)</li> <li>Uses a non-linear, deterministic state space model with transition dynamics     parametrized using an LSTM-RNN</li> </ul> </li> <li>Praises:<ul> <li>Implemented in GluonTS (https://github.com/awslabs/gluon-ts/pull/497) by one     of the coauthors who works on time series forecasting at AWS</li> <li>Code for the paper at     https://github.com/mbohlkeschneider/gluon-ts/tree/mv_release</li> <li>Good choice of baselines comparisons</li> <li>Demonstrates the importance of data transformations</li> </ul> </li> <li>Next steps:<ul> <li>Use in cases where we have a large number of time series known to have     meaningful correlations</li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html#2014_the_topology_of_macro_financial_flow_using_stochastic_flow_diagrams","title":"2014, The topology of macro financial flow using stochastic flow diagrams","text":"<ul> <li>Paper authors: Calkin, De Prado</li> <li>Link</li> <li>Review author / date: GP, 2020-01-17</li> <li>Score: 1 / 5</li> <li>Summary:</li> <li>Praises:<ul> <li>PCA on futures sectors</li> <li>Interesting graphical representation<ul> <li>Width of the arc represents strength of relationship (in terms of $R^2$)</li> <li>Color (green / red) and intensity represent sign and magnitude</li> <li>Lags are delays</li> <li>Geometric topology represents relationships better than tables</li> <li>Connectivity of a vertex represents importance</li> </ul> </li> <li>Agreed that econometrics as it is, is close to a pseudo-science that more     complex techniques are needed than inverting a matrix</li> </ul> </li> <li>Critiques:<ul> <li>Various inflammatory remarks and very little content</li> </ul> </li> <li>Next steps:<ul> <li>None</li> </ul> </li> </ul>"},{"location":"general_background/all.literature_review.reference.html#computer_engineering","title":"Computer engineering","text":""},{"location":"general_background/all.literature_review.reference.html#2015_hidden_technical_debt_in_machine_learning_systems","title":"2015, Hidden technical debt in machine learning systems","text":"<ul> <li>Paper authors: Sculler et al.</li> <li>Link</li> <li>Review author / date: GP, 2020-01-07</li> <li>Score: 4/5</li> <li>Summary:<ul> <li>Many interesting little observations about ML practices and engineering</li> </ul> </li> <li>Praises:<ul> <li>Validates how approach of minimizing technical debt and paying it off the     interest, e.g.,<ul> <li>Treat configuration as code, as we do</li> <li>Design abstraction carefully</li> <li>Routinely clean up the code</li> <li>No distinction in quality between research and production</li> <li>Use a single language for everything</li> <li>Need for committing to the healthy engineering practices</li> </ul> </li> </ul> </li> <li>Critiques:<ul> <li>None</li> </ul> </li> <li>Next steps:<ul> <li>None</li> </ul> </li> </ul>"},{"location":"general_background/all.reading_list.reference.html","title":"All.reading list.reference","text":""},{"location":"general_background/all.reading_list.reference.html#reading_list","title":"Reading List","text":""},{"location":"general_background/all.reading_list.reference.html#reading_list_1","title":"Reading List","text":""},{"location":"general_background/all.reading_list.reference.html#git","title":"Git","text":"<ul> <li>Short tutorial</li> <li>Pro Git book<ul> <li>To achieve mastery</li> </ul> </li> </ul>"},{"location":"general_background/all.reading_list.reference.html#bash_linux","title":"Bash / Linux","text":"<ul> <li>Short tutorial</li> <li>Missing semester of CS</li> </ul>"},{"location":"general_background/all.reading_list.reference.html#coding","title":"Coding","text":"<ul> <li> <p>The Pragmatic Programmer</p> <ul> <li>Aka the Black Book</li> <li>Reading and (really) understanding this is equivalent to accumulate 20 years     of coding</li> <li>It will change your life</li> </ul> </li> <li> <p>The Joel Test</p> <ul> <li>We should probably listen to the guy that started StackOverflow</li> <li>Today the 12 steps are obvious, but in 2000 these simple ideas were     revolutionary</li> <li>And, yes you are correct noticing that Joel is holding the table tennis     racquet incorrectly in the picture</li> </ul> </li> </ul>"},{"location":"general_background/all.reading_list.reference.html#data_analysis","title":"Data analysis","text":"<ul> <li>Python for Data Analysis<ul> <li>Reading is not enough: you should have tried all the examples of the book</li> <li>Remember: whatever you want to do, there is a more effective pandas way to     do it in one line</li> </ul> </li> </ul>"},{"location":"general_background/all.reading_list.reference.html#sre","title":"SRE","text":"<ul> <li>Site Reliability Engineering<ul> <li>\"Members of the SRE team explain how their engagement with the entire     software lifecycle has enabled Google to build, deploy, monitor, and     maintain some of the largest software systems in the world.\"</li> <li>An outstanding reference drawing on a wealth of experience</li> </ul> </li> </ul>"},{"location":"general_background/all.reading_list.reference.html#arbitrage","title":"Arbitrage","text":"<ul> <li>Trading and Arbitrage in Cryptocurrency Markets</li> </ul>"},{"location":"onboarding/all.dev_must_read_checklist.reference.html","title":"All.dev must read checklist.reference","text":""},{"location":"onboarding/all.dev_must_read_checklist.reference.html#must-read_documents_for_a_new_team_member","title":"Must-read documents for a new team member","text":""},{"location":"onboarding/all.dev_must_read_checklist.reference.html#org","title":"Org","text":"<ul> <li>[ ] General rules of collaboration</li> </ul>"},{"location":"onboarding/all.dev_must_read_checklist.reference.html#coding","title":"Coding","text":"<ul> <li>[ ] Coding style guide</li> <li>[ ] How to write unit tests</li> <li>[ ] How to run unit tests</li> <li>[ ] Creating a Jupyter Notebook</li> </ul>"},{"location":"onboarding/all.dev_must_read_checklist.reference.html#submitting_your_code_for_review","title":"Submitting your code for review","text":"<ul> <li>[ ] What to do before opening a PR</li> <li>[ ] Code review process</li> </ul>"},{"location":"onboarding/all.dev_must_read_checklist.reference.html#github","title":"Git(Hub)","text":"<ul> <li>[ ] Git workflows and best practices</li> <li>[ ] GitHub organization</li> </ul>"},{"location":"onboarding/all.dev_must_read_checklist.reference.html#buildmeister_duties","title":"Buildmeister duties","text":"<ul> <li>[ ] Buildmeister Guide</li> </ul>"},{"location":"onboarding/all.dev_must_read_checklist.reference.html#writing_docs","title":"Writing docs","text":"<ul> <li>[ ] Tips for writing documentation</li> </ul>"},{"location":"onboarding/all.dev_must_read_checklist.reference.html#beyond_the_must-read","title":"Beyond the must-read","text":"<ul> <li>There are many, many more docs in our knowledge base than the ones listed     above</li> <li>Feel free to browse and read them at your leisure</li> <li>If you have a question, first try to find the answer in one of the docs<ul> <li>You can, for example, <code>grep</code> for relevant keywords</li> </ul> </li> <li>If the answer is not in the docs, reach out to your team leader, and, if it is     deemed useful for other (future) team members, file a PR to update the docs</li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html","title":"All.development.how to guide","text":""},{"location":"onboarding/all.development.how_to_guide.html#development","title":"Development","text":""},{"location":"onboarding/all.development.how_to_guide.html#setting_up_git_credentials","title":"Setting up Git credentials","text":""},{"location":"onboarding/all.development.how_to_guide.html#preamble","title":"Preamble","text":"<ul> <li>Git allows setting credentials at different \"levels\":<ul> <li>System (set for all the users in <code>/etc/git</code>)</li> <li>Global (set for a single user in <code>$HOME/.gitconfig</code> or     <code>$HOME/.config/git/config</code>)</li> <li>Local (set on a per client basis in <code>.git/config</code> in the repo root)</li> <li>Git uses a hierarchical config approach in which settings of a broader scope     are inherited if not overridden.</li> </ul> </li> <li>Refs:<ul> <li>How to customize Git:     https://git-scm.com/book/en/v2/Customizing-Git-Git-Configuration</li> <li>Details on <code>git config</code>: https://git-scm.com/docs/git-config</li> </ul> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#check_git_credentials","title":"Check Git credentials","text":"<ul> <li>You can check the Git credentials that will be used to commit in a client by     running:     <code>bash     &gt; git config -l | grep user     user.name=saggese     user.email=saggese@gmail.com     github.user=gpsaggese</code></li> <li>To know at which level each variable is defined, run     <code>bash     &gt; git config --show-origin user.name     file:/Users/saggese/.gitconfig saggese</code></li> <li>You can see all the <code>Authors</code> in a Git repo history with:     <code>bash     &gt; git log | grep -i Author | sort | uniq     ...</code></li> <li> <p>Git doesn't do any validation of <code>user.name</code> and <code>user.email</code> but it just uses     these values to compose a commit message like:</p> <p>```bash</p> <p>git log -2 commit 31052d05c226b1c9834d954e0c3d5586ed35f41e (HEAD -&gt; AmpTask1290_Avoid_committing_to_master_by_mistake) Author: saggese saggese@gmail.com Date: Mon Jun 21 16:22:25 2021</p> <p>Update hooks ```</p> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#setting_git_credentials","title":"Setting Git credentials","text":"<ul> <li>To keep things simple and avoid variability, our convention is to use:<ul> <li>As <code>user.name</code> our Linux user name on the local computer we are using to     commit which is returned by <code>whoami</code> (e.g., <code>user.name=saggese</code>)</li> <li>As <code>user.email</code> the email that corresponds to that user (e.g,.     <code>user.email=saggese@gmail.com</code>)</li> </ul> </li> <li>To accomplish the set-up above you can:<ul> <li>Use in <code>/Users/saggese/.gitconfig</code> the values for our open-source account,     so that they are used by default ```bash <p>git config --global user.name $(whoami) git config --global user.email YOUR_EMAIL ```</p> </li> </ul> </li> <li>Use the correct user / email in the repos that are not open-source     <code>bash     &gt; cd $GIT_ROOT     &gt; git config --local user.name $(whoami)     &gt; git config --local user.email YOUR_EMAIL</code></li> <li>Note that you need to set these local values on each Git client that you have     cloned, since Git doesn't version control these values</li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#enforcing_git_credentials","title":"Enforcing Git credentials","text":"<ul> <li>We use Git hooks to enforce that certain emails are used for certain repos     (e.g., we should commit to our open-source repos only using our personal     non-corporate email).</li> <li>You need to install the hooks in each Git client that you use. Conceptually     this step is part of <code>git clone</code>: every time you clone a repo locally you need     to set the hooks.</li> <li> <p>TODO(gp): We could create a script to automate cloning a repo and setting it     up.</p> <p>```bash</p> <p>cd //amp ./dev_scripts/git/git_hooks/install_hooks.py --action install cd //lem ./amp/dev_scripts/git/git_hooks/install_hooks.py --action install ```</p> </li> <li> <p>This procedure creates some links from <code>.git/hook</code> to the scripts in the repo.</p> </li> <li>You can also use the action <code>status</code> to see the status and <code>remove</code> to the     hooks.</li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#create_the_thin_env","title":"Create the thin env","text":"<ul> <li> <p>You can follow the</p> <p>```bash</p> </li> <li> <p>The installation is successful if you see at the end of the output</p> <p>```verbatim ...</p> </li> <li> <p>To configure each shell, you should run:     <code>bash     &gt; source dev_scripts/setenv_amp.sh</code>     which should output     <code>verbatim     ...     alias w='which'     # Enable invoke autocompletion.     ==&gt; SUCCESS &lt;==</code></p> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#build_the_client_env","title":"Build the client env.","text":"<p>dev_scripts/client_setup/build.sh 2&gt;&amp;1 | tee tmp.build.log source dev_scripts/setenv_amp.sh ```</p>"},{"location":"onboarding/all.development.how_to_guide.html#installation","title":"Installation","text":""},{"location":"onboarding/all.development.how_to_guide.html#configure_your_client_with","title":"Configure your client with:","text":"<p>source dev_scripts/setenv_amp.sh ```</p>"},{"location":"onboarding/all.development.how_to_guide.html#publish_a_notebook","title":"Publish a notebook","text":"<ul> <li><code>publish_notebook.py</code> is a little tool that allows to:<ol> <li>Opening a notebook in your browser (useful for read-only mode)</li> <li>E.g., without having to use Jupyter notebook (which modifies the file in    your client) or github preview (which is slow or fails when the notebook    is too large)</li> <li>Sharing a notebook with others in a simple way</li> <li>Pointing to detailed documentation in your analysis Google docs</li> <li>Reviewing someone's notebook</li> <li>Comparing multiple notebooks against each other in different browser  windows</li> <li>Taking a snapshot / checkpoint of a notebook as a backup or before making  changes</li> <li>This is a lightweight alternative to \"unit testing\" to capture the    desired behavior of a notebook</li> <li>One can take a snapshot and visually compare multiple notebooks    side-by-side for changes</li> </ol> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#detailed_instructions","title":"Detailed instructions","text":"<ul> <li> <p>You can get details by running:</p> <p>```bash</p> <p>dev_scripts/notebooks/publish_notebook.py -h ```</p> </li> <li> <p>Plug-in for Chrome     my-s3-browser</p> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#publish_notebooks","title":"Publish notebooks","text":"<ul> <li> <p>Make sure that your environment is set up properly</p> <p>```bash</p> <p>more ~/.aws/credentials [ck] aws_access_key_id= aws_secret_access_key=</p> <p>printenv | grep CK_ CSFY_AWS_PROFILE=ck ```</p> </li> <li> <p>If you don't have them, you need to re-run <code>source dev_scripts/setenv.sh</code> in     all the shells. It might be easier to kill that tmux session and restart it</p> <p>```bash</p> <p>tmux kill-session --t limeXYZ</p> <p>~/go_lem.sh XYZ ```</p> </li> <li> <p>Inside or outside a Docker bash run</p> <p>```bash</p> <p>publish_notebook.py --file http://127.0.0.1:2908/notebooks/notebooks/Task40_Optimizer.ipynb --action publish_on_s3 ```</p> </li> <li> <p>The file is copied to S3</p> <p><code>bash Copying './Task40_Optimizer.20210717_010806.html' to 's3://alphamatic-data/notebooks/Task40_Optimizer.20210717_010806.html'</code></p> </li> <li> <p>You can also save the data locally:</p> <p>```bash</p> <p>publish_notebook.py --file amp/oms/notebooks/Master_forecast_processor_reader.ipynb --action publish_on_s3 --aws_profile saml-spm-sasm ```</p> </li> <li> <p>You can also use a different path or profile by specifying it directly</p> <p>```bash</p> <p>publish_notebook.py \\ --file http://127.0.0.1:2908/notebooks/notebooks/Task40_Optimizer.ipynb \\ --action publish_on_s3 \\ --s3_path s3://alphamatic-data/notebooks \\ --aws_profile am ```</p> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#open_a_published_notebook","title":"Open a published notebook","text":""},{"location":"onboarding/all.development.how_to_guide.html#start_a_server","title":"Start a server","text":"<ul> <li> <p><code>(cd /local/home/share/html/published_notebooks; python3 -m http.server 8000)</code></p> </li> <li> <p>Go to the page in the local browser</p> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#using_the_dev_box","title":"Using the dev box","text":"<ul> <li> <p>To open a notebook saved on S3, *outside* a Docker container run:</p> <p>```bash</p> <p>publish_notebook.py --action open --file s3://alphamatic-data/notebooks/Task40_Optimizer.20210717_010806.html ```</p> </li> <li> <p>This opens a Chrome window through X-windows.</p> </li> <li> <p>To open files faster you can open a Chrome window in background with</p> <p>```bash</p> <p>google-chrome ```</p> </li> <li> <p>And then navigate to the path (e.g.,     <code>/local/home/share/html/published_notebooks/Master_forecast_processor_reader.20220810-112328.html</code>)</p> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#using_windows_browser","title":"Using Windows browser","text":"<ul> <li> <p>Another approach is:</p> <p>```bash</p> <p>aws s3 presign --expires-in 36000 s3://alphamatic-data/notebooks/Task40_Optimizer.20210716_194400.html | xclip ```</p> </li> <li> <p>Open the link saved in the clipboard in the Windows browser</p> </li> <li>For some reason, Chrome saves the link instead of opening, so you need to     click on the saved link</li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#how_to_create_a_private_fork","title":"How to create a private fork","text":"<ul> <li>Https://stackoverflow.com/questions/10065526/github-how-to-make-a-fork-of-public-repository-private</li> <li> <p>From     https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/duplicating-a-repository</p> <p>```bash</p> <p>git clone --bare git@github.com:alphamatic/amp.git amp_bare</p> <p>git push --mirror https://github.com/cryptomtc/cmamp.git ```</p> </li> <li> <p>It worked only as cryptomtc, but not using my key</p> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#integrate_public_to_private_amp_-_cmamp","title":"Integrate public to private: <code>amp</code> -&gt; <code>cmamp</code>","text":""},{"location":"onboarding/all.development.how_to_guide.html#set-up","title":"Set-up","text":"<pre><code>&gt; git remote add public git@github.com:alphamatic/amp\n\n## Go to cmamp\n&gt; cd /data/saggese/src/cmamp1\n&gt; cd /Users/saggese/src/cmamp1\n\n## Add the remote\n## git remote add public https://github.com/exampleuser/public-repo.git\n&gt; git remote add public git@github.com:alphamatic/amp\n\n&gt; git remote -v\norigin https://github.com/cryptomtc/cmamp.git (fetch)\norigin https://github.com/cryptomtc/cmamp.git (push)\npublic git@github.com:alphamatic/amp (fetch)\npublic git@github.com:alphamatic/amp(push)\n</code></pre>"},{"location":"onboarding/all.development.how_to_guide.html#ours_vs_theirs","title":"Ours vs theirs","text":"<ul> <li> <p>From     https://stackoverflow.com/questions/25576415/what-is-the-precise-meaning-of-ours-and-theirs-in-git/25576672</p> </li> <li> <p>When merging:</p> <ul> <li>Ours = branch checked out (git checkout *ours*)</li> <li>Theirs = branch being merged (git merge *theirs*)</li> </ul> </li> <li>When rebasing the role is swapped<ul> <li>Ours = branch being rebased onto (e.g., master)</li> <li>Theirs = branch being rebased (e.g., feature)</li> </ul> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#sync_the_repos_after_double_integration","title":"Sync the repos (after double integration)","text":"<pre><code>&gt; git fetch origin; git fetch public\n\n## Pull from both repos\n\n&gt; git pull public master -X ours\n\n## You might want to use `git pull -X theirs` or `ours`\n\n&gt; git pull -X theirs\n\n&gt; git pull public master -s recursive -X ours\n\n## When there is a file added it is better to add\n\n&gt; git diff --name-status --diff-filter=U | awk '{print $2}'\n\nim/ccxt/db/test/test_ccxt_db_utils.py\n\n## Merge branch\n\n&gt; gs\n+ git status\nOn branch AmpTask1786_Integrate_20211128_02 Your branch and 'origin/AmpTask1786_Integrate_20211128_02' have diverged, and have 861 and 489\ndifferent commits each, respectively. (use \"git pull\" to merge the remote branch into yours)\n\nYou are in a sparse checkout with 100% of tracked files present.\n\nnothing to commit, working tree clean\n\n&gt; git pull -X ours\n\n### Make sure it's synced at ToT\n\n&gt; rsync --delete -r /Users/saggese/src/cmamp2/ /Users/saggese/src/cmamp1\n--exclude='.git/'\n\n&gt; diff -r --brief /Users/saggese/src/cmamp1 /Users/saggese/src/cmamp2 | grep -v \\.git\n</code></pre>"},{"location":"onboarding/all.development.how_to_guide.html#updated_sync","title":"Updated sync","text":"<pre><code>&gt; git fetch origin; git fetch public\n</code></pre>"},{"location":"onboarding/all.development.how_to_guide.html#check_that_things_are_fine","title":"Check that things are fine","text":"<pre><code>&gt; git diff origin/master... &gt;patch.txt\n\n&gt; cd /Users/saggese/src/cmamp2\n\n## Create a branch\n\n&gt; git checkout -b Cmamp114_Integrate_amp_cmamp_20210928\n&gt; git apply patch.txt\n\n## Compare branch with references\n\n&gt; dev_scripts/diff_to_vimdiff.py --dir1 /Users/saggese/src/cmamp1/im --dir2\n/Users/saggese/src/cmamp2/im\n\n&gt; diff -r --brief /Users/saggese/src/lemonade3/amp \\~/src/cmamp2 | grep -v \"/im\"\n\n## Creates a merge commit\n&gt; git push origin master\n</code></pre>"},{"location":"onboarding/all.development.how_to_guide.html#integrate_private_to_public_cmamp_-_amp","title":"Integrate private to public: <code>cmamp</code> -&gt; <code>amp</code>","text":"<pre><code>&gt; cd /data/saggese/src/cmamp1\n&gt; tar cvzf patch.tgz $(git diff --name-onlyorigin/master public/master | grep -v repo_config.py)\n\n&gt; cd /Users/saggese/src/amp1 git remote add cmamp\n&gt; git@github.com:cryptomtc/cmamp.git\n\n&gt; GIT_SSH_COMMAND=\"ssh -i \\~/.ssh/cryptomatic/id_rsa.cryptomtc.github\" git fetch\n&gt; git@github.com:cryptomtc/cmamp.git\n\n&gt; git checkout -b Integrate_20210928\n\n&gt; GIT_SSH_COMMAND=\"ssh -i \\~/.ssh/cryptomatic/id_rsa.cryptomtc.github\" git pull\n&gt; cmamp master -X ours\n</code></pre>"},{"location":"onboarding/all.development.how_to_guide.html#squash_commit_of_everything_in_the_branch","title":"Squash commit of everything in the branch","text":"<ul> <li> <p>From     https://stackoverflow.com/questions/25356810/git-how-to-squash-all-commits-on-branch</p> <p>```bash</p> <p>git checkout yourBranch git reset $(git merge-base master $(git branch --show-current)) git add -A git commit -m \"Squash\" git push --force ```</p> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#double_integration_cmamp_--_amp","title":"Double integration <code>cmamp</code> &lt; -- &gt; <code>amp</code>","text":"<ul> <li>The bug is https://github.com/alphamatic/amp/issues/1786</li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#script_set-up","title":"Script set-up","text":"<pre><code>&gt; vi /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh\nUpdate the date\n\n&gt; vi /Users/saggese/src/amp1/dev_scripts/integrate_repos/*\n\n&gt; cd \\~/src/amp1\n&gt; source /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh\n\n&gt; cd \\~/src/cmamp1\n&gt; source /Users/saggese/src/amp1/dev_scripts/integrate_repos/setup.sh\n</code></pre>"},{"location":"onboarding/all.development.how_to_guide.html#manual_set-up_branches","title":"Manual set-up branches","text":"<pre><code>## Go to cmamp1\n&gt; go_amp.sh cmamp 1\n\n## Set up the env vars in both clients\n&gt; export AMP_DIR=/Users/saggese/src/amp1; export\nCMAMP_DIR=/Users/saggese/src/cmamp1; echo \"$AMP_DIR\"; ls\n$AMP_DIR; echo \"$CMAMP_DIR\"; ls $CMAMP_DIR\n\n## Create two branches\n&gt; export BRANCH_NAME=AmpTask1786_Integrate_20211010 export BRANCH_NAME=AmpTask1786_Integrate_2021117\n...\n&gt; cd $AMP_DIR\n\n## Create automatically\n&gt; i git_create_branch -b $BRANCH_NAME\n\n## Create manually\n&gt; git checkout -b $BRANCH_NAME\n&gt; git push --set-upstream origin $BRANCH_NAME\n\n&gt; cd $CMAMP_DIR\n&gt; i git_create_branch -b $BRANCH_NAME\n</code></pre>"},{"location":"onboarding/all.development.how_to_guide.html#high-level_plan","title":"High-level plan","text":"<ul> <li>SUBDIR=im<ul> <li>Typically <code>cmamp</code> is copied on top of <code>amp</code></li> </ul> </li> <li>SUBDIR=devops<ul> <li><code>cmamp</code> and <code>amp</code> need to be different (until we unify the Docker flow)</li> </ul> </li> <li>Everything else<ul> <li>Typically <code>amp</code> -&gt; <code>cmamp</code></li> </ul> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#sync_im_cmamp_-_amp","title":"Sync <code>im</code> <code>cmamp</code> -&gt; <code>amp</code>","text":"<pre><code>SUBDIR=im\n\n## Check different files\n&gt; diff -r --brief $AMP_DIR/$SUBDIR $CMAMP_DIR/$SUBDIR | grep -v .git\n\n## Diff the entire dirs with vimdiff\n&gt; dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR/$SUBDIR --dir2 $CMAMP_DIR/$SUBDIR\n\n## Find different files\n&gt; find $AMP_DIR/$SUBDIR -name \"*\"; find $CMAMP_DIR/$SUBDIR -name \"*\" sdiff\n/tmp/dir1 /tmp/dir2\n\n## Copy cmamp -&gt; amp\n&gt; rsync --delete -au $CMAMP_DIR/$SUBDIR/ $AMP_DIR/$SUBDIR\n-a = archive\n-u = ignore newer\n\n## Add all the untracked files\n&gt; cd $AMP_DIR/$SUBDIR &amp;&amp; git add $(git ls-files -o --exclude-standard)\n\n## Check that there are no differences after copying\n&gt; dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR/$SUBDIR --dir2 $CMAMP_DIR/$SUBDIR\n\n==========\n\n&gt; rsync --delete -rtu $AMP_DIR/$SUBDIR/ $CMAMP_DIR/$SUBDIR\n\n&gt; rsync --dry-run -rtui --delete $AMP_DIR/$SUBDIR/ $CMAMP_DIR/$SUBDIR/ .d..t.... ./\n&gt; f..t.... __init__.py\ncd+++++++ features/\n&gt; f+++++++ features/__init__.py\n&gt; f+++++++ features/pipeline.py\ncd+++++++ features/test/\n&gt; f+++++++ features/test/test_feature_pipeline.py\ncd+++++++ features/test/TestFeaturePipeline.test1/\ncd+++++++ features/test/TestFeaturePipeline.test1/output/\n&gt; f+++++++ features/test/TestFeaturePipeline.test1/output/test.txt\n.d..t.... price/\n.d..t.... real_time/\n&gt; f..t.... real_time/__init__.py\n.d..t.... real_time/notebooks/\n&gt; f..t.... real_time/notebooks/Implement_RT_interface.ipynb\n&gt; f..t.... real_time/notebooks/Implement_RT_interface.py\n.d..t.... real_time/test/\ncd+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/\ncd+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/output/\n&gt; f+++++++ real_time/test/TestRealTimeReturnPipeline1.test1/output/test.txt\n.d..t.... returns/\n&gt; f..t.... returns/__init__.py\n&gt; f..t.... returns/pipeline.py\n.d..t.... returns/test/\n&gt; f..t.... returns/test/test_returns_pipeline.py\n.d..t.... returns/test/TestReturnsBuilder.test_equities1/\n.d..t.... returns/test/TestReturnsBuilder.test_equities1/output/\n.d..t.... returns/test/TestReturnsBuilder.test_futures1/\n.d..t.... returns/test/TestReturnsBuilder.test_futures1/output/\n\n&gt; rsync --dry-run -rtui --delete $CMAMP_DIR/$SUBDIR/ $AMP_DIR/$SUBDIR/\n&gt; f..t.... price/__init__.py\n&gt; f..t.... price/pipeline.py\n&gt; f..t.... real_time/pipeline.py\n&gt; f..t.... real_time/test/test_dataflow_amp_real_time_pipeline.py\n&gt; f..t.... returns/test/TestReturnsBuilder.test_equities1/output/test.txt\n&gt; f..t.... returns/test/TestReturnsBuilder.test_futures1/output/test.txt\n</code></pre>"},{"location":"onboarding/all.development.how_to_guide.html#sync_everything","title":"Sync everything","text":"<pre><code>## Check if there is anything in cmamp more recent than amp\n&gt; rsync -au --exclude='.git' --exclude='devops' $CMAMP_DIR/ $AMP_DIR\n\n## vimdiff\n&gt; dev_scripts/diff_to_vimdiff.py --dir1 $AMP_DIR --dir2\n$CMAMP_DIR\n\nF1: skip\nF9: choose left (i.e., amp)\nF10: choose right (i.e,. cmamp)\n\n## Copy\n\n&gt; rsync -au --delete --exclude='.git' --exclude='devops' --exclude='im'\n$AMP_DIR/\n$CMAMP_DIR\n\n## Add all the untracked files\n\n&gt; (cd $CMAMP_DIR/$SUBDIR &amp;&amp; git add $(git ls-files -o --exclude-standard))\n\n&gt; diff -r --brief $AMP_DIR $CMAMP_DIR | grep -v .git | grep Only\n</code></pre>"},{"location":"onboarding/all.development.how_to_guide.html#files_that_need_to_be_different","title":"Files that need to be different","text":"<ul> <li> <p><code>amp</code> needs an <code>if False</code> <code>helpers/lib_tasks.py</code></p> </li> <li> <p><code>amp</code> needs two tests disabled <code>im/ccxt/data/load/test/test_loader.py</code></p> <p><code>im/ccxt/data/load/test/test_loader.py</code></p> </li> </ul> <p>TODO(gp): How to copy files in vimdiff including last line?</p> <ul> <li>Have a script to remove all the last lines</li> <li>Some files end with an <code>0x0a</code></li> <li> <p><code>tr -d '\\\\r'</code></p> <p>```bash</p> <p>find . -name \"*.txt\" | xargs perl -pi -e 's/\\r\\n/\\n/g'</p> </li> </ul>"},{"location":"onboarding/all.development.how_to_guide.html#remove_no_newline_at_end_of_file","title":"Remove <code>No newline at end of file</code>","text":"<p>find . -name \"*.txt\" | xargs perl -pi -e 'chomp if eof' ```</p>"},{"location":"onboarding/all.development.how_to_guide.html#testing","title":"Testing","text":"<ul> <li>Run <code>amp</code> on my laptop (or on the server)</li> <li>IN PROGRESS: Get <code>amp</code> PR to pass on GH</li> <li>IN PROGRESS: Run lemonade on my laptop</li> <li>Run <code>cmamp</code> on the dev server</li> <li>Get <code>cmamp</code> PR to pass on GH</li> </ul>"},{"location":"onboarding/all.development_documents.reference.html","title":"All.development documents.reference","text":""},{"location":"onboarding/all.development_documents.reference.html#development_documents","title":"Development Documents","text":""},{"location":"onboarding/all.development_documents.reference.html#on-boarding","title":"On-boarding","text":"<ul> <li>Signing up for KaizenFlow</li> </ul>"},{"location":"onboarding/all.development_documents.reference.html#how_to_start_developing","title":"How to start developing","text":"<p>This contains the absolute minimal amount of info to start developing</p> <ul> <li> <p>General Rules of Collaboration</p> </li> <li> <p>Quick start for developing</p> </li> <li> <p>Set up the development environment</p> </li> <li> <p>How to organize your work</p> </li> <li> <p>First Review Process</p> </li> <li> <p>KaizenFlow Python coding style guide</p> </li> <li> <p>Jupyter notebook best practices</p> </li> <li> <p>Writing unit tests</p> </li> <li> <p>Running unit tests</p> </li> </ul>"},{"location":"onboarding/all.development_documents.reference.html#project_management","title":"Project management","text":"<ul> <li> <p>Contributor Feedback</p> <ul> <li>How we give feedback to contributors</li> </ul> </li> <li> <p>Code review</p> <ul> <li>PR review workflow from the code author and code reviewer points of view</li> </ul> </li> <li> <p>Development workflow</p> <ul> <li>Extended instructions on how to deploy your working environment and interact     with it</li> </ul> </li> <li> <p>GitHub workflows</p> <ul> <li>How to use GitHub, file a PR, review GitHub workflows</li> </ul> </li> <li> <p>Git workflow and best practices</p> <ul> <li><code>git</code> workflows, branch management, troubleshooting, common procedures</li> </ul> </li> <li> <p>Documentation about guidelines</p> <ul> <li>How to write guidelines, format documentation, convert Gdocs to markdown     files and vice versa</li> </ul> </li> <li> <p>Imports and packages</p> <ul> <li>How to import code, use packages, and resolve circular dependencies</li> </ul> </li> <li> <p>How to integrate repos</p> <ul> <li>Detailed instruction on how to integrate repos</li> </ul> </li> </ul>"},{"location":"onboarding/all.development_documents.reference.html#learn_how_to_become_efficient_at_developing","title":"Learn how to become efficient at developing","text":"<p>This contains a set of resources that over time will make 10x more productive</p> <ul> <li> <p>Docker</p> </li> <li> <p>Reading List</p> </li> <li> <p>Design Philosophy</p> </li> <li> <p>Codebase Clean-up</p> </li> <li> <p>Scrum Methodology</p> </li> </ul>"},{"location":"onboarding/all.development_documents.reference.html#in-depth_docs","title":"In-depth docs","text":"<ul> <li> <p>Buildmeister process</p> </li> <li> <p>Profiling</p> </li> <li> <p>Visual Studio Code</p> </li> <li> <p>Gsheet into pandas</p> </li> <li> <p>Email</p> </li> <li> <p>Glossary</p> </li> <li> <p>Type Hints</p> </li> <li> <p>PyCharm</p> </li> </ul>"},{"location":"onboarding/all.development_documents.reference.html#defi","title":"DeFi","text":"<ul> <li> <p>DeFi README</p> </li> <li> <p>From 0 to Web3</p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html","title":"All.development setup.how to guide","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#development_setup","title":"Development Setup","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#high-level_view","title":"High-level view","text":"<ul> <li> <p>In order to develop in <code>cmamp</code> you must run on one of our <code>dev</code> servers, by     creating a VPN connection to our VPC and then ssh-ing to a <code>dev</code> server from     your laptop</p> </li> <li> <p>Don't waste time trying to setting up the dev env on your laptop since certain     resources (e.g., AWS resources, DBs, etc) are not visible outside our VPN</p> </li> <li> <p>If there is a problem with the <code>dev</code> servers (e.g., too slow, tools that you     want that don't work, etc) we can make your dev env better, so that everyone     can benefit</p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#configure_bash_for_your_laptop","title":"Configure bash for your laptop","text":"<ul> <li>Make sure you are connected to the VPN before proceeding</li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#linux_and_macos","title":"Linux and MacOS","text":"<ul> <li> <p>Locally on your laptop tweak your <code>.bashrc</code> to add your server credentials:</p> <p>```bash</p> <p>vi .bashrc</p> </li> <li> <p>For MacOS you might want to tweak <code>.bash_profile</code>. After tweaking your bashrc,     you need to restart the shell session with <code>exit; bash</code> or do <code>source ~/.bash</code></p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#append_at_the_end","title":"Append at the end.","text":"<p>export CM_SSH_KEY=~/.ssh/cryptomatic.pem</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#eg_export_cm_ssh_keysshcksaggese-cryptomaticpem","title":"E.g., export CM_SSH_KEY=~/.ssh/ck/saggese-cryptomatic.pem","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#xyz_for_your_remote_linux_username_eg_saggese_danya_maxs","title":"XYZ for your remote Linux username, e.g., saggese, danya, maxs","text":"<p>export CM_USER=saggese export CM_IP=172.30.2.136 ```</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#windows","title":"Windows","text":"<ul> <li> <p>Download and install <code>putty.exe</code>, <code>pscp.exe</code>, and <code>puttygen.exe</code> from the     PuTTY Download Page</p> </li> <li> <p>Open <code>PuTTYgen</code> and click on the load button     </p> </li> <li> <p>Locate the private SSH key that you have downloaded</p> </li> <li> <p>Click the Save private key button to create the ppk file     </p> </li> <li> <p>Open <code>PuTTY</code>, under Session, enter your Hostname i.e. our server IP:     172.30.2.136</p> <p></p> </li> <li> <p>Under Connection, click Data and Enter your username as the Auto-login     username </p> </li> <li> <p>Under SSH, choose 2 from the Preferred SSH Protocol Version     </p> </li> <li> <p>Under SSH -&gt; Auth -&gt; Credentials, you will need to specify where your private     key can be found</p> </li> <li> <p>Remember, this is the ppk file you generated using <code>PuTTygen</code> earlier</p> </li> <li> <p>Click the Browse to locate the file on your computer     </p> </li> <li> <p>Under Sessions, type a name (such as \"my site\") in the Saved Sessions box and     then click Save. Hit Open to Run the session</p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#configure_ssh_for_your_laptop","title":"Configure .ssh for your laptop","text":"<ul> <li>To connect to the server use your previously generated private key, i.e. not     the public key that the Infra team sends you back</li> <li>Move the public-private key pair to <code>~/.ssh</code> and set <code>pem</code> as the private key     extension, see     more about <code>pem</code> extension. E.g.,     <code>~/.ssh/cryptomatic.pem</code> (private key) that you will use to connect.</li> <li>Set the permissions for your private key     <code>bash     laptop&gt; chmod 600 ~/.ssh/cryptomatic.pem</code></li> <li> <p>Now you can connect to the server through a VPN (see     <code>/docs/onboarding/ck.setup_vpn_and_dev_server_access.how_to_guide.md</code>)</p> <p>```bash laptop&gt; ssh -i $CM_SSH_KEY $CM_USER@$CM_IP</p> <p>E.g., laptop&gt; ssh -i ~/.ssh/cryptomatic.pem $CM_USER@$CM_IP ```</p> </li> <li> <p>You can also pass the user and IP manually, but adding those exports above     saves time in the long run.</p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#configure_ssh_for_the_server","title":"Configure .ssh for the server","text":"<ul> <li><code>ssh</code> into the server with     <code>bash     laptop&gt; ssh -i $CM_SSH_KEY $CM_USER@$CM_IP</code></li> <li>Your home directory should be under <code>/data</code> <code>bash     server&gt; echo $HOME     /data/saggese</code></li> <li>The directory on the server should look like:     <code>bash     server&gt; ls -l ~/.ssh/     total 16     -rw------- 1 saggese saggese  382 Nov 19 18:15 authorized_keys     -rw------- 1 saggese saggese  381 Nov 25 17:41 cryptomatic.pem.pub     -rw-r--r-- 1 saggese saggese  666 Nov 25 17:43 known_hosts</code></li> <li>(Optional step) If that's not the case, talk to IT to copy the public ssh key     to the server from your laptop.</li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#configure_bash_for_the_server_advanced","title":"Configure bash for the server (advanced)","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#configure_bash","title":"Configure bash","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#linux_and_macos_1","title":"Linux and MacOS","text":"<ul> <li>Now you want to configure the server <code>~/.bash</code> and the environment to your     preference.</li> <li> <p>E.g., you might want to copy <code>.vimrc</code>, pimp <code>.bashrc</code></p> <p>```bash</p> </li> <li> <p>Tweak permissions on the server     <code>bash     server&gt; ls -l ~/.ssh     total 32     -rw------- 1 saggese saggese  382 Nov 19 18:15 authorized_keys     -r-------- 1 saggese saggese 1679 Nov 25 17:40 cryptomatic.pem     -r-------- 1 saggese saggese  381 Nov 25 17:41 cryptomatic.pem.pub     -r-------- 1 saggese saggese 3243 Nov 30 17:54 id_rsa.cryptomtc.hub     -r-------- 1 saggese saggese  732 Nov 30 17:54 id_rsa.cryptomtc.hub.pub     -r-------- 1 saggese saggese 1675 Nov 30 17:48 id_rsa.gpsaggese.hub     -r-------- 1 saggese saggese  389 Nov 30 17:50 id_rsa.gpsaggese.hub.pub     -rw-r--r-- 1 saggese saggese  888 Nov 30 17:43 known_hosts</code></p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#copy_your_hub_keys_to_the_dev_server","title":"Copy your hub keys to the dev server","text":"<p>laptop&gt; export CM_SSH_KEY=~/.ssh/cryptomatic/saggese-cryptomatic.pem laptop&gt; export CM_USER=saggese laptop&gt; export CM_IP=172.30.2.136</p> <p>laptop&gt; scp -i $CM_SSH_KEY ~/.ssh/id_rsa $CM_USER@$CM_IP:/data/$CM_USER/.ssh/id_rsa.gpsaggese.hub</p> <p>laptop&gt; scp -i ~/.ssh/cryptomatic/saggese-cryptomatic.pem ~/.ssh/id_rsa.pub saggese@172.30.2.136:/data/saggese/.ssh/id_rsa.gpsaggese.hub.pub</p> <p>laptop&gt; scp -i ~/.ssh/cryptomatic/saggese-cryptomatic.pem ~/.ssh/cryptomatic/id_rsa.cryptomtc.hub saggese@172.30.2.136:/data/saggese/.ssh/id_rsa.cryptomtc.github</p> <p>laptop&gt; scp -i ~/.ssh/cryptomatic/saggese-cryptomatic.pem ~/.ssh/cryptomatic/id_rsa.cryptomtc.hub.pub saggese@172.30.2.136:/data/saggese/.ssh/id_rsa.cryptomtc.github.pub ```</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#windows_1","title":"Windows","text":"<ul> <li> <p>Use the above steps from Configure bash of Linux and MacOS. Just use scp in     the following way:</p> <p>```bash laptop&gt; scp -i $CM_SSH_KEY ~/.ssh/id_rsa $CM_USER@$CM_IP:/data/$CM_USER/.ssh/id_rsa.gpsaggese.hub</p> <p>laptop&gt; pscp -scp -i $PPK_KEY_GENERATED_USING_PUTTYGEN ~/.ssh/id_rsa $CM_USER@$CM_IP:/data/$CM_USER/.ssh/id_rsa.gpsaggese.hub ```</p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#configure_tmux_optional","title":"Configure tmux (optional)","text":"<ul> <li> <p>Create this file under <code>~/.tmux.conf</code> on the server. The prefix is <code>CTRL-g</code>.     Copy-paste the code below into the <code>.tmux.conf</code> and save the changes.</p> <p>```bash</p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#use_c-x","title":"Use C-x.","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#set_-g_prefix_c-x","title":"set -g prefix C-x","text":"<p>set -g prefix C-g unbind-key C-b bind-key C-g send-prefix</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#set_status_bar","title":"Set status bar.","text":"<p>set -g status-bg black set -g status-fg white set -g status-left '#[fg=green]#H'</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#use_vi_shortcuts","title":"Use vi shortcuts.","text":"<p>set-window-option -g mode-keys vi</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#set_-g_status-right_fgblues_ip_fgyellow_d_b_y_fggreen_lm_p_date_-u_awk_print_4","title":"set -g status-right \"#[fg=blue]#S #I:#P #[fg=yellow]:: %d %b %Y #[fg=green]:: %l:%M %p :: #(date -u | awk '{print $4}')::\"","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#100k_lines","title":"100k lines","text":"<p>set-option -g history-limit 100000</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#highlight_active_window","title":"Highlight active window.","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#set-window-option_-g_window-status-current-bg_red","title":"set-window-option -g window-status-current-bg red","text":"<p>set -g window-status-current-style bg=red</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#try_to_speed_up_the_processing_of_shortcuts","title":"Try to speed up the processing of shortcuts.","text":"<p>set-option -g repeat-time 0 set -sg escape-time 0</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#to_get_notification_when_a_command_is_done","title":"To get notification when a command is done.","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#todo_this_seems_to_give_problems_with_my_mac_and_being_removed","title":"TODO: This seems to give problems with my mac and being removed.","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#set_-g_visual-content_on","title":"set -g visual-content on","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#setw_-g_monitor-content_cmd_done","title":"setw -g monitor-content CMD_DONE","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#for_mac_only","title":"For Mac only.","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#workaround_to_lsopenurlswithrole_failed_with_error_-10810","title":"Workaround to 'LSOpenURLsWithRole() failed with error -10810'.","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#set_-g_default-command_reattach-to-user-namespace_-l_binbash","title":"set -g default-command \"reattach-to-user-namespace -l /bin/bash\"","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#set-option_-g_default-command_which_reattach-to-user-namespace_devnull_reattach-to-user-namespace_-l_shell_shell","title":"set-option -g default-command \"which reattach-to-user-namespace &gt; /dev/null &amp;&amp; #reattach-to-user-namespace -l $SHELL || $SHELL\"","text":"<p>```</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#spruce_up_your_environment","title":"Spruce up your environment","text":"<ul> <li> <p>If you have multiple users, you can decide which one to use</p> <p>```bash</p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#test_hub_connection","title":"Test hub connection","text":"<p>ssh -i ~/.ssh/id_rsa.gpsaggese.hub -T git@github.com _SSH_COMMAND=\"ssh -i ~/.ssh/id_rsa.gpsaggese.github\" git clone git@github.com:gpsaggese/dotted_files.git</p> <p>_SSH_COMMAND=\"ssh -i ~/.ssh/id_rsa.gpsaggese.github\" git clone --recurse git@github.com:alphamatic/amp.git amp1 ```</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#set_up_aws","title":"Set up AWS","text":"<ul> <li> <p>We use AWS profile called <code>ck</code></p> <ul> <li>It is used to access S3 bucket data and other AWS resources</li> <li>API credentials are included in the <code>zip</code> archive sent via e-mail during     on-boarding process</li> <li>Ask the Infra team if you have not obtained the credentials</li> </ul> </li> <li> <p>Create dir with proper permissions:     <code>bash     &gt; mkdir ~/.aws; chmod 700 ~/.aws</code></p> </li> <li> <p>Put the following into <code>~/.aws/config</code>:</p> <p><code>bash [profile ck] region = eu-north-1</code></p> </li> <li> <p>Put the following into <code>~/.aws/credentials</code>:</p> <p><code>bash [ck] aws_access_key_id=*** aws_secret_access_key=*** aws_s3_bucket=cryptokaizen-data</code></p> </li> <li> <p>Note that AWS uses <code>[profile XYZ]</code> vs <code>[XYZ]</code> in the two types of files     (config and credentials), yes, this is insane</p> </li> <li>Change permissions to ready-only and only for your user since it contains     credentials:     <code>bash     &gt; chmod 400 ~/.aws/*</code></li> <li> <p>You should get to permissions that look like:</p> <p>```bash saggese@ip-172-30-2-136:~$ ls -ld ~/.aws drwx------ 2 saggese saggese 4096 Dec  3 17:09 /data/saggese/.aws</p> <p>saggese@ip-172-30-2-136:~$ ls -l ~/.aws total 8 -r-------- 1 saggese saggese  32 Dec  3 17:08 config -r-------- 1 saggese saggese 403 Dec  3 17:09 credentials ```</p> </li> <li> <p>Test the access to the S3 bucket</p> <ul> <li>For the ck:     <code>bash     &gt; aws s3 ls s3://cryptokaizen-data --profile ck      PRE daily_staged/      PRE db_archive/      PRE historical/      PRE reorg/      PRE unit_test/</code></li> </ul> </li> <li> <p>Test accessing AWS ECR to pull containers:</p> <p>```bash</p> <p>eval $(aws ecr get-login --profile am --no-include-email --region us-east-1) WARNING! Using --password via the CLI is insecure. Use --password-stdin. WARNING! Your password will be stored unencrypted in /data/saggese/.docker/config.json. Configure a credential helper to remove this warning. See https://docs.docker.com/engine/reference/commandline/login/#credentials-store</p> <p>Login Succeeded ```</p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#clone_the_hub_repo","title":"Clone the Hub repo","text":""},{"location":"onboarding/all.development_setup.how_to_guide.html#ssh_keys","title":"SSH Keys","text":"<ul> <li>For hub you can use your key otherwise you can generate an ssh key for hub</li> <li>Note:<ul> <li>You can use any SSH key, even a \"personal\" one that one uses for his/her     projects</li> <li>In practice, the logic is \"user authenticates with GH, we tell GH to grant     access to a private repo, GH is happy\"</li> </ul> </li> <li>Typically you can save the Hub authentication in <code>~/.ssh/id_rsa</code> and     <code>~/.ssh/id_rsa.pub</code></li> <li>Note: make sure permissions are read-only, otherwise change permissions, e.g.,     <code>chmod 400 ~/.ssh/id_ed25519</code></li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#git_clone","title":"Git clone","text":"<ul> <li>In order to use our automation scripts, the path to local copy of the repos     needs look like this <code>${HOME}/src/{REPO_NAME}{IDX}</code>, e.g.,     <code>/data/saggese/src/cmamp1</code>.</li> <li>Clone a repo using an SSH key, we do not use HTTPS</li> <li>Clone the repo with:     <code>bash     &gt; mkdir ~/src     &gt; cd ~/src     # In general form.     &gt; git clone --recursive git@github.com:causify-ai/{repo_name}.git ~/src/{repo_name}{index}     # Example for cmamp.     &gt; git clone --recursive git@github.com:causify-ai/cmamp.git ~/src/cmamp1</code></li> <li>You can have multiple cloned repos like <code>cmamp2</code>, <code>cmamp3</code> and so on to work     on unrelated changes at the same time</li> <li> <p>If the repo contains submodules we need to checkout master in all submodules.</p> <p>```bash</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#before","title":"Before.","text":"<p>git status HEAD detached at bd69850bb nothing to commit, working tree clean</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#in_general_form","title":"In general form.","text":"<p>cd {submodule} git checkout master git pull</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#example_for_orange_which_contains_cmamp_which_contains_helpers","title":"Example for <code>orange</code> which contains <code>cmamp</code> which contains <code>helpers</code>.","text":"<p>cd amp git checkout master git pull cd helpers_root git checkout master git pull</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#after","title":"After.","text":"<p>git status On branch master Your branch is up to date with 'origin/master'.</p> <p>Changes not staged for commit:     (use \"git add ...\" to update what will be committed)     (use \"git restore ...\" to discard changes in working directory)                 modified:   helpers_root (new commits) ```"},{"location":"onboarding/all.development_setup.how_to_guide.html#set_up_the_thin_environment","title":"Set up the thin environment","text":"<ul> <li>Build the thin environment we use to call invoke and Docker. For all the repos     we use the thin environment from the <code>helpers</code> repo, i.e. to build one we need     to run:     <code>bash         &gt; cd amp (only if amp is submodule)         &gt; cd helpers_root         &gt; ./dev_scripts_helpers/thin_client/build.py</code></li> <li>This script to build the environment is run rarely when the dependencies are     changed.</li> <li>This environment is shared by multiple repos (e.g., all the <code>cmamp</code> and     <code>tutorials</code>), so once you build the environment you can use it for all the     repos and for multiple clients</li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#activate_the_thin_environment","title":"Activate the thin environment","text":"<ul> <li>It is okay to skip this step as long as you use <code>tmux</code>. The thin environment     is activated automatically within a <code>tmux</code> session.</li> <li>To activate the thin environment, run the <code>setenv.sh</code> script. The script is     located under <code>dev_scripts_{repo_name}/thin_client</code>, e.g.:     <code>bash     &gt; source dev_scripts_cmamp/thin_client/setenv.sh     &gt; source dev_scripts_orange/thin_client/setenv.sh</code></li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#create_a_tmux_session","title":"Create a tmux session","text":"<ul> <li> <p>Create a soft link. The cmd below will create a file <code>~/go_{repo_name}.py</code></p> <p>```bash</p> <p>dev_scripts_{repo_name}/thin_client/tmux.py --create_global_link ```</p> </li> <li> <p>Create a tmux session. Choose <code>index</code> based on the dir name, e.g., <code>--index 1</code>     if the dir name is <code>~/src/cmamp1</code>.</p> <p>```bash</p> <p>dev_scripts_{repo_name}/thin_client/tmux.py --index 1 ```</p> </li> <li> <p>You need to create the tmux environment once per client and then you can     re-connect with:</p> <p>```bash</p> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#check_the_available_environments","title":"Check the available environments.","text":"<p>tmux ls cmamp1: 4 windows (created Fri Dec  3 18:27:09 2021) (attached)</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#attach_an_environment","title":"Attach an environment.","text":"<p>tmux attach -t cmamp1 ```</p>"},{"location":"onboarding/all.development_setup.how_to_guide.html#configure_gh","title":"Configure <code>gh</code>","text":"<ul> <li> <p>We enjoy using Hub CLI</p> <ul> <li>Create a Personal Access Token (classic) on Hub following the official guide</li> <li> <p>Put the following permissions:</p> <p></p> </li> </ul> </li> <li> <p>Create personal access token (aka PAT) as done above and save it to     <code>~/hub_pat.txt</code></p> </li> <li>You can login with a single command     <code>bash     &gt; cat ~/hub_pat.txt | gh auth login --hostname github.com --with-token</code></li> <li>Sometimes you need to create the file to make <code>gh</code> happy     <code>bash     &gt; touch ~/.config/gh/config.yml</code></li> <li>Check the current authentication status of <code>gh</code>:     <code>bash     &gt; gh auth status     hub.com         \u2713 Logged in to hub.com as gpsaggese (/Users/saggese/.config/gh/hosts.yml)         \u2713  operations for github.com configured to use ssh protocol.         \u2713 Token: *******************</code></li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#fix_permissions","title":"Fix permissions","text":"<ul> <li>Sometimes for whatever reason a file is created by <code>root</code> which prevents     executing certain commands being a non-root user w/o sudo privileges</li> <li>To change permissions we need sudo privileges that we do not have outside a     Docker container but we have inside a Docker container     ```bash     (amp.client_venv) dan@ip-172-31-12-40:~/src/cmamp1$ gh auth login     ? What account do you want to log into? Hub.com     ? What is your preferred protocol for  operations? SSH     ? Upload your SSH public key to your Hub account? Skip     ? How would you like to authenticate Hub CLI? Paste an authentication token     Tip: you can generate a Personal Access Token here https://hub.com/settings/tokens     The minimum required scopes are 'repo', 'read:org'.     ? Paste your authentication token: ******<ul> <li>gh config set -h hub.com git_protocol ssh \u2713 Configured  protocol open /data/dan/.config/gh/config.yml: permission denied (amp.client_venv) dan@ip-172-31-12-40:~/src/cmamp1$ cat ~/hub_pat.txt | gh auth login --hostname github.com --with-token open /data/dan/.config/gh/config.yml: permission denied ```</li> <li>Get inside the docker container using <code>i docker_bash</code></li> <li>Change dir using <code>cd ~/.config</code></li> <li>Run <code>sudo chown -R username:username gh</code> where username is your username     inside the container, e.g. <code>user_1022:user_1022</code></li> <li>Run <code>ls -l gh | grep root</code> if nothing, it's a good sign</li> <li>Close container printing <code>exit</code> or ctrl+D</li> <li>Log in again using <code>gh auth login</code> and follow the instructions above</li> <li>Check your auth status using <code>gh auth status</code></li> </ul> </li> </ul>"},{"location":"onboarding/all.development_setup.how_to_guide.html#begin_working","title":"Begin working","text":"<ul> <li>Conceptually the steps are:<ul> <li>Clone a repo in <code>~/src/...</code><ul> <li>You can have as many clients as you want (<code>cmamp1</code>, <code>cmamp2</code>, ...,     <code>helpers1</code>, ...)</li> </ul> </li> <li>Build the \"thin\" environment:<ul> <li>Needs to be done only once for all your clients     <code>bash     &gt; cd helpers_root     &gt; ./dev_scripts_helpers/thin_client/build.py</code></li> </ul> </li> <li>Start a tmux session<ul> <li>Choose <code>index</code> based on folder name, e.g., <code>--index 1</code> if repo name is     <code>~/src/cmamp1</code> <code>bash     &gt; dev_scripts_{repo_name}/thin_client/tmux.py --index 1</code></li> </ul> </li> <li>Activate environment:<ul> <li>It is activated automatically in all the tmux sessions</li> <li>Otherwise, run:     <code>bash     &gt; source dev_scripts_{repo_name}/thin_client/setenv.sh</code></li> </ul> </li> <li>Pull <code>cmamp</code> docker images:<ul> <li>This image is shared all your clients (actually all users on the server     use the same image)     <code>bash     &gt; i docker_pull     Digest: sha256:308d52a0354845d1a2aa2014282efa66864329e7e22bf15dad518c219b5d5a3d     Status: Downloaded newer image for 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:dev     665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:dev</code></li> </ul> </li> <li>Check <code>docker_bash</code>:     <code>bash     &gt; i docker_bash     INFO: &gt; cmd='/data/saggese/src/venv/helpers.client_venv/bin/invoke docker_bash'     ## docker_bash:     IMAGE=665840871993.dkr.ecr.us-east-1.amazonaws.com/helpers:dev \\                     docker-compose \\                     --file /data/saggese/src/helpers1/devops/compose/docker-compose.yml \\                     --env-file devops/env/default.env \\                     run \\                     --rm \\                     --user $(id -u):$(id -g) \\                     app \\                     bash             user_1005@9e4373e01213:/app$</code><ul> <li>Exit the container with <code>exit</code> and get back to the server prompt</li> </ul> </li> <li>Run regressions:     <code>bash     &gt; i run_fast_tests</code><ul> <li>If you can run all the regressions without errors, then you are totally     set up to start developing</li> </ul> </li> </ul> </li> </ul>"},{"location":"onboarding/all.hiring_process.how_to_guide.html","title":"All.hiring process.how to guide","text":""},{"location":"onboarding/all.hiring_process.how_to_guide.html#hiring_process","title":"Hiring process","text":""},{"location":"onboarding/all.hiring_process.how_to_guide.html#pre-hiring_bounty","title":"Pre-hiring: Bounty","text":"<ul> <li>We have hundreds of candidates who want to contribute to our projects and work     with us, but we can't easily find the ones that have the right skills to join     the team</li> <li>One approach is for us to go through the applications, rank the candidates,     on-board them and help them achieve the quality of code we expect from team     members</li> <li>Another approach is to let the candidates show that they can deliver through     bounty hunting</li> <li>The idea is to have challenging projects (bounties) that potential hires can     take on<ul> <li>From a couple of days to ~2 weeks of work</li> <li>Lots of coding with clear specs</li> <li>Not on the critical path, \"nice to have\" projects that will become useful in     the future</li> </ul> </li> <li>Those who excel at the bounties get paid and get onto the     hiring path</li> <li>More information and instructions for bounty hunters are provided in     <code>/docs/onboarding/bounty.onboarding_checklist.reference.md</code></li> </ul>"},{"location":"onboarding/all.hiring_process.how_to_guide.html#hiring_stages","title":"Hiring stages","text":"<ul> <li>We separate the hiring process into the following stages:</li> <li>Intern<ul> <li>Not a part of the company</li> <li>Paid, but not through the company payroll</li> <li>Interns only have access to public repos (e.g., <code>helpers</code>, <code>tutorials</code>)</li> <li>Can be full-time or part-time<ul> <li>Since most are students, during summer they can work full-time</li> <li>In any case we expect at least 20 hrs/week</li> </ul> </li> <li>Interns go through     \"light\" on-boarding<ul> <li>Read the \"must-read\" docs about how we do things</li> <li>Set up for development on their laptops</li> </ul> </li> <li>Interns get simple tasks to work on, so that we can assess     how well they can follow directions and work independently<ul> <li>The tasks would be most likely related to Linter, documentation, AI, DS</li> <li>How to test potential hires to the Infra team? We decide case by case</li> </ul> </li> <li>Ideal goal: minimal effort for us to give a chance to somebody, high     probability of failing</li> <li>Interns are managed only by the hiring team without any input from team     leaders</li> <li>Interns receive feedback on various skills every 2 weeks (see more     below)</li> <li>After 1-2 months (or whenever we feel we have enough information to make an     informed decision), we decide whether the intern can move on to the     following stage of the hiring pipeline</li> </ul> </li> <li>3-month trial<ul> <li>Before an intern can turn into a permanent team member, they go through a     3-month trial period</li> <li>At this stage, they work full-time and have all the markings of a Causify     team member</li> <li>They go through     full on-boarding,     receiving company e-mail, server access, access to private repos, etc.</li> <li>They receive feedback on various skills every month to make sure they are     adapting and improving in the way we expect</li> <li>After 3 months, we make a decision whether to make an offer or not<ul> <li>The probability of failing at this stage should be low</li> </ul> </li> <li>This stage is run by the hiring team with some input from team leaders</li> <li>During this stage, we decide which team they go on: Infra, Dev, DS, AI</li> </ul> </li> <li>Permanent team member<ul> <li>Successful candidates receive an offer for a permanent full-time position     after the 3-month trial</li> <li>At this point they should already be fully on-boarded and ready to hit the     ground running</li> <li>They get assigned to a particular team/project and start being managed by     the corresponding team leader</li> <li>For now, we don't consider hiring new people to part-time positions since     it's too difficult to plan their involvement in projects</li> </ul> </li> </ul>"},{"location":"onboarding/all.hiring_process.how_to_guide.html#hiringmeister","title":"HiringMeister","text":"<ul> <li>Members of the hiring team alternate being the HiringMeister for 2 weeks</li> <li>To see who is the HiringMeister now, refer to     Rotation Meisters</li> <li>HiringMeister's duties include:<ul> <li>Organizing the hiring and onboarding process (see below for     more details)</li> <li>Communicating with applicants by email</li> <li>Supervising interns and people on a 3-month trial period</li> <li>Keeping all the relevant documentation up-to-date</li> <li>Improving the process when needed</li> <li>Managing the hiring project on Asana</li> </ul> </li> <li>In other words, HiringMeister is the first point of contact for triaging,     making sure nothing falls through the cracks</li> </ul>"},{"location":"onboarding/all.hiring_process.how_to_guide.html#step_by_step","title":"Step by step","text":"<ul> <li> <p>We look for candidates based on the specific hiring needs in our     teams/projects. There are several avenues:</p> <ul> <li>Emails to CS and related depts<ul> <li>Especially for PhD students</li> </ul> </li> <li>Using our LinkedIn pipeline<ul> <li>We are going to start targeting graduates from top schools</li> </ul> </li> <li>Job postings on LinkedIn</li> <li>Job postings on Upwork</li> </ul> </li> <li> <p>HiringMeister/GP: send an email to the applicant with a link to the     questionnaire     to gather information about them</p> </li> <li> <p>(Optional) HiringMeister/GP: send an email inviting the applicant to     participate in bounty hunting</p> <ul> <li>All further steps are put on hold while the applicant works on bounty tasks</li> </ul> </li> <li> <p>Hiring team: decide whether we take on the applicant as an intern</p> <ul> <li>Decided at a review meeting that takes place every 2 weeks</li> <li>We want to increase the quality of the collaborators, so if there is a red     flag (e.g., no GitHub, low GPA, undergrad) we can decide not to offer the     internship</li> <li>The goal is to avoid onboarding people that will likely disappoint us</li> <li>If we receive a single candidate application and find no red flags in the     profile, we should proceed further in the process</li> <li>It's ok to ask more team members to take a look</li> <li>If the candidate is a no-go, GP sends an email of rejection</li> <li>If we decide to on-board the candidate, continue with the steps below</li> </ul> </li> <li> <p>HiringMeister/GP: send an email asking to confirm if they are still interested     and ready to go</p> <ul> <li>Proceed with the steps below only if they respond with a confirmation</li> </ul> </li> <li> <p>HiringMeister: create a task on Asana in the     Hiring     project</p> <ul> <li>The task goes in the \"Onboarding\" section</li> <li>The name of the task is the name of the intern</li> <li> <p>In the task description, use the following template:</p> <p>```verbatim Pronoun: Personal email: Work status (e.g., when graduates):</p> <p>GitHub:     Onboarding issue:x     Issues:     PRs:</p> <p>Hiring info:     Google Form:     CV:     LinkedIn:     GitHub user:     TG:     Working hours / week:     Best piece of code:     How good (1-5): ```     - Fill in the template based on their questionnaire responses     - We use this Asana task to communicate about the intern</p> </li> </ul> </li> <li> <p>HiringMeister/GP: send invitations to GitHub repos with <code>write</code> permissions:</p> <ul> <li><code>helpers</code></li> <li><code>tutorials</code></li> </ul> </li> <li> <p>HiringMeister: create a GitHub issue for onboarding the intern</p> <ul> <li>Follow the instructions in     <code>intern.onboarding_checklist.reference.md</code></li> </ul> </li> <li> <p>HiringMeister: regularly check the updates made by the intern in the     onboarding issue and help resolve any errors they face</p> <ul> <li>This \"light\" onboarding process should take 2-3 days max</li> <li>The goal is to make sure that all the mechanisms for developing are tested     and working, so that we can focus on the next stages (\"can the intern fix a     simple bug?\", \"can the intern write more complex code?\", etc.)</li> </ul> </li> <li> <p>HiringMeister: once the onboarding is complete, assign a     warm-up issue to the intern</p> <ul> <li>If an intern shows lack of progress on their assigned warm-up task, ping     them twice for updates. If no progress is made, reassign the task to a more     promising intern</li> <li>Once the intern submits a pull request, review it to ensure adherence to our     processes. Provide constructive feedback on areas for improvement and     ascertain if the task's objectives have been fully met</li> <li>If the task is completed, assign a new one</li> </ul> </li> <li> <p>Hiring team: after 1 month, decide whether we should continue to collaborate     with the intern or not</p> </li> <li> <p>HiringMeister: when the intern moves on to the 3-month trial period,     create a GitHub issue for further onboarding</p> <ul> <li>Follow the instructions in     <code>all.onboarding_checklist.reference.md</code></li> </ul> </li> <li> <p>HiringMeister: once the full onboarding is complete, organize more complex     tasks to test their development and problem-solving skills</p> <ul> <li>This is done together with team leaders</li> </ul> </li> <li> <p>Hiring team: after 3 months, decide whether we want to offer a permanent     position on the team</p> <ul> <li>If the offer is made, decide which team/project the new team member is     joining, based on their background, skills, performance during internship     and 3-month trial, and, last but not least, company needs</li> </ul> </li> </ul>"},{"location":"onboarding/all.hiring_process.how_to_guide.html#warm-up_tasks","title":"Warm-up tasks","text":"<ul> <li>The HiringMeister should collaborate with the team to identify potential     warm-up tasks to give to the interns upon completion of their onboarding     process</li> <li>The goal of a warm-up issue is to write a bit of code and show they can follow     the process, the goal is not to check if they can solve a complex coding     problem</li> <li>It should take 1-2 days to get it done</li> <li>This helps us understand if they can<ul> <li>Follow the process (or at least show that they have read the docs and     somehow internalized them)</li> <li>Solve some trivial problems</li> <li>Write Python code</li> <li>Interact on GitHub</li> <li>Communicate with the team</li> </ul> </li> <li>The warm-up tasks should be straightforward to integrate, immediately     beneficial, unrelated to new features, and should not rely on the code in     private repos</li> <li>It is important that specs are provided in a manner that is easy to be     understood by interns, and any queries they may have regarding the task are     addressed quickly by the HiringMeister or another permanent team member</li> <li>Warm-up tasks are collected in advance:<ul> <li>Issues marked with the \"good first issue\" label on GH<ul> <li>Extremely simple (e.g., changing a few lines of code, converting Gdoc to     Markdown...)</li> <li>The goal is to practice following our procedure</li> </ul> </li> <li>Issues marked with the \"good second issue\" label on GH<ul> <li>Still simple but require a little bit more coding than the first issue</li> <li>The goal is to start testing the skills as well as confirm the     understanding of our process</li> </ul> </li> <li>Outsourceable issues gdoc</li> </ul> </li> </ul>"},{"location":"onboarding/all.hiring_process.how_to_guide.html#giving_feedback","title":"Giving feedback","text":"<ul> <li>Every 2 weeks interns are provided feedback that includes scores given to     their skills and performance</li> <li>The process and scoring criteria are defined in     <code>all.contributor_feedback.how_to_guide.md</code></li> <li>Scoring should be done by all members of the hiring team</li> <li>Interns with a low score should be let go</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html","title":"All.onboarding checklist.reference","text":""},{"location":"onboarding/all.onboarding_checklist.reference.html#onboarding_checklist","title":"Onboarding Checklist","text":""},{"location":"onboarding/all.onboarding_checklist.reference.html#onboarding_process_for_a_new_team_member","title":"Onboarding Process for a New Team Member","text":""},{"location":"onboarding/all.onboarding_checklist.reference.html#meta","title":"Meta","text":""},{"location":"onboarding/all.onboarding_checklist.reference.html#make_on-boarding_automatic","title":"Make On-Boarding Automatic","text":"<ul> <li>We want to make the onboarding process as automatic as possible</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#be_patient","title":"Be Patient","text":"<ul> <li>Let's use all the communication tools we have (screen sharing, Google Meet,     phone, Slack, email) to keep the process smooth and personal</li> <li>There are many (hidden) dependencies in the process, so something will     inevitably go wrong<ul> <li>Let's all be patient with each other</li> </ul> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#ask_for_confirmation","title":"Ask for Confirmation","text":"<ul> <li>Use checklists any time to make sure things get done</li> <li>Ask for confirmation of all the actions, e.g.,<ul> <li>\"Does this and that work?\"</li> <li>\"Did you receive the email?\"</li> <li>\"Can you log in?\"</li> </ul> </li> <li>Make the new team member follow the instructions so that they can get familiar     with the systems</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#make_on-boarding_similar_to_our_work_routine","title":"Make On-Boarding Similar to Our Work Routine","text":"<ul> <li> <p>Provide tools for interacting with the team</p> <ul> <li>During the process the new teammate will ask questions and he / she should     use the appropriate tools for each type of communication<ul> <li>Slack: only for interactive communications</li> <li>GitHub issues: for coding related things</li> <li>Asana tasks: for business/high-level tasks</li> <li>Email: everything else</li> </ul> </li> </ul> </li> <li> <p>We want to nudge a new hire to operate through the tools we use during our     daily work</p> <ul> <li>That is why we asking him to create a first issue about his / her experience     with the process</li> <li>He / she should put his / her notes in the GH issue while proceeding during     the onboarding process</li> </ul> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#improve_on-boarding_process","title":"Improve On-Boarding Process","text":"<ul> <li> <p>Review, improve, clarify process</p> <ul> <li>We want to point the new team member to a document rather than explaining     over and over the same things</li> <li>Always keep this document updated, ask candidates to follow it carefully,     and update it with a PR</li> <li>We want to use each new team-member experience to improve our workflow</li> <li>We want new team-members to improve the existing documentation using our     standard GitHub process</li> </ul> </li> <li> <p>Have the new team member:</p> <ul> <li>Open the first GitHub bug on the first day</li> <li>Open the first PR by the second day</li> </ul> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#people_involved_in_the_on-boarding","title":"People Involved in the On-Boarding","text":"<ul> <li>Each task is performed by one of the persons involved in the on-boarding<ul> <li>Team leader (e.g., GP, Paul, Grisha, Samarth)</li> <li>Team member (i.e., the person on-boarded)</li> <li>IT (e.g., Shayan)</li> <li>HR (e.g., Rose)</li> </ul> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#checklist","title":"Checklist","text":"<ul> <li>Source:     <code>all.onboarding_checklist.reference.md</code></li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#before_the_start_date","title":"Before the Start Date","text":"<ul> <li> <p>[ ] Team leader: talk to the teams about the new team member joining</p> </li> <li> <p>[ ] Team leader: Send some information to the new team member as a preview</p> <ul> <li>E.g., some documentation</li> </ul> </li> <li> <p>[ ] Team leader: establish contact with the new hire with a few words             about the next steps</p> </li> <li> <p>[ ] Team leader: do a proper intro with an email to the team:</p> <ul> <li>\"Hi team, please join me in welcoming XYZ to the team\"</li> <li>Whom is going to report to</li> <li>What is going to work on initially</li> <li>LinkedIn link</li> </ul> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#the_first_day","title":"The First Day!","text":""},{"location":"onboarding/all.onboarding_checklist.reference.html#team_member_info","title":"Team Member Info","text":"<ul> <li> <p>[ ] Team member: send the following information to your team leader</p> <ul> <li>Full name:</li> <li>Aka:</li> <li>Personal email:</li> <li>Github user:</li> <li>Laptop OS: Windows, Linux, or Mac</li> <li>Physical location and timezone</li> <li>User's SSH public key</li> </ul> </li> <li> <p>[ ] Team member: Set a profile picture where your face is clearly visible,             and use your @mention name as your display name on GitHub and other             communication channels.</p> </li> <li> <p>[ ] Team leader: update the             Team member info gdoc</p> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#nda","title":"NDA","text":"<ul> <li> <p>[ ] HR: send the team member an NDA to sign             NDAs Development Team</p> </li> <li> <p>[ ] Team member: send back signed copy of the NDA</p> </li> <li>[ ] HR: store in             signed directory</li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#hubstaff","title":"Hubstaff","text":"<ul> <li> <p>[ ] HR: Update Hubstaff</p> <ul> <li>Add user here</li> <li>We respect our own privacy by not using any of the screenshot or typing     things BS</li> <li>We use it only to track the time automatically and as HR</li> </ul> </li> <li> <p>[ ] Team member:</p> <ul> <li>Confirm access to Hubstaff</li> <li>Read     Instructions for Hubstaff</li> </ul> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#it_setup","title":"IT Setup","text":"<ul> <li> <p>[ ] Team leader: File an issue with this checklist</p> <ul> <li>The title is \"Onboarding {{Name}}\"</li> <li>Copy-and-paste the whole checklist starting from here</li> </ul> </li> <li> <p>[ ] IT: create account info bundle following             New team member addition - admin side tutorial</p> <ul> <li>TODO(Shayan): convert this into Markdown</li> </ul> </li> <li> <p>[ ] IT: send the following information to team member</p> <ul> <li>Linux user: name + first letter of last name</li> <li>Send the account info bundle</li> </ul> </li> <li> <p>[ ] IT: create company email <code>@causify.ai</code></p> <ul> <li>Exchange username and password</li> </ul> </li> <li> <p>[ ] IT: Add team member to mailing lists</p> <ul> <li><code>all@causify.ai</code> mailing lists<ul> <li>https://groups.google.com/my-groups</li> <li>https://admin.google.com/ac/groups/01qoc8b13iwic0z/info</li> </ul> </li> </ul> </li> <li> <p>[ ] Team member: Confirm receipt of emails to <code>all@</code>/<code>eng@</code></p> </li> <li> <p>[ ] IT: Add the team member to the company's Slack workspace</p> </li> <li> <p>[ ] Team member: Confirm access to the company's Slack workspace</p> </li> <li> <p>[ ] Team member: Confirm monthly all-hands meeting (usually on Monday             morning)</p> <ul> <li>This happens automatically through <code>all@causify.ai</code></li> <li>https://calendar.google.com/calendar/u/0/r</li> </ul> </li> <li> <p>[ ] Team member: Confirm access to the Google Drive documentation</p> <ul> <li>This should be automatically granted as being in <code>all@causify.ai</code></li> <li>Tech</li> <li>Process</li> </ul> </li> <li> <p>[ ] Team member: Set up the laptop to connect to the server following             VPN and dev server access setup instructions</p> </li> <li> <p>[ ] Team member: Configure your server environment following             Development setup instructions</p> </li> <li> <p>[ ] Team member (On per-need basis): We have implemented a             self-registration process for our Airflow on Kubernetes deployment. Please             register and create your personal accounts as we will no longer be using             the shared Airflow Admin user. Make sure to use the same username that you             have on the dev servers.</p> <ul> <li>Airflow - Registration</li> <li>TODO(Shayan): Update this</li> </ul> </li> <li> <p>[ ] IT: Add team member to all GitHub repos:</p> <ul> <li>[ ] kaizenflow</li> <li>[ ] helpers</li> <li>[ ] tutorials</li> <li>[ ] cmamp</li> <li>[ ] orange (On per-need basis)</li> <li>[ ] UMD_data605 (On per-need             basis)</li> </ul> </li> <li> <p>[ ] Team member: Confirm access to GitHub repos</p> </li> <li> <p>[ ] IT: Server set-up</p> <ul> <li>We use the personal laptop as a thin client only to connect to the servers,     where the work is typically carried out. We don't check out code on our     laptop</li> </ul> </li> <li> <p>[ ] IT: Create the Linux user on all the servers</p> <ul> <li>Create a keypair and share it with the new team member</li> </ul> </li> <li> <p>[ ] Team member: Check whether he / she can log in on one of the servers</p> <ul> <li>TODO(Shayan): How?</li> </ul> </li> <li> <p>[ ] IT: Create AWS UI credentials</p> <ul> <li>Only if necessary, for developer just starting, this is usually not needed     (CK login)</li> </ul> </li> <li> <p>[ ] Team member: check access to AWS on the server</p> <ul> <li><code>&gt; aws s3 --profile ck ls s3://cryptokaizen-unit-test/</code></li> <li><code>&gt; aws s3 --profile ck ls s3://cryptokaizen-data/</code></li> </ul> </li> <li> <p>[ ] Team member: Add your photo to the Slack and GitHub accounts</p> <ul> <li>Please use the same photo in all accounts with a recognizable Gravatar</li> <li>We use the picture to select Issues and conversations</li> <li>Please use a picture where you look somehow professional (e.g., not a     picture of you on the beach taking Tequila shots): the best picture is just     one of your face, so we can virtually get to know each other</li> </ul> </li> <li> <p>[ ] Team member: Make sure you have access to the             OOTO / Vacation calendar</p> <ul> <li>The link should be accessible and you should see the calendar in the list of     calendars at calendar.google.com (when accessing via your corporate email)</li> </ul> </li> <li> <p>[ ] Team member: Add your usual working hours by going to             calendar.google.com (using your corporate email), heading to the settings             section by clicking the gear icon on top right</p> <ul> <li></li> </ul> </li> <li> <p>[ ] Team member: Confirm you can access the anonymous form to ask anything             https://forms.gle/KMQgobqbyxhoTR9n6</p> </li> <li> <p>[ ] Team member: File your first Issue on GitHub</p> <ul> <li>It should be called \"Document review while onboarding $TEAM_MEMBER\"</li> <li>Track what is not clear in the onboarding process / documentation and what     should / could be improved</li> </ul> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#quick_checklists_to_verify_that_everything_works","title":"Quick Checklists to Verify That Everything Works","text":"<ul> <li>Team member<ul> <li>[ ] VPN to dev server</li> <li>[ ] ssh into the dev server</li> <li>[ ] Check access to AWS on the server (refer to instructions above)</li> <li>[ ] Clone the code from Git</li> <li>[ ] Connect to server with VisualStudio Code or PyCharm<ul> <li>There is an extension for VSCode, which allows to develop remotely     Remote - SSH</li> <li>Follow     this instruction     on how to set it up in your Visual Studio Code</li> </ul> </li> <li>[ ] Run the unit tests and make sure they all pass</li> <li>[ ] Run a docker container     <code>bash     &gt; i docker_bash</code></li> <li>[ ] Run a jupyter notebook<ul> <li>Follow     this instruction     on how to access the Jupyter server running on the remote server through     your local machine ```bash <p>i docker_jupyter ```</p> </li> </ul> </li> </ul> </li> </ul>"},{"location":"onboarding/all.onboarding_checklist.reference.html#the_second_day","title":"The Second Day","text":"<ul> <li> <p>[ ] Team member: carefully study all the documents in             the must-read list</p> <ul> <li>[ ] General rules of collaboration</li> <li>[ ] Coding style guide</li> <li>[ ] How to write unit tests</li> <li>[ ] How to run unit tests</li> <li>[ ] Creating a Jupyter Notebook</li> <li>[ ] What to do before opening a PR</li> <li>[ ] Code review process</li> <li>[ ] Git workflows and best practices</li> <li>[ ] GitHub organization</li> <li>[ ] Tips for writing documentation</li> <li>They will help you get up to speed with our practices and development style</li> <li>Read them carefully one by one</li> <li>Ask questions</li> <li>Memorize / internalize all the information</li> <li>Take notes</li> <li>Mark the reading as done</li> <li>Use your first Issue to document all the improvements to the documentation</li> </ul> </li> <li> <p>[ ] Team member: Do PRs following the process to improve documentation</p> <ul> <li>Do not be shy</li> </ul> </li> <li> <p>[ ] Team member: exercise all the important parts of the systems</p> <ul> <li>[ ] Create a GitHub issue</li> <li>[ ] Check out and pull the latest version of the code on the server</li> <li>[ ] Create a branch</li> <li>[ ] Run regressions on the server (<code>i run_fast_tests</code>)</li> <li>[ ] Start a Docker container on the server (<code>i docker_bash</code>)</li> <li>[ ] Start a Jupyter server (<code>i docker_jupyter</code>) from the server</li> <li>[ ] Run Linter on the server (<code>i lint --files=\"...\"</code>)</li> <li>[ ] Do a PR</li> </ul> </li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html","title":"All.organize email.how to guide","text":""},{"location":"onboarding/all.organize_email.how_to_guide.html#organize_email","title":"Organize Email","text":""},{"location":"onboarding/all.organize_email.how_to_guide.html#email","title":"Email","text":""},{"location":"onboarding/all.organize_email.how_to_guide.html#mailing_lists","title":"Mailing lists","text":"<ul> <li><code>all@</code> is the mailing list with everybody at the company</li> <li><code>contributors@</code> is the mailing list with every open-source contributor/intern</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#organizing_email_flow","title":"Organizing email flow","text":"<ul> <li>We receive tons of emails, and the inflow is going to keep increasing<ul> <li>At a large company you can get 10k emails per day (no kidding)</li> </ul> </li> <li>The goal is to read all the emails and always be on top of it</li> <li>How can one do that?<ul> <li>As usual the answer is get organized</li> <li>Filter emails in folders</li> <li>Separate emails in folders based on the action that they require (e.g.,     ignore, just read and be aware of it, read and respond)</li> <li>Read email and decide what to do about each of it:<ul> <li>No reply needed</li> <li>Reply right away</li> <li>Follow up later (e.g., to read, reply, think about it)</li> </ul> </li> <li>Use flags to distinguish what needs to be followed up later or if you are     waiting for a response</li> </ul> </li> <li>A possible organization in folders is:<ul> <li>GitHub<ul> <li>Commits in all the repos (be aware of it)</li> <li>Issue updates (read and respond)</li> <li>PRs</li> <li>Commits directly to <code>master</code> (read and review)</li> <li>Commits into <code>documents</code> dir of <code>master</code> (read and review)</li> <li>Emails generated by my GH activities (ignore and mark as read)</li> </ul> </li> <li>Asana<ul> <li>Daily activities assigned report</li> <li>New comment/activity</li> <li>New task assignment</li> </ul> </li> </ul> </li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#anatomy_of_email_messages_from_infra","title":"Anatomy of email messages from infra","text":"<ul> <li>The goal is to classify emails so that we can filter email effectively</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#filtering_emails_with_gmail","title":"Filtering emails with Gmail","text":"<ul> <li>Personally (GP) I prefer an email client (Mozilla Thunderbird and more     recently Apple Mail) rather than using Gmail web interface<ul> <li>People are able to use it</li> </ul> </li> <li>Personally I prefer to use filters on the Gmail (server) side<ul> <li>Pros<ul> <li>I don't get emails on my cell phone continuously</li> <li>The emails are organized as they arrive</li> <li>Folders are on the server side, so my client can simply sync</li> </ul> </li> <li>Cons<ul> <li>The Gmail interface for filtering emails is horrible</li> </ul> </li> </ul> </li> <li>The web interface is     https://mail.google.com/mail/u/0/#settings/filters</li> <li>Note that Gmail distinguish different email accounts using different indices,     e.g.,     https://mail.google.com/mail/u//#inbox"},{"location":"onboarding/all.organize_email.how_to_guide.html#notifications_from_github","title":"Notifications from GitHub","text":"<ul> <li>https://help.github.com/en/categories/receiving-notifications-about-activity-on-github</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#github_pull_requests","title":"GitHub pull requests","text":"<ul> <li> <p>These emails look like:</p> <pre><code>        Samarth KaPatel &lt;notifications@github.com&gt;\n        to cryptokaizen/orange, Subscribed\n\n        cryptokaizen/cmamp#4765\n        - Refactoring amp_path on orange\n        - PR in cmamp - cryptokaizen/cmamp#4788\n        ________________________________________________________________\n\n        You can view, comment on, or merge this pull request online at:\n        https://github.com/cryptokaizen/orange/pull/411\n\n        Commit Summary\n        - B2b4940 orange fix\n        - 850b2b2 amp\n\n        File Changes (2 files)\n        - M amp (2)\n        - M dataflow_orange/system/Cx/test/test_master_pnl_real_time_observer_notebook.py (5)\n\n        Patch Links:\n        - Https://github.com/cryptokaizen/orange/pull/411.patch\n        - Https://github.com/cryptokaizen/orange/pull/411.diff\n</code></pre> </li> <li> <p>These emails have the words: \"You can view, comment on, or merge this pull     request online at:\" in the body of the email</p> </li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#github_issue_activity","title":"GitHub issue activity","text":"<ul> <li> <p>These emails look like:</p> <pre><code>        Samarth KaPatel &lt;notifications@github.com&gt;\n        to Review, kaizen-ai/kaizenflow\n\n        @samarth9008 requested your review on: #436 Update Signing_up_for_Kaizenflow.md.\n        __\n        Reply to this email directly, view it on GitHub, or unsubscribe.\n        You are receiving this because your review was requested.\n</code></pre> <p>or:</p> <pre><code>        Grigorii Pomazkin &lt;notifications@github.com&gt;\n        to cryptokaizen/cmamp, Mention\n\n        @PomazkinG commented on this pull request.\n        ________________________________________________________________________\n\n        In helpers/lib_tasks_pytest.py:\n\n        &gt; @@ -671,17 +671,25 @@ def run_coverage_report(  # type: ignore\n     :param aws_profile: the AWS profile to use for publishing HTML report\n     \"\"\"\n     # TODO(Grisha): allow user to specify which tests to run.\n     ...\n        obsolete, resolving\n        __\n        Reply to this email directly, view it on GitHub, or unsubscribe.\n        You are receiving this because you were mentioned.\n</code></pre> </li> <li> <p>These emails can be recognized by the fact that have the words \"You are     receiving this because\" in the email body</p> </li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#commits","title":"Commits","text":"<ul> <li> <p>These emails look like:</p> <pre><code>        Grigorii Pomazkin &lt;notifications@github.com&gt;\n        to cryptokaizen/cmamp, Push\n\n        @PomazkinG pushed 1 commit.\n        - 65496bb Merge branch 'master' into CmTask4707_fix_coverage_test\n        __\n        View it on GitHub or unsubscribe.\n        You are receiving this because you are subscribed to this thread.\n</code></pre> </li> <li> <p>These emails can be recognized by the fact that have the words \"pushed commit\"     in the email body</p> </li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#gdocs","title":"Gdocs","text":"<ul> <li>These emails have <code>comments-noreply@docs.google.com</code> or (Google Docs) in the     \"subject\" field</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#todo_emails","title":"TODO emails","text":"<ul> <li>These emails have TODO in the subject</li> </ul>"},{"location":"onboarding/all.organize_email.how_to_guide.html#asana","title":"Asana","text":"<ul> <li>These emails are received from <code>no-reply@asana.com</code> address</li> <li>These emails may contain:<ul> <li>List of tasks assigned to you for today</li> <li>New activities in the tasks assigned to you</li> <li>New task assignment</li> </ul> </li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html","title":"All.receive crypto payment.how to guide","text":""},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#receive_crypto_payment","title":"Receive Crypto Payment","text":""},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#choose_a_crypto_wallet_type","title":"Choose a crypto wallet type","text":"<ul> <li>Https://academy.binance.com/en/articles/crypto-wallet-types-explained</li> <li>Https://www.blockchain-council.org/blockchain/types-of-crypto-wallets-explained/</li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#public_private_keys","title":"Public / private keys","text":"<p>See the explanation here</p> <p>TLDR:</p> <ul> <li>Send a public key to receive a transaction, see it as a wallet address</li> <li>Private key is needed to prove ownership or spend the funds associated with     your public address, see it as a password</li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#what_do_people_in_the_team_use","title":"What do people in the team use","text":""},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#beginner","title":"Beginner","text":"<p>Mobile / desktop hot wallet Exodus</p> <p>Pros:</p> <ul> <li>Easy to set up</li> <li>Has a mobile app which is easy to use</li> <li>Built-in exchange (e.g., allows to exchange BTC to ETH inside the app)</li> <li>Allows to sell crypto and receive fiat money to a bank account via the app,     see     here</li> <li>Customer support</li> </ul> <p>Cons:</p> <ul> <li>A swap may include a     spread</li> <li>As any hot wallet is vulnerable to hackers attacks<ul> <li>It is not recommended to store significant amount of money</li> </ul> </li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#advanced","title":"Advanced","text":"<p>If you are experienced and / or own a lot of crypto you might want to have a cold wallet on your laptop, holding the bulk of your bitcoin and transfer among different website on-line wallets.</p> <p>https://bitcoin.org/en/choose-your-wallet</p>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#choose_a_crypto_currency_to_receive_money_in","title":"Choose a crypto currency to receive money in","text":"<ul> <li>One can either receive a payment in BTC or in one of the Stablecoins (USDT is     a preference)</li> <li>People in the team prefer to use Stablecoins since Stablecoins pursue price     stability</li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#send_a_crypto_wallet_address","title":"Send a crypto wallet address","text":"<p>Send your crypto wallet address to GP together with an invoice via email. E.g.,</p> <pre><code>Hi GP,\n\nPlease find my invoice for the period [a, b] attached. I would like to receive the money in USDT ERC20.\n\nMy crypto wallet address is: 0x5ce3d650703f745B9C0cf20E322204b00bF59205\n</code></pre>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#do_a_test_transfer","title":"Do a test transfer","text":"<p>For the first time GP sends a test transfer (e.g., 100$) just to confirm that a wallet address provided works.</p> <p>After a wallet address is \"verified\" send the full amount (exluding the test transfer amount).</p>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#cash_out_crypto","title":"Cash out crypto","text":""},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#bank_account","title":"Bank account","text":"<p>There are some 3rd party services that buy crypto money from you and send fiat money to your bank account</p> <p>E.g.,</p> <ul> <li>Built-in Exodus crypto withdrawal service</li> <li>Coinbase cash out serivce</li> <li>Binance P2P</li> </ul>"},{"location":"onboarding/all.receive_crypto_payment.how_to_guide.html#cash","title":"Cash","text":"<p>There are some crypto ATMs in the USA, see here</p>"},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html","title":"All.track time with hubstaff.how to guide","text":""},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#track_time_with_hubstaff","title":"Track Time With Hubstaff","text":""},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#hubstaff","title":"Hubstaff","text":""},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#general","title":"General","text":"<ul> <li>Hubstaff is a tool for remote working that automates:<ul> <li>Time tracking</li> <li>Invoice creation</li> <li>Payment</li> </ul> </li> <li>The goal is to use Hubstaff for automatically getting paid for the worked hours.</li> </ul>"},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#time_tracking","title":"Time Tracking","text":""},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#privacy","title":"Privacy","text":"<ul> <li>We have decided to turn off the feature of taking screenshots of the laptop as     proof-as-work, URL, and app tracking.<ul> <li></li> </ul> </li> <li>Although this seems to be standard practice for remote teams (e.g., it's     default for UpWork), we don't believe in surveilling people, rather we prefer     to assume that people do the right thing (until proven wrong, of course).<ul> <li></li> <li></li> </ul> </li> <li>We left enabled \"the keyboard and mouse activity\" since this is used to     automatically track the worked time. The tool does not store what you type,     only if you type as a proxy for \"working\".<ul> <li>     We let people decide if they want to monitor the usage of programs or not.     Some people like it (I do since it gives an insight on which program I spend     my time)</li> <li></li> </ul> </li> <li>You can change your mind over time.</li> </ul>"},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#tracking_time_automatically","title":"Tracking time automatically","text":"<ul> <li>You can use the desktop App<ul> <li>I prefer this since it's completely automated</li> <li>For Linux (Ubuntu) follow this     guide</li> </ul> </li> <li>Both options update the time information to Hubstaff automatically.<ul> <li></li> <li></li> </ul> </li> <li>See     https://hubstaff.com/how-tracking-works</li> </ul>"},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#tracking_time_manually","title":"Tracking time manually","text":"<ul> <li>You can track time using the     Google Chrome extension:<ul> <li>Chrome Time Tracking - Hubstaff Time Tracker</li> <li>Start the timer when you start the work</li> <li>Stop the timer when the work is paused</li> </ul> </li> </ul>"},{"location":"onboarding/all.track_time_with_hubstaff.how_to_guide.html#overriding_time_tracking","title":"Overriding time tracking","text":"<ul> <li>In case you forgot to turn on the timer or you are tracking time manually, go     to<ul> <li>     and fill the timesheet for the past day</li> <li></li> </ul> </li> <li>See the     official guide     for a lot of details</li> </ul>"},{"location":"onboarding/bounty.onboarding_checklist.reference.html","title":"Bounty.onboarding checklist.reference","text":""},{"location":"onboarding/bounty.onboarding_checklist.reference.html#bounty_hunters_onboarding","title":"Bounty hunters onboarding","text":""},{"location":"onboarding/bounty.onboarding_checklist.reference.html#checklist","title":"Checklist","text":"<ul> <li> <p>Source:     <code>bounty.onboarding_checklist.reference.md</code></p> </li> <li> <p>[ ] Contributor: Clone the repos</p> <ul> <li>[ ] helpers</li> <li>[ ] tutorials</li> </ul> </li> <li>[ ] Contributor: Set up the development environment following instructions             in             <code>intern.set_up_development_on_laptop.how_to_guide.md</code></li> <li>[ ] Contributor: Carefully study all the documents in the must-read list:<ul> <li>[ ] Carefully study all the documents in             the must-read list</li> <li>[ ] General rules of collaboration</li> <li>[ ] Coding style guide</li> <li>[ ] How to write unit tests</li> <li>[ ] How to run unit tests</li> <li>[ ] Creating a Jupyter Notebook</li> <li>[ ] What to do before opening a PR</li> <li>[ ] Code review process</li> <li>[ ] Git workflows and best practices</li> <li>[ ] GitHub organization</li> <li>[ ] Tips for writing documentation</li> <li>They will help you get up to speed with our practices and development style</li> <li>Read them carefully one by one</li> <li>Ask questions</li> <li>Memorize / internalize all the information</li> <li>Take notes</li> <li>Mark the reading as done</li> <li>Open a GH issue/PR to propose improvements to the documentation</li> </ul> </li> </ul>"},{"location":"onboarding/bounty.onboarding_checklist.reference.html#final_checks","title":"Final checks","text":"<ul> <li>[ ] Contributor: Exercise all the important parts of the systems<ul> <li>[ ] Check out and pull the latest version of the repo code</li> <li>[ ] Create a branch</li> <li>[ ] Run regressions (<code>i run_fast_tests</code>)</li> <li>[ ] Run Linter (<code>i lint --files=\"...\"</code>)</li> <li>[ ] Start a Docker container (<code>i docker_bash</code>)</li> <li>[ ] Start a Jupyter server (<code>i docker_jupyter</code>)</li> </ul> </li> </ul>"},{"location":"onboarding/bounty.onboarding_checklist.reference.html#instructions","title":"Instructions","text":""},{"location":"onboarding/bounty.onboarding_checklist.reference.html#general_organization","title":"General organization","text":"<ul> <li>All the collaboration happens on GitHub as a typical open-source project</li> <li>The bounties are published in a     Gdoc with specs<ul> <li>We are going to be adding more and more bounties depending on the demand</li> </ul> </li> <li>There is a     sign-up sheet     for the bounties<ul> <li>There can be at most 2 people working on each bounty</li> <li>If 2 people are working on the same bounty, they are competing with each     other<ul> <li>You get paid only if you do the task better than the other person</li> </ul> </li> <li>At some point (e.g., the task is close to being done) we reserve the right     to make the bounty \"locked\", meaning that other people can no longer take it     on</li> </ul> </li> <li>There will be minimal interaction from us<ul> <li>In general we don't answer questions, contributors need to figure things out     themselves</li> <li>If they believe there is a genuine bug somewhere in the codebase, they     should<ul> <li>Check the existing issues in case this bug has already been reported</li> <li>If not, file an issue with a proper report and ideally propose a fix too</li> </ul> </li> </ul> </li> <li>If a contributor succeeds, we pay the bounty $XYZ as a bank transfer or in     Tether<ul> <li>If both sides agree, the contributor gets a paid internship / RA / TA</li> <li>We then on-board the contributors as part of our team and dedicate our time     to train them</li> </ul> </li> </ul>"},{"location":"onboarding/bounty.onboarding_checklist.reference.html#working_on_a_bounty","title":"Working on a bounty","text":"<ul> <li>Take time to peruse the description of the bounty<ul> <li>No need to rush, there is always time and work to do</li> </ul> </li> <li>Before any implementation, the contributor should create an issue for the task     and post a detailed plan of action there<ul> <li>By default the contributor is then free to proceed according to their plan     (implement, file a PR)</li> <li>We reserve the right to review and propose changes at any point</li> </ul> </li> <li>For each bounty, the contributor should spend ~1 hour looking for a package or     already existing solutions on GitHub<ul> <li>One should report the findings even if nothing has been found (e.g.,     explaining how the search was done)</li> <li>It's totally ok (and actually recommended) to re-use packages and other     people's work to get stuff done</li> <li>If you find an implementation of the bounty in the wild, congrats, you made     money with very little work</li> </ul> </li> <li>All code needs to be written using our coding style<ul> <li>See <code>code_template.py</code>,     coding style doc,     examples of good code</li> <li>We provide our Linter and our AI code-styler / reviewer (once available) to     help with formatting and style fixes</li> </ul> </li> <li>All code needs to be unit tested according to our     standards and infrastructure</li> <li>The project needs to be documented in the way we     document software</li> <li>To get used to our process, for the first couple of PRs post the     PR checklist     in a comment and check the boxes when the requirements are met</li> </ul>"},{"location":"onboarding/intern.onboarding_checklist.reference.html","title":"Intern.onboarding checklist.reference","text":""},{"location":"onboarding/intern.onboarding_checklist.reference.html#intern_onboarding_checklist","title":"Intern onboarding checklist","text":""},{"location":"onboarding/intern.onboarding_checklist.reference.html#intern_onboarding_vs_full_onboarding","title":"Intern onboarding vs. full onboarding","text":"<ul> <li>The onboarding process documented here is intended for people who have not     become permanent members of the team yet, i.e., interns</li> <li>Upon completing this onboarding, the intern will be able to develop in our     common environment, open GitHub issues and PRs, use our extensive coding     toolchain</li> <li>However, some of the steps of the     full onboarding process     (like creating a company email) are postponed until the intern \"graduates\" to     a permanent position</li> </ul>"},{"location":"onboarding/intern.onboarding_checklist.reference.html#checklist","title":"Checklist","text":"<ul> <li>Source:     <code>intern.onboarding_checklist.reference.md</code></li> </ul>"},{"location":"onboarding/intern.onboarding_checklist.reference.html#org","title":"Org","text":"<ul> <li> <p>[ ] HiringMeister: File an issue with this checklist</p> <ul> <li>The title is \"Onboarding {{Name}}\"</li> <li>Copy-and-paste the whole checklist starting from here</li> <li>The issue should be assigned to the intern</li> </ul> </li> <li> <p>[ ] Intern: Join the Telegram channel -             https://t.me/+DXZXsWoEHR1mNWIx</p> </li> <li> <p>[ ] HiringMeister: Establish contact by email or Telegram with the intern             with a few words about the next steps</p> </li> <li> <p>[ ] Intern: Post your laptop's OS (Windows, Linux, Mac) in the comments of             this issue</p> <ul> <li>Our dev environment is only adapted for Linux and Mac. Windows users have to     install dual boot or a VM with Linux (Ubuntu)</li> </ul> </li> <li> <p>[ ] Intern: Confirm access to the public GH repos</p> <ul> <li>[ ] helpers</li> <li>[ ] tutorials</li> </ul> </li> <li> <p>[ ] HiringMeister: Give the intern write access to the current             Intern-focused project on GH</p> </li> <li> <p>[ ] IT: @Shayawnn Add the intern to the mailing group             <code>contributors@causify.ai</code> so that they can send             morning TODO emails</p> <ul> <li>The intern's personal e-mail address can be found in the corresponding Asana     task in the     Hiring project</li> </ul> </li> <li> <p>[ ] HiringMeister: Update the             Access Tracker             spreadsheet</p> <ul> <li>Put \"Yes\" in the columns that the intern now has access to</li> </ul> </li> </ul>"},{"location":"onboarding/intern.onboarding_checklist.reference.html#it_setup","title":"IT setup","text":"<ul> <li>[ ] Intern: Set up the development environment following instructions in             <code>intern.set_up_development_on_laptop.how_to_guide.md</code><ul> <li>We expect the intern to independently solve all the problems they encounter     through trial and error, googling and ChatGPT-ing</li> <li>If you cannot solve some problem no matter how hard you try:<ul> <li>Check if the problem has already been reported and discussed in one of the     existing issues</li> <li>If not, create a new issue and provide a full report<ul> <li>Describe your problem in detail, including the full context to reproduce     (e.g., copy-and-paste the command you ran and the output you got; do not     use screenshots)</li> <li>Describe what you have already tried to fix it and what was the result</li> <li>Tag the hiring team</li> </ul> </li> </ul> </li> <li>After the problem is resolved, then, if applicable, do a PR to fix the bug /     add a description of the problem and the solution to the docs</li> <li>See also     <code>Collaboration for problem solving</code></li> </ul> </li> </ul>"},{"location":"onboarding/intern.onboarding_checklist.reference.html#must-read","title":"Must-read","text":"<ul> <li>[ ] Intern: Carefully study all the documents in             the must-read list<ul> <li>[ ] General rules of collaboration</li> <li>[ ] Coding style guide</li> <li>[ ] How to write unit tests</li> <li>[ ] How to run unit tests</li> <li>[ ] Creating a Jupyter Notebook</li> <li>[ ] What to do before opening a PR</li> <li>[ ] Code review process</li> <li>[ ] Git workflows and best practices</li> <li>[ ] GitHub organization</li> <li>[ ] Tips for writing documentation</li> <li>They will help you get up to speed with our practices and development style</li> <li>Read them carefully one by one</li> <li>Ask questions</li> <li>Memorize / internalize all the information</li> <li>Take notes</li> <li>Mark the reading as done</li> <li>Open a GH issue/PR to propose improvements to the documentation</li> </ul> </li> </ul>"},{"location":"onboarding/intern.onboarding_checklist.reference.html#final_checks","title":"Final checks","text":"<ul> <li>[ ] Intern: Exercise all the important parts of the systems<ul> <li>[ ] Create a GitHub issue</li> <li>[ ] Check out and pull the latest version of the repo code</li> <li>[ ] Create a branch</li> <li>[ ] Run regressions (<code>i run_fast_tests</code>)</li> <li>[ ] Run Linter (<code>i lint --files=\"...\"</code>)</li> <li>[ ] Start a Docker container (<code>i docker_bash</code>)</li> <li>[ ] Start a Jupyter server (<code>i docker_jupyter</code>)</li> <li>[ ] Do a PR</li> </ul> </li> <li>Tip: a good approach to the \"final checks\" is to perform all the steps     (opening an issue, creating a branch, filing a PR) for something very small     but useful -- like fixing a typo in the docs.</li> <li>To get used to our process, for the first couple of PRs post the     PR checklist     in a comment and check the boxes when the requirements are met</li> </ul>"},{"location":"onboarding/intern.prepare_for_development.how_to_guide.html","title":"Intern.prepare for development.how to guide","text":""},{"location":"onboarding/intern.prepare_for_development.how_to_guide.html#prepare_for_development_in_kaizenflow","title":"Prepare for development in KaizenFlow","text":""},{"location":"onboarding/intern.prepare_for_development.how_to_guide.html#introduction","title":"Introduction","text":"<p>This document outlines the steps to take in order to start contributing to KaizenFlow. Documenting this set up, we aim to streamline the information flow and make the contribution process seamless by creating a collaborative and efficient coding environment for all contributors.</p> <p>Happy coding!</p>"},{"location":"onboarding/intern.prepare_for_development.how_to_guide.html#technologies_used","title":"Technologies used","text":"<ul> <li>UMD DATA605 Big Data Systems     contains     lectures and     tutorials     about most of the technologies we use in KaizenFlow, e.g., Dask, Docker,     Docker Compose, Git, github, Jupyter, MongoDB, Pandas, Postgres, Apache Spark</li> <li>You can go through the lectures and tutorials on a per-need basis, depending     on what it's useful for you to develop</li> <li>As an additional resource to become proficient in using Linux and shell, you     can refer to     The Missing Semester of Your CS Education</li> </ul>"},{"location":"onboarding/intern.prepare_for_development.how_to_guide.html#set_up_the_dev_environment","title":"Set up the dev environment","text":"<ul> <li>Set up the development environment following the instructions in     <code>intern.set_up_development_on_laptop.how_to_guide.md</code></li> </ul>"},{"location":"onboarding/intern.prepare_for_development.how_to_guide.html#coding_style","title":"Coding style","text":"<ul> <li>Adopt the coding style outlined     here</li> <li>Internalize the guidelines to maintain code consistency</li> </ul>"},{"location":"onboarding/intern.prepare_for_development.how_to_guide.html#linter","title":"Linter","text":"<ul> <li>Linter is in charge of reformatting the code according to our conventions and     reporting potential problems</li> </ul>"},{"location":"onboarding/intern.prepare_for_development.how_to_guide.html#run_linter_and_check_results","title":"Run Linter and check results","text":"<ul> <li> <p>Run Linter on the changed files in the PR branch</p> <p>```bash</p> <p>i lint --files=\"file1 file2...\" ```</p> </li> <li> <p>More information about Linter can be found     here</p> </li> </ul>"},{"location":"onboarding/intern.prepare_for_development.how_to_guide.html#writing_and_contributing_code","title":"Writing and contributing code","text":"<ul> <li>If needed, always start with creating an issue first, providing a summary of     what you want to implement and assign it to yourself and your team</li> <li>Create a branch of your assigned issues/bugs<ul> <li>E.g., for a GitHub issue with the name: \"Expose Linter container to     KaizenFlow contributors #63\", The GitHub issue and the branch name should be     called <code>SorrTask63_Expose_linter_container_to_Kaizenflow_contributors</code></li> </ul> </li> <li>Implement the code based on the requirements in the assigned issue</li> <li>Run Linter on your code before pushing</li> <li>Do <code>git commit</code> and <code>git push</code> together so the latest changes are readily     visible</li> <li>Make sure your branch is up-to-date with the master branch</li> <li>Create a Pull Request (PR) from your branch</li> <li>Add your assigned reviewers for your PR so that they are informed of your PR</li> <li>After being reviewed, the PR will be merged to the master branch by your     reviewers</li> <li>Do not respond to emails for replies to comments in issues or PRs. Use the     GitHub GUI instead, as replying through email adds unwanted information</li> </ul>"},{"location":"onboarding/intern.running_causify_on_virtual_machine.how_to_guide.html","title":"Intern.running causify on virtual machine.how to guide","text":""},{"location":"onboarding/intern.running_causify_on_virtual_machine.how_to_guide.html#running_causify_on_virtual_machine_how-to-guide","title":"Running Causify On Virtual Machine How-to-Guide","text":""},{"location":"onboarding/intern.running_causify_on_virtual_machine.how_to_guide.html#step_1_download_vmware_workstation_pro","title":"Step 1: Download VMware Workstation Pro","text":"<ul> <li> <p>There are a lot of errors while setting up the Oracle VirtualBox, so it is     recommended to go ahead with VMWare Workstation Pro</p> </li> <li> <p>Go to the official     VMware Workstation and Fusion     page</p> </li> <li>Click on <code>Download Fusion or Workstation</code></li> <li>The user will be redirected to the Broadcom registration page.     Register and     Login yourself on the     website</li> <li>Once logged in, go to the     Downloads Homepage. Under     <code>My Downloads</code>, click on <code>Free Software Downloads available HERE</code></li> <li>Select <code>VMware Workstation Pro</code> and choose the latest release (or 17.6.3)</li> <li>Fill out few details and proceed to download</li> <li>Follow the installation prompts to set up VMware on the system</li> </ul>"},{"location":"onboarding/intern.running_causify_on_virtual_machine.how_to_guide.html#step_2_set_up_a_new_virtual_machine","title":"Step 2: Set Up a New Virtual Machine","text":"<ul> <li> <p>Before proceeding, ensure that you have downloaded the latest version or     Ubuntu (24.04.2 LTS) ISO file. It can be downloaded     here</p> </li> <li> <p>Open VMware Workstation Pro and click on <code>New Virtual Machine</code></p> </li> <li>Choose the <code>Typical (Recommended)</code> configuration</li> <li>Under the <code>Installer disc image file (iso)</code> option, browse and select the path     to your downloaded Ubuntu ISO file</li> <li>Set the disk size to 35GB and select <code>Store virtual disk as a single file</code><ul> <li>If you select <code>Split virtual disk into multiple files</code>, the 35GB will be     split, so you might need to allocate more space for your section in the     future but it works fine as well</li> </ul> </li> <li>Completing the setup will take you directly to the Ubuntu installation screen</li> </ul>"},{"location":"onboarding/intern.running_causify_on_virtual_machine.how_to_guide.html#step_3_install_ubuntu","title":"Step 3: Install Ubuntu","text":"<ul> <li>Choose <code>English</code> as your preferred language</li> <li>Select <code>Use wired connection</code>, even if you plan to use Wi-Fi</li> <li>Begin the installation and choose <code>Interactive Installation</code> option</li> <li>Proceed with the <code>Default Selection</code> during the installation</li> <li>Select both:<ul> <li><code>Install third-party software for graphics and Wi-Fi hardware</code></li> <li><code>Download and install support for additional media formats</code></li> </ul> </li> <li>Choose the <code>Erase disk and Install Ubuntu</code> option</li> <li> <p>Set up your <code>name</code>, <code>password</code>, and <code>timezone</code> as per your preferences</p> </li> <li> <p>In case the installation fails or asks to report the issue, simply restart the     virtual machine and repeat the process. It should work on the second try</p> </li> </ul>"},{"location":"onboarding/intern.running_causify_on_virtual_machine.how_to_guide.html#step_4_check_internet_connection","title":"Step 4: Check Internet Connection","text":"<ul> <li>Check your internet connection in the browser. If it works, you can skip this     section!</li> </ul>"},{"location":"onboarding/intern.running_causify_on_virtual_machine.how_to_guide.html#recommended_method","title":"Recommended Method","text":"<ul> <li>Shutdown the virtual machine</li> <li>In VMware Workstation, go to <code>Edit</code> in the top menu and select     <code>Virtual Network Editor</code></li> <li>Click <code>Change Settings</code> and ensure the following:<ul> <li><code>VMnet0</code>: Bridged</li> <li><code>VMnet1</code>: Host-only</li> <li><code>VMnet8</code>: NAT</li> <li>Both <code>VMnet1</code> and <code>VMnet8</code> should be connected and enabled</li> </ul> </li> <li>For <code>VMnet0</code>, set the <code>Bridged to: Automatic</code> option and click on     <code>Automatic Settings</code>. Uncheck all options except the one corresponding to your     Wi-Fi address (you can find it in your host system's <code>Network Connections</code>)     and apply the changes</li> </ul>"},{"location":"onboarding/intern.running_causify_on_virtual_machine.how_to_guide.html#alternative_1","title":"Alternative 1","text":"<ul> <li>Instead of using <code>Automatic</code> for Bridged, manually select your Wi-Fi network     from the dropdown list and apply the changes. Always restart the VM after any     modifications</li> </ul>"},{"location":"onboarding/intern.running_causify_on_virtual_machine.how_to_guide.html#alternative_2","title":"Alternative 2","text":"<ul> <li>Before starting the VM, go to <code>Network Adapter</code> under <code>Devices</code>, select     <code>Bridged</code>, and check the option <code>Replicate physical network connection state</code></li> <li>If it doesn't work, change to <code>NAT</code> and apply the settings. Restart the VM     after each change</li> </ul>"},{"location":"onboarding/intern.running_causify_on_virtual_machine.how_to_guide.html#conclusion","title":"Conclusion","text":"<ul> <li>After completing these steps, you should have VMware Workstation Pro running     Ubuntu with internet connectivity, ready to set up Docker</li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html","title":"Intern.set up development on laptop.how to guide","text":""},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#set_up_development_environment_locally","title":"Set up development environment locally","text":"<ul> <li>One only develops locally on their laptop during the intern stage. All     permanent members of the team should develop on our server. Interns will get     access to the server once they \"graduate\" to a permanent position.</li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#clone_the_code","title":"Clone the code","text":"<ul> <li> <p>To clone the repo, use the cloning command described in the official GitHub     documentation</p> </li> <li> <p>Example of the cloning command:</p> <p>```bash</p> <p>git clone --recursive git@github.com:causify-ai/{repo_name}.git ~/src/{repo_name}1 ``` - The previous command might not work sometimes, in which case try the     alternative command using HTTP instead of SSH:</p> <p>```bash</p> <p>git clone --recursive https://github.com/causify-ai/{repo_name}.git ~/src/{repo_name}1 ```</p> </li> <li> <p>All the source code should go under <code>~/src</code> (e.g., <code>/Users/&lt;YOUR_USER&gt;/src</code> on     a Mac)</p> </li> <li>The path to the local repo folder should look like this     <code>~/src/{REPO_NAME}{IDX}</code> where<ul> <li><code>REPO_NAME</code> is a name of the repository</li> <li>IDX is an integer</li> </ul> </li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#set_up_github_ssh_access_and_personal_access_token_pat","title":"Set up GitHub SSH access and personal access token (PAT)","text":"<ul> <li> <p>To generate a new SSH key, follow the official     GitHub instructions</p> </li> <li> <p>Ensure that you save the SSH key with the below name format and at the     specified location</p> <p>File location: <code>~/.ssh/id_rsa.causify-ai.github</code></p> <p>Example command to generate SSH key:</p> <p>```bash</p> <p>ssh-keygen -t ed25519 -C \"your_email@example.com\" -f ~/.ssh/id_rsa.causify-ai.github ```</p> </li> <li> <p>To create a Personal Access Token (classic) with necessary scopes like <code>repo</code>,     <code>workflow</code>, etc., go to     https://github.com/settings/tokens and     click \"Generate new token (classic)\".</p> </li> <li> <p>After obtaining the token, store it in a file named     <code>github_pat.causify-ai.txt</code> at the specified path</p> <p>File location: <code>~/.ssh/github_pat.causify-ai.txt</code></p> <p>Example command to save using <code>vim</code>:</p> <p>```bash</p> <p>vim ~/.ssh/github_pat.causify-ai.txt ```</p> </li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#build_and_activate_the_thin_environment","title":"Build and activate the thin environment","text":"<ul> <li> <p>Create the \"thin environment\" which contains the minimum set of dependencies     needed for running our Dev Docker container</p> </li> <li> <p>Build the thin environment; this is done once per client</p> <ul> <li> <p>If you are in the <code>helpers</code> repo:</p> <p>```bash</p> <p>./dev_scripts_helpers/thin_client/build.py ```     - Otherwise:</p> <p>```bash</p> <p>cd helpers_root ./dev_scripts_helpers/thin_client/build.py ```</p> </li> </ul> </li> <li> <p>While building the thin environment, the     GitHub CLI will also be installed system-wide</p> </li> <li> <p>NB! You can skip the rest of this section if you use tmux.</p> </li> <li> <p>Activate the thin environment; make sure it is always activated</p> <p>```bash</p> <p>source dev_scripts_{repo_name}/thin_client/setenv.sh ``` - E.g.,</p> <p>```bash</p> <p>source dev_scripts_tutorials/thin_client/setenv.sh ```</p> </li> <li> <p>If you see output like below, your environment is successfully built!     <code>bash     ...     alias sp='echo '\\''source ~/.profile'\\''; source ~/.profile'     alias vi='/usr/bin/vi'     alias vim='/usr/bin/vi'     alias vimdiff='/usr/bin/vi -d'     alias vip='vim -c \"source ~/.vimrc_priv\"'     alias w='which'     ==&gt; SUCCESS &lt;==</code></p> </li> <li>If you encounter any issues, please post them by creating a new issue on     GitHub and assign it to @gpsaggese<ul> <li>You should report as much information as possible: what was the command,     what is your platform, output of the command (copy-and-paste, do not use     screenshots)</li> </ul> </li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#install_and_test_docker","title":"Install and test Docker","text":""},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#supported_os","title":"Supported OS","text":"<ul> <li>Our systems support Mac (both x86 and Apple Silicon) and Linux Ubuntu</li> <li>For Linux Ubuntu, we actively test and support development on:<ul> <li>Operating System: Ubuntu 24.04 LTS</li> <li>Docker: v28</li> </ul> </li> <li>We do not support Windows and WSL: we have tried several times to port the     toolchain to it, but there are always subtle incompatible behaviors that drive     everyone crazy<ul> <li>If you are using Windows, we suggest to use dual boot with Linux or use a     virtual machine with Linux</li> <li>Install VMWare software</li> <li>Reference video for     installing Ubuntu on VMWare software</li> <li>Make sure you set up your git and github</li> <li>Install     docker     on your Ubuntu VM</li> </ul> </li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#install_docker","title":"Install Docker","text":"<ul> <li> <p>Get familiar with Docker if you are not, e.g.,     https://docs.docker.com/get-started/overview/</p> </li> <li> <p>We work in a Docker container that has all the required dependencies installed</p> <ul> <li>You can use PyCharm / VSCode on your laptop to edit code, but you want to     run code inside the dev container since this makes sure everyone is running     with the same system, and it makes it easy to share code and reproduce     problems</li> </ul> </li> <li> <p>Install Docker Desktop on your PC</p> <ul> <li>Links:<ul> <li>Mac</li> <li>Linux</li> </ul> </li> </ul> </li> <li> <p>Follow     https://docs.docker.com/engine/install/</p> </li> <li> <p>For Mac you can also install <code>docker-cli</code> without the GUI using</p> <p>```bash</p> <p>brew install docker brew link docker brew install colima ```</p> </li> <li> <p>After installing, make sure Docker works on your laptop (of course the version     will be newer)</p> <p>```bash</p> <p>docker version Client:    Cloud integration: v1.0.24    Version:           20.10.17    API version:       1.41    Go version:        go1.17.11    Git commit:        100c701    Built:             Mon Jun  6 23:04:45 2022    OS/Arch:           darwin/amd64    Context:           default    Experimental:      true</p> <p>Server: Docker Desktop 4.10.1 (82475)    Engine:     Version:          20.10.17     API version:      1.41 (minimum version 1.12)     Go version:       go1.17.11     Git commit:       a89b842     Built:            Mon Jun  6 23:01:23 2022     OS/Arch:          linux/amd64     Experimental:     false    containerd:     Version:          1.6.6     GitCommit:        10c12954828e7c7c9b6e0ea9b0c02b01407d3ae1    runc:     Version:          1.1.2     GitCommit:        v1.1.2-0-ga916309    docker-init:     Version:          0.19.0     GitCommit:        de40ad0 ```</p> </li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#checking_docker_installation","title":"Checking Docker installation","text":"<ul> <li>Check the installation by running:     <code>bash     &gt; docker pull hello-world     Using default tag: latest     latest: Pulling from library/hello-world     Digest: sha256:fc6cf906cbfa013e80938cdf0bb199fbdbb86d6e3e013783e5a766f50f5dbce0     Status: Image is up to date for hello-world:latest     docker.io/library/hello-world:latest</code></li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#docker_installation_troubleshooting","title":"Docker installation troubleshooting","text":"<ul> <li>Common problems with Docker<ul> <li>Mac DNS problem, try step 5 from the     article     and repeat the cmd below:     <code>bash     &gt; docker pull hello-world     Error response from daemon: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</code></li> <li>Linux sudo permission denied problem, see     here     for the solution     <code>bash     &gt; docker pull hello-world     Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get   http://%2Fvar%2Frun%2Fdocker.sock/v1.40/containers/json: dial unix /var/run/docker.sock: connect: permission denied</code></li> </ul> </li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#add_user_to_docker_group_and_sudoers","title":"Add user to Docker group and sudoers","text":"<ul> <li>This section only applies to Linux, macOS users can skip this. Docker Desktop     handles permissions automatically on Mac.</li> <li> <p>Add your user to the <code>docker</code> group to run Docker without <code>sudo</code></p> <p>```bash</p> <p>sudo usermod -aG docker $USER</p> </li> </ul> <p># Restart your shell session (log out and log back in), or run:</p> <p>newgrp docker     ```</p> <ul> <li>Add yourself to the sudoers file     <code>bash     &gt; sudo visudo</code><ul> <li>Add this line to the file(replace <code>your_username</code>):     <code>bash     your_username ALL=(ALL) NOPASSWD:ALL</code></li> </ul> </li> <li>You should see <code>docker</code> in the output     <code>bash     &gt; groups     your_username sudo ... docker</code></li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#tmux","title":"Tmux","text":"<ul> <li> <p>Using tmux is optional but recommended</p> </li> <li> <p>The thin environment is activated     automatically within a tmux session. Ensure you have already run the     <code>/dev_scripts_helpers/thin_client/build.py</code>     command as outlined in the     thin environment setup</p> </li> <li> <p>Create a soft link. The command below will create a file <code>~/go_{repo_name}.py</code></p> <p>```bash</p> <p>dev_scripts_{repo_name}/thin_client/tmux.py --create_global_link ```</p> </li> <li> <p>Create a tmux session. Choose <code>index</code> based on the dir name, e.g., <code>--index 1</code>     if the dir name is <code>~/src/tutorials1</code>.</p> <p>```bash</p> <p>dev_scripts_{repo_name}/thin_client/tmux.py --index 1 ```</p> </li> <li> <p>You need to create the tmux environment once per client and then you can     re-connect with:</p> <p>```bash</p> </li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#check_the_available_environments","title":"Check the available environments.","text":"<p>tmux ls tutorials1: 4 windows (created Fri Dec  3 18:27:09 2021) (attached)</p>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#attach_an_environment","title":"Attach an environment.","text":"<p>tmux attach -t tutorials1 ```</p>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#shell_support","title":"Shell support","text":"<ul> <li>We only support <code>bash</code>, no other shells like <code>zsh</code>, etc.</li> <li>We recommended that you make <code>bash</code> the default shell on your system, to avoid     possible compatibility issues</li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#set_up_aws_credentials","title":"Set up AWS credentials","text":"<ul> <li>NB! This section is only for interns/collaborators who have been assigned     tasks that require access to data on AWS S3</li> <li> <p>NB! Skip this section if you haven't been assigned an AWS user with us yet</p> </li> <li> <p>Create dir with proper permissions:</p> <p>```bash</p> <p>mkdir ~/.aws; chmod 700 ~/.aws ```</p> </li> <li> <p>Place the following into <code>~/.aws/config</code>:</p> <p><code>bash [profile ck] region = eu-north-1</code></p> </li> <li> <p>Place the following into <code>~/.aws/credentials</code> and populate the values with     your credentials:</p> <p><code>bash [ck] aws_access_key_id=*** aws_secret_access_key=***</code></p> </li> <li> <p>Test access to the S3 bucket:     <code>bash     &gt; aws s3 ls s3://causify-data-collaborators --profile ck                                                         PRE causal_automl/</code></p> </li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#some_useful_workflows","title":"Some useful workflows","text":"<ul> <li> <p>Check the installation by running:</p> <p>```bash</p> <p>docker pull hello-world Using default tag: latest ```</p> </li> <li> <p>Pull the latest image; this is done once</p> <p>```bash</p> <p>i docker_pull ```</p> </li> <li> <p>Pull the latest <code>helpers</code> image containing Linter; this is done once</p> <p>```bash</p> <p>i docker_pull_helpers ```</p> </li> <li> <p>Get the latest version of <code>master</code></p> <p>```bash</p> </li> <li> <p>Run Linter</p> <p>```bash</p> <p>i lint --files=\"dir1/file1.py dir2/file2.py\" ```</p> </li> <li> <p>Start a Docker container</p> <p>```bash</p> <p>i docker_bash ```</p> </li> <li> <p>You can ignore all the warnings that do not prevent you from running the     tests, e.g.,</p> <p>```bash WARNING: The AM_AWS_ACCESS_KEY_ID variable is not set. Defaulting to a blank string. WARNING: The AM_AWS_DEFAULT_REGION variable is not set. Defaulting to a blank string. WARNING: The AM_AWS_SECRET_ACCESS_KEY variable is not set. Defaulting to a blank string. WARNING: The AM_FORCE_TEST_FAIL variable is not set. Defaulting to a blank string. WARNING: The CK_AWS_ACCESS_KEY_ID variable is not set. Defaulting to a blank string. WARNING: The CK_AWS_DEFAULT_REGION variable is not set. Defaulting to a blank string. WARNING: The CK_AWS_SECRET_ACCESS_KEY variable is not set. Defaulting to a blank string. WARNING: The CK_TELEGRAM_TOKEN variable is not set. Defaulting to a blank string.</p> <p>This code is not in sync with the container: code_version='1.4.1' != container_version='1.4.0'</p> <p>You need to: - merge origin/master into your branch with <code>invoke git_merge_master</code> - pull the latest container with <code>invoke docker_pull</code> ```</p> </li> <li> <p>If you are prompted to enter sudo password, do not enter anything and press     Ctrl-C to resolve</p> <p><code>bash WARN  hserver.py _raise_invalid_host:342   Don't recognize host: host_os_name=Linux, am_host_os_name=None [sudo] password for ubuntu:</code></p> </li> <li> <p>Start a Jupyter server</p> <p>```bash</p> <p>i docker_jupyter ```</p> </li> <li> <p>To open a Jupyter notebook in a local web-browser:</p> <ul> <li>In the output from the cmd above find an assigned port, e.g.,     <code>bash     [I 14:52:26.824 NotebookApp] http://0044e866de8d:10091/ -&gt; port is 10091</code></li> <li>Add the port to the link like so: <code>http://localhost:10091/</code> or     <code>http://127.0.0.1:10091</code></li> <li>Copy-paste the link into a web-browser and update the page</li> </ul> </li> </ul>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#to_update_your_feature_branch_with_the_latest_changes_from_master_run","title":"To update your feature branch with the latest changes from master, run","text":""},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#the_cmd_below_from_a_feature_branch_ie_not_from_master","title":"the cmd below from a feature branch, i.e. not from master.","text":"<p>i git_merge_master</p>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#if_you_are_on_master_just_pull_the_remote_changes","title":"If you are on <code>master</code>, just pull the remote changes.","text":"<p>i git_pull ```</p>"},{"location":"onboarding/intern.set_up_development_on_laptop.how_to_guide.html#hack_use_a_local_container_if_needed","title":"Hack: use a local container if needed","text":"<ul> <li> <p>If the commands above do not work for you, then, as     a temporary workaround, you can run the commands in a local Docker container</p> </li> <li> <p>Build a local container (the version number is not that important but make     sure it follows the format of <code>NUM.NUM.NUM</code>)</p> </li> </ul> <pre><code>&gt; i docker_build_local_image --version 1.1.0\n</code></pre> <ul> <li>Run all the commands with the flags     <code>--stage local --version &lt;YOUR_VERSION_NUMBER&gt;</code>, e.g.:</li> </ul> <pre><code># Starting bash in a Docker container.\n&gt; i docker_bash --stage local --version 1.1.0\n# Starting a Jupyter server.\n&gt; i docker_jupyter --stage local --version 1.1.0\n# Running Linter.\n&gt; i lint --stage local --version 1.1.0 --files=\"dir1/file1.py dir2/file2.py\"\n</code></pre> <ul> <li>The hack is only there to unblock you and shouldn't be relied on forever. File     an issue to figure out why the commands don't work for you as-is</li> </ul>"},{"location":"onboarding/intern.signing_up.how_to_guide.html","title":"Intern.signing up.how to guide","text":""},{"location":"onboarding/intern.signing_up.how_to_guide.html#signing_up_to_the_project","title":"Signing up to the project","text":"<ul> <li> <p>Look around the repo and make sure that you are really interested in what we     are doing, and you have time to contribute</p> <ul> <li>Ponder on the IMPORTANT note about committing to contribute to the     project here</li> <li>Please fork, star, watch the KaizenFlow repo so that GitHub promotes our     repo (this will help us promote our effort)</li> </ul> </li> <li> <p>Fill out the     Contributor Info form.</p> <ul> <li>It's meant to just get basic contact info and technical skills about you</li> <li>Don't worry: by working with us, you will quickly become a coding machine</li> </ul> </li> <li> <p>Communication with the project teams happens through:</p> <ul> <li>GitHub on the repo like in any     open-source development</li> <li>Slack IM channel for higher bandwidth discussion</li> <li>Through the KaizenFlow mailing list</li> </ul> </li> <li> <p>Accept the invite from GitHub that we will send you</p> <ul> <li>Again please don't forget to fork, star, watch the KaizenFlow repo so that     GitHub promotes our repo</li> </ul> </li> <li> <p>Accept the invite to join the Google Drive that we will send you</p> </li> <li> <p>On-boarding Tasks</p> <ul> <li>Once the invitation is accepted, an issue will be created by the title     <code>On-board \\&lt;YOUR FIRST NAME LAST NAME\\&gt; Aka &lt;YOUR GITHUB_HANDLE&gt;</code>. If not     assinged, assign it to yourself and go through the particular checklist from     the issue one-by-one, marking each item as done when it's actually done.</li> </ul> </li> </ul>"},{"location":"tools/all.ai_review.how_to_guide.html","title":"All.ai review.how to guide","text":""},{"location":"tools/all.ai_review.how_to_guide.html#operations","title":"Operations","text":"<ul> <li> <p>There are several operations we want to perform using LLMs:</p> <ul> <li>Apply a transformation to a chunk of text<ul> <li>E.g., create a unit test</li> </ul> </li> <li>Create comments and lints in the form of a <code>cfile</code><ul> <li>E.g., lint or AI review based on certain criteria</li> </ul> </li> <li>Apply modifications from a <code>cfile</code> to a set of files<ul> <li>E.g., from linter and AI review</li> </ul> </li> <li>Add TODOs from a <code>cfile</code> to Python or markdown files</li> <li>Apply a set of transformations to an entire Python file<ul> <li>E.g., styling / formatting code</li> </ul> </li> <li>Rewrite an entire markdown to fix English mistakes without changing its     structure<ul> <li>E.g., styling / formatting a markdown</li> </ul> </li> </ul> </li> <li> <p>You should always commit your code before applying automatic transforms, in the     same way that we run the <code>linter</code> on a clean tree</p> <ul> <li>In this way, modifying a file is a separate commit and it's easy to review</li> </ul> </li> </ul>"},{"location":"tools/all.ai_review.how_to_guide.html#use_templates","title":"Use templates","text":"<ul> <li> <p>We use templates for code and documentation to show and describe how a document     or code should look like, e.g.,</p> <ul> <li><code>code_template.py</code> shows our coding style</li> <li><code>unit_test_template.py</code> shows how our unit tests look like</li> <li><code>all.how_to_guide_template_doc.md</code> shows how a Diataxis how to guide should     be structured and look like (same for <code>explanation</code>, <code>tutorial</code>, <code>reference</code>)</li> </ul> </li> <li> <p>The same templates can have multiple applications for:</p> <ul> <li>Humans:<ul> <li>Understand how to write documentation and code</li> <li>As boilerplate<ul> <li>E.g., \"copy the template and customize it to achieve a certain goal\"</li> </ul> </li> </ul> </li> <li>LLMs:<ul> <li>As reference style to apply transforms</li> <li>To report violations of coding styles</li> <li>As boilerplate<ul> <li>E.g., \"explain this piece of code using this template\"</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/all.ai_review.how_to_guide.html#tools","title":"Tools","text":""},{"location":"tools/all.ai_review.how_to_guide.html#llm_transformpy","title":"<code>llm_transform.py</code>","text":"<ul> <li> <p>There are several classes of transforms:</p> <ul> <li><code>code_*</code>: transform Python code<ul> <li><code>code_fix_*</code>: fix a specific chunk of code according to a prompt</li> <li><code>code_transform_*</code>: apply a series of transformations</li> <li><code>code_write_*</code>: write from scratch</li> </ul> </li> <li><code>latex_*</code>: process Latex code</li> <li><code>md_*</code>: process markdown <code>md</code> text and <code>txt</code> notes</li> <li><code>review_*</code>: process Python code to extract reviews</li> <li><code>scratch_*</code>: misc and one-off transforms</li> <li><code>slide_*</code>: process markdown slides in <code>txt</code> format</li> <li><code>text_*</code>: process free form (not markdown) text</li> </ul> </li> <li> <p>You can list the available transformations with:</p> <p>```bash</p> <p>llm_transform.py -p list</p> </li> </ul>"},{"location":"tools/all.ai_review.how_to_guide.html#available_prompt_tags","title":"Available prompt tags:","text":"<p>code_fix_by_using_f_strings code_fix_by_using_perc_strings code_fix_code code_fix_comments code_fix_complex_assignments code_fix_docstrings ... ```</p>"},{"location":"tools/all.ai_review.how_to_guide.html#transform_notespy","title":"<code>transform_notes.py</code>","text":"<ul> <li> <p>Some transformations don't need LLMs and are implemented as code</p> </li> <li> <p>You can see the available transforms with:</p> <p>```bash</p> <p>transform_notes.py -a list test: compute the hash of a string to test the flow format_headers: format the headers increase_headers_level: increase the level of the headers md_list_to_latex: convert a markdown list to a latex list md_remove_formatting: remove the formatting md_clean_up: clean up removing all weird characters md_only_format: reflow the markdown md_colorize_bold_text: colorize the bold text md_format: reflow the markdown and colorize the bold text ```</p> </li> </ul>"},{"location":"tools/all.ai_review.how_to_guide.html#ai_reviewpy","title":"<code>ai_review.py</code>","text":"<ul> <li>The rules for AI are saved in the file     ./docs/code_guidelines/all.coding_style_guidelines.reference.md</li> <li>This file has a special structure:     ```bash     &gt; extract_headers_from_markdown.py -i ./docs/code_guidelines/all.coding_style_guidelines.reference.md --max_level 2<ul> <li>All Style Guide<ul> <li>Summary</li> </ul> </li> <li>General<ul> <li>Spelling</li> </ul> </li> <li>Python<ul> <li>Naming</li> <li>Docstrings</li> <li>Comments</li> <li>Code Implementation</li> <li>Code Design</li> <li>Imports</li> <li>Type Annotations</li> <li>Functions</li> <li>Scripts</li> <li>Logging</li> <li>Misc</li> </ul> </li> <li>Unit Tests<ul> <li>Rules</li> </ul> </li> <li>Notebooks<ul> <li>General</li> <li>Plotting</li> <li>Jupytext</li> </ul> </li> <li>Markdown<ul> <li>General</li> <li>Headers</li> <li>Text ```</li> </ul> </li> <li>The first level represents the target language (e.g. <code>General</code>, <code>Python</code>)</li> <li>The second level represents a rule topic (e.g., <code>Imports</code>, <code>Functions</code>)</li> <li>The third level represents instructions for an LLM vs Linter, since some     instructions:<ul> <li>Are easier to enforce by an LLM</li> <li>While others should be enforced by the <code>linter</code> (even if they are temporary not     enforced by the <code>linter</code> but by LLM or by humans)</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/all.ai_review.how_to_guide.html#inject_todospy","title":"<code>inject_todos.py</code>","text":""},{"location":"tools/all.ai_review.how_to_guide.html#apply_todospy","title":"<code>apply_todos.py</code>","text":""},{"location":"tools/all.ai_review.how_to_guide.html#some_typical_workflows","title":"Some typical workflows","text":""},{"location":"tools/all.ai_review.how_to_guide.html#a_transform_workflow","title":"A transform workflow","text":"<ul> <li>There are 3 types of transforms and review tasks<ul> <li><code>llm</code>: executed by an LLM since they are difficult to implement otherwise<ul> <li>E.g., \"apply this style to a certain file\"</li> </ul> </li> <li><code>linter_llm</code>: executed by an LLM for now to get something in place, even if     they should be moved to code / linter<ul> <li>E.g., mainly formatting tasks</li> </ul> </li> <li><code>linter</code>: executed by the Linter using code and regex</li> </ul> </li> </ul>"},{"location":"tools/all.ai_review.how_to_guide.html#an_editing_workflow","title":"An editing workflow","text":"<ul> <li>Use <code>llm_transform.py</code> to:<ul> <li>Edit files manually applying specific transformations to chunks of code</li> <li>Apply transforms to an entire file</li> <li>Read and apply a list of transforms (from a <code>cfile</code>) and apply them</li> <li>Format the style of a template to a file</li> </ul> </li> </ul>"},{"location":"tools/all.ai_review.how_to_guide.html#a_reviewer_workflow","title":"A reviewer workflow","text":"<ul> <li> <p>This workflow can be used by the author of the code directly or by a reviewer</p> <ul> <li>Initially, reviewers use these tools as part of dogfooding of the workflows</li> <li>The goal is to make these tools robust enough so that they can be used     directly by the author and potentially integrated in the <code>linter</code> flow     itself</li> </ul> </li> <li> <p>Go to the Git branch with the code to review</p> </li> <li> <p>Check which files are modified</p> <p>```bash</p> <p>invoke git_branch_diff_with -t base --only-print-files ```</p> </li> <li> <p>Run <code>ai_review.py</code> on each file to generate a list of comments on the code</p> <ul> <li>This is equivalent to running a <code>review</code> target with <code>llm_transform.py</code>     (e.g., <code>llm_transform.py -p review_*</code>) but it is a separated flow for     clarify</li> <li>It generates a <code>cfile</code> with a list of comments</li> </ul> </li> <li> <p>Review the TODOs using cfile to jump around files:</p> <p>```bash</p> <p>vim -c \"cfile cfile\" ``` - You can fix the code according to the TODOs directly - Discard a TODO as a false positive or not important</p> </li> <li> <p>Run <code>inject_todos.py</code> to add TODOs to the files for someone (human or LLM)     else to fix it later</p> <ul> <li>E.g., in a code review you want to ask the author to perform that task</li> </ul> </li> <li> <p>Run <code>apply_todos.py</code> to automatically apply the TODOs using an LLM</p> <ul> <li>This can be a risky move</li> </ul> </li> </ul>"},{"location":"tools/all.ai_review.how_to_guide.html#example","title":"Example","text":"<ul> <li> <p>There are multiple targets for the <code>ai_review.py</code></p> <p>```bash</p> </li> <li> <p>Run the <code>ai_review.py</code> tool:     <code>bash     # Run.     &gt; ai_review.py -i $FILE -p $PROMPT</code></p> </li> <li> <p>Sometimes you want to edit the tools in a different client while running it on     a different client:     <code>bash     # To copy all the reviewer code.     &gt; \\cp -f /Users/saggese/src/helpers1/helpers/lib_tasks_lint.py helpers &amp;&amp; \\         i lint_sync_code &amp;&amp; ai_review.py -i $FILE -p $PROMPT</code></p> </li> <li> <p>Review and apply the changes:     ```bash     &gt; vi -c \"cfile cfile\"</p> <p>inject_todos.py --cfile cfile</p> <p>llm_transform.py -i $FILE -p code_fix_code ```</p> </li> </ul>"},{"location":"tools/all.ai_review.how_to_guide.html#specify_the_target_prompt_for_ai_reviewpy","title":"Specify the target prompt for ai_review.py.","text":"<p>PROMPT=review_llm PROMPT=review_correctness PROMPT=review_linter PROMPT=review_architecture</p>"},{"location":"tools/all.ai_review.how_to_guide.html#specify_the_target_file","title":"Specify the target file.","text":"<p>FILE=dev_scripts_helpers/github/dockerized_sync_gh_repo_settings.py ```</p>"},{"location":"tools/all.ai_review.how_to_guide.html#how_to_improve_the_reviewing_tools_while_reviewing","title":"How to improve the reviewing tools while reviewing","text":"<ul> <li> <p>Problem: we often want to improve one of our tools (e.g., <code>linter.py</code>,     <code>ai_review.py</code>) while reviewing somebody's else code</p> </li> <li> <p>There are two use cases</p> <ol> <li>When the code to review is in a different repo than <code>//helpers</code></li> <li>Solution:</li> <li>Create a branch in <code>//helpers</code></li> <li>Modify the code for the tools in place</li> <li>When the code to review is in the repo <code>//helpers</code></li> <li>Solution:</li> <li>Use a different Git client to develop and edit the tools</li> <li>Automatically copy <code>linter.py</code> / <code>ai_review.py</code> code from the client we      are developing in to the one we are reviewing      <code>bash      &gt; \\cp -f /Users/saggese/src/helpers1/helpers/lib_tasks_lint.py helpers &amp;&amp; i lint_sync_code</code></li> <li>Run the tools in the different client</li> <li>Before committing the review, we then revert the      <code>linter.py / ai_review.py</code> code      <code>bash      &gt; i lint_sync_code -r</code></li> </ol> </li> </ul>"},{"location":"tools/all.bfg_repo_cleaner.how_to_guide.html","title":"All.bfg repo cleaner.how to guide","text":""},{"location":"tools/all.bfg_repo_cleaner.how_to_guide.html#bfg_repo-cleaner","title":"BFG Repo-Cleaner","text":"<p>BFG dockerized.</p>"},{"location":"tools/all.bfg_repo_cleaner.how_to_guide.html#build","title":"Build","text":"<pre><code>docker build . --tag bfg\n</code></pre>"},{"location":"tools/all.bfg_repo_cleaner.how_to_guide.html#usage","title":"Usage","text":"<p>You could run BFG in a container by executing the following <code>docker</code> command.</p> <pre><code>docker run -it --rm \\\n    --volume \"$PWD:/home/bfg/workspace\" \\\n    koenrh/bfg \\\n    --delete-files id_rsa\n</code></pre> <p>You could make this command more easily accessible by putting it in an executable, and make sure that it is available in your <code>$PATH</code>. Alternatively, you could create wrapper functions for your <code>docker run</code> commands (example).</p> <pre><code>bfg() {\n    docker run -it --rm \\\n        --volume \"$PWD:/home/bfg/workspace\" \\\n        --name bfg \\\n        koenrh/bfg \"$@\"\n}\n</code></pre>"},{"location":"tools/all.blog.how_to_guide.html","title":"All.blog.how to guide","text":""},{"location":"tools/all.blog.how_to_guide.html#how_to_write_a_blog_for_causify","title":"How to Write a Blog for Causify","text":"<p>Writing a blog at Causify is about communicating our ideas clearly, while re-using and connecting with the broader body of internal documentation, white papers, tutorials, and research notes we already create. This guide explains how blogs fit into our documentation ecosystem, what principles to follow, and provides a checklist to get started.</p>"},{"location":"tools/all.blog.how_to_guide.html#position_of_blogs_in_our_documentation_ecosystem","title":"Position of Blogs in Our Documentation Ecosystem","text":"<ul> <li>There is often overlap between:<ul> <li>Internal white papers (aka an internal report)</li> <li>Journal / conference papers</li> <li>Blogs</li> <li>Internal documentation</li> <li>Research Google Docs</li> </ul> </li> </ul>"},{"location":"tools/all.blog.how_to_guide.html#invariants","title":"Invariants","text":"<ul> <li>Internal documentation: Contains all the information at maximum detail,     organized using the Diataxis framework</li> <li>Internal white paper: Abstracted version of internal documentation,     emphasizing architecture and high-level results</li> <li>Journal / conference paper: A succinct and highly structured version of a     white paper</li> <li>Blog: Often the union of several documentation pieces. A blog should give a     high-level view of the work and why it matters.<ul> <li>Point to detailed technical resources (internal docs, repos, tutorials)     instead of duplicating them.</li> <li>Do a comparison with the state-of-the-art (SOTA), and explain why our     approach is better, different, or more practical.</li> <li>Keep it accessible as blogs are not white papers.</li> </ul> </li> <li>Research Google Docs: Living documents to store ongoing project information     before being consolidated.</li> </ul> <p>Note: It is common for a project to start with a blog and then evolve into a journal/conference paper.</p>"},{"location":"tools/all.blog.how_to_guide.html#practical_guidelines_for_blogs","title":"Practical Guidelines for Blogs","text":"<ul> <li>Level of detail<ul> <li>Blogs should stay at a high level</li> <li>Only go deep if it is explicitly a tutorial-style blog</li> </ul> </li> <li>References<ul> <li>Always link to technical resources (internal docs, repos, code, or white     papers) for implementation details.</li> </ul> </li> <li>Comparisons<ul> <li>Compare against SOTA or industry norms, and clearly explain why our approach     is novel, impactful, or more effective.</li> </ul> </li> <li>Workflow &amp; Tools<ul> <li>Markdown first: When writing technical blogs (that don\u2019t need exec input),     keep everything in markdown and work directly in GitHub.<ul> <li>Easier to manage diffs, PRs, reviews, and formatting.</li> <li>Enables diagrams via Mermaid, and integration with internal rendering     tools.</li> </ul> </li> <li>Google Docs only if needed: Use Google Docs when we need broader     collaboration (e.g., exec reviews, heavy commenting).<ul> <li>Conversion tools exist to move from GDocs \u2192 Markdown.</li> </ul> </li> </ul> </li> <li>Assets and Diagrams<ul> <li>Use <code>./helpers_root/dev_scripts_helpers/documentation/render_images.py</code> to     render images/diagrams.</li> </ul> </li> </ul>"},{"location":"tools/all.blog.how_to_guide.html#checklist_for_preparing_a_blog_file_in_a_github_issue","title":"Checklist for Preparing a Blog (File in a Github Issue)","text":"<p>When starting a blog on a project, open a GitHub Issue and track the following:</p>"},{"location":"tools/all.blog.how_to_guide.html#learn_how_we_write_and_organize_documentation","title":"Learn How We Write and Organize Documentation","text":"<ul> <li>[ ] Read docs in [<code>//helpers/docs/documentation_meta</code>]<ul> <li>[ ] [<code>//helpers/docs/documentation_meta/all.diataxis.explanation.md</code>]</li> <li>[ ] [<code>//helpers/docs/documentation_meta/all.google_technical_writing.how_to_guide.md</code>]</li> <li>[ ] [`//helpers/docs/documentation_meta/all.writing_docs.how_to_guide.md]</li> </ul> </li> </ul>"},{"location":"tools/all.blog.how_to_guide.html#learn_our_style_and_tooling","title":"Learn Our Style and Tooling","text":"<ul> <li>Read rules enforced by <code>linter</code> and <code>ai_review.py</code><ul> <li>[ ] Read ./docs/code_guidelines/all.coding_style_guidelines.reference.md</li> </ul> </li> </ul>"},{"location":"tools/all.blog.how_to_guide.html#learn_how_we_organize_projects_and_tutorials","title":"Learn How We Organize Projects and Tutorials","text":"<ul> <li>[ ] Read [<code>//tutorials/class_project_instructions/README.md</code>]</li> <li>[ ] Read [<code>//tutorials/docs/all.learn_X_in_60_minutes.how_to_guide.md</code>]</li> </ul>"},{"location":"tools/all.blog.how_to_guide.html#read_examples_of_past_blogs","title":"Read Examples of Past Blogs","text":"<ul> <li>[ ] [<code>//tutorials/blogs</code>]</li> </ul>"},{"location":"tools/all.blog.how_to_guide.html#apply_blog-specific_best_practices","title":"Apply Blog-Specific Best Practices","text":"<ul> <li>[ ] Keep content at a high level (unless it\u2019s a tutorial).</li> <li>[ ] Link to internal docs, repos, or white papers for details.</li> <li>[ ] Compare against SOTA and articulate why our work is different/better.</li> <li>[ ] Decide workflow:<ul> <li>[ ] Markdown-first (default)</li> <li>[ ] Google Docs (if executive review required)</li> </ul> </li> <li>[ ] Use image rendering tools (<code>render_images.py</code>) or Mermaid for diagrams.</li> <li>[ ] If using GDocs, plan for Markdown conversion before publication.</li> </ul>"},{"location":"tools/all.blog.how_to_guide.html#summary","title":"Summary","text":"<p>A Causify blog is not just a write-up, it\u2019s a way of:</p> <ul> <li>Communicating why a project matters,</li> <li>Placing our work in the context of SOTA, and</li> <li>Connecting readers to deeper resources if they want details.</li> </ul> <p>By following this guide and checklist, contributors can ensure our blogs are consistent, informative, and impactful.</p>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html","title":"All.causify blog platform.how to guide","text":""},{"location":"tools/all.causify_blog_platform.how_to_guide.html#causify_blog_platform_documentation","title":"Causify Blog Platform Documentation","text":""},{"location":"tools/all.causify_blog_platform.how_to_guide.html#introduction","title":"Introduction","text":"<ul> <li>The blog platform is accessible at blog.causify.ai</li> <li>It enables us to share company updates, thought leadership content, and     technical insights</li> <li>The audience includes both internal and external stakeholders</li> <li>The platform is built on Ghost, an open-source, modern publishing solution</li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#stakeholders","title":"Stakeholders","text":"<ul> <li>Executives: Gain visibility into platform reach, editorial calendar, and     content quality</li> <li>Marketing &amp; Communications: Publish and manage content (blog posts and     announcements)</li> <li>Technical Team: Manage and maintain underlying infrastructure, security,     and updates</li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#accessing_the_blog","title":"Accessing the Blog","text":""},{"location":"tools/all.causify_blog_platform.how_to_guide.html#public_access","title":"Public Access","text":"<ul> <li>Navigate to blog.causify.ai. Readers can see     published posts without logging in</li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#admin_access_staff_portal","title":"Admin Access (Staff Portal)","text":"<ul> <li>Go to blog.causify.ai/ghost</li> <li>Enter your email and password (provided via staff invites)</li> <li>After logging in, you can create, edit, or publish content\u2014depending on your     role</li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#getting_started_as_a_staff_user","title":"Getting Started as a Staff User","text":""},{"location":"tools/all.causify_blog_platform.how_to_guide.html#receiving_an_invitation","title":"Receiving an Invitation","text":"<ul> <li>An admin invites you by entering your email address in the Staff \u2192 Invite     People section</li> <li>You receive an email link prompting you to create or confirm an account</li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#logging_in","title":"Logging In","text":"<ul> <li>Open blog.causify.ai/ghost</li> <li>Enter your credentials (email and password)</li> <li>You'll be taken to the Ghost Admin Dashboard, where you can see recent posts,     drafts, and announcements</li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#how_to_create_and_publish_a_blog","title":"How to create and publish a blog","text":""},{"location":"tools/all.causify_blog_platform.how_to_guide.html#jekyll","title":"Jekyll","text":""},{"location":"tools/all.causify_blog_platform.how_to_guide.html#set_up_jekyll","title":"Set up Jekyll","text":"<p>https://github.com/jekyll/jekyll</p> <ul> <li> <p>Install Jekyll locally</p> <ul> <li> <p>Install Ruby and Bundler:     <code>bash     &gt; gem install jekyll bundler</code></p> </li> <li> <p>Create a new site: ```bash</p> <p>jekyll new my-blog cd my-blog ```</p> </li> <li> <p>Build and serve locally: ```bash</p> <p>bundle exec jekyll serve ```</p> </li> <li> <p>Open your browser at http://localhost:4000 to see your site</p> </li> </ul> </li> <li> <p>Use Docker</p> <ul> <li> <p>Instructions at https://github.com/envygeeks/jekyll-docker/blob/master/README.md</p> </li> <li> <p>Pull docker     <code>&gt; docker pull jekyll/jekyll</code></p> </li> <li> <p>On Mac     &gt; docker run --platform linux/amd64 -v $(pwd):/site jekyll/jekyll jekyll new blog</p> <p><code>export JEKYLL_VERSION=3.8 docker run --rm \\     --platform linux/amd64 \\     --volume=\"$PWD:/srv/jekyll:Z\" \\     -it jekyll/jekyll:$JEKYLL_VERSION \\     jekyll build</code></p> </li> </ul> </li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#add_blog_posts_on_github","title":"Add blog posts on GitHub","text":"<pre><code>- Create a `_posts` folder if it doesn't exist\n- Add Markdown files named in the format `YYYY-MM-DD-title.md`, for example:\n    ```text\n    _posts/2025-07-06-my-first-post.md\n    ```\n\n- Each post needs front matter at the top:\n```text\n    ---\n    layout: post\n    title: \"My First Blog Post\"\n    date: 2025-07-06\n    ---\n\n    This is my first blog post written in Markdown\n    ```\n</code></pre>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#configure_github_pages","title":"Configure GitHub Pages","text":"<pre><code>- Push your code to GitHub\n- Go to the repository's Settings &gt; Pages\n- Set the source branch to main and the folder to / (root) or /docs if your\n    site is in a docs folder\n- Save your settings. Your site will be published at:\n    `https://yourusername.github.io/my-blog/`\n</code></pre> <ul> <li>Add a custom domain (optional)<ul> <li>Buy a domain name and update your DNS to point to <code>yourusername.github.io</code></li> <li>In your repo, create a file named CNAME with your custom domain name:     <code>www.yourdomain.com</code></li> </ul> </li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#configure","title":"Configure","text":"<ul> <li> <p>Example <code>_config.yml</code> to configure your site:     ```text     title: My Blog     description: A blog powered by Jekyll and GitHub Pages     baseurl: \"\"     url: \"https://yourusername.github.io\"</p> <p>remote_theme: jekyll/minima</p> <p>plugins:     - jekyll-feed ```</p> </li> <li> <p>Add an <code>index.md</code> at the root of your repo:     <code>text     ---     layout: home     ---</code></p> </li> <li> <p>Folder structure example     <code>text     \u251c\u2500\u2500 _config.yml     \u251c\u2500\u2500 _posts/     \u2502   \u2514\u2500\u2500 2025-07-06-my-first-post.md     \u251c\u2500\u2500 index.md     \u2514\u2500\u2500 CNAME  # only if using a custom domain</code></p> </li> <li> <p>Every time you push changes to GitHub, GitHub Pages will rebuild and publish your site automatically</p> </li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#hugo","title":"Hugo","text":""},{"location":"tools/all.causify_blog_platform.how_to_guide.html#ananke","title":"Ananke","text":"<p>https://themes.gohugo.io/tags/blog/ Download a them from https://github.com/theNewDynamic/gohugo-theme-ananke</p> <p>unzip blog/gohugo-theme-ananke-main.zip</p> <p>mv gohugo-theme-ananke-main blog/themes/ananke</p> <p>Add theme = \"ananke\" config.toml</p> <p>docker run --rm -it -v $(pwd):/src klakegg/hugo:ext new posts/my-first-post.md WARN 2025/07/06 15:41:14 Module \"ananke\" is not compatible with this Hugo version; run \"hugo mod graph\" for more information. Content \"/src/content/posts/my-first-post.md\" created</p> <p>vi ./content/posts/my-first-post.md</p>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#hugo-book","title":"hugo-book","text":"<p>https://themes.gohugo.io/themes/hugo-book/ https://github.com/alex-shpak/hugo-book# https://hugo-book-demo.netlify.app/</p>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#pre-built_image","title":"pre-built image","text":"<ul> <li>This image is too old and doesn't work with many themes</li> </ul> <p>docker run --rm -it -v $(pwd):/src klakegg/hugo:latest new site blog --force</p> <p>Ananke is not compatible with the klakegg version</p>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#dockerfile","title":"Dockerfile","text":"<p>docker build -t hugo .</p> <p>docker run --rm -it -p 1313:1313 -v $(pwd):/src hugo hugo server --bind 0.0.0.0</p>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#to_clean","title":"To clean","text":"<p>hugo mod clean rm -rf public</p>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#ghost","title":"Ghost","text":""},{"location":"tools/all.causify_blog_platform.how_to_guide.html#writing_publishing_content","title":"Writing &amp; Publishing Content","text":"<ul> <li> <p>Create a New Post</p> <ul> <li>Click New post in the admin panel</li> <li>Enter a title and start writing. Ghost uses a rich text/markdown hybrid     editor with slash commands (<code>/</code>) for quick embeds</li> </ul> </li> <li> <p>Add Images &amp; Media</p> <ul> <li>Drag and drop images directly into the editor, or click the \"+\" button and     select Image</li> <li>(Optional): We can have S3 or external storage configured, so the files are     automatically uploaded to that location</li> </ul> </li> <li> <p>Using Cards &amp; Embeds</p> <ul> <li>Type <code>/</code> in a new line to see options (e.g., Markdown card, HTML, YouTube     embed, etc.)</li> </ul> </li> <li> <p>Preview &amp; Publish</p> <ul> <li>Click Preview to see how the post will appear to readers</li> <li>When ready, click Publish \u2192 Publish now or schedule a specific time</li> </ul> </li> <li> <p>Tags &amp; Organization</p> <ul> <li>Add tags to group similar posts (e.g., <code>Company News</code>, <code>Technical</code>,     <code>Research</code>)</li> </ul> </li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#example","title":"Example","text":"<ul> <li> <p>Consider the case where we want to push the Markdown file     <code>all.invoke_git_branch_copy.how_to_guide.md</code> to the blog</p> </li> <li> <p>After creating an account, follow these steps:</p> </li> <li> <p>Step 1: Open the Ghost Admin panel</p> <ul> <li>Open blog.causify.ai/ghost</li> <li> <p>Enter your credentials to login</p> <p></p> </li> <li> <p>Open the Ghost Admin Panel</p> <p></p> </li> </ul> </li> <li> <p>Step 2: Create a new post</p> <ul> <li> <p>Click New post on the top right</p> <p></p> </li> </ul> </li> <li> <p>Step 3: Insert Markdown card</p> <ul> <li> <p>Use the <code>/markdown</code> command to insert a Markdown card</p> <p></p> </li> </ul> </li> <li> <p>Step 4: Paste the contents to post</p> <ul> <li> <p>Copy and paste the content of <code>all.invoke_git_branch_copy.how_to_guide.md</code>     into the Markdown card</p> <p></p> </li> </ul> </li> <li> <p>Step 5: Perform edits to the content</p> <ul> <li>Move the level 1 title (e.g., <code># The Git Branch Copy Workflow</code>) into the Ghost     post title field</li> <li> <p>Delete the level 1 header from the Markdown card content</p> <p></p> </li> </ul> </li> <li> <p>Step 6: Handle authorship attribution</p> <ul> <li> <p>If you are a Contributor:</p> <ul> <li>Add a temporary <code>## Authorship</code> section at the very top of the Markdown card     content</li> <li>List everyone who contributed (e.g., wrote code, reviewed, edited)</li> </ul> <p></p> </li> <li> <p>If you are an Author or higher:</p> <ul> <li>Add contributors directly in the Authors field using the Ghost sidebar</li> <li>You do not need to include a <code>## Authorship</code> section in the content</li> </ul> </li> <li> <p>If you are reviewing a blog created by a Contributor:</p> <ul> <li>You need to have Author or higher access to do this</li> <li>Check the <code>## Authorship</code> section at the top of the Markdown card</li> <li>Manually copy the listed names into the Ghost Authors field</li> <li>Delete the <code>## Authorship</code> section from the Markdown content before     publishing</li> </ul> </li> </ul> </li> <li> <p>Step 7: Preview post</p> <ul> <li> <p>Click Preview on the top right to verify formatting (headings, TOC, code     blocks)</p> <p> </p> </li> </ul> </li> <li> <p>Step 8: Optionally add metadata like excerpt, tags, or scheduled time</p> <ul> <li>These actions require Author access or higher</li> <li> <p>Contributors cannot modify metadata or scheduling options</p> <p></p> </li> </ul> </li> <li> <p>Step 9: Publish the blog</p> <ul> <li>Click Publish \u2192 Publish now (or schedule the post for later) to publish     the blog</li> <li>If you are a Contributor, you cannot publish or schedule posts directly</li> <li>A staff member with Author or higher access must review and approve the     post before it goes live</li> </ul> </li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#themes_design","title":"Themes &amp; Design","text":"<ul> <li> <p>Default Theme (Casper)</p> <ul> <li>The blog currently uses Ghost's default theme, \"Casper\", providing a clean     and responsive layout</li> </ul> </li> <li> <p>Customizing Design</p> <ul> <li>Navigate to Settings \u2192 Design to upload a new theme or activate an     existing one</li> <li>Use Code Injection (under the same area) for tracking scripts (e.g.,     Google Analytics) or custom CSS</li> </ul> </li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#additional_guides_tutorials","title":"Additional Guides &amp; Tutorials","text":"<ul> <li> <p>Below are step-by-step mini-tutorials to help users and contributors     understand some advanced or less obvious features of Ghost</p> </li> <li> <p>Creating and Managing Tags</p> <ul> <li> <p>What are tags?</p> <ul> <li>Tags let you organize content into categories (e.g., <code>Product Updates</code>,     <code>Engineering</code>)</li> </ul> </li> <li> <p>How to add tags</p> <ul> <li>When editing a post, find the Tags section on the right panel, type a new     tag name or select an existing one</li> <li>Posts can have multiple tags (e.g., <code>Engineering</code> + <code>Announcements</code>)</li> </ul> </li> <li> <p>Managing tags</p> <ul> <li>Go to Posts \u2192 Tags in the admin panel to rename or delete tags</li> </ul> </li> </ul> </li> <li> <p>Scheduling Posts</p> <ul> <li> <p>Why schedule posts?</p> <ul> <li>Perfect for timing announcements, product releases, or holiday-themed posts</li> </ul> </li> <li> <p>How to schedule</p> <ul> <li>Write your post, click Publish \u2192 Schedule it for later</li> <li>Pick a date and time (in your local timezone)</li> </ul> </li> <li> <p>Editing a scheduled post</p> <ul> <li>Navigate to Posts \u2192 Scheduled, then click the post to edit or change the     scheduled time</li> </ul> </li> </ul> </li> <li> <p>Using Markdown &amp; HTML Blocks</p> <ul> <li> <p>Markdown basics</p> <ul> <li>Ghost's editor supports inline markdown. For advanced formatting, insert a     Markdown card by typing <code>/markdown</code></li> <li>You can use headings, bold, italics, blockquotes, lists, etc.</li> </ul> </li> <li> <p>HTML cards</p> <ul> <li>Type <code>/html</code> to embed raw HTML for custom iframes, custom scripts, or     specialized formatting</li> </ul> </li> </ul> </li> <li> <p>Excerpt and Meta Data</p> <ul> <li> <p>Excerpt(Optional)</p> <ul> <li>Under Post settings \u2192 Excerpt, you can define a short preview text for     your post</li> <li>This snippet appears on the home page or in RSS feeds, depending on your     theme</li> </ul> </li> <li> <p>Meta title and description</p> <ul> <li>For SEO benefits, set custom meta titles and descriptions under Post     settings \u2192 Meta data</li> </ul> </li> </ul> </li> <li> <p>Integrations &amp; Webhooks</p> <ul> <li> <p>Built-in integrations</p> <ul> <li>Under Settings \u2192 Integrations, you can quickly connect Slack, Zapier, or     other tools to notify our team when new posts are published</li> </ul> </li> <li> <p>Custom webhooks</p> <ul> <li>You can create webhooks for publish events, letting external systems react     (e.g., cross-posting to social media)</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#frequently_asked_questions_faq","title":"Frequently Asked Questions (FAQ)","text":"<ul> <li> <p>How do I reset my password?</p> <ul> <li>Click Forgot Password? on the login page. Check your inbox for a reset     link (ensure it's not caught in spam)</li> </ul> </li> <li> <p>Why isn't my invitation email received?</p> <ul> <li>Check your spam folder</li> </ul> </li> <li> <p>Can we schedule posts for later?</p> <ul> <li>Yes, while publishing a new post, choose Publish \u2192 Schedule it for later</li> </ul> </li> <li> <p>How do I integrate analytics?</p> <ul> <li>Use Settings \u2192 Code Injection to add your analytics script in the     site-wide header/footer</li> </ul> </li> <li> <p>How to reset a staff user's password if they can't access email?</p> <ul> <li>Admin can go to Staff \u2192 Select User \u2192 Resend Invitation or Change     Password. Alternatively, the user can use the Forgot Password? link</li> </ul> </li> <li> <p>I see a \"502 Bad Gateway\" error when accessing blog.causify.ai. What do I do?</p> <ul> <li>Contact the Infra/DevOps team to investigate network issues</li> </ul> </li> <li> <p>Why can't I see the scheduling option?</p> <ul> <li>Make sure you have an Author (or higher) role. Contributors can't schedule     posts</li> </ul> </li> <li> <p>Can we add custom pages (e.g., \"About,\" \"Contact,\" \"Terms of Service\")?</p> <ul> <li>Yes. Create a new post and mark it as a page in Post Settings and toggle     \"Turn this post into a static page.\"</li> <li>Adjust your theme's navigation to link these pages</li> </ul> </li> </ul>"},{"location":"tools/all.causify_blog_platform.how_to_guide.html#summary_next_steps","title":"Summary &amp; Next Steps","text":"<ul> <li> <p>We have successfully deployed a secure, scalable Ghost blog at     blog.causify.ai/ghost</p> <ul> <li>Executives and stakeholders can review content for corporate alignment and     brand consistency</li> <li>Authors can quickly draft and publish posts, while editors ensure quality</li> <li>Technical teams maintain the underlying Kubernetes infrastructure, backups,     and security posture</li> </ul> </li> <li> <p>Next Steps:</p> <ul> <li>Continue inviting staff members who need author access</li> <li>Customize the theme to fit the Causify brand</li> <li>Review and refine settings as you collect feedback from readers and     contributors</li> </ul> </li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html","title":"All.chatgpt api.how to guide","text":""},{"location":"tools/all.chatgpt_api.how_to_guide.html#chatgpt_api","title":"Chatgpt Api","text":""},{"location":"tools/all.chatgpt_api.how_to_guide.html#openai_assistant_runner_manager","title":"OpenAI Assistant Runner &amp; Manager","text":""},{"location":"tools/all.chatgpt_api.how_to_guide.html#what_is_openai_assistant","title":"What is OpenAI Assistant","text":"<ul> <li>An assistant is similar to a modified GPT that has mastered some knowledge and     is able to use that knowledge for future tasks</li> <li> <p>The official OpenAI documentation is at     https://platform.openai.com/docs/assistants/overview</p> </li> <li> <p>Normally you \"chat\" with ChatGPT, that means anything you send to it is     treated as input</p> </li> <li>GPT will forget what you previously said or files uploaded as the total length     of the conversation grows</li> <li>By creating an Assistant, you build a new \"instance\" of ChatGPT and can give     it some knowledge to learn</li> <li>This knowledge can be in many formats and up to 20 files (512MB each) at a     time</li> <li>With an instruction string, you define its behavior about how it should make     use of those knowledge</li> <li>When talking to an assistant, you can still add files in the message</li> <li>These files does not count towards its 20 files' knowledge limit, as they are     considered as input and will be forgotten eventually</li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#general_pattern","title":"General pattern","text":"<ul> <li>Creation:<ul> <li>Send some guideline or example files for one type of task to the Assistant     itself as knowledge</li> <li>Send an instruction about how it should deal with tasks using those     knowledge</li> </ul> </li> <li>Running:<ul> <li>Whenever a specific task comes, send the task and its data files in the     message as input</li> <li>Let the assistant solve the task based on the knowledge it learned</li> <li>It will forget everything in this task, and be ready for the next task like     this one never happens</li> </ul> </li> <li>Chatting:<ul> <li>You can continue the conversation if you are not satisfied with its reply</li> <li>Chatting is not yet implemented in our code, since command line scripts     cannot save conversations.</li> </ul> </li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#code_organization","title":"Code organization","text":"<ul> <li> <p>Libraries are under <code>helpers</code>, e.g.,</p> <ul> <li><code>helpers/hchatgpt.py</code></li> <li><code>helpers/hchatgpt_instructions.py</code></li> </ul> </li> <li> <p>Scripts are under <code>dev_scripts/chatgpt</code>, e.g.,</p> <ul> <li><code>dev_scripts/chatgpt/manage_chatgpt_assistant.py</code></li> <li><code>dev_scripts/chatgpt/run_chatgpt.py</code></li> </ul> </li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#how_to_use","title":"How to use","text":"<ul> <li>Add the API KEY     <code>bash     &gt; export OPENAI_API_KEY=&lt;your secret key&gt;</code></li> <li>Each API key can be bound to an OpenAI Organization</li> <li>Assistants are organization-wide, an assistant created under our Org can be     accessed by any API key that belongs to our Org</li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#assistant_manager","title":"Assistant Manager","text":"<ul> <li> <p>The interface is like:</p> <p>```bash</p> <p>manage_chatgpt_assistant.py -h Manage the ChatGPT Assistants in our OpenAI Organization</p> <p>optional arguments:     -h, --help            show this help message and exit     --create CREATE_NAME  Name of the assistant to be created     --edit EDIT_NAME      Name of the assistant to be edited     --delete DELETE_NAME  Name of the assistant to be deleted, will ignore all other arguments     --new_name NEW_NAME   New name for the assistant, only used in -e     --model MODEL         Model used by the assistant     --instruction_name INSTRUCTION_NAME                                                 Name of the instruction for the assistant, as shown in helpers.hchatgpt_instructions     --input_files [INPUT_FILE_PATHS ...]                                                 Files needed for the assistant, use relative path from project root     --retrieval_tool, --no-retrieval_tool                                                 Enable the retrieval tool. Use --no-r to disable     --code_tool, --no-code_tool                                                 Enable the code_interpreter tool. Use --no-c to disable     --function FUNCTION   Apply certain function tool to the assistant, not implemented yet     -v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}                                                 Set the logging level ```</p> </li> <li> <p>Use this script to create, modify, or delete an assistant in our Org</p> </li> <li>A set of instructions are in <code>helpers/hchatgpt_instructions.py</code></li> <li>Feel free to add more instructions for different tasks here</li> <li> <p>For better understanding, the name of an assistant created should be related     to its instruction name</p> </li> <li> <p>E.g., create a doc writer assistant with name <code>DocWriter-1</code> and use     <code>instruction=DocWriter</code> from <code>helpers/hchatgpt_instructions.py</code> <code>bash     &gt; manage_chatgpt_assistant.py \\             --create DocWriter-1 \\             --model \"gpt-4-1106-preview\" \\             --instruction_name DocWriter \\             --input_files docs/work_tools/all.bfg_repo_cleaner.how_to_guide.md docs/marketing/dropcontact.how_to_guide.md docs/coding/all.hplayback.how_to_guide.md \\             --retrieval_tool --code_tool</code></p> </li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#running_assistant","title":"Running Assistant","text":"<ul> <li> <p>The script <code>dev_scripts/chatgpt/run_chatgpt.py</code> runs an assistant</p> </li> <li> <p>The interface is like:</p> <p>```bash</p> <p>run_chatgpt.py -h Use ChatGPT Assistant to process a file or certain text.</p> <p>optional arguments:     -h, --help            show this help message and exit     --list                Show all currently available assistants and exit     --assistant_name ASSISTANT_NAME                                                 Name of the assistant to be used     --input_files [INPUT_FILE_PATHS ...]                                                 Files needed in this run, use relative path from project root     --model MODEL         Use specific model for this run, overriding existing assistant config     --output_file OUTPUT_FILE                                                 Redirect the output to the given file     --input_text INPUT_TEXT                                                 Take a text input from command line as detailed instruction     --vim                 Disable -i (but not -o), take input from stdin and output to stdout forcely     -v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}                                                 Set the logging level ```</p> </li> <li> <p><code>dev_scripts/run_chatgpt.py -l</code> will show all the available assistants in our     Org.</p> </li> <li> <p>Refer to <code>helpers/hchatgpt_instructions.py</code> to see how they are instructed</p> <p>```bash</p> <p>python dev_scripts/chatgpt/run_chatgpt.py \\     -n MarkdownLinter \\ # Use the assistant \"MarkdownLinter\"     -f dev_scripts/chatgpt/example_data/corrupted_dropcontact.how_to_guide.md \\ # Give this corrupted markdown file     -o dev_scripts/chatgpt/example_data/gpt_linted_dropcontact.how_to_guide.md # Redirect its output to this file ```</p> </li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#api_library","title":"API library","text":"<ul> <li><code>helpers/hchatgpt.py</code> provides methods that wrap and interact with OpenAI API</li> <li> <p>By using these methods, you can easily build an assistant and chat to it with     our files</p> </li> <li> <p>Key functionalities include:</p> <ul> <li>Uploading and removing files from OpenAI</li> <li>Adding and removing files for an assistant</li> <li>Creating threads and messages for user input</li> <li>Running threads with certain assistants</li> <li>End-to-end communication method between users and the assistant</li> </ul> </li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#usage","title":"Usage","text":"<p>The following snippets provide a basic overview of the code usage.</p>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#file_structure","title":"File structure","text":"<ul> <li>Since OpenAI File manager does not hold folder structure, you use a cache     dictionary to save the relation between our file (with folder) and OpenAI File     IDs</li> <li>This dictionary will be constantly accessed and saved back to     <code>project_root/gpt_id.json</code></li> <li>If you find anything buggy, try deleting this cache file and rerun the code so     that it can be regenerated from scratch</li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#uploading_and_retrieving_files","title":"Uploading and retrieving Files","text":"<ul> <li> <p>To upload a file to OpenAI, which you can later attach to messages/assistants:</p> <p><code>python file_id = upload_to_gpt('path_to_your_file')</code></p> </li> <li> <p>If you want to retrieve a file that has been uploaded to OpenAI by its path or     ID:     <code>python file_object = get_gpt_file_from_path('path_to_your_file')</code>     or     <code>python file_object = get_gpt_file_from_id(file_id)</code></p> </li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#managing_assistants","title":"Managing Assistants","text":"<p>You can specify files an assistant should constantly use (like guidelines):</p> <pre><code>set_assistant_files_by_name('assistant_name', ['file_path_1', 'file_path_2'])\n</code></pre> <p>Add or remove specific files to/from an existing assistant by file paths or IDs:</p> <pre><code>add_files_to_assistant_by_name('assistant_name', ['new_file_path'])\ndelete_file_from_assistant_by_name('assistant_name', 'file_path_to_remove')\n</code></pre>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#chatgpt_communication","title":"ChatGPT communication","text":"<ul> <li> <p>Create a thread and send a message, with or without attaching files:</p> <p><code>python thread_id = create_thread() message_id = create_message_on_thread_with_file_names(thread_id, 'Your message content', ['file_name_1'])</code></p> </li> <li> <p>Run a thread on an assistant to get the Assistant's response:     <code>python run_id = run_thread_on_assistant_by_name('assistant_name', thread_id) response_messages = wait_for_run_result(thread_id, run_id)</code></p> </li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#e2e_assistant_runner","title":"E2E assistant runner","text":"<ul> <li>Interact with an assistant conveniently with the <code>e2e_assistant_runner</code>     function</li> <li>This function can take user input, send it to the assistant, and manage file     attachments in one call:     ```python response = e2e_assistant_runner(         'assistant_name',         'Your question or statement here',         input_file_names=['file_name_1'])</li> </ul>"},{"location":"tools/all.chatgpt_api.how_to_guide.html#outputs_the_assistants_response","title":"Outputs the assistant's response.","text":"<p>print(response)     ```</p>"},{"location":"tools/all.code_coverage.how_to_guide.html","title":"All.code coverage.how to guide","text":""},{"location":"tools/all.code_coverage.how_to_guide.html#python_code_coverage","title":"Python Code Coverage","text":""},{"location":"tools/all.code_coverage.how_to_guide.html#overview","title":"Overview","text":"<ul> <li>Code coverage is a metric used to measure how much of your source code is     exercised during test execution</li> <li>In Python, combining <code>pytest</code> (a testing framework) with the <code>coverage</code> module     allows you to assess and improve the quality of your tests</li> </ul>"},{"location":"tools/all.code_coverage.how_to_guide.html#tools_required","title":"Tools Required","text":"<ul> <li>pytest: A framework for writing and running Python tests.</li> <li> <p>coverage.py: A tool to measure code coverage of Python programs.</p> </li> <li> <p>Install both using pip:     <code>bash     &gt; pip install pytest coverage pytest-cov</code></p> </li> </ul>"},{"location":"tools/all.code_coverage.how_to_guide.html#basic_workflow","title":"Basic Workflow","text":"<ul> <li> <p>Assume you have a <code>test_math.py</code> containing unit tests:</p> <p>```python from my_module import add</p> </li> </ul> <p>def test_add():         assert add(2, 3) == 5     ```</p> <ul> <li>Run tests with coverage and collects coverage data:     <code>bash     &gt; pytest --cov=test_math.py</code>     or     <code>bash     &gt; coverage run -m pytest</code></li> <li> <p>The options for <code>pytest coverage</code> are:     <code>bash     coverage reporting with distributed testing support:         --cov=[SOURCE]        Path or package name to measure during execution (multi-                                                                                                 allowed). Use --cov= to not do any source filtering and                                                                                                 record everything.         --cov-reset           Reset cov sources accumulated in options so far.         --cov-report=TYPE     Type of report to generate: term, term-missing,                                                                                                 annotate, html, xml, json, lcov (multi-allowed). term,                                                                                                 term-missing may be followed by \":skip-covered\".                                                                                                 annotate, html, xml, json and lcov may be followed by                                                                                                 \":DEST\" where DEST specifies the output location. Use                                                                                                 --cov-report= to not generate any output.         --cov-config=PATH     Config file for coverage. Default: .coveragerc         --no-cov-on-fail      Do not report coverage if test run fails. Default: False         --no-cov              Disable coverage report completely (useful for                                                                                                 debuggers). Default: False         --cov-fail-under=MIN  Fail if the total coverage is less than MIN.         --cov-append          Do not delete coverage but append to current. Default:                                                                                                 False         --cov-branch          Enable branch coverage.         --cov-precision=COV_PRECISION                                                                                                 Override the reporting precision.         --cov-context=CONTEXT                                                                                                 Dynamic contexts to use. \"test\" for now.</code></p> </li> <li> <p>After running the tests, display a coverage summary in the terminal:</p> <p>```bash</p> <p>coverage report</p> </li> <li> <p>To view a detailed coverage report in your browser:     <code>bash     &gt; coverage html</code></p> </li> <li> <p>This generates an htmlcov directory containing an index.html file you can open     in a browser     ```bash     &gt; coverage --help     Coverage.py, version 7.8.0 with C extension     Measure, collect, and report on code coverage in Python programs.</p> <p>usage: coverage  [options] [args] <p>Commands:             annotate    Annotate source files with execution information.             combine     Combine a number of data files.             debug       Display information about the internals of coverage.py             erase       Erase previously collected coverage data.             help        Get help on using coverage.py.             html        Create an HTML report.             json        Create a JSON report of coverage results.             lcov        Create an LCOV report of coverage results.             report      Report coverage stats on modules.             run         Run a Python program and measure code execution.             xml         Create an XML report of coverage results.</p> <p>Use \"coverage help \" for detailed help on any command. Full documentation is at https://coverage.readthedocs.io/en/7.8.0 ```"},{"location":"tools/all.code_coverage.how_to_guide.html#name_stmts_miss_cover","title":"Name           Stmts   Miss  Cover","text":"<p>my_module.py      10      2    80% ```</p>"},{"location":"tools/all.code_coverage.how_to_guide.html#use_examples","title":"Use Examples","text":"<ul> <li> <p>You want to measure the coverage of the codebase from a given unit tests     ```bash     &gt; pytest helpers/test/test_hmarkdown*.py --cov 2&gt;&amp;1 | tee log.txt     ...     collected 117 items</p> <p>helpers/test/test_hmarkdown.py::Test_header_list_to_vim_cfile1::test_get_header_list1 (0.00 s) PASSED [  0%] helpers/test/test_hmarkdown.py::Test_header_list_to_markdown1::test_mode_headers1 (0.00 s) PASSED [  1%] helpers/test/test_hmarkdown.py::Test_header_list_to_markdown1::test_mode_list1 (0.00 s) PASSED [  2%] helpers/test/test_hmarkdown.py::Test_replace_fenced_blocks_with_tags1::test1 (0.00 s) PASSED [  3%] ...</p> <p>================================ tests coverage ================================ ___ coverage: platform linux, python 3.12.3-final-0 ____</p> </li> <li> <p>Generate the coverage for only certain target code (e.g.,     <code>helpers/hmarkdown*.py</code>)</p> <p>```bash</p> <p>coverage report --include=helpers/hmarkdown*.py Name                                 Stmts   Miss Branch BrPart  Cover</p> <p>helpers/hmarkdown.py                     9      0      0      0   100% helpers/hmarkdown_bullets.py            91     24     42      5    68% helpers/hmarkdown_coloring.py           60     19     16      2    62% helpers/hmarkdown_comments.py           28     12     10      4    53% helpers/hmarkdown_fenced_blocks.py      55      0     14      0   100% helpers/hmarkdown_filtering.py          53     43      6      0    17% helpers/hmarkdown_formatting.py        101     34     18      1    69% helpers/hmarkdown_headers.py           226      6     88      7    96% helpers/hmarkdown_rules.py             101     19     42      3    80% helpers/hmarkdown_slides.py             52      2     16      4    91%</p> <p>TOTAL                                  776    159    252     26    78% ```</p> </li> <li> <p>Generate the coverage sorted by % covered</p> <p>```bash</p> <p>coverage report --include=helpers/hmarkdown*.py --sort=cover Name                                 Stmts   Miss Branch BrPart  Cover</p> <p>helpers/hmarkdown_filtering.py          53     43      6      0    17% helpers/hmarkdown_comments.py           28     12     10      4    53% helpers/hmarkdown_coloring.py           60     19     16      2    62% ... helpers/hmarkdown.py                     9      0      0      0   100% helpers/hmarkdown_fenced_blocks.py      55      0     14      0   100%</p> <p>TOTAL                                  776    159    252     26    78% ```</p> </li> <li> <p>Analyze the coverage through browser:     <code>&gt; coverage html --include=helpers/hmarkdown*.py     &gt; open htmlcov/index.html</code></p> </li> </ul>"},{"location":"tools/all.code_coverage.how_to_guide.html#name_stmts_miss_branch_brpart_cover","title":"Name                                     Stmts   Miss Branch BrPart  Cover","text":"<p>init.py                                  0      0      0      0   100% conftest.py                                 79     38     18      7    49% helpers/init.py                          0      0      0      0   100% helpers/hcoverage.py                        69     53     14      0    19% helpers/hdbg.py                            394    267    136     23    29% helpers/hdocker.py                         564    510    102      0     8% helpers/henv.py                            216     92     46      4    50% helpers/hgit.py                            573    491    130      0    12% helpers/hintrospection.py                  125    102     48      0    13% helpers/hio.py                             345    244    118     12    25% ... helpers/test/test_hmarkdown_bullets.py     132      0      2      0   100%</p> <p>TOTAL                                     6956   3824   1700    168    41% ```</p>"},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html","title":"All.code coverage subprocess.how to guide","text":""},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#enhanced_pytest_coverage_for_subprocesses_and_docker","title":"Enhanced Pytest Coverage for Subprocesses and Docker","text":""},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#overview","title":"Overview","text":"<p>This guide provides comprehensive test coverage for Python subprocesses and Dockerized applications using coverage hooks and parallel data collection.</p>"},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#quick_start","title":"Quick Start","text":"<p>For most use cases, use the automated invoke task:</p> <pre><code># Run Coverage for Entire Project\ninvoke run_coverage_subprocess\n\n# Run Coverage for Specific Directory\ninvoke run_coverage_subprocess --target-dir=dev_scripts_helpers/llms\n\n# Generate HTML Report\ninvoke run_coverage_subprocess --generate-html-report\n</code></pre> <p>This task automatically:</p> <ul> <li>Installs coverage hooks</li> <li>Sets up subprocess environment</li> <li>Runs all tests with coverage tracking</li> <li>Combines coverage data from all processes</li> <li>Generates reports (text, XML, and optionally HTML)</li> <li>Cleans up hooks when finished</li> </ul>"},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#manual_setup_advanced","title":"Manual Setup (Advanced)","text":""},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Python project with pytest tests</li> <li><code>coverage</code>, <code>pytest</code>, <code>pytest-cov</code> packages</li> <li>Docker installed (if testing containerized code)</li> </ul>"},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#step_1_configure_coverage_for_parallel_execution","title":"Step 1: Configure Coverage for Parallel Execution","text":"<p>Update <code>.coveragerc</code> in project root:</p> <pre><code>[run]\nbranch = True\nparallel = True\nconcurrency = multiprocessing\nsigterm = True\n\n[paths]\nsource =\n        .\n        /app\n</code></pre>"},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#step_2_install_coverage_hooks","title":"Step 2: Install Coverage Hooks","text":"<pre><code>python -c \"import helpers.hcoverage as hcovera; hcovera.inject()\"\nexport COVERAGE_PROCESS_START=.coveragerc\n</code></pre>"},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#step_3_prepare_coverage_data_directory","title":"Step 3: Prepare Coverage Data Directory","text":"<pre><code>python3 -c \"import helpers.hcoverage as hcovera; hcovera.coverage_commands_subprocess()\"\n</code></pre>"},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#step_4_update_docker_containers_if_applicable","title":"Step 4: Update Docker Containers (If Applicable)","text":"<p>Containers built with <code>hdocker.build_container_image()</code> automatically include coverage support.</p> <p>For manual Docker setups, add to your Dockerfile:</p> <pre><code>RUN pip install --no-cache-dir coverage pytest pytest-cov\n\n# Create Coverage Data Directory with Proper Permissions.\nRUN mkdir -p /app/coverage_data &amp;&amp; chmod 777 /app/coverage_data\n\n# Setup Coverage Configuration.\nCOPY .coveragerc /app/coverage_data/.coveragerc\nENV COVERAGE_PROCESS_START=/app/coverage_data/.coveragerc\n\n# Create Coverage.Pth File for Automatic Startup.\n# This Ensures Coverage Tracking Starts Automatically When Python Runs.\nRUN python - &lt;&lt;PYCODE\nimport site, os\nsite_dir = site.getsitepackages()[0]\npth_file = os.path.join(site_dir, 'coverage.pth')\nwith open(pth_file, 'w') as f:\n        f.write('import coverage; coverage.process_startup()')\nPYCODE\n</code></pre>"},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#step_5_run_tests_with_coverage","title":"Step 5: Run Tests with Coverage","text":"<pre><code>coverage run --parallel-mode -m pytest your_test_file.py\n</code></pre>"},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#step_6_collect_and_merge_coverage_data","title":"Step 6: Collect and Merge Coverage Data","text":"<pre><code>python3 -c \"import helpers.hcoverage as hcovera; hcovera.coverage_combine()\"\n</code></pre>"},{"location":"tools/all.code_coverage_subprocess.how_to_guide.html#step_7_view_coverage_report","title":"Step 7: View Coverage Report","text":"<p>Generate text report:</p> <pre><code>coverage report\n</code></pre> <p>Generate HTML report:</p> <pre><code>coverage html\npython3 -m http.server --directory htmlcov 8000\n# Navigate to Http://Localhost:8000\n</code></pre>"},{"location":"tools/all.code_coverage_subprocess.reference.html","title":"All.code coverage subprocess.reference","text":""},{"location":"tools/all.code_coverage_subprocess.reference.html#coverage_subprocess_reference","title":"Coverage Subprocess Reference","text":""},{"location":"tools/all.code_coverage_subprocess.reference.html#invoke_task","title":"Invoke Task","text":"<pre><code>invoke run_coverage_subprocess [--target-dir=DIR] [--generate-html-report]\n</code></pre> <p>Parameters:</p> <ul> <li><code>target_dir</code>: Directory to measure coverage (default: \".\")</li> <li><code>generate_html_report</code>: Generate HTML coverage report (default: False)</li> </ul>"},{"location":"tools/all.code_coverage_subprocess.reference.html#configuration_options","title":"Configuration Options","text":"<p>Required in <code>.coveragerc</code>:</p> <pre><code>[run]\nparallel = True              # Enables separate coverage files\nconcurrency = multiprocessing # Handles concurrent processes\nsigterm = True               # Saves coverage data on termination\n\n[paths]\nsource =\n        .                        # Host path\n        /app                     # Container path\n</code></pre> <p>Environment variables:</p> <ul> <li><code>COVERAGE_PROCESS_START</code>: Points to <code>.coveragerc</code></li> <li><code>COVERAGE_FILE</code>: Specifies coverage data file location</li> </ul>"},{"location":"tools/all.code_coverage_subprocess.reference.html#function_reference","title":"Function Reference","text":"<p><code>hcoverage</code> module functions:</p> <ul> <li><code>inject()</code>: Installs coverage hooks in site-packages</li> <li><code>remove()</code>: Removes coverage hooks</li> <li><code>coverage_commands_subprocess()</code>: Prepares coverage data directory</li> <li><code>coverage_combine()</code>: Merges coverage data and generates reports</li> </ul>"},{"location":"tools/all.code_coverage_subprocess.reference.html#docker_integration","title":"Docker Integration","text":"<p>Containers built with <code>hdocker.build_container_image()</code> automatically include:</p> <ul> <li>Coverage tools installation</li> <li>Coverage data directory setup</li> <li>Hook installation</li> <li>Runtime environment variables</li> </ul> <p>Manual Docker setup requires:</p> <pre><code>RUN pip install --no-cache-dir coverage pytest pytest-cov\n\n# Create Coverage Data Directory with Proper Permissions.\nRUN mkdir -p /app/coverage_data &amp;&amp; chmod 777 /app/coverage_data\n\n# Setup Coverage Configuration.\nCOPY .coveragerc /app/coverage_data/.coveragerc\nENV COVERAGE_PROCESS_START=/app/coverage_data/.coveragerc\n\n# Create Coverage.Pth File for Automatic Startup.\n# This Ensures Coverage Tracking Starts Automatically When Python Runs.\nRUN python - &lt;&lt;PYCODE\nimport site, os\nsite_dir = site.getsitepackages()[0]\npth_file = os.path.join(site_dir, 'coverage.pth')\nwith open(pth_file, 'w') as f:\n        f.write('import coverage; coverage.process_startup()')\nPYCODE\n</code></pre>"},{"location":"tools/all.code_coverage_subprocess.reference.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing Coverage Data: Check hooks with     <code>python -c \"import site; print(site.getsitepackages())\"</code>.</li> <li>Container Permissions: Ensure 777 permissions for coverage directory, 644 for     <code>.coveragerc</code>.</li> <li>Path Mapping Issues: Verify <code>[paths]</code> maps <code>/app</code> to <code>.</code> in <code>.coveragerc</code>.</li> </ul>"},{"location":"tools/all.code_review_tools.reference.html","title":"All.code review tools.reference","text":""},{"location":"tools/all.code_review_tools.reference.html#code_review_tools","title":"Code Review Tools","text":""},{"location":"tools/all.code_review_tools.reference.html#review_systems","title":"Review Systems","text":""},{"location":"tools/all.code_review_tools.reference.html#github_copilot_code_review","title":"Github Copilot Code Review","text":"<ul> <li> <p>Link:     https://docs.github.com/en/copilot/using-github-copilot/code-review/using-copilot-code-review</p> </li> <li> <p>Key Features:</p> <ul> <li>AI-generated review comments</li> <li>Suggestions for bug fixes and style improvements</li> <li>Context-aware feedback</li> </ul> </li> <li> <p>Usage / Integration</p> <ul> <li>Seamless integration within GitHub</li> <li>Works directly on PRs in the GitHub ecosystem</li> </ul> </li> <li> <p>Ease of Use</p> <ul> <li>Very easy for teams already on GitHub</li> <li>Minimal setup required</li> </ul> </li> </ul>"},{"location":"tools/all.code_review_tools.reference.html#graphitedev","title":"Graphite.dev","text":"<ul> <li> <p>Link:     https://graphite.dev/docs/code-review</p> </li> <li> <p>Key Features:</p> <ul> <li>Automated code review assistance</li> <li>Consistency and style checking</li> <li>Customizable rule sets</li> </ul> </li> <li> <p>Usage / Integration</p> <ul> <li>Integrates into CI/CD pipelines and possibly GitHub workflows</li> <li>Can be configured for team needs</li> </ul> </li> <li> <p>Ease of Use</p> <ul> <li>User-friendly dashboard</li> <li>Requires initial configuration to align with project-specific rules</li> </ul> </li> </ul>"},{"location":"tools/all.code_review_tools.reference.html#coderabbit","title":"CodeRabbit","text":"<ul> <li> <p>Link: https://docs.coderabbit.ai/</p> </li> <li> <p>Key Features:</p> <ul> <li>Advanced AI-driven code review</li> <li>Highly context-aware suggestions</li> <li>Focus on catching subtle issues and recommending optimizations</li> </ul> </li> <li> <p>Usage / Integration</p> <ul> <li>Early stage integrations with popular platforms (GitHub, Bitbucket, etc.)</li> <li>Designed for modern development workflows</li> </ul> </li> <li> <p>Ease of Use</p> <ul> <li>Modern UI and workflow</li> <li>May involve a learning curve as the product matures</li> </ul> </li> </ul>"},{"location":"tools/all.code_review_tools.reference.html#deepcode_now_integrated_with_snyk_code","title":"Deepcode (now integrated with Snyk Code)","text":"<ul> <li> <p>Link https://docs.snyk.io/</p> </li> <li> <p>Key Features:</p> <ul> <li>AI-powered static analysis</li> <li>Detects bugs, security issues, and anti-patterns</li> <li>Machine learning-based code analysis</li> </ul> </li> <li> <p>Usage / Integration</p> <ul> <li>Integrates with GitHub and other SCM platforms via Snyk Code</li> <li>Often used as part of a broader security/quality toolchain</li> </ul> </li> <li> <p>Ease of Use</p> <ul> <li>Integration may require initial setup within Snyk ecosystem</li> <li>Familiarity with Snyk platforms can help</li> </ul> </li> </ul>"},{"location":"tools/all.code_review_tools.reference.html#prices","title":"Prices","text":""},{"location":"tools/all.code_review_tools.reference.html#github_copilot_code_review_1","title":"Github Copilot Code Review","text":"<ul> <li> <p>Link:     https://docs.github.com/en/copilot/about-github-copilot/plans-for-github-copilot</p> </li> <li> <p>Cost per User:</p> <ul> <li>Pro: $10/month</li> <li>Pro+: $39/month</li> <li>Business: $19/month</li> <li>Enterprise: $39/month</li> </ul> </li> <li> <p>Cost for 10 Users:</p> <ul> <li>Pro: $100/month</li> <li>Pro+: $390/month</li> <li>Business: $190/month</li> <li>Enterprise: $390/month</li> </ul> </li> <li> <p>Cost for 20 Users:</p> <ul> <li>Pro: $200/month</li> <li>Pro+: $780/month</li> <li>Business: $380/month</li> <li>Enterprise: $780/month</li> </ul> </li> </ul>"},{"location":"tools/all.code_review_tools.reference.html#grapitedev","title":"Grapite.dev","text":"<ul> <li> <p>Link:     https://graphite.dev/docs/graphite-standard</p> </li> <li> <p>Cost per User:</p> <ul> <li>Annual Subscription: $25/month</li> <li>Monthly Subscription: $29/month</li> </ul> </li> <li> <p>Cost for 10 Users (Annual):</p> <ul> <li>Base (3 seats): $900/year</li> <li>Additional 7 seats: $2,100/year</li> <li>Total: $3,000/year (~$250/month)</li> </ul> </li> <li> <p>Cost for 20 Users (Annual):</p> <ul> <li>Base (3 seats): $900/year</li> <li>Additional 17 seats: $5,100/year</li> <li>Total: $6,000/year (~$500/month)</li> </ul> </li> <li> <p>Note: Graphite is free for personal repositories, teams with 10 or fewer     GitHub collaborators, open-source projects, and students/educators.</p> </li> </ul>"},{"location":"tools/all.code_review_tools.reference.html#coderabbit_1","title":"CodeRabbit","text":"<ul> <li> <p>Link: https://www.coderabbit.ai/pricing</p> </li> <li> <p>Cost per User:</p> <ul> <li>Lite: $12/month (billed annually) or $15/month</li> <li>Pro: $24/month (billed annually) or $30/month</li> </ul> </li> <li> <p>Cost for 10 Users:</p> <ul> <li>Lite (Annual): $1,440/year (~$120/month)</li> <li>Pro (Annual): $2,880/year (~$240/month)</li> </ul> </li> <li> <p>Cost for 20 Users:</p> <ul> <li>Lite (Annual): $2,880/year (~$240/month)</li> <li>Pro (Annual): $5,760/year (~$480/month)</li> </ul> </li> <li> <p>Note: CodeRabbit is free for open-source projects and offers a 14-day free     trial.</p> </li> </ul>"},{"location":"tools/all.code_review_tools.reference.html#takeaways","title":"Takeaways","text":""},{"location":"tools/all.code_review_tools.reference.html#core_needs","title":"Core Needs","text":"<ol> <li> <p>Automated Code Review vs. Code Generation         - GitHub Copilot is primarily an AI-assisted code generation tool             (though it also has a \"Code Review\" beta). Its strength lies in offering             real-time suggestions as you code.         - Graphite is focused on PR workflow optimization and automated checks,             helping streamline how pull requests are created, reviewed, and merged.         - Coderabbit is geared toward AI-based code review and analysis, aiming             to provide feedback on potential bugs, performance issues, and style             concerns.</p> </li> <li> <p>Integration with GitHub         - Copilot integrates directly within GitHub (especially if you're using             GitHub for everything).         - Graphite integrates well with GitHub but provides its own dashboard             and workflow enhancements. Coderabbit is newer but also targets GitHub             integration. You'll want to verify how mature their integration is and             whether it fits your team's workflows.</p> </li> <li> <p>Budget &amp; Licensing         - GitHub Copilot is priced per user per month. For 30\u201340 developers, the             cost scales linearly.         - Graphite and Coderabbit typically use a per-seat or tiered pricing             model as well.         - If you already have a GitHub Enterprise or Team plan, adding Copilot can             be straightforward. Graphite and Coderabbit would be additional monthly             expenses.</p> </li> </ol>"},{"location":"tools/all.code_review_tools.reference.html#strengths_and_weaknesses","title":"Strengths and Weaknesses","text":""},{"location":"tools/all.code_review_tools.reference.html#github_copilot","title":"Github Copilot","text":"<ul> <li> <p>Strengths</p> <ul> <li>Seamless integration if your team is already deeply invested in GitHub.</li> <li>Great for boosting productivity during coding (boilerplate, tests,     refactors).</li> <li>Provides a familiar in-IDE experience.</li> </ul> </li> <li> <p>Weaknesses</p> <ul> <li>The \"Copilot for Code Review\" feature is relatively new and may not offer     deep analysis compared to specialized tools.</li> <li>Can produce \"best guess\" suggestions that require human oversight.</li> </ul> </li> </ul>"},{"location":"tools/all.code_review_tools.reference.html#graphitedev_1","title":"Graphite.dev","text":"<ul> <li> <p>Strengths</p> <ul> <li>Focuses on improving the pull request workflow, making it faster and more     organized.</li> <li>Catches common style or lint issues early, reducing manual review overhead.</li> <li>Ideal for teams seeking to structure their PR process rigorously (e.g.,     stacked diffs, streamlined merges).</li> </ul> </li> <li> <p>Weaknesses</p> <ul> <li>Not as advanced in AI-based code analysis as some specialized code-review     tools.</li> <li>May require your team to adapt to a new PR workflow or dashboard.</li> </ul> </li> </ul>"},{"location":"tools/all.code_review_tools.reference.html#coderabbit_2","title":"CodeRabbit","text":"<ul> <li> <p>Strengths</p> <ul> <li>Billed as an AI-driven code review tool, aiming for deeper analysis of     logic, bugs, and potential improvements.</li> <li>If effective, it could reduce the time spent on finding edge cases or hidden     issues.</li> </ul> </li> <li> <p>Weaknesses</p> <ul> <li>A relatively new entrant; maturity and coverage (languages, frameworks) need     to be confirmed.</li> <li>Integration details might not be as polished or comprehensive yet.</li> </ul> </li> </ul>"},{"location":"tools/all.code_review_tools.reference.html#references","title":"References","text":"<ul> <li> <p>https://www.awesomecodereviews.com/tools/ai-code-review-tools</p> </li> <li> <p>https://bito.ai/blog/best-automated-ai-code-review-tools/</p> </li> <li> <p>https://www.codacy.com/</p> </li> <li> <p>https://www.sonarsource.com/</p> </li> <li> <p>https://usetrag.com/</p> </li> <li> <p>https://medium.com/@anyu6686/use-llm-for-code-review-70385c3f7457</p> </li> </ul>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html","title":"All.codebase clean up.how to guide","text":""},{"location":"tools/all.codebase_clean_up.how_to_guide.html#codebase_clean_up","title":"Codebase Clean Up","text":""},{"location":"tools/all.codebase_clean_up.how_to_guide.html#codebase_clean-up_scripts","title":"Codebase clean-up scripts","text":""},{"location":"tools/all.codebase_clean_up.how_to_guide.html#problem","title":"Problem","text":"<ol> <li>Since we have multiple repos, we can't always easily replace code in one repo    (e.g., with PyCharm) and have all the other repos work properly</li> <li>Sometimes it is required to rename files and replace text in files at the    same time (e.g., when renaming an import)</li> <li> <p>While developing a change for the entire repo, we want to be able to \"reset\"    the work and apply the change from scratch</p> </li> <li> <p>E.g., during the review of the PR applying lots of text replaces:</p> <ul> <li>The code in master might be changing creating conflicts</li> <li>The reviewers might ask some changes</li> <li>This creates a lot of manual changes</li> </ul> </li> </ol>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#solution_script_approach","title":"Solution: script approach","text":"<ul> <li>Create a shell <code>sh</code> script that applies the correct changes to all the repos     using     <code>/dev_scripts_helpers/system_tools/replace_text.py</code></li> <li> <p>For more complex problems we can extend <code>replace_text.py</code> with custom     regex-based operations</p> </li> <li> <p>The script approach solves all the problems above</p> </li> <li> <p>We apply the script to all the repos</p> </li> <li>The script can rename text and files at the same time</li> <li>We can check out a clean master, run the script to apply the changes    automatically, regress and merge</li> </ul>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#using_the_script_approach","title":"Using the script approach","text":"<ul> <li> <p>We want to apply clean-up changes to the code base with a script</p> </li> <li> <p>Ideally we would like to apply all the changes automatically through the     script</p> <ul> <li>E.g., in SorrTask258 a     script that replaces <code>pytest.raises</code> with <code>self.assertRaises</code> everywhere in     the code</li> </ul> </li> <li> <p>We are ok to make the vast majority (like 95%) of the changes automatically,     and the rest manually</p> </li> <li> <p>We want to keep together in a single PR</p> <ul> <li>The script performing automatically the changes; and</li> <li>The manual changes from the outcome of the script</li> </ul> </li> <li> <p>We want to create a separate PR to communicate with the reviewers the output     of running the script</p> <ul> <li>The author/reviewers should run the script on all the repos, run the unit     tests, and merge (through a PR as usual)</li> </ul> </li> </ul>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#how_to_use_replace_textpy","title":"How to use <code>replace_text.py</code>","text":"<ul> <li> <p>See <code>-h</code> for updated list of options</p> </li> <li> <p>Replace an instance of text in the content of all the files with extensions:     <code>.py</code>, <code>.ipynb</code>, <code>.txt</code>, <code>.md</code></p> </li> <li><code>--old</code>: regular expression or string that should be replaced with <code>--new</code></li> <li><code>--new</code>: regular expression or string that should replace <code>--old</code></li> <li><code>--preview</code>: see script result without making actual changes</li> <li><code>--only_dirs</code>: provide space-separated list of directories to process only</li> <li><code>--only_files</code>: provide space-separated list of files to process only</li> <li><code>--exclude_files</code>: provide space-separated list of files to exclude from     replacements</li> <li><code>--exclude_dirs</code>: provide space-separated list of dir to exclude from     replacements</li> <li> <p><code>--ext</code>: process files with specified extensions</p> <ul> <li>Defaults are <code>py, ipynb, txt, sh</code></li> <li>Use <code>_all_</code> for all files</li> </ul> </li> <li> <p>The goal of the script is to replace an instance of text in the content of all     the files with extensions <code>.py</code>, <code>.ipynb</code>, <code>.txt</code>, <code>.md</code> and to do a <code>git mv</code>     for files based on certain criteria</p> </li> </ul>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#rename_a_file","title":"Rename a file","text":"<ul> <li>Preview the change     <code>bash     &gt; replace_text.py \\             --old research_backtest_utils \\             --new backtest_api \\             --preview</code></li> <li>Then you can look at the changes to be performed with <code>vic</code> /     <code>vim -c \"cfile cfile\"</code></li> <li>If you are satisfied you can re-run the command with <code>--preview</code> to apply the     change</li> <li>Rename file     <code>bash     &gt; git mv ./dataflow/backtest/{research_backtest_utils.py,backtest_api.py}</code></li> </ul>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#replace_an_import_with_a_new_one","title":"Replace an import with a new one","text":"<pre><code>&gt; replace_text.py \\\n        --old \"import core.fin\" \\\n        --new \"import core.finance\"\n</code></pre>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#replace_text_in_a_specific_directory","title":"Replace text in a specific directory","text":"<pre><code>&gt; replace_text.py \\\n        --old \"exec \" \\\n        --new \"execute \" \\\n        --preview \\\n        --dirs dev_scripts \\\n        --exts None\n</code></pre>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#revert_all_files_but_this_one","title":"Revert all files but this one","text":"<ul> <li>There is an option <code>--revert_all</code> to apply this before the script     <code>bash     &gt; gs -s | \\             grep -v dev_scripts_helpers/system_tools/replace_text.py | \\             grep -v \"\\?\" | \\             awk '{print $2}' | \\             xargs git checkout --</code></li> </ul>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#custom_flows","title":"Custom flows","text":"<pre><code>&gt; replace_text.py --custom_flow _custom1\n</code></pre> <ul> <li>Custom flow for AmpTask14     <code>bash     &gt; replace_text.py --custom_flow _custom2 --revert_all</code></li> </ul>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#usage_examples","title":"Usage examples","text":"<ul> <li> <p>See SorrIssue259 and the     related PR for reference</p> <ul> <li>We wanted to make <code>_to_multiline_cmd()</code> from     <code>/helpers/lib_tasks_utils.py</code> a public     function</li> <li>This would require to rename <code>_to_multiline_cmd()</code> to <code>to_multiline_cmd()</code>     with the script</li> <li>This     script     will make the replacement smoothly everywhere in the code except for the     dirs specified <code>--exclude_dirs</code> flag.</li> </ul> </li> <li> <p>See SorrIssue258 and the     related PR for reference</p> <ul> <li>We wanted to replace <code>pytest.raises</code> with <code>self.assertRaises</code></li> <li>This     script     will replace it everywhere in the code</li> <li>Note the use of <code>--ext</code> flag to specify the file extentions the script     should work on</li> </ul> </li> <li>Of course the changes need to be applied in one repo and then propagated to     all the other repos if the tests are successful</li> </ul>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#instructions_for_the_pr_author","title":"Instructions for the PR author","text":"<ul> <li>Create a local branch called <code>...TaskXYZ_..._script</code> containing:<ul> <li>The code that needs to be changed manually<ul> <li>E.g.: Replacing <code>pytest.raises</code> with <code>self.assertRaises</code></li> </ul> </li> <li>More contextual changes<ul> <li>E.g.: Adding unit tests to the new functions</li> </ul> </li> <li>The script for the replacement of the caller named after the GH issue<ul> <li>The script should:<ul> <li>Prepare the target Git client</li> <li>Merge this <code>script</code> branch with the manual changes</li> <li>Make the automated changes<ul> <li>E.g.: Rename a function or replace certain word in comments /     docstring</li> </ul> </li> </ul> </li> </ul> </li> <li>Notes in the files that need to be changed manually after the automatic     script</li> </ul> </li> <li>Run from scratch the script getting the regression to pass<ul> <li>Any time there is a change needed by hand, the change should be added to the     script branch</li> <li>The goal is to be able to run the script</li> </ul> </li> <li>File a PR of the <code>...TaskXYZ_..._script</code></li> <li>(Optional) Create a PR with the result of the script<ul> <li>The author can request a review on this PR, but still the goal is to     automate as much as possible</li> </ul> </li> <li>Finally, the PR author merges the PR with the results of the script</li> </ul>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#example","title":"Example","text":"<ul> <li>The name of script should be related to the task. E.g:     <code>SorrTask259_Make_to_multi_line_cmd_public.sh</code></li> <li>The script should have a system call to <code>replace_text.py</code> to execute the     required functionality as provided in the above examples</li> <li>Create a PR only with the script and the changes</li> </ul>"},{"location":"tools/all.codebase_clean_up.how_to_guide.html#instructions_for_the_subrepo_integrator","title":"Instructions for the subrepo integrator","text":"<ul> <li>Do a <code>git checkout</code> of the <code>...TaskXYZ_..._script</code></li> <li>Run the script</li> <li>Review carefully the changes to make sure we are not screwing things up</li> <li>Run the regressions</li> <li>Merge the resulting <code>...TaskXYZ...</code> PR</li> <li>Ensure that the <code>...TaskXYZ_..._script</code> is merged in <code>master</code></li> <li>Delete the <code>...TaskXYZ_..._script</code></li> </ul>"},{"location":"tools/all.codecov.how_to_guide.html","title":"All.codecov.how to guide","text":""},{"location":"tools/all.codecov.how_to_guide.html#codecov_integration_and_coverage_setup_documentation","title":"Codecov Integration and Coverage Setup Documentation","text":"<p>This documentation describes the setup and usage of Codecov integration for our repos. The purpose is to explain how Codecov coverage tracking is configured, the functionalities implemented, and how developers can interpret, interact with, and extend the coverage results.</p>"},{"location":"tools/all.codecov.how_to_guide.html#setting_up_codecov","title":"Setting Up Codecov","text":"<p>Codecov was integrated by adding necessary files and configuration steps:</p> <p>Files and Directories Added</p> <ul> <li>.coveragerc: Configures directories and files under coverage     collection</li> <li>/.github/gh_requirements.txt: Lists     dependencies necessary for the coverage workflow</li> <li>/.github/workflows/coverage_tests.yml:     Defines GitHub Actions workflow for automated coverage runs</li> </ul> <p>External setup</p> <p>Applies default coverage settings across all repos to ensure consistency in reporting and behavior. If needed, individual repos can include their own <code>.codecov.yml</code> at the root to override specific global settings.</p> <ul> <li>Global YAML: Configures     coverage collection of Codecov</li> </ul>"},{"location":"tools/all.codecov.how_to_guide.html#coverage_configuration","title":"Coverage Configuration","text":"<p>The <code>.coveragerc</code> file defines coverage measurement settings:</p> <ul> <li><code>Excluded Files</code>: These files are omitted from coverage reporting.     <code>ini     [report]     omit =             */devops/compose/*             */helpers/test/outcomes/*/tmp.scratch/*</code></li> </ul>"},{"location":"tools/all.codecov.how_to_guide.html#github_actions_workflow","title":"GitHub Actions Workflow","text":"<p>Coverage tests are automated via GitHub Actions - <code>.github/workflows/coverage_tests.yml</code>.</p>"},{"location":"tools/all.codecov.how_to_guide.html#workflow_schedule","title":"Workflow Schedule","text":"<ul> <li>Runs daily at midnight (UTC)</li> <li>Can be manually triggered (<code>workflow_dispatch</code>)</li> <li>Action fails if coverage drops by <code>1%</code> (including <code>fast</code>, <code>slow</code> and     <code>superslow</code> tests)</li> </ul>"},{"location":"tools/all.codecov.how_to_guide.html#workflow_jobs","title":"Workflow Jobs","text":"<ol> <li> <p>Fast Tests Coverage:</p> </li> <li> <p>Runs daily or on manual trigger</p> </li> <li> <p>Uploads report flagged as <code>fast</code></p> </li> <li> <p>Slow Tests Coverage:</p> </li> <li> <p>Runs daily or on manual trigger</p> </li> <li> <p>Uploads report flagged as <code>slow</code></p> </li> <li> <p>Superslow Tests Coverage:</p> </li> <li> <p>Runs weekly on Monday or on manual trigger.</p> </li> <li> <p>Uploads report flagged as <code>superslow</code>.</p> </li> <li> <p>Each job:</p> </li> <li> <p>Generates an <code>XML</code> coverage report (<code>Global YAML</code>)</p> </li> <li>Uploads reports to <code>Codecov</code> with respective flags (<code>fast</code>, <code>slow</code>,     <code>superslow</code>)</li> </ol>"},{"location":"tools/all.codecov.how_to_guide.html#codecov_configuration","title":"Codecov Configuration","text":"<ul> <li>Coverage flags and project-level checks are configured at Global yml -     https://app.codecov.io/account/gh/causify-ai/yaml/.</li> <li> <p>If required, you can add distinct repository configurations to the     <code>.codecov.yml</code> file at the root of your repository. - You can change the     threshold of coverage drops for a specific repo - You can add/ change patch     settings or flags for a specific repo</p> </li> <li> <p>Flag Management</p> </li> </ul> <p>The <code>carryforward</code> option allows Codecov to reuse the previous coverage data    if a report is not submitted for a given flag in the current CI run. This is    useful when certain test suites (e.g., slow or superslow) don't run in every    cycle but should still be reflected in the coverage summary.</p> <p><code>yml    flag_management:    individual_flags:      - name: fast        carryforward: true      - name: slow        carryforward: true      - name: superslow        carryforward: true</code></p> <ol> <li>Comment Behavior</li> </ol> <p>Codecov can automatically post a summary comment on PRs. This comment can be    customized in layout, behavior, and verbosity.    - <code>layout: \"reach, diff, files\"</code> : Displays overall coverage, diff coverage,      and file-level detail    - <code>behavior: default</code> : Overwrites the previous comment instead of posting a      new one    - <code>require_changes: false</code> : Posts the comment even when coverage doesn't      change    - <code>show_critical_paths: false</code> : Disables per-line comments in the PR diff      view</p> <p><code>yml    comment:      layout: \"reach, diff, files\"      behavior: default      require_changes: false      show_critical_paths: false</code>    - When PR comment is enabled:</p> <pre><code> &lt;img src=\"figs/coverage/image1.png\" alt=\"alt text\" width=\"1000\"/&gt;\n</code></pre> <ul> <li> <p>When per-line comments in PR files is enabled:</p> <p></p> </li> <li> <p>GitHub Check Annotations</p> </li> </ul> <p>Inline annotations in the GitHub <code>Files changed</code> view are disabled using the    <code>github_checks.annotations</code> flag. This ensures a cleaner PR experience    without coverage-based highlights on each line.</p> <pre><code> &lt;img src=\"figs/coverage/image7.png\" alt=\"alt text\" width=\"1000\"/&gt;\n</code></pre> <ol> <li>Coverage Status Check:</li> </ol> <p>This section configures the status checks that appear in GitHub pull    requests. It defines both patch-level and project-level coverage checks and    sets conditions for when they should run and how they should behave.    - <code>project.default</code>: Defines the overall coverage check behavior.    - <code>target</code>: auto automatically compares against the base branch of the PR.    - <code>threshold: 1%</code> means the check will fail if coverage drops by <code>1%</code> or      more.    - <code>flags</code> scopes the project-level check to specific test suites (<code>fast</code>,      <code>slow</code>, <code>superslow</code>).    - <code>branches</code> limits the check to PRs targeting the <code>master</code> branch.    - <code>patch: true</code>: Ensures Codecov always checks coverage on the changed lines      in a PR, regardless of the base branch or flag.</p> <p><code>yml    coverage:      status:        project:          default:            target: auto            threshold: 1%            flags:              - fast              - slow              - superslow            branches:              - master        patch: true</code></p> <pre><code> &lt;img src=\"figs/coverage/image3.png\" alt=\"alt text\" width=\"1000\"/&gt;\n</code></pre>"},{"location":"tools/all.codecov.how_to_guide.html#viewing_coverage_reports","title":"Viewing Coverage Reports","text":"<p>Coverage results for the helpers repository are accessible via Codecov.</p> <ul> <li>Codecov UI link for helpers -     https://app.codecov.io/gh/causify-ai/helpers</li> <li>Master Build Dashboard Notebook:     http://172.30.2.44/build/buildmeister_dashboard/Master_buildmeister_dashboard.latest.html#Code-coverage-HTML-page</li> </ul>"},{"location":"tools/all.codecov.how_to_guide.html#running_coverage_locally","title":"Running Coverage Locally","text":"<p>Developers can manually run coverage tasks locally via Invoke commands and generate html report:</p> <ul> <li> <p>Fast Tests:</p> <p><code>yml invoke run_coverage --suite fast --generate-html-report</code></p> </li> <li> <p>Slow Tests:</p> <p><code>yml invoke run_coverage --suite slow --generate-html-report</code></p> </li> <li> <p>Superslow Tests:</p> <p><code>yml invoke run_coverage --suite superslow --generate-html-report</code></p> </li> <li> <p>Review HTML coverage report</p> <ul> <li>Run a local HTTP server to serve the HTML:     <code>bash         cd htmlcov         python3 -m http.server 8000</code></li> <li>If you're running this on a remote server, set up SSH port forwarding:     <code>bash         ssh -i ~/.ssh/&lt;private_key&gt; -L 8000:localhost:8000 &lt;user_name&gt;@&lt;server_ip&gt;</code></li> <li> <p>Then open your browser and go to:     http://localhost:8000</p> <p></p> </li> </ul> </li> </ul>"},{"location":"tools/all.codecov.how_to_guide.html#running_targeted_coverage_suites_for_a_single_pr_manual_triggering","title":"Running Targeted Coverage Suites for a Single PR (Manual Triggering)","text":"<p>The primary reason for doing this is to measure the coverage impact of the changes introduced in a specific pull request, including newly added or modified files. This helps determine whether the PR maintains, increases, or decreases test coverage in the areas it touches. This section explains how to trigger an isolated test suite (e.g., <code>fast</code>,<code>slow</code>,<code>superslow</code>) for a specific PR and ensure that Codecov coverage reflects the results correctly.</p>"},{"location":"tools/all.codecov.how_to_guide.html#logic_behind_the_workflow","title":"Logic Behind the Workflow","text":"<ul> <li>A GitHub Actions workflow can be triggered manually via the     <code>workflow_dispatch</code> event</li> <li>The input field <code>suite</code> (<code>fast</code>, <code>slow</code>, or <code>superslow</code>) determines which     test group runs</li> <li>To attribute the results to the PR, the test suite must run on a branch that     is part of the PR history (not <code>master</code> or a detached manual run)</li> <li>When the upload is done from a commit not associated with the PR, Codecov will     not link the results to the correct diff</li> </ul>"},{"location":"tools/all.codecov.how_to_guide.html#steps_to_trigger_manual_pr-specific_coverage","title":"Steps to Trigger Manual PR-Specific Coverage","text":"<p>This approach ensures that the Codecov flag is uploaded against your PR commit, so the coverage diff is accurate.</p> <ol> <li>Open the Actions tab of the repository</li> <li>Select the Test coverage workflow (or its current name)</li> <li>Select the branch name where the PR is open</li> <li>In the suite input box, enter the test you want to run:</li> <li>Fast</li> <li>Slow</li> <li>Superslow</li> <li> <p>Click Run workflow to start the job.</p> <p></p> </li> <li> <p>Monitor the run</p> </li> <li> <p>The workflow executes only on the selected test.</p> </li> <li> <p>On success, it uploads the coverage report to Codecov with the test flag.</p> </li> <li> <p>Verify the updated flag in Codecov</p> </li> <li> <p>Return to the pull-request page on GitHub.</p> </li> <li> <p>There should be a comment from the codecov bot with the PR coverage details</p> <p></p> </li> </ol>"},{"location":"tools/all.codecov.how_to_guide.html#system_behavior_when_the_test_coverage_workflow_fails_or_continues","title":"System Behavior: When the Test Coverage Workflow Fails or Continues","text":"<p>This section documents how the Test coverage workflow behaves under various failure conditions, specifically regarding the fast, slow, and superslow test suites.</p> <ol> <li>Dependency / Setup Steps</li> </ol> <p>Steps included:</p> <ul> <li>AWS credential configuration</li> <li>Docker login</li> <li>Cleanup</li> <li>Code checkout</li> <li>PYTHONPATH update</li> <li>Dependency installation</li> </ul> <p>Behavior:</p> <ul> <li>If any of these steps fail, the workflow fails immediately</li> <li>No test suites (fast, slow, superslow) will run</li> <li> <p>This is intentional to prevent test execution in a broken or incomplete     environment</p> </li> <li> <p>Fast / Slow Test Steps</p> </li> </ul> <p>Steps included:</p> <ul> <li><code>run_fast</code></li> <li><code>upload_fast</code></li> <li><code>run_slow</code></li> <li><code>upload_slow</code></li> </ul> <p>These steps use <code>continue-on-error: true</code>.</p> <p>Behavior:</p> <ul> <li>If any of these steps fail, the workflow continues without immediate failure</li> <li>The superslow test will still run if the workflow is triggered on Monday     (scheduled) or manually (workflow_dispatch)</li> <li> <p>However, the workflow may still fail at the end if fast/slow failures are     detected by the final failure check step</p> </li> <li> <p>Superslow Test Steps</p> </li> </ul> <p>Steps included:</p> <ul> <li><code>run_superslow</code></li> <li><code>upload_superslow</code></li> </ul> <p>These steps do not use <code>continue-on-error</code>.</p> <p>Behavior:</p> <ul> <li>These steps run only:<ul> <li>On scheduled workflows that fall on a Monday (DAY_OF_WEEK == 1)</li> <li>Or when the workflow is manually triggered</li> </ul> </li> <li>If either step fails, the workflow fails immediately</li> <li> <p>If both pass, the workflow continues to the final fast/slow check</p> </li> <li> <p>Final Failure Check (Fast/Slow Only)</p> </li> </ul> <p>Step included:</p> <ul> <li>Fail if fast/slow test or upload failed</li> </ul> <p>Behavior:</p> <ul> <li>This step runs at the very end of the workflow</li> <li>It checks whether any of the fast/slow test or upload steps failed</li> <li>If any of them failed, this step causes the entire job to fail using exit 1</li> <li>This ensures that silent failures in fast/slow coverage are surfaced, even if     superslow passes</li> </ul> Step Type Step Failed? Superslow Runs? Job Fails? Setup Step Yes No Yes Fast Test Yes Yes (Mon/dispatch only) Yes (after final check) Slow Test Yes Yes (Mon/dispatch only) Yes (after final check) Superslow Test Yes n/a Yes Final Fail Check Yes (if fast/slow failed) Already ran Yes"},{"location":"tools/all.codecov.how_to_guide.html#additional_functionalities_and_extensions","title":"Additional Functionalities and Extensions","text":"<p>Additional functionalities provided by Codecov that can be utilized or extended include:</p> <ul> <li><code>Pull Request Comments</code>: Automatically generate detailed coverage summaries or     line-by-line coverage comments directly in GitHub pull requests</li> <li> <p><code>Coverage Badges</code>: Integrate coverage badges in the repository <code>README</code> to     visibly show current coverage status</p> <p></p> </li> <li> <p><code>Report Customization</code>: Configure detailed reporting settings to specify what     information to display or omit in coverage summaries</p> </li> </ul>"},{"location":"tools/all.codecov.how_to_guide.html#coverage_behavior_and_best_practices","title":"Coverage Behavior and Best Practices","text":"<ul> <li>Coverage reports are uploaded regardless of test success to ensure coverage     tracking consistency</li> <li> <p>Coverage flags (<code>fast</code>, <code>slow</code>, <code>superslow</code>) allow separate visibility and     tracking</p> <ul> <li>Fast test coverage:</li> </ul> <p></p> </li> <li> <p>Regular review of coverage differences (visible in PR checks and Codecov UI)     is encouraged to maintain code quality</p> </li> </ul>"},{"location":"tools/all.codecov.how_to_guide.html#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Check GitHub Actions logs for errors in coverage upload steps</li> <li>Ensure <code>CODECOV_TOKEN</code> is correctly set as a GitHub secret</li> <li>Validate workflow and coverage configuration files for correctness if issues     arise</li> </ul>"},{"location":"tools/all.evaluating_external_tools.how_to_guide.html","title":"All.evaluating external tools.how to guide","text":""},{"location":"tools/all.evaluating_external_tools.how_to_guide.html#tool_evaluation_guide","title":"Tool Evaluation Guide","text":"<p>This guide outlines how to consistently evaluate third-party tools (e.g., coverage reporting, code review, CI add-ons) across our repositories such as <code>cmamp</code>, <code>helpers</code>, <code>tutorials</code>, etc.</p>"},{"location":"tools/all.evaluating_external_tools.how_to_guide.html#1_define_comparison_attributes","title":"1. Define Comparison Attributes","text":"<p>Use the following attributes to evaluate and compare tools:</p> Attribute Description Tool Name Name of the tool Website Official website link Integration Method GitHub Action, CLI, manual, automatic pull, etc. Deployment Model SaaS only, Local only, or Both Open Source Yes / No Free for OSS Yes / No Pricing Cost per month, usage tiers, or credits Pricing Page Link to official pricing documentation Setup Effort Point to repo / Needs config / Requires signup or subscription Use Cases Coverage, code review, security, dependency management, etc. Reference Link Blog post, GitHub repo, YouTube demo, testimonial, etc. Pros Key strengths or features Cons Known limitations or pain points"},{"location":"tools/all.evaluating_external_tools.how_to_guide.html#2_track_in_a_shared_google_sheet","title":"2. Track in a Shared Google Sheet","text":"<ul> <li>Use a shared Google Sheet to create a comparison matrix with the attributes     above</li> <li>Each row represents one tool</li> <li>Columns can include notes, ratings, or checkboxes</li> <li>Assign owners for research if needed</li> </ul>"},{"location":"tools/all.evaluating_external_tools.how_to_guide.html#3_pick_top_5_tools_to_investigate_further","title":"3. Pick Top 5 Tools to Investigate Further","text":"<ul> <li>Based on research, select the top 5 tools that appear promising</li> <li>Prioritize tools with:<ul> <li>Minimal setup requirements</li> <li>Strong community or support</li> <li>Transparent pricing</li> </ul> </li> </ul>"},{"location":"tools/all.evaluating_external_tools.how_to_guide.html#4_try_23_tools_hands-on","title":"4. Try 2\u20133 Tools Hands-On","text":"<p>Choose 2\u20133 tools to test directly in one of the repositories.</p>"},{"location":"tools/all.evaluating_external_tools.how_to_guide.html#example_tool_categories","title":"Example Tool Categories","text":"<ul> <li> <p>Tools requiring minimal setup</p> <p>These tools can be tested with little to no configuration. You often just sign in with GitHub and point them to a repository. - Codecov \u2013 Add their GitHub App and upload a coverage file from CI. No     manual config needed for basic use. Works out of the box with popular test     runners like <code>pytest</code> (Python) and <code>jest</code> (JavaScript) - Coveralls \u2013 GitHub integration and CI upload. Just set the     <code>COVERALLS_REPO_TOKEN</code> secret and push coverage results - Code Climate \u2013 GitHub integration with default support for several     languages. Free for open source but may need a <code>.codeclimate.yml</code> file for     customization</p> </li> <li> <p>Tools requiring moderate setup</p> <p>These tools require sign-up, access configuration, or CLI installation. Setup is still manageable but not zero-effort. - SonarCloud \u2013 Requires GitHub login, project import, and token setup in     CI. Needs a <code>sonar-project.properties</code> file. Provides static analysis and     coverage reporting - DeepSource \u2013 GitHub login and permission to scan repos. Auto-detects     config, but may need a <code>.deepsource.toml</code> to fine-tune rules or exclusions</p> </li> <li> <p>Tools that may require subscription or approval</p> <p>These tools focus on enterprise or compliance use cases. Setup may require contacting sales, credit card details, or legal approval. - Snyk \u2013 Security scanning for dependencies. Free for public repos but     requires signup and CLI setup. Some features (e.g., PR gating, private repo     scanning) require a paid plan - FOSSA \u2013 License compliance scanning. Requires signup, and although an     open-source tier exists, full access may require approval or paid plans</p> </li> </ul>"},{"location":"tools/all.evaluating_external_tools.how_to_guide.html#5_review_and_demo","title":"5. Review and Demo","text":"<ul> <li>After testing, note impressions and any issues</li> <li>Schedule a 30-minute demo to share findings with the team</li> <li>Decide as a group whether to adopt, postpone, or reject each tool</li> </ul>"},{"location":"tools/all.evaluating_external_tools.how_to_guide.html#notes","title":"Notes","text":"<ul> <li>Not all tools can be judged fairly from documentation alone</li> <li>Actual usage is necessary to evaluate UX, speed, noise, and integration pain     points</li> </ul>"},{"location":"tools/all.explanation_template.html","title":"All.explanation template","text":"<p> <pre><code>&lt;title&gt;Explanation: &amp;lt;FileName&amp;gt;&lt;/title&gt;\n\n&lt;section name=\"IntroductionMotivation\"&gt;\n</code></pre>"},{"location":"tools/all.explanation_template.html#introduction_motivation","title":"Introduction / Motivation","text":"<pre><code>    &lt;description&gt;\n        &lt;bullet&gt;TODO: What is this about?&lt;/bullet&gt;\n        &lt;bullet&gt;TODO: What problem or limitation does it solve?&lt;/bullet&gt;\n        &lt;bullet&gt;TODO: Who needs to understand it?&lt;/bullet&gt;\n    &lt;/description&gt;\n\n    &lt;example&gt;\n        Traditional file conversion methods focused on quality rather than file size. This method aims to optimize file size while ensuring quality.\n    &lt;/example&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"CoreConcepts\"&gt;\n</code></pre>"},{"location":"tools/all.explanation_template.html#core_concepts","title":"Core Concepts","text":"<pre><code>    &lt;concepts&gt;\n        &lt;bullet&gt;&lt;b&gt;Concept 1:&lt;/b&gt; TODO: Brief explanation&lt;/bullet&gt;\n        &lt;bullet&gt;&lt;b&gt;Concept 2:&lt;/b&gt; TODO: How it relates to others&lt;/bullet&gt;\n        &lt;bullet&gt;&lt;b&gt;Concept 3:&lt;/b&gt; TODO: Dependencies or assumptions&lt;/bullet&gt;\n    &lt;/concepts&gt;\n    &lt;!-- Optional diagram or chart --&gt;\n    &lt;code_block language=\"text\"&gt;\n\n&lt;!--&gt; Insert diagram or logical model here --&gt;\n\n    &lt;/code_block&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"HowItWorks\"&gt;\n</code></pre>"},{"location":"tools/all.explanation_template.html#how_it_works","title":"How It Works","text":"<pre><code>    &lt;steps&gt;\n        &lt;bullet&gt;TODO: Step-by-step or component-based explanation&lt;/bullet&gt;\n        &lt;bullet&gt;TODO: Key data transformations or control flow&lt;/bullet&gt;\n    &lt;/steps&gt;\n    &lt;code_block language=\"text\"&gt;\n</code></pre> <p>[Component A] \u2192 transforms \u2192 [Component B] \u2192 outputs \u2192 [Result] </p> <pre><code>&lt;/section&gt;\n\n&lt;section name=\"DesignRationale\"&gt;\n</code></pre>"},{"location":"tools/all.explanation_template.html#design_rationale","title":"Design Rationale","text":"<pre><code>    &lt;rationale&gt;\n        &lt;bullet&gt;TODO: What goals guided its design?&lt;/bullet&gt;\n        &lt;bullet&gt;TODO: What constraints shaped its form? (e.g., performance, usability)&lt;/bullet&gt;\n        &lt;bullet&gt;TODO: What would go wrong with an alternate approach?&lt;/bullet&gt;\n    &lt;/rationale&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"TradeoffsAndAlternatives\"&gt;\n</code></pre>"},{"location":"tools/all.explanation_template.html#tradeoffs_and_alternatives","title":"Tradeoffs And Alternatives","text":""},{"location":"tools/all.explanation_template.html#current_approach","title":"Current Approach","text":"<pre><code>    &lt;comparison&gt;\n        &lt;current_approach&gt;\n            &lt;title&gt;Current Approach&lt;/title&gt;\n            &lt;advantages&gt;\n                &lt;bullet&gt;TODO: Advantage #1&lt;/bullet&gt;\n                &lt;bullet&gt;TODO: Advantage #2&lt;/bullet&gt;\n            &lt;/advantages&gt;\n            &lt;drawbacks&gt;\n                &lt;bullet&gt;TODO: Drawback #1&lt;/bullet&gt;\n                &lt;bullet&gt;TODO: Drawback #2&lt;/bullet&gt;\n            &lt;/drawbacks&gt;\n        &lt;/current_approach&gt;\n</code></pre>"},{"location":"tools/all.explanation_template.html#alternative_approach","title":"Alternative Approach","text":"<pre><code>        &lt;alternative_approach&gt;\n            &lt;title&gt;Alternative Approach&lt;/title&gt;\n            &lt;advantages&gt;\n                &lt;bullet&gt;TODO: Advantage #1&lt;/bullet&gt;\n                &lt;bullet&gt;TODO: Advantage #2&lt;/bullet&gt;\n            &lt;/advantages&gt;\n            &lt;drawbacks&gt;\n                &lt;bullet&gt;TODO: Drawback #1&lt;/bullet&gt;\n                &lt;bullet&gt;TODO: Drawback #2&lt;/bullet&gt;\n            &lt;/drawbacks&gt;\n        &lt;/alternative_approach&gt;\n    &lt;/comparison&gt;\n\n&lt;/section&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html","title":"All.how to guide template","text":"<p> <pre><code>&lt;title&gt;How-To Guide: script.py&lt;/title&gt;\n\n&lt;section name=\"WhatItDoes\"&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html#what_it_does","title":"What It Does","text":"<pre><code>    &lt;bullet&gt;TODO: Explain the purpose of this script or workflow&lt;/bullet&gt;\n    &lt;bullet&gt;TODO: Describe what outcome or goal this achieves&lt;/bullet&gt;\n\n    &lt;example&gt;\n        Automatically render images from fenced code blocks in Markdown using a custom Python script.\n    &lt;/example&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"AssumptionsRequirements\"&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html#assumptions_requirements","title":"Assumptions / Requirements","text":"<pre><code>    &lt;bullet&gt;Docker installed and running&lt;/bullet&gt;\n    &lt;bullet&gt;Basic command-line familiarity&lt;/bullet&gt;\n    &lt;bullet&gt;TODO: List any additional dependencies or permissions&lt;/bullet&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"Instructions\"&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html#instructions","title":"Instructions","text":"<pre><code>    &lt;step name=\"Step1\" title=\"Fetch Input\"&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html#step_1_fetch_input","title":"Step 1: Fetch Input","text":"<pre><code>        &lt;description&gt;TODO: Describe the input file(s) and how to prepare or provide them.&lt;/description&gt;\n\n        &lt;code_block language=\"bash\"&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html#example_download_markdown_input_file","title":"Example: Download Markdown input file","text":"<p>wget https://example.com/input.md -O input.md  </p> <pre><code>    &lt;step name=\"Step2\" title=\"Describe Action\"&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html#step_2_describe_action","title":"Step 2: Describe Action","text":"<pre><code>        &lt;description&gt;TODO: Describe the main script command to execute the action.&lt;/description&gt;\n\n        &lt;code_block language=\"bash\"&gt;\n</code></pre> <p>python render_images.py -i input.md -o output.md --action render  </p> <pre><code>    &lt;step name=\"Step3\" title=\"Review Output\"&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html#step_3_review_output","title":"Step 3: Review Output","text":"<pre><code>        &lt;description&gt;TODO: Explain how to verify the results of the script run.&lt;/description&gt;\n\n        &lt;code_block language=\"bash\"&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html#preview_the_output_file","title":"Preview the output file","text":"<p>cat output.md | less  </p> <pre><code>&lt;/section&gt;\n\n&lt;section name=\"Examples\"&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html#examples","title":"Examples","text":"<pre><code>    &lt;examples&gt;\n        &lt;example title=\"Basic Render\"&gt;\n            &lt;description&gt;Render all diagrams in a Markdown file to embedded images.&lt;/description&gt;\n            &lt;code_block language=\"bash\"&gt;\n</code></pre> <p>python render_images.py -i diagrams.md -o diagrams.output.md --action render  </p> <pre><code>        &lt;example title=\"Render + Open in Browser\"&gt;\n            &lt;description&gt;Render diagrams and open the rendered file in browser preview.&lt;/description&gt;\n            &lt;code_block language=\"bash\"&gt;\n</code></pre> <p>python render_images.py -i diagrams.md --action render --run_dockerized  </p> <pre><code>        &lt;example title=\"Dry Run for Debugging\"&gt;\n            &lt;description&gt;Check what will be processed without rendering the output.&lt;/description&gt;\n            &lt;code_block language=\"bash\"&gt;\n</code></pre> <p>python render_images.py -i diagrams.md -o /tmp/preview.md --dry_run  </p> <pre><code>&lt;/section&gt;\n\n&lt;section name=\"Troubleshooting\"&gt;\n</code></pre>"},{"location":"tools/all.how_to_guide_template.html#troubleshooting","title":"Troubleshooting","text":"<pre><code>    &lt;issue_block&gt;\n        &lt;issue&gt;TODO: Describe a common issue&lt;/issue&gt;\n        &lt;cause&gt;TODO: Likely cause&lt;/cause&gt;\n        &lt;solution&gt;TODO: Suggested fix or workaround&lt;/solution&gt;\n    &lt;/issue_block&gt;\n\n    &lt;issue_block&gt;\n        &lt;issue&gt;TODO: Another possible issue&lt;/issue&gt;\n        &lt;cause&gt;TODO: Likely cause&lt;/cause&gt;\n        &lt;solution&gt;TODO: How to resolve it&lt;/solution&gt;\n    &lt;/issue_block&gt;\n\n&lt;/section&gt;\n</code></pre>"},{"location":"tools/all.import_check.reference.html","title":"All.import check.reference","text":"<ul> <li>show_imports             * Basic usage             * Visualize dependencies at a directory level             * Visualize external dependencies             * Visualize level X dependencies             * Visualize cyclic dependencies             * Pydeps-dependent limitations          * NotModuleError          * Modules above the target directory             * Run the tool on our codebase -- pre-docker procedure</li> <li>detect_import_cycles             * Basic usage</li> </ul>"},{"location":"tools/all.import_check.reference.html#show_imports","title":"show_imports","text":"<p>A tool for visualizing dependencies among files and packages.</p>"},{"location":"tools/all.import_check.reference.html#basic_usage","title":"Basic usage","text":"<pre><code>&gt;./show_imports.py [flags] &lt;target_directory&gt;\n</code></pre> <p>The script will produce by default an output <code>.png</code> file named <code>&lt;target_directory&gt;_dependencies.png</code>, you can change the default output name or image format by specifying the <code>--out_filename</code> and <code>--out_format</code> options.</p> <p>In the following examples we will analyze an example input directory <code>example/input</code> that you can find in the <code>import_check/</code> dir. It is structured as follows:</p> <pre><code>example\n\u251c\u2500\u2500 input\n        \u251c\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 subdir1\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n        \u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n        \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n        \u251c\u2500\u2500 subdir2\n        \u2502    \u251c\u2500\u2500 file1.py\n        \u2502    \u251c\u2500\u2500 file2.py\n        \u2502    \u251c\u2500\u2500 __init__.py\n        \u2502    \u2514\u2500\u2500 subdir3\n        \u2502        \u251c\u2500\u2500 file1.py\n        \u2502        \u251c\u2500\u2500 file2.py\n        \u2502        \u251c\u2500\u2500 file3.py\n        \u2502        \u2514\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 subdir4\n                \u251c\u2500\u2500 file1.py\n                \u251c\u2500\u2500 file2.py\n                \u251c\u2500\u2500 file3.py\n                \u2514\u2500\u2500 __init__.py\n</code></pre> <p>Basic usage example:</p> <pre><code>&gt;./show_imports.py --out_filename example/output/basic.png example/input\n</code></pre> <p>Will produce the following output:</p> <p></p>"},{"location":"tools/all.import_check.reference.html#visualize_dependencies_at_a_directory_level","title":"Visualize dependencies at a directory level","text":"<p>To visualize dependencies at a directory level, specify <code>--dir</code> option.</p> <p>Example:</p> <pre><code>&gt;./show_imports.py --dir --out_filename example/output/directory_deps.png example/input\n</code></pre> <p>Output:</p> <p></p>"},{"location":"tools/all.import_check.reference.html#visualize_external_dependencies","title":"Visualize external dependencies","text":"<p>By default, external dependencies are not visualized. You can turn them on by specifying the <code>--ext</code> option.</p> <p>Example:</p> <pre><code>&gt;./show_imports.py --ext --out_filename example/output/external_deps.png example/input\n</code></pre> <p>Output:</p> <p></p>"},{"location":"tools/all.import_check.reference.html#visualize_level_x_dependencies","title":"Visualize level X dependencies","text":"<p>When you want to stop analyzing dependencies at a certain directory level, you can set the <code>--max_level</code> option.</p> <p>Example:</p> <pre><code>&gt;./show_imports.py --max_level 2 --out_filename example/output/max_level_deps.png example/input\n</code></pre> <p>Output:</p> <p></p>"},{"location":"tools/all.import_check.reference.html#visualize_cyclic_dependencies","title":"Visualize cyclic dependencies","text":"<p>When you want to visualize cyclic dependencies only, you can set the <code>--show_cycles</code> option.</p> <p>Example:</p> <pre><code>&gt;./show_imports.py --show_cycles --out_filename example/output/cyclic_deps.png example/input\n</code></pre> <p>Output:</p> <p></p>"},{"location":"tools/all.import_check.reference.html#pydeps-dependent_limitations","title":"Pydeps-dependent limitations","text":"<p><code>show_imports</code> is based on the <code>pydeps</code> tool for detecting dependencies among imports, therefore it shares some of the its limitations:</p> <ul> <li>The output contains only files which have at least one import, or are imported     in at least one other file</li> <li>Only files that can be found by using the Python import machinery will be     considered (e.g., if a module is missing or not installed, it will not be     included regardless of whether it is being imported)</li> <li>All the imports inside submodules should be absolute</li> <li>There are certain requirements related to the presence of modules in and     above the target directory, which are described in detail below<ul> <li>Here, a module is a directory that contains an <code>__init__.py</code> file</li> </ul> </li> </ul>"},{"location":"tools/all.import_check.reference.html#notmoduleerror","title":"NotModuleError","text":"<p>Suppose we run the <code>show_imports</code> script on a target directory <code>input_dir</code>. The script will check the <code>input_dir</code> and all of its subdirectories of any level. A <code>NotModuleError</code> will be raised if any of them</p> <ul> <li>Contain Python files (directly or in any of their subdirectories of any level)     and</li> <li>Are not modules</li> </ul> <p>Example of an acceptable structure for <code>input_dir</code> as a target directory:</p> <pre><code>input_dir\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 subdir1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 subdir2\n        \u2514\u2500\u2500 __init__.py\n</code></pre> <p>Examples of input directories for which <code>show_imports</code> will fail with a <code>NotModuleError</code>:</p> <ul> <li><code>input_dir/subdir1</code> contains Python files but is not a module</li> </ul> <pre><code>input_dir\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 subdir1\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 file1.py\n\u2514\u2500\u2500 subdir2\n        \u2514\u2500\u2500 __init__.py\n</code></pre> <pre><code>__main__.NotModuleError: The following dirs have to be modules (add `__init__.py`): ['input_dir/subdir1']\n</code></pre> <ul> <li><code>input_dir</code> contains subdirectories with Python files (<code>input_dir/subdir1</code>)     but is not a module</li> </ul> <pre><code>input_dir\n\u251c\u2500\u2500 subdir1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n\u2514\u2500\u2500 subdir2\n        \u2514\u2500\u2500 __init__.py\n</code></pre> <pre><code>__main__.NotModuleError: The following dirs have to be modules (add `__init__.py`): ['input_dir']\n</code></pre>"},{"location":"tools/all.import_check.reference.html#modules_above_the_target_directory","title":"Modules above the target directory","text":"<p>Suppose we run the <code>show_imports</code> script on a target directory <code>input_dir</code>. The dependencies will be retrieved and shown for the files</p> <ul> <li>Under <code>input_dir</code> (including the files in its subdirectories of any level)     and</li> <li>In the directories above <code>input_dir</code><ul> <li>If these directories are modules themselves and there is no non-module     directory between them and <code>input_dir</code></li> </ul> </li> </ul> <p>For example,</p> <ul> <li>All the directories in following structure are modules</li> <li>Therefore, if <code>import_check</code> or any of the subdirectories are passed as a     target directory (e.g. <code>example/input</code>, <code>example/input/subdir1</code>,     <code>example/input/subdir2/subdir3</code>, etc), all the dependencies in <code>import_check</code>     will be shown</li> </ul> <pre><code>import_check\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 show_imports.py\n\u251c\u2500\u2500 detect_import_cycles.py\n\u2514\u2500\u2500 example\n        \u251c\u2500\u2500 __init__.py\n        \u2514\u2500\u2500 input\n                \u251c\u2500\u2500 __init__.py\n                \u251c\u2500\u2500 subdir1\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n                \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n                \u2514\u2500\u2500 subdir2\n             \u251c\u2500\u2500 file1.py\n             \u251c\u2500\u2500 file2.py\n             \u251c\u2500\u2500 __init__.py\n             \u2514\u2500\u2500 subdir3\n                 \u251c\u2500\u2500 file1.py\n                 \u251c\u2500\u2500 file2.py\n                 \u251c\u2500\u2500 file3.py\n                 \u2514\u2500\u2500 __init__.py\n</code></pre> <ul> <li>In the following structure <code>import_check/example</code> is not a module</li> <li>Therefore, if <code>import_check/example/input</code> or any of its subdirectories are     passed as a target directory, all the dependencies in     <code>import_check/example/input</code> will be shown, but not the dependencies of the     files above it</li> <li>If <code>import_check</code> or <code>import_check/example</code> are passed as a target directory,     a <code>NotModuleError</code> will be raised, see above</li> </ul> <pre><code>import_check\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 show_imports.py\n\u251c\u2500\u2500 detect_import_cycles.py\n\u2514\u2500\u2500 example\n        \u2514\u2500\u2500 input\n                \u251c\u2500\u2500 __init__.py\n                \u251c\u2500\u2500 subdir1\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 file1.py\n                \u2502\u00a0\u00a0 \u251c\u2500\u2500 file2.py\n                \u2502\u00a0\u00a0 \u2514\u2500\u2500 __init__.py\n                \u2514\u2500\u2500 subdir2\n             \u251c\u2500\u2500 file1.py\n             \u251c\u2500\u2500 file2.py\n             \u251c\u2500\u2500 __init__.py\n             \u2514\u2500\u2500 subdir3\n                 \u251c\u2500\u2500 file1.py\n                 \u251c\u2500\u2500 file2.py\n                 \u251c\u2500\u2500 file3.py\n                 \u2514\u2500\u2500 __init__.py\n</code></pre> <p>In practice, this means that if all the directories containing Python files in the repository are modules, the output of the <code>show_imports</code> script will always show the dependencies for the whole repository.</p> <p>If it is necessary to run <code>show_imports</code> only for a specific directory, it has to be located directly inside a non-module directory (like <code>import_check/example/input</code>, which is located in a non-module <code>import_check/example</code>).</p>"},{"location":"tools/all.import_check.reference.html#run_the_tool_on_our_codebase_--_pre-docker_procedure","title":"Run the tool on our codebase -- pre-docker procedure","text":"<ul> <li>Activate <code>helpers</code> environment:<ul> <li>From the <code>helpers</code> root:     <code>bash     poetry shell; export PYTHONPATH=$PYTHONPATH:$(pwd)</code></li> </ul> </li> <li>Run the tool on the target repo. E.g., analyze for cyclic dependencies     <code>bash     &lt;path for the show_imports.py script&gt; --show_cycles \\                                                                                 --out_format svg \\                                                                                 --out_filename cyclic_dependencies.svg \\                                                                                 &lt;absolute path for the target repo&gt;</code></li> </ul>"},{"location":"tools/all.import_check.reference.html#detect_import_cycles","title":"detect_import_cycles","text":"<p>A tool for detecting circular dependencies among files and packages.</p>"},{"location":"tools/all.import_check.reference.html#basic_usage_1","title":"Basic usage","text":"<pre><code>&gt;./detect_import_cycles.py &lt;target_directory&gt;\n</code></pre> <p>The script will either exit with an error, logging the groups of files with circular dependencies, or will pass, logging that no cyclic imports have been detected.</p> <p>The script uses <code>show_imports.py</code> for the dependency retrieval and therefore inherits its limitations.</p> <p>For the <code>import_check/example/input</code> directory, the script will produce the following output, detecting two import cycles:</p> <pre><code>&gt;./detect_import_cycles.py example/input\n</code></pre> <pre><code>ERROR detect_import_cycles.py _main:73    Cyclic imports detected: (input.subdir2.subdir3.file1, input.subdir2.subdir3.file2)\nERROR detect_import_cycles.py _main:73    Cyclic imports detected: (input.subdir4.file1, input.subdir4.file2, input.subdir4.file3)\n</code></pre>"},{"location":"tools/all.imports_and_packages.how_to_guide.html","title":"All.imports and packages.how to guide","text":""},{"location":"tools/all.imports_and_packages.how_to_guide.html#imports_and_packages","title":"Imports And Packages","text":""},{"location":"tools/all.imports_and_packages.how_to_guide.html#imports_and_packages_1","title":"Imports and packages","text":"<ul> <li>TODO(gp): Consolidate here any other rule from other gdoc</li> </ul>"},{"location":"tools/all.imports_and_packages.how_to_guide.html#goals_of_packages","title":"Goals of packages","text":"<ul> <li> <p>The goal of creating packages is to:</p> <ul> <li>Simplify the import from clients</li> <li>Hide in which file the actual code is, so that we can reorganize the code     without having to change all the client code</li> <li>Organize the code in related units</li> <li>Make it simpler to avoid import loops by enforcing that there are no import     loops in any module and no import loops among modules</li> </ul> </li> <li> <p>E.g., referring to package from a different package looks like     <code>python import dataflow.core as dtfcore dtfcore.ArmaGenerator(...)</code></p> </li> <li>Importing the specific file:     <code>python import dataflow_amp.system.source_nodes as dtfsysonod dtfsysonod.ArmaGenerator(...)</code></li> </ul>"},{"location":"tools/all.imports_and_packages.how_to_guide.html#circular_dependency_aka_import_cycle_import_loop","title":"Circular dependency (aka import cycle, import loop)","text":"<ul> <li>The simplest case of circular import is a situation when in lib <code>A</code> we have     <code>import B</code>, and in lib B we have <code>import A</code></li> <li>The presence of circular imports can be checked with an invoke     <code>i lint_detect_cycles</code>. By default, it will run on the whole repo, which takes     a couple of minutes, but it will provide the most reliable and thorough check     for circular imports</li> </ul>"},{"location":"tools/all.imports_and_packages.how_to_guide.html#rules_for_imports","title":"Rules for imports","text":"<ul> <li>We follow rules to avoid import loops:<ul> <li>Code inside a package should import directly a file in the same package and     not use the package<ul> <li> <p>E.g., <code>datapull/common/data/client/data_frame_im_clients.py</code></p> <ul> <li> <p>Good</p> <p><code>python import datapull.common.data.client.abstract_im_clients as imvcdcaimcl</code>             - Bad <code>python import datapull.common.data.client as icdc</code>     - Code from a package should import other packages, instead of importing         directly the file     - We don't allow any import loop that can be detected statically (i.e., by         inspecting the code without executing it)         - This guarantees that there are no dynamic import loops, which are even             more difficult to detect and disruptive     - We allow only imports at the module level and not inside functions         - We don't accept using local imports to break import loops, unless it's             temporary to solve a more important problem     - We allow nested packages         - TODO(gp): Clarify the rules here     - We don't want to abuse packaging by creating too many of them         - Rationale:             - There is overhead in organizing and maintaining code in packages and we want to pay the overhead only if we get enough benefit from this     - We specify a short import in the <code>__init__.py</code> file for a package manually         because Linter cannot do it automatically yet         - We use the first letters to build a short import and try to keep it less             than 8 chars long, e.g., <code>datapull.talos.data.client</code> -&gt; <code>itdcl</code>         - We insert an import docstring in the <code>__init__.py</code> file manually and then             we use the specified short import everywhere in the codebase. E.g.,</p> </li> </ul> <p>```python Import as:</p> </li> </ul> </li> </ul> </li> </ul> <p>import datapull.talos.data.client as itdcl             ```</p>"},{"location":"tools/all.imports_and_packages.how_to_guide.html#how_to_import_code_from_unit_tests","title":"How to import code from unit tests","text":"<ul> <li> <p>To avoid churning client code when code is moved among files, we allow unit     tests to both:</p> <ol> <li>Import the package when testing code exported from the package</li> <li>E.g., in <code>market_data/test/market_data_test_case.py</code> you can import the    package even if it's included    <code>python import market_data as mdata ... mdata.AbstractMarketData ...</code></li> <li>Import the files directly with the code and not the package</li> <li>E.g.,    <code>python import market_data.abstract_market_data as mdabmada ... mdabmada.AbstractMarketData ...</code></li> </ol> </li> <li> <p>To justify, one can argue that unit tests are clients of the code and should     import packages like any other client</p> </li> <li> <p>To justify, one can interpret that unit tests are tied to specific files, so     they should be kept in sync with the low-level code and not with the public     interface. In fact, we already allow unit tests to call private functions,     acknowledging that unit tests are not regular clients</p> </li> <li> <p>Given that both explanations are valid, we allow both styles</p> </li> </ul>"},{"location":"tools/all.imports_and_packages.how_to_guide.html#common_unit_test_code","title":"Common unit test code","text":"<ul> <li>Unit tests should not import from each other<ul> <li>If there is common code, it should go in libraries inside or outside <code>test</code>     directories<ul> <li>E.g., we use <code>foobar_example.py</code> files containing builders for mocks and     examples of objects to be used by tests</li> <li>E.g., we use <code>test/foobar_test_case.py</code> or <code>test/foobar_utils.py</code></li> </ul> </li> <li>In other terms, test files are always leaves of the import graph</li> </ul> </li> </ul>"},{"location":"tools/all.imports_and_packages.how_to_guide.html#packagelib_hierarchy_and_cycle_prevention","title":"Package/lib hierarchy and cycle prevention","text":"<ul> <li>Static import cycles can be detected by the invoke <code>lint_detect_cycles</code></li> <li>To prevent import cycles, we want to enforce that certain packages don't     depend on other packages<ul> <li>E.g., <code>helpers</code> should not depend on any other package, besides external     libraries</li> <li><code>core</code> should only depend on <code>helpers</code></li> <li><code>dataflow</code> should only depend on <code>core</code> and <code>helpers</code></li> <li>These constraints can be expressed in terms of \"certain nodes of the import     graph are sources\" or \"certain edges in the import graph are forbidden\"</li> </ul> </li> <li>We also want to enforce that certain libs don't import others within a single     package. For example, in <code>helpers</code>, the following hierarchy should be     respected:<ol> <li><code>hwarnings</code>, <code>hserver</code>, <code>hlogging</code></li> <li><code>hdbg</code></li> <li><code>hintrospection</code>, <code>hprint</code></li> <li><code>henv</code>, <code>hsystem</code>, <code>hio</code>, <code>hversio</code> (this is the base layer to access env  vars and execute commands)</li> <li><code>hgit</code> (Git requires accessing env vars and system calls)</li> </ol> </li> <li>A library can only import libs that precede it or are on the same level in the     hierarchy above.<ul> <li>E.g., <code>henv</code> can import <code>hdbg</code>, <code>hprint</code>, and <code>hio</code>, but it cannot import     <code>hgit</code></li> <li>While importing a lib on the same level, make sure you are not creating an     import cycle</li> </ul> </li> <li>In addition, keep in mind the following rules to prevent import cycles:<ul> <li>Any import inside a function is just a temporary hack waiting to create     problems</li> <li>Any time we can break a file into smaller pieces, we should do that since     this helps control the dependencies</li> </ul> </li> </ul>"},{"location":"tools/all.imports_and_packages.how_to_guide.html#anatomy_of_a_package","title":"Anatomy of a package","text":"<ul> <li>TODO(gp): Let's use <code>dataflow</code> as a running example</li> <li>A package has a special <code>__init__.py</code> exporting public methods</li> </ul>"},{"location":"tools/all.integrate_repos.how_to_guide.html","title":"All.integrate repos.how to guide","text":""},{"location":"tools/all.integrate_repos.how_to_guide.html#integrate_repos","title":"Integrate Repos","text":""},{"location":"tools/all.integrate_repos.how_to_guide.html#how_to_integrate_repos","title":"How to integrate repos","text":""},{"location":"tools/all.integrate_repos.how_to_guide.html#concepts","title":"Concepts","text":"<ul> <li>We have two dirs storing two forks of the same repo<ul> <li>Files are touched (e.g., added, modified, deleted) in each forks</li> <li>The most problematic files are the files that are modified in both forks</li> <li>Files that are added or deleted in one fork, should be added / deleted also     in the other fork</li> </ul> </li> <li>Often we can integrate \"by directory\", i.e., finding entire directories that     were touched in one branch but not in the other<ul> <li>In this case we can simply copy the entire dir from one repo to the other</li> </ul> </li> <li> <p>Other times we need to integrate each file</p> </li> <li> <p>There are various interesting Git reference points:</p> <ol> <li>The branch point for each fork, at which the integration branch was started</li> <li>The last integration point for each fork, at which the repos are the same,  or at least aligned</li> </ol> </li> </ul>"},{"location":"tools/all.integrate_repos.how_to_guide.html#invariants_for_the_integration_workflows","title":"Invariants for the integration workflows","text":"<ul> <li>The user runs commands in an abs dir, e.g., <code>/Users/saggese/src/{amp1,cmamp1}</code></li> <li>The user refers in the command line to <code>dir_basename</code>, which is the basename     of the integration directories (e.g., <code>amp1</code>, <code>cmamp1</code>, <code>kaizenflow1</code>)<ul> <li>The <code>src_dir_basename</code> is the one where the command is issued</li> <li>The <code>dst_dir_basename</code> is assumed to be parallel to the <code>src_dir_basename</code></li> </ul> </li> <li>The dirs are then transformed in absolute dirs <code>abs_src_dir</code></li> </ul>"},{"location":"tools/all.integrate_repos.how_to_guide.html#integration_process","title":"Integration process","text":""},{"location":"tools/all.integrate_repos.how_to_guide.html#preparation","title":"Preparation","text":"<ul> <li> <p>Pull master</p> </li> <li> <p>Crete the integration branches</p> <p>```bash</p> <p>cd cmamp1 git checkout master i integrate_create_branch --dir-basename cmamp1 cd kaizenflow1 git checkout master i integrate_create_branch --dir-basename kaizenflow1 ```</p> </li> <li> <p>In one line</p> <p><code>bash cd $HOME/cmamp1 &amp;&amp; \\     git checkout master &amp;&amp; \\     i integrate_create_branch --dir-basename cmamp1 &amp;&amp; \\     cd $HOME/kaizenflow1 &amp;&amp; \\     git checkout master &amp;&amp; \\     i integrate_create_branch --dir-basename kaizenflow1</code></p> </li> <li> <p>Remove white spaces from both source and destination repos:</p> <p>```bash</p> <p>dev_scripts/clean_up_text_files.sh git commit -am \"Remove white spaces\"; git push <code>- One should still run the regressions out of paranoia since some golden     outcomes can be changed</code>     &gt; i gh_create_pr --no-draft     &gt; i gh_workflow_list     ```</p> </li> <li> <p>Remove empty files:</p> <p>```bash</p> <p>find . -type f -empty -print | grep -v .git | grep -v init | grep -v \".log$\" | grep -v \".txt$\" | xargs git rm <code>`` - TODO(gp): Add this step to</code>dev_scripts/clean_up_text_files.sh`</p> </li> <li> <p>Align <code>lib_tasks.py</code>:</p> <p>```bash</p> <p>vimdiff ~/src/{cmamp1, kaizenflow1}/tasks.py; diff_to_vimdiff.py --dir1 ~/src/cmamp1 --dir2 ~/src/kaizenflow1 --subdir helpers ```</p> </li> <li> <p>Lint both dirs:</p> <p>```bash</p> <p>cd amp1 i lint --dir-name . --only-format cd cmamp1 i lint --dir-name . --only-format ```</p> <p>or at least the files touched by both repos:</p> <p>```bash</p> <p>i integrate_files --file-direction only_files_in_src cat tmp.integrate_find_files_touched_since_last_integration.cmamp1.txt tmp.integrate_find_files_touched_since_last_integration.amp1.txt | sort | uniq &gt;files.txt FILES=$(cat files.txt) i lint --only-format -f \"$FILES\" ``` - This should be done as a single separated PR to be reviewed separately</p> </li> <li> <p>Align <code>lib_tasks.py</code>:     <code>bash     &gt; vimdiff ~/src/{amp1,cmamp1}/tasks.py; diff_to_vimdiff.py --dir1 ~/src/amp1 --dir2 ~/src/cmamp1 --subdir helpers</code></p> </li> </ul>"},{"location":"tools/all.integrate_repos.how_to_guide.html#integration","title":"Integration","text":"<ul> <li> <p>Create the integration branches:</p> <p>```bash</p> <p>cd amp1 i integrate_create_branch --dir-basename amp1 i integrate_create_branch --dir-basename kaizenflow1 cd cmamp1 i integrate_create_branch --dir-basename cmamp1 ```</p> </li> <li> <p>Check what files were modified in each fork since the last integration:</p> <p>```bash</p> <p>i integrate_files --file-direction common_files i integrate_files --file-direction common_files --src-dir-basename cmamp1 --dst-dir-basename kaizenflow1</p> <p>i integrate_files --file-direction only_files_in_src i integrate_files --file-direction only_files_in_dst ```</p> </li> <li> <p>Look for directory touched on only one branch:     <code>bash     &gt; i integrate_files --file-direction common_files --mode \"print_dirs\"     &gt; i integrate_files --file-direction only_files_in_src --mode \"print_dirs\"     &gt; i integrate_files --file-direction only_files_in_dst --mode \"print_dirs\"</code></p> </li> <li> <p>If we find dirs that are touched in one branch but not in the other we can     copy / merge without running risks</p> <p>```bash</p> <p>i integrate_diff_dirs --subdir $SUBDIR -c ```</p> </li> <li> <p>Check which change was made in each side since the last integration</p> <p>```bash</p> </li> <li> <p>Check which files are different between the dirs:</p> <p>```bash</p> <p>i integrate_diff_dirs ```</p> </li> <li> <p>Diff dir by dir</p> <p>```bash</p> <p>i integrate_diff_dirs --subdir dataflow/system ```</p> </li> <li> <p>Copy by dir</p> <p>```bash</p> <p>i integrate_diff_dirs --subdir market_data -c ```</p> </li> <li> <p>Sync a dir to handle moved files</p> </li> <li>Assume that there is a dir where files were moved     <code>bash     &gt; invoke integrate_diff_dirs     ...     ... Only in .../cmamp1/.../alpha_numeric_data_snapshots: alpha     ... Only in .../amp1/.../alpha_numeric_data_snapshots: latest</code></li> <li>You can accept one side with:     <code>bash     &gt; invoke integrate_rsync $(pwd)/marketing</code></li> <li>This corresponds to:     <code>bash     &gt; rsync --delete -a -r {src_dir}/ {dst_dir}/</code></li> </ul>"},{"location":"tools/all.integrate_repos.how_to_guide.html#find_the_integration_point","title":"Find the integration point:","text":"<p>i integrate_files --file-direction common_files ... last_integration_hash='813c7e763'</p>"},{"location":"tools/all.integrate_repos.how_to_guide.html#diff_the_changes_in_each_side_from_the_integration_point","title":"Diff the changes in each side from the integration point:","text":"<p>i git_branch_diff_with -t hash -h 813c7e763 -f ... git difftool 813c7e763 ... ```</p>"},{"location":"tools/all.integrate_repos.how_to_guide.html#double-check_the_integration","title":"Double-check the integration","text":"<ul> <li> <p>Check that the regressions are passing on GH</p> <p>```bash</p> <p>i gh_create_pr --no-draft ```</p> </li> <li> <p>Check the files that were changed in both branches (i.e., the \"problematic     ones\") since the last integration and compare them to the base in each branch</p> <p>```bash</p> <p>cd amp1 i integrate_diff_overlapping_files --src-dir-basename \"amp1\" --dst-dir-basename \"cmamp1\" cd cmamp1 i integrate_diff_overlapping_files --src-dir-basename \"cmamp1\" --dst-dir-basename \"amp1\" ```</p> </li> <li> <p>Read the changes to Python files:</p> <p>```bash</p> <p>cd amp1 i git_branch_diff_with -t base --keep-extensions py cd cmamp1 i git_branch_diff_with -t base --keep-extensions py ```</p> </li> <li> <p>Quickly scan all the changes in the branch compared to the base:     <code>&gt; cd amp1     &gt; i git_branch_diff_with -t base     &gt; cd cmamp1     &gt; i git_branch_diff_with -t base</code></p> </li> </ul>"},{"location":"tools/all.integrate_repos.how_to_guide.html#run_tests","title":"Run tests","text":"<ul> <li> <p>Check <code>amp</code> / <code>cmamp</code> using GH actions:</p> <p>```bash</p> <p>i gh_create_pr --no-draft i pytest_collect_only i gh_workflow_list ```</p> </li> <li> <p>Check <code>lem</code> on dev1</p> <p>```bash</p> </li> <li> <p>Check <code>lime</code> on dev4</p> </li> <li> <p>Check <code>orange</code> on dev1</p> </li> </ul>"},{"location":"tools/all.integrate_repos.how_to_guide.html#clean_everything","title":"Clean everything.","text":"<p>git reset --hard; git clean -fd; git pull; (cd amp; git reset --hard; git clean -fd; git pull)</p> <p>i git_pull</p> <p>AM_BRANCH=AmpTask1786_Integrate_20220916 (cd amp; gco $AM_BRANCH)</p> <p>i pytest_collect_only i pytest_buildmeister</p> <p>i git_branch_create -b $AM_BRANCH ```</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html","title":"All.invoke bash print tree.how to guide","text":""},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#the_bash_print_tree_workflow","title":"The Bash Print Tree Workflow","text":""},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#introduction","title":"Introduction","text":"<ul> <li>The <code>bash_print_tree</code> <code>invoke</code> workflow prints a directory tree and optionally     updates an existing Markdown file</li> <li>It is used for maintaining documentation of structured files preserving inline     comments</li> <li>The tree can be printed to standard output or embedded into a file between     markers like <code>&lt;!-- tree:start:{name} --&gt;</code> and <code>&lt;!-- tree:end --&gt;</code></li> </ul>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#workflow_explanation","title":"Workflow Explanation","text":"<ul> <li>The task walks the specified directory up to a maximum depth</li> <li>By default, the tree includes all subdirectories and files (excluding Python     and test files), and outputs to the terminal only</li> <li>You can configure whether to:<ul> <li>Include or exclude Python files</li> <li>Include or exclude test files</li> <li>Show only directories</li> <li>Output to a file, while preserving existing inline comments</li> <li>Clean untracked files</li> </ul> </li> <li>If an output file is given and contains tree markers, the script:<ul> <li>Extracts inline comments from the existing tree</li> <li>Preserves these comments while updating the tree section</li> </ul> </li> </ul>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#usage_instructions","title":"Usage Instructions","text":"<ul> <li> <p>Some examples of workflows are:</p> <p>```bash</p> </li> </ul>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#print_the_current_directory_tree","title":"Print the current directory tree.","text":"<p>i bash_print_tree</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#limit_depth_to_2_and_include_test_files","title":"Limit depth to 2 and include test files.","text":"<p>i bash_print_tree --path=\"devops\" --depth=2 --include-tests</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#include_only_python_files","title":"Include only Python files.","text":"<p>i bash_print_tree --include-python</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#print_only_directory_names","title":"Print only directory names.","text":"<p>i bash_print_tree --only-dirs</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#overwrite_tree_block_in_a_markdown_file_while_preserving_comments","title":"Overwrite tree block in a Markdown file while preserving comments.","text":"<p>i bash_print_tree --path=\"devops\" --output=\"README.md\"</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#clean_untracked_files_before_generating_the_tree","title":"Clean untracked files before generating the tree.","text":"<p>i bash_print_tree --clean ```</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#use_cases","title":"Use Cases","text":"<ul> <li>Documenting file structures in <code>README.md</code> or similar documentation</li> <li>Auditing the layout of codebases and shared folders</li> <li>Tracking structural changes over time via version control</li> </ul>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#example","title":"Example","text":"<ul> <li>Consider the case where we want to document the DevOps code organization     located in the directory <code>devops</code></li> </ul>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#step_1_create_a_new_file_with_tree_output","title":"Step 1: Create a new file with tree output","text":"<ul> <li> <p>Create the initial file with the structure of the tree</p> <p>```bash</p> <p>i bash_print_tree --path=\"devops\" --output=\"all.devops_docker.reference.md\" ```</p> </li> <li> <p>The output file <code>all.devops_docker.reference.md</code> should look like this:</p> <p>```markdown </p> <p>devops - compose     - tmp.docker-compose.yml - docker_build     - create_users.sh     - dev.Dockerfile     - dockerignore.dev     - dockerignore.prod     - etc_sudoers     - fstab     - install_cprofile.sh     - install_dind.sh     - install_os_packages.sh     - install_publishing_tools.sh     - install_python_packages.sh     - pip_list.txt     - poetry.lock     - poetry.toml     - prod.Dockerfile     - pyproject.python_data_stack.toml     - pyproject.toml     - update_os.sh     - utils.sh - docker_run     - bashrc     - docker_setenv.sh     - entrypoint.sh     - run_jupyter_server.sh - env     - default.env      ```</p> </li> </ul>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#step_2_add_comments_to_explain_important_files","title":"Step 2: Add comments to explain important files","text":"<ul> <li> <p>You edit the output file <code>all.devops_docker.reference.md</code> and more information     and comments to the files in the tree</p> <p>```markdown</p> </li> </ul>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#introduction_1","title":"Introduction","text":"<p>This file documents the code organization and Docker-based DevOps structure.</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#directory_structure","title":"Directory Structure","text":"<p>devops - compose # Contains Docker compose files.     - tmp.docker-compose.yml - docker_build # Building Docker image.     - create_users.sh # Create container users.     - dev.Dockerfile     - dockerignore.dev     - dockerignore.prod     - etc_sudoers # Gives sudo permissions.     - fstab     - install_cprofile.sh     - install_dind.sh # Installs docker-in-docker.     - install_os_packages.sh     - install_publishing_tools.sh     - install_python_packages.sh     - pip_list.txt     - poetry.lock     - poetry.toml     - prod.Dockerfile     - pyproject.python_data_stack.toml     - pyproject.toml     - update_os.sh     - utils.sh - docker_run # Running Docker image.     - bashrc     - docker_setenv.sh     - entrypoint.sh     - run_jupyter_server.sh - env     - default.env     </p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#docker_invoke_flow","title":"Docker invoke flow","text":"<p>There exists <code>docker_bash</code> and <code>docker_jupyter</code>. ```</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#step_3_add_more_files_and_subdirectories_to_devops","title":"Step 3: Add more files and subdirectories to <code>devops</code>","text":"<ul> <li> <p>Over time, additional files and subdirectories will be added to <code>devops</code></p> <p>```bash</p> </li> </ul>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#create_a_new_directory","title":"Create a new directory.","text":"<p>mkdir -p devops/debug</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#add_a_new_file","title":"Add a new file.","text":"<p>touch devops/docker_build/install_tools.sh ```</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#step_4_re-run_the_workflow_to_update_the_tree","title":"Step 4: Re-run the workflow to update the tree","text":"<ul> <li> <p>Update the documentation file with the updated structure of <code>devops</code></p> <p>```bash</p> <p>i bash_print_tree --path=\"devops\" --output=\"all.devops_docker.reference.md\" ```</p> </li> <li> <p>The updated tree will reflect the new files while preserving comments:</p> <p>```markdown</p> </li> </ul>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#introduction_2","title":"Introduction","text":"<p>This file documents the code organization and Docker-based DevOps structure.</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#directory_structure_1","title":"Directory Structure","text":"<p>devops - compose # Contains Docker compose files.     - tmp.docker-compose.yml - debug - docker_build # Building Docker image.     - create_users.sh # Create container users.     - dev.Dockerfile     - dockerignore.dev     - dockerignore.prod     - etc_sudoers # Gives sudo permissions.     - fstab     - install_cprofile.sh     - install_dind.sh # Installs docker-in-docker.     - install_os_packages.sh     - install_publishing_tools.sh     - install_python_packages.sh     - install_tools.sh     - pip_list.txt     - poetry.lock     - poetry.toml     - prod.Dockerfile     - pyproject.python_data_stack.toml     - pyproject.toml     - update_os.sh     - utils.sh - docker_run # Running Docker image.     - bashrc     - docker_setenv.sh     - entrypoint.sh     - run_jupyter_server.sh - env     - default.env     </p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#docker_invoke_flow_1","title":"Docker invoke flow","text":"<p>There exists <code>docker_bash</code> and <code>docker_jupyter</code>. ```</p>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#known_limitations","title":"Known Limitations","text":"<ul> <li>Inline comment preservation only works if the tree is wrapped with markers     like <code>&lt;!-- tree:start:{name} --&gt;</code> and <code>&lt;!-- tree:end --&gt;</code></li> <li>The function does not support excluding arbitrary file patterns beyond test or     Python filters</li> </ul>"},{"location":"tools/all.invoke_bash_print_tree.how_to_guide.html#future_improvements","title":"Future Improvements","text":"<ul> <li>Add support for custom exclude or include glob patterns</li> <li>Support multiple tree blocks per file (e.g., for different directories)</li> <li>Improve standardization of how tree blocks are rendered in Markdown files by     wrapping the content between markers in a code block (e.g., using <code>```bash</code>)     so that only the tree, not the markers, is displayed in the final output</li> </ul>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html","title":"All.invoke git branch copy.how to guide","text":""},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#the_git_branch_copy_workflow","title":"The Git Branch Copy Workflow","text":""},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#introduction","title":"Introduction","text":"<ul> <li>The <code>git_branch_copy</code> <code>invoke</code> workflow creates a new Git branch derived from     the current branch and copies its content exactly</li> <li>It's primarily useful in scenarios where a large PR (Pull Request) needs to be     split into smaller, independently mergeable PRs to improve code review     efficiency and reduce integration complexity</li> <li>This approach allows to:<ul> <li>Develop a large feature in one-shot, but merge it in chunks</li> <li>Easily extract a sub-PR, since often \"removing\" code chunks is simpler than     \"adding\" code chunks one by one, e.g., by reverting an entire file</li> <li>Overlap developing and testing / reviewing</li> <li>Make it simple to extract a sub-PR and then merge it back into the feature     branch easily</li> </ul> </li> </ul>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#workflow_explanation","title":"Workflow Explanation","text":"<ul> <li>The workflow executes the following operations:<ul> <li>Cleans up untracked files in the working directory</li> <li>Optionally merges the latest changes from <code>master</code> before creating the new     branch</li> <li>Creates and switches to a new branch<ul> <li>Automatically generates a suitable branch name if not explicitly provided</li> <li>Validates the new branch name according to established naming conventions,     e.g, <code>{Repo}Task{Number}_{Description}</code></li> </ul> </li> <li>Squash merge changes from the current branch into the new branch<ul> <li>Changes are staged, but uncommitted, to allow developers to selectively     stage, commit, and push specific parts.</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#usage_instructions","title":"Usage Instructions","text":"<ul> <li> <p>Go to the target branch to merge</p> <p>```bash</p> <p>git checkout CmTask5874_Document_PR_flow ```</p> </li> <li> <p>You can execute the workflow with one of these command:</p> <p>```bash</p> </li> </ul>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#create_new_branch_with_an_automatically_generated_name_eg_cmtask5874_document_pr_flow_02","title":"Create new branch with an automatically generated name (e.g., <code>CmTask5874_Document_PR_flow_02</code>).","text":"<p>i git_branch_copy</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#create_new_branch_with_given_name","title":"Create new branch with given name.","text":"<p>i git_branch_copy --new-branch-name=\"CmTask5874_PR_flow_Images\"</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#create_a_new_branch_without_merging_master","title":"Create a new branch without merging <code>master</code>.","text":"<p>i git_branch_copy --skip-git-merge-master --new-branch-name=\"CmTask5874_PR_flow_Images\"</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#create_a_new_branch_without_checking_for_naming_conventions","title":"Create a new branch without checking for naming conventions.","text":"<p>i git_branch_copy --new-branch-name=\"wrongname_123\" --no-check-branch-name ```</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#example","title":"Example","text":"<ul> <li>Consider the case where we are working on a large feature branch     <code>CmTask5874_Document_PR_flow</code></li> <li> <p>For various reasons we might want to extract and merge just a few files, e.g.,</p> <ul> <li>The regression tests are not passing and we want to test and merge only a     subset of changes</li> <li>The branch contains unrelated changes (e.g., while implementing a feature,     we ended up fixing another issue)</li> <li>The PR is too complex to review in one shot and we want to break it down in     reviewable pieces</li> </ul> </li> <li> <p>Rather than merging the main PR, we create one or more children PRs, and merge     them into <code>master</code> one at a time</p> </li> </ul>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#step_1_make_sure_your_branch_is_up_to_date","title":"Step 1: Make sure your branch is up to date","text":"<ul> <li> <p>Use this workflow to ensure your branch is up to date with <code>master</code></p> <p>```bash</p> </li> <li> <p>The output looks like:</p> <p>```bash INFO: &gt; cmd='/data/sameepp/src/venv/amp.client_venv/bin/invoke git_branch_diff_with -t base --only-print-files' 04:58:35 - INFO  lib_tasks_git.py _git_diff_with_branch:726</p> </li> <li> <p>As shown above, three files have been modified</p> </li> <li> <p>Suppose we only want to partially merge this PR, specifically, just the <code>.png</code>     files, while continuing development in the main feature branch</p> </li> </ul>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#first_switch_to_your_feature_branch","title":"First switch to your feature branch.","text":"<p>git checkout CmTask5874_Document_PR_flow</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#make_sure_that_the_branch_is_up-to-date_with_master","title":"Make sure that the branch is up-to-date with master.","text":"<p>i git_merge_master</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#commit_and_push_the_changes_that_you_have_made_to_the_branch","title":"Commit and push the changes that you have made to the branch.","text":"<p>git commit -m \"Merge\" git push</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#check_the_diff_between_your_branch_and_master","title":"Check the diff between your branch and master.","text":"<p>i git_branch_diff_with -t base --only-print-files ```</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#_1","title":"All.invoke git branch copy.how to guide","text":""},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#files3","title":"files=3","text":""},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#_2","title":"All.invoke git branch copy.how to guide","text":"<p>04:58:35 - INFO  lib_tasks_git.py _git_diff_with_branch:727 ./figs/development/Fig1.png ./figs/development/Fig2.png docs/work_tools/all.development.how_to_guide.md 04:58:35 - WARN  lib_tasks_git.py _git_diff_with_branch:732             Exiting as per user request with --only-print-files ```</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#step_2_create_a_new_branch_from_your_feature_branch","title":"Step 2: Create a new branch from your feature branch","text":"<ul> <li> <p>Create a new branch (e.g., <code>CmTask5874_Document_PR_flow_02</code>) derived from our     feature branch <code>CmTask5874_Document_PR_flow</code></p> <p>```bash</p> </li> </ul>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#create_a_derived_branch_from_the_feature_branch","title":"Create a derived branch from the feature branch.","text":"<p>i git_branch_copy INFO: &gt; cmd='/data/sameepp/src/venv/amp.client_venv/bin/invoke git_branch_copy' git clean -fd invoke git_merge_master --ff-only From github.com:cryptokaizen/cmamp e59affd79..d6e6ed8e4  master     -&gt; master INFO: &gt; cmd='/data/sameepp/src/venv/amp.client_venv/bin/invoke git_merge_master --ff-only'</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#git_merge_master","title":"git_merge_master:","text":""},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#git_fetch_master","title":"git_fetch_master:","text":"<p>git fetch origin master:master git submodule foreach 'git fetch origin master:master' git merge master --ff-only Already up to date. 07:04:46 - INFO  lib_tasks_git.py git_branch_copy:599                   new_branch_name='CmTask5874_Document_PR_flow_2' git checkout master &amp;&amp; invoke git_branch_create -b 'CmTask5874_Document_PR_flow_2' Switched to branch 'master' Your branch is up to date with 'origin/master'. INFO: &gt; cmd='/data/sameepp/src/venv/amp.client_venv/bin/invoke git_branch_create -b CmTask5874_Document_PR_flow_2'</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#git_branch_create","title":"git_branch_create:","text":"<p>07:05:00 - INFO  lib_tasks_git.py git_branch_create:413                 branch_name='CmTask5874_Document_PR_flow_2' git pull --autostash --rebase Current branch master is up to date. Switched to a new branch 'CmTask5874_Document_PR_flow_2' remote: remote: Create a pull request for 'CmTask5874_Document_PR_flow_2' on GitHub by visiting: remote:      https://github.com/cryptokaizen/cmamp/pull/new/CmTask5874_Document_PR_flow_2 remote: To github.com:causify-ai/cmamp.git [new branch] CmTask5874_Document_PR_flow_2 -&gt; CmTask5874_Document_PR_flow_2 git checkout -b CmTask5874_Document_PR_flow_2 git push --set-upstream origin CmTask5874_Document_PR_flow_2 Branch 'CmTask5874_Document_PR_flow_2' set up to track remote branch 'CmTask5874_Document_PR_flow_2' from 'origin'. git merge --squash --ff CmTask5874_Document_PR_flow &amp;&amp; git reset HEAD Updating d6e6ed8e4..a264a6f30 Fast-forward Squash commit -- not updating HEAD docs/work_tools/figs/development/Fig1.png | Bin 27415 -&gt; 0 bytes docs/work_tools/figs/development/Fig2.png | Bin 35534 -&gt; 0 bytes 2 files changed, 0 insertions(+), 0 deletions(-) delete mode 100644 docs/work_tools/figs/development/Fig1.png delete mode 100644 docs/work_tools/figs/development/Fig2.png Unstaged changes after reset: D docs/work_tools/figs/development/Fig1.png D docs/work_tools/figs/development/Fig2.png ```</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#step_3_confirm_the_new_branch_is_created","title":"Step 3: Confirm the new branch is created","text":"<ul> <li> <p>After running the command, a new branch <code>CmTask5874_Document_PR_flow_2</code> is     created, containing all changes from the original feature branch</p> <p>```bash</p> <p>git status On branch CmTask5874_Document_PR_flow_2 Your branch is up to date with 'origin/CmTask5874_Document_PR_flow_2'.</p> <p>Untracked files: (use \"git add ...\" to include in what will be committed)         ./figs/development/Fig1.png         ./figs/development/Fig2.png         docs/work_tools/all.invoke_workflows.how_to_guide.md ``` <li> <p>You can now stage and commit only the files you want to merge (e.g., the     <code>.png</code> files), and proceed to create a PR to merge those changes into <code>master</code></p> <p>```bash</p> </li> <li> <p>From this you can create a PR, test it, review it, and merge it</p> </li> <li> <p>Once the chunk is merged into <code>master</code>, you can go to the father branch and     merge <code>master</code></p> <p>```bash</p> <p>git checkout CmTask5874_Document_PR_flow i git_merge_master git commit -am \"Merge\" git push ```</p> </li> <li> <p>Note that the merge it's typically very simple, since the incoming code is the     same one that is already in the branch</p> </li>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#add_commit_and_push_only_the_required_files","title":"Add, commit and push only the required files.","text":"<p>git add ./figs/development/Fig1.png ./figs/development/Fig2.png git commit -m \"Checkpoint\" git push origin CmTask5874_Document_PR_flow_2 ```</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#execution_flow_diagram","title":"Execution flow diagram","text":"<pre><code>%%{init: {'theme': 'base', 'gitGraph': {'mainBranchName': 'master'}}}%%\ngitGraph\n        commit id: \"Initial commit (master)\"\n        branch CmTask5874_Document_PR_flow\n        checkout CmTask5874_Document_PR_flow\n        commit id: \"Feature change 1\"\n        commit id: \"Feature change 2\"\n        checkout master\n        branch CmTask5874_Document_PR_flow_2\n        checkout CmTask5874_Document_PR_flow_2\n        merge CmTask5874_Document_PR_flow id: \"Squash feature changes\"\n        commit id: \"Commit .png files only\"\n        checkout master\n        merge CmTask5874_Document_PR_flow_2 id: \"Partial PR merged\"\n        checkout CmTask5874_Document_PR_flow\n        merge master id: \"Sync with master\"\n</code></pre>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#alternative_manual_workflow","title":"Alternative Manual Workflow","text":"<ul> <li>You can also manually extract part of a PR using patch-based workflows or     squash merging.</li> </ul>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#using_invoke_git_create_patch","title":"Using invoke <code>git_create_patch</code>","text":"<ul> <li> <p>If you're working with multiple Git clients or prefer working outside your     development tree:</p> <p>```bash</p> </li> </ul>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#in_your_feature_branch","title":"In your feature branch:","text":"<p>i git_patch_create --branch</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#copy_the_patch_file_to_a_clean_git_client_or_switch_to_master","title":"Copy the patch file to a clean Git client or switch to <code>master</code>","text":"<p>git checkout master</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#apply_the_patch_generated_earlier","title":"Apply the patch generated earlier","text":"<p>git apply ~/patch.amp.8f9cda97.20210609_080439.patch</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#the_patch_should_apply_cleanly_if_you_get_conflicts_your_feature_branch_may_not_be_up-to-date_with_master","title":"The patch should apply cleanly. If you get conflicts, your feature branch may not be up-to-date with <code>master</code>.","text":""},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#clean_up_unwanted_files_before_committing","title":"Clean up unwanted files before committing","text":"<p>git diff git checkout master -- files/to/revert</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#commit_and_push_selected_changes","title":"Commit and push selected changes","text":"<p>git commit -m \"Partial merge from feature branch\" git push</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#create_a_pr_to_trigger_github_tests","title":"Create a PR to trigger GitHub tests","text":"<p>i gh_create_pr --no-draft</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#run_regression_tests","title":"Run regression tests","text":"<p>i run_fast_tests</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#merge_pr_into_master","title":"Merge PR into <code>master</code>","text":""},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#return_to_your_feature_branch_and_merge_updates_from_master","title":"Return to your feature branch and merge updates from <code>master</code>","text":"<p>git checkout CmTask5874_Document_PR_flow git pull ```</p>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#known_limitations","title":"Known Limitations","text":"<ul> <li>Currently, the <code>use_patch</code> option for creating branches via patches is     unimplemented and reserved for future enhancements</li> <li>The workflow doesn't explicitly handle merge conflicts during the squash merge     step, manual resolution may be required</li> </ul>"},{"location":"tools/all.invoke_git_branch_copy.how_to_guide.html#future_improvements","title":"Future Improvements","text":"<ul> <li>Implement the <code>use_patch</code> option to support patch-based branch creation</li> <li>Introduce enhanced automation for conflict resolution and interactive conflict     handling during branch operations</li> <li>Provide options to handle specific file types or directories selectively     during branch creation</li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html","title":"All.invoke workflows.how to guide","text":""},{"location":"tools/all.invoke_workflows.how_to_guide.html#invoke_workflows","title":"Invoke Workflows","text":""},{"location":"tools/all.invoke_workflows.how_to_guide.html#introduction","title":"Introduction","text":"<ul> <li>We use <code>invoke</code> to implement workflows (aka \"tasks\") similar to Makefile     targets, but using Python</li> <li> <p>The official documentation for <code>invoke</code> is     here</p> </li> <li> <p>We use <code>invoke</code> to automate tasks and package workflows for:</p> <ul> <li>Docker: <code>docker_*</code></li> <li>Git: <code>git_*</code></li> <li>GitHub (relying on <code>gh</code> integration): <code>gh_*</code></li> <li>Running tests: <code>run_*</code></li> <li>Branch integration: <code>integrate_*</code></li> <li>Releasing tools and Docker images: <code>docker_*</code></li> <li>Lint: <code>lint_*</code></li> </ul> </li> <li> <p>Each set of commands starts with the name of the corresponding topic:</p> <ul> <li>E.g., <code>docker_*</code> for all the tasks related to Docker</li> </ul> </li> <li>The best approach to getting familiar with the tasks is to browse the list and     then check the output of the help</li> <li> <p><code>i</code> is the shortcut for the <code>invoke</code> command</p> <p>```bash</p> <p>invoke --help command i -h gh_issue_title Usage: inv[oke] [--core-opts] gh_issue_title [--options] [other tasks here ...]</p> <p>Docstring: Print the title that corresponds to the given issue and repo_short_name. E.g., AmpTask1251_Update_GH_actions_for_amp.</p> <p>:param pbcopy: save the result into the system clipboard (only on macOS)</p> <p>Options: -i STRING, --issue-id=STRING -p, --[no-]pbcopy -r STRING, --repo-short-name=STRING ```</p> </li> <li> <p>We can guarantee you a 2x improvement in performance if you master the     workflows, but it takes some time and patience</p> </li> <li> <p><code>TAB</code> completion available for all the tasks, e.g.,</p> <p>```bash</p> <p>i gh_ gh_create_pr      gh_issue_title    gh_login          gh_workflow_list  gh_workflow_run ``` - Tabbing after typing a dash (-) or double dash (--) will display valid     options/flags for the current context."},{"location":"tools/all.invoke_workflows.how_to_guide.html#listing_all_the_tasks","title":"Listing All the Tasks","text":"<ul> <li> <p>New commands are always being added, but a list of valid tasks is below</p> <p>````bash</p> <p>invoke --list INFO: &gt; cmd='/Users/saggese/src/venv/amp.client_venv/bin/invoke --list' Available tasks: copy_ecs_task_definition_image_url                    Copy image URL from one task definition to another.</p> <p>docker_bash                                           Start a bash shell inside the container corresponding to a stage. docker_build_local_image                              Build a local image, i.e., a release candidate \"dev\" image. docker_build_prod_image                               Build a prod image from a dev image. docker_cmd                                            Execute the command <code>cmd</code> inside a container corresponding to a stage. docker_create_candidate_image                         Create new prod candidate image and update the specified ECS task docker_images_ls_repo                                 List images in the logged in repo_short_name. docker_jupyter                                        Run Jupyter notebook server. docker_kill                                           Kill the last Docker container started. docker_login                                          Log in the target registry and skip if we are in kaizenflow. docker_ps                                             List all the running containers. docker_pull                                           Pull latest dev image corresponding to the current repo from the registry. docker_pull_helpers                                   Pull latest prod image of <code>helpers</code> from the registry. docker_push_dev_image                                 Push the \"dev\" image to ECR. docker_push_prod_candidate_image                      (ONLY CI/CD) Push the \"prod\" candidate image to ECR. docker_push_prod_image                                Push the \"prod\" image to ECR. docker_release_dev_image                              Build, test, and release to ECR the latest \"dev\" image. docker_release_multi_build_dev_image                  Build, test, and release the latest multi-arch \"dev\" image. docker_release_prod_image                             Build, test, and release to ECR the prod image. docker_rollback_dev_image                             Rollback the version of the dev image. docker_rollback_prod_image                            Rollback the version of the prod image. docker_stats                                          Report last started Docker container stats, e.g., CPU, RAM. docker_tag_local_image_as_dev                         Mark the \"local\" image as \"dev\". docker_tag_push_multi_build_local_image_as_dev        Mark the multi-arch \"local\" image as \"dev\" and push it. docker_update_prod_task_definition                    Update image in prod task definition to the desired version.</p> <p>find                                                  Find symbols, imports, test classes and so on. find_check_string_output                              Find output of <code>check_string()</code> in the test running find_dependency                                       E.g., <code>`` find_test_class                                       Report test files containing</code>class_name<code>in a format compatible with find_test_decorator                                   Report test files containing</code>class_name` in pytest format.</p> <p>fix_perms                                             :param action:</p> <p>gh_create_pr                                          Create a draft PR for the current branch in the corresponding gh_issue_title                                        Print the title that corresponds to the given issue and repo_short_name. gh_login gh_publish_buildmeister_dashboard_to_s3               Run the buildmeister dashboard notebook and publish it to S3. gh_workflow_list                                      Report the status of the GH workflows. gh_workflow_run                                       Run GH workflows in a branch.</p> <p>git_add_all_untracked                                 Add all untracked files to Git. git_branch_copy                                       Create a new branch with the same content of the current branch. git_branch_create                                     Create and push upstream branch <code>branch_name</code> or the one corresponding to git_branch_delete_merged                              Remove (both local and remote) branches that have been merged into master. git_branch_diff_with                                  Diff files of the current branch with master at the branching point. git_branch_files                                      Report which files were added, changed, and modified in the current branch git_branch_next_name                                  Return a name derived from the current branch so that the branch doesn't git_branch_rename                                     Rename current branch both locally and remotely. git_clean                                             Clean the repo_short_name and its submodules from artifacts. git_fetch_master                                      Pull master without changing branch. git_files                                             Report which files are changed in the current branch with respect to git_last_commit_files                                 Print the status of the files in the previous commit. git_merge_master                                      Merge <code>origin/master</code> into the current branch. git_patch_create                                      Create a patch file for the entire repo_short_name client from the base git_pull                                              Pull all the repos. git_roll_amp_forward                                  Roll amp forward.</p> <p>integrate_create_branch                               Create the branch for integration of <code>dir_basename</code> (e.g., amp1) in the integrate_diff_dirs                                   Integrate repos from dirs <code>src_dir_basename</code> to <code>dst_dir_basename</code> by diffing integrate_diff_overlapping_files                      Find the files modified in both branches <code>src_dir_basename</code> and integrate_file                                        Diff corresponding files in two different repos. integrate_files                                       Find and copy the files that are touched only in one branch or in both. integrate_find_files                                  Find the files that are touched in the current branch since last integrate_find_files_touched_since_last_integration   Print the list of files modified since the last integration for this dir. integrate_rsync                                       Use <code>rsync</code> to bring two dirs to sync.</p> <p>lint                                                  Lint files. lint_check_if_it_was_run                              Check if Linter was run in the current branch. lint_check_python_files                               Compile and execute Python files checking for errors. lint_check_python_files_in_docker                     Compile and execute Python files checking for errors. lint_create_branch                                    Create the branch for linting in the current dir. lint_detect_cycles                                    Detect cyclic imports in the directory files.</p> <p>print_env                                             Print the repo configuration. print_setup                                           Print some configuration variables. print_tasks                                           Print all the available tasks in <code>lib_tasks.py</code>.</p> <p>pytest_add_untracked_golden_outcomes                  Add the golden outcomes files that are not tracked under git. pytest_buildmeister                                   Run the regression tests. pytest_buildmeister_check                             :param print_output: print content of the file with the output of the pytest_clean                                          Clean pytest artifacts. pytest_collect_only pytest_compare_logs                                   Diff two log files removing the irrelevant parts (e.g., timestamps, object pytest_find_unused_goldens                            Detect mismatches between tests and their golden outcome files. pytest_rename_test                                    Rename the test and move its golden outcome. pytest_repro                                          Generate commands to reproduce the failed tests after a <code>pytest</code> run.</p> <p>release_dags_to_airflow                               Copy the DAGs to the shared Airflow directory.</p> <p>run_blank_tests                                       (ONLY CI/CD) Test that pytest in the container works. run_coverage_report                                   Compute test coverage stats. run_coverage_subprocess                               Run comprehensive coverage using subprocess mode with hcoverage injection and direct                                                                                                             coverage run. run_fast_slow_superslow_tests                         Run fast, slow, superslow tests back-to-back. run_fast_slow_tests                                   Run fast and slow tests back-to-back. run_fast_tests                                        Run fast tests. check <code>gh auth status</code> before invoking to avoid auth run_qa_tests                                          Run QA tests independently. run_slow_tests                                        Run slow tests. run_superslow_tests                                   Run superslow tests. run_tests                                             :param test_lists: comma separated list with test lists to run (e.g., <code>fast_test,slow_tests</code>)</p> <p>traceback                                             Parse the traceback from Pytest and navigate it with vim. ````</p> </li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#getting_help_for_a_specific_workflow","title":"Getting Help for a Specific Workflow","text":"<ul> <li> <p>You can get a more detailed help with</p> <p>```bash</p> <p>invoke --help run_fast_tests Usage: inv[oke] [--core-opts] run_fast_tests [--options] [other tasks here ...]</p> <p>Docstring: Run fast tests.</p> <p>:param stage: select a specific stage for the Docker image :param pytest_opts: option for pytest :param pytest_mark: test list to select as <code>@pytest.mark.XYZ</code> :param dir_name: dir to start searching for tests :param skip_submodules: ignore all the dir inside a submodule :param coverage: enable coverage computation :param collect_only: do not run tests but show what will be executed</p> <p>Options: -c, --coverage -d STRING, --dir-name=STRING -k, --skip-submodules -o, --collect-only -p STRING, --pytest-opts=STRING -s STRING, --stage=STRING -y STRING, --pytest-mark=STRING ```</p> </li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#implementation_details","title":"Implementation Details","text":"<ul> <li>By convention all invoke targets are in <code>*_lib_tasks.py</code>, e.g.,<ul> <li><code>helpers/lib_tasks.py</code> - tasks to be run in <code>cmamp</code></li> <li><code>optimizer/opt_lib_tasks.py</code> - tasks to be run in <code>cmamp/optimizer</code></li> </ul> </li> <li> <p>All invoke tasks are functions with the <code>@task</code> decorator, e.g.,</p> <p>```python from invoke import task</p> </li> </ul> <p>@task def invoke_task(...):     ...     ```</p> <ul> <li>To run a task we use <code>context.run(...)</code>, see     the official docs</li> <li>To be able to run a specified invoke task one should import it in <code>tasks.py</code><ul> <li>E.g., see <code>cmamp/tasks.py</code></li> </ul> </li> <li>A task can be run only in a dir where it is imported in a corresponding     <code>tasks.py</code>, e.g.,<ul> <li><code>invoke_task1</code> is imported in <code>cmamp/tasks.py</code> so it can be run only from     <code>cmamp</code></li> <li><code>invoke_task2</code> is imported in <code>cmamp/optimizer/tasks.py</code> so it can be run     only from <code>cmamp/optimizer</code><ul> <li>In other words one should do <code>cd cmamp/optimizer</code> before doing     <code>i invoke_task2 ...</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#git","title":"Git","text":""},{"location":"tools/all.invoke_workflows.how_to_guide.html#merge_master_in_the_current_branch","title":"Merge Master in the Current Branch","text":"<pre><code>&gt; i git_merge_master\n</code></pre>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#github","title":"Github","text":"<ul> <li> <p>Get the official branch name corresponding to an Issue</p> <p>```bash</p> <p>i gh_issue_title -i 256</p> </li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#gh_issue_title_issue_id256_repo_short_namecurrent","title":"gh_issue_title: issue_id='256', repo_short_name='current'","text":""},{"location":"tools/all.invoke_workflows.how_to_guide.html#copied_to_system_clipboard","title":"Copied to system clipboard:","text":"<p>AmpTask256_Part_task2236_jenkins_cleanup_split_scripts: https://github.com/alphamatic/amp/pull/256 ```</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#create_a_pr","title":"Create a PR","text":"<p>TODO(gp): Describe</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#extract_a_pr_from_a_larger_one","title":"Extract a PR From a Larger One","text":"<ul> <li>For splitting large PRs, use the dedicated workflow `git_branch_copy'</li> <li>See detailed guide in     all.invoke_git_branch_copy.how_to_guide.md</li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#systematic_code_transformation","title":"Systematic Code Transformation","text":"<ul> <li>Can be done with the help of     <code>dev_scripts_helpers/system_tools/replace_text.py</code></li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#generate_a_local_amp_docker_image","title":"Generate a Local <code>amp</code> Docker Image","text":"<ul> <li>This is a manual flow used to test and debug images before releasing them to     the team.</li> <li> <p>The flow is similar to the dev image, but by default tests are not run and the     image is not released.</p> <p>```bash</p> </li> <li> <p>To implement the entire Docker QA process of a dev image</p> <p>```bash</p> </li> <li> <p>To install packages in an image, do <code>i docker_bash</code></p> <p>```bash</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#build_the_local_image_and_update_poetry_dependencies_if_needed","title":"Build the local image (and update Poetry dependencies, if needed).","text":"<p>i docker_build_local_image --update-poetry ... docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local</p> <p>REPOSITORY TAG IMAGE ID CREATED SIZE 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp local 9b3f8f103a2c 1 second ago 1.72GB</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#test_the_new_local_image","title":"Test the new \"local\" image","text":"<p>i docker_bash --stage \"local\" python -c \"import async_solipsism\" python -c \"import async_solipsism; print(async_solipsism.version)\"</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#run_the_tests_with_local_image","title":"Run the tests with local image","text":""},{"location":"tools/all.invoke_workflows.how_to_guide.html#make_sure_the_new_image_is_used_eg_add_an_import_and_trigger_the_tests","title":"Make sure the new image is used: e.g., add an import and trigger the tests.","text":"<p>i run_fast_tests --stage \"local\" --pytest-opts core/dataflow/test/test_real_time.py i run_fast_slow_tests --stage \"local\"</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#promote_a_local_image_to_dev","title":"Promote a local image to dev.","text":"<p>i docker_tag_local_image_as_dev i docker_push_dev_image ```</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#update_the_dev_amp_docker_image","title":"Update the dev <code>amp</code> Docker image","text":""},{"location":"tools/all.invoke_workflows.how_to_guide.html#clean_all_the_docker_images_locally_to_make_sure_there_is_no_hidden_state","title":"Clean all the Docker images locally, to make sure there is no hidden state.","text":"<p>docker system prune --all</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#update_the_needed_packages","title":"Update the needed packages.","text":"<p>devops/docker_build/pyproject.toml</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#visually_inspect_the_updated_packages","title":"Visually inspect the updated packages.","text":"<p>git diff devops/docker_build/poetry.lock</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#run_entire_release_process","title":"Run entire release process.","text":"<p>i docker_release_dev_image ```</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#experiment_in_a_local_image","title":"Experiment in a local image","text":""},{"location":"tools/all.invoke_workflows.how_to_guide.html#switch_to_root_and_install_package","title":"Switch to root and install package.","text":"<p>sudo su - source /venv/bin/activate pip install"},{"location":"tools/all.invoke_workflows.how_to_guide.html#switch_back_to_user","title":"Switch back to user.","text":"<p>exit ```</p> <li> <p>You should test that the package is installed for your user, e.g.,     <code>bash     &gt; source /venv/bin/activate python -c \"import foobar; print(foobar);print(foobar.__version__)\"</code></p> </li> <li>You can now use the package in this container. Note that if you exit the     container, the modified image is lost, so you need to install it again.</li> <li>You can save the modified image, tagging the new image as local, while the     container is still running.</li> <li>Copy your Container ID. You can find it<ul> <li>In the docker bash session, e.g., if the command line in the container     starts with <code>user_1011@da8f3bb8f53b:/app$</code>, your Container ID is     <code>da8f3bb8f53b</code></li> <li>By listing running containers, e.g., run <code>docker ps</code> outside the container</li> </ul> </li> <li>Commit image     <code>bash     &gt; docker commit &lt;Container ID&gt; &lt;IMAGE&gt;/cmamp:local-$USER</code><ul> <li>E.g.     <code>docker commit da8f3bb8f53b 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:local-julias</code></li> </ul> </li> <li>If you are running inside a notebook using <code>i docker_jupyter</code> you can install     packages using a one liner <code>! sudo su -; source ...;</code></li>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#github_actions_ci","title":"Github Actions (CI)","text":"<ul> <li>To run a single test in GH Action<ul> <li>Create a branch</li> <li>Change .github/workflows/fast_tests.yml     <code>bash     run: invoke run_fast_tests     --pytest-opts=\"helpers/test/test_git.py::Test_git_modified_files1::test_get_modified_files_in_branch1     -s --dbg\"</code></li> </ul> </li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#pytest","title":"Pytest","text":"<ul> <li> <p>From     https://gist.github.com/kwmiebach/3fd49612ef7a52b5ce3a</p> </li> <li> <p>More details on running unit tests with <code>invoke</code> is     /docs/coding/all.run_unit_tests.how_to_guide.md</p> </li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#run_with_coverage","title":"Run with Coverage","text":"<pre><code>&gt; i run_fast_tests --pytest-opts=\"core/test/test_finance.py\" --coverage\n</code></pre>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#capture_output_of_a_pytest","title":"Capture Output of a Pytest","text":"<ul> <li> <p>Inside the <code>dev</code> container (i.e., docker bash)</p> <p><code>bash docker&gt; pytest_log ...</code> - This captures the output in a file <code>tmp.pytest_script.txt</code></p> </li> <li> <p>Get the failed tests (inside or outside the container)     <code>bash     [docker]&gt; i pytest_failed     dataflow/model/test/test_run_notebooks.py::Test_run_master_research_backtest_analyzer::test_run_notebook1     dataflow/system/test/test_real_time_runner.py::TestRealTimeDagRunner1::test_simulated_replayed_time1     dataflow/model/test/test_dataframe_modeler.py::TestDataFrameModeler::test_dump_json1     ...</code></p> </li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#run_only_one_test_based_on_its_name","title":"Run Only One Test Based on Its Name","text":"<ul> <li> <p>Outside the <code>dev</code> container</p> <p>```bash</p> <p>i find_test_class Test_obj_to_str1 INFO: &gt; cmd='/Users/saggese/src/venv/amp.client_venv/bin/invoke find_test_class Test_obj_to_str1'</p> </li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#find_test_class_class_name_abs_dir_pbcopy","title":"find_test_class: class_name abs_dir pbcopy","text":"<p>10:18:42 - INFO  lib_tasks_find.py _find_test_files:44                  Searching from '.'</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#copied_to_system_clipboard_1","title":"Copied to system clipboard:","text":"<p>./helpers/test/test_hobject.py::Test_obj_to_str1 ```</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#iterate_on_stacktrace_of_failing_test","title":"Iterate on Stacktrace of Failing Test","text":"<ul> <li>Inside docker bash     <code>bash     docker&gt; pytest ...</code></li> <li> <p>The test fails: switch to using <code>pytest_log</code> to save the stacktrace to a file</p> <p>```bash</p> <p>pytest_log dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals ... =================================== FAILURES =================================== _ Testevaluate_weighted_forecasts.test_combine_two_signals ___ Traceback (most recent call last):     File \"/app/dataflow/model/test/test_tiled_flows.py\", line 78, in test_combine_two_signals         bar_metrics = dtfmotiflo.evaluate_weighted_forecasts(     File \"/app/dataflow/model/tiled_flows.py\", line 265, in evaluate_weighted_forecasts         weighted_sum = hpandas.compute_weighted_sum( TypeError: compute_weighted_sum() got an unexpected keyword argument 'index_mode' ============================= slowest 3 durations ============================== 2.18s call     dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals 0.01s setup    dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals 0.00s teardown dataflow/model/test/test_tiled_flows.py::Test_evaluate_weighted_forecasts::test_combine_two_signals ```</p> </li> <li> <p>Then from outside <code>dev</code> container launch <code>vim</code> in quickfix mode</p> <p>```bash</p> <p>invoke traceback ```</p> </li> <li> <p>The short form is <code>it</code></p> </li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#iterating_on_a_failing_regression_test","title":"Iterating on a Failing Regression Test","text":"<ul> <li> <p>The workflow is:</p> <p>```bash</p> </li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#run_a_lot_of_tests_eg_the_entire_regression_suite","title":"Run a lot of tests, e.g., the entire regression suite.","text":"<p>pytest ...</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#some_tests_fail","title":"Some tests fail.","text":""},{"location":"tools/all.invoke_workflows.how_to_guide.html#run_the_pytest_repro_to_summarize_test_failures_and_to_generate_commands_to_reproduce_them","title":"Run the <code>pytest_repro</code> to summarize test failures and to generate commands to reproduce them.","text":"<p>invoke pytest_repro ```</p>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#detect_mismatches_with_golden_test_outcomes","title":"Detect Mismatches with Golden Test Outcomes","text":"<ul> <li> <p>The command is</p> <p>```bash</p> <p>i pytest_find_unused_goldens ```</p> </li> <li> <p>The specific dir to check can be specified with the <code>dir_name</code> parameter.</p> </li> <li>The invoke detects and logs mismatches between the tests and the golden     outcome files.<ul> <li>When goldens are required by the tests but the corresponding files do not     exist<ul> <li>This usually happens if the tests are skipped or commented out.</li> <li>Sometimes it's a FP hit (e.g. the method doesn't actually call     <code>check_string</code> but instead has it in a string, or <code>check_string</code> is called     on a missing file on purpose to verify that an exception is raised).</li> </ul> </li> <li>When the existing golden files are not actually required by the     corresponding tests.<ul> <li>In most cases it means the files are outdated and can be deleted.</li> <li>Alternatively, it can be a FN hit: the test method A, which the golden     outcome corresponds to, doesn't call <code>check_string</code> directly, but the     test's class inherits from a different class, which in turn has a method B     that calls <code>check_string</code>, and this method B is called in the test method     A.</li> </ul> </li> </ul> </li> <li>For more details see     CmTask528.</li> </ul>"},{"location":"tools/all.invoke_workflows.how_to_guide.html#lint","title":"Lint","text":""},{"location":"tools/all.invoke_workflows.how_to_guide.html#lint_everything","title":"Lint Everything","text":"<pre><code>&gt; i lint --phases=\"amp_isort amp_class_method_order amp_normalize_import\namp_format_separating_line amp_black\" --files='$(find . -name \"\\*.py\")'\n</code></pre>"},{"location":"tools/all.llm_transform.how_to_guide.html","title":"All.llm transform.how to guide","text":""},{"location":"tools/all.llm_transform.how_to_guide.html#available_prompt_tags","title":"Available prompt tags:","text":"<p>code_1_unit_test code_apply_refactoring code_comment code_docstring code_propose_refactoring code_review code_review_and_improve code_type_hints code_unit_test md_rewrite md_summarize_short slide_colorize slide_colorize_points slide_improve</p>"},{"location":"tools/all.parquet.explanation.html","title":"All.parquet.explanation","text":""},{"location":"tools/all.parquet.explanation.html#pyarrow_parquet_management","title":"Pyarrow Parquet management","text":""},{"location":"tools/all.parquet.explanation.html#introduction","title":"Introduction","text":"<ul> <li> <p>What is Parquet?</p> <ul> <li>Parquet is a columnar storage file format that provides efficient data     compression and encoding schemes with enhanced performance to handle complex     data in bulk. It is designed to support complex data structures and is ideal     for big data processing.</li> </ul> </li> <li> <p>Core features of Parquet:</p> <ul> <li>Efficient storage and compression: Parquet uses efficient encoding and     compression techniques to store data in a columnar format, reducing the     storage space and improving the read performance</li> <li>Support for various compression algorithms: Parquet supports various     compression algorithms such as Snappy, Gzip, and LZO</li> <li>Support for complex data types: Parquet supports complex data types such as     nested fields, arrays, and maps, making it suitable for handling complex     data structures</li> <li>Efficient encoding and decoding schemes: Parquet provides efficient encoding     and decoding schemes for complex data types, improving the read and write     performance</li> </ul> </li> <li> <p>Pyarrow</p> <ul> <li>Pyarrow is a cross-language development platform for in-memory data that     provides efficient data interchange between Python and other languages. It     is designed to support complex data structures and is ideal for big data     processing.</li> <li>Pyarrow provides efficient data interchange between Python and other     languages, enabling seamless data exchange between different systems</li> <li>It supports various data types and complex data structures, making it     suitable for handling complex data processing tasks</li> </ul> </li> </ul>"},{"location":"tools/all.parquet.explanation.html#implementation_details","title":"Implementation details","text":"<p>The <code>helpers.hparquet</code> module provides a set of helper functions to manage Parquet files using the pyarrow library.</p>"},{"location":"tools/all.parquet.explanation.html#writing_parquet","title":"Writing Parquet","text":"<ul> <li> <p><code>to_parquet()</code></p> <ul> <li>Writes a Pandas DataFrame to a Parquet file</li> </ul> </li> <li> <p><code>to_partitioned_parquet()</code></p> <ul> <li>Writes a Pandas DataFrame to a partitioned Parquet files</li> </ul> </li> </ul>"},{"location":"tools/all.parquet.explanation.html#reading_parquet","title":"Reading Parquet","text":"<ul> <li><code>from_parquet()</code><ul> <li>Reads a Parquet file or partitional Parquet files into a Pandas DataFrame</li> </ul> </li> </ul>"},{"location":"tools/all.parquet.explanation.html#change_log","title":"Change log","text":""},{"location":"tools/all.parquet.explanation.html#2024-02-26_cmamp-1140","title":"2024-02-26: cmamp-1.14.0","text":"<ul> <li>Upgraded pyarrow to 14.0.2 -&gt; 15.0.0</li> <li>Update outomes in the tests due to the new version of pyarrow changed the size     of the some Parquet files</li> <li>Delete <code>partition_filename</code> from <code>to_partitioned_parquet()</code> function<ul> <li>Delete <code>partition_filename_cb</code> in the <code>pq.write_to_dataset()</code> call</li> </ul> </li> <li>Delete <code>partition_filename</code> arguments from the all calls of     <code>to_partitioned_parquet()</code> function</li> <li>In the <code>list_and_merge_pq_files()</code><ul> <li>In the <code>pq.ParquetDataset()</code> change <code>use_legacy_dataset=True</code> to     <code>partitioning=None</code></li> </ul> </li> <li>Introduce <code>purify_parquet_file_names()</code> in the <code>helpers/hunit_test.py</code></li> </ul>"},{"location":"tools/all.parquet.explanation.html#rationale","title":"Rationale","text":"<ul> <li>The upgrade to pyarrow 15.0.0 is necessary to keep the library up-to-date and     benefit from the latest features and improvements</li> <li>Due the <code>partition_filename_cb</code> is deprecated in the new version of pyarrow,     it is necessary to remove it from the <code>to_partitioned_parquet()</code> function<ul> <li>After discussion with the team, we decided to remove the     <code>partition_filename</code> from the <code>to_partitioned_parquet()</code> function</li> <li>The consequence of this change is that the Parquet files will be saved with     the default names like <code>&lt;guid&gt;-&lt;number&gt;.parquet</code> for example     <code>f3b3e3e33e3e3e3e3e3e3e3e3e3e3e3e-0.parquet</code></li> </ul> </li> <li>In the pyarrow 1.15.0 the <code>use_legacy_dataset</code> is deprecated and the     <code>partitioning</code> should be used instead<ul> <li>When we use the <code>partitioning=None</code> in <code>pq.ParquetDataset()</code> then we will     not use the partitioning and will not add the partitioned columns to the     dataset</li> <li>Some tests expect the Parquet files with the name <code>data.parquet</code>. The     <code>purify_parquet_file_names()</code> changes the names of the Parquet files to     <code>data.parquet</code> in the goldens</li> </ul> </li> </ul>"},{"location":"tools/all.parquet.explanation.html#2024-03-11_cmamptask7331_remove_ns_vs_us_hacks_related_to_pyarrow_1402","title":"2024-03-11: CmampTask7331 Remove <code>ns vs us</code> hacks related to Pyarrow 14.0.2","text":"<ul> <li>Remove time unit casting to <code>us</code> in the <code>to_parquet()</code></li> <li>Keep time unit casting to <code>ns</code> in the <code>from_parquet()</code></li> </ul>"},{"location":"tools/all.parquet.explanation.html#time_unit_conversion_when_writing_to_parquet","title":"Time unit conversion when writing to Parquet","text":"<ul> <li>Context: Before the upgrade to pyarrow 15.0.0, casting the time unit to     <code>us</code> was necessary to avoid the <code>pyarrow.lib.ArrowInvalid</code> exception.</li> <li>Problem: In pyarrow 15.0.0, this exception is no longer raised, and the     time unit is preserved correctly.</li> <li>Insight: Casting the time unit to <code>us</code> in the <code>to_parquet()</code> function is     no longer necessary and can be removed. At the same time, casting to <code>us</code> in     the <code>to_parquet()</code> function does not make sense since the time unit will be     converted back to <code>ns</code> in the <code>from_parquet()</code> function.</li> <li>Solution: Remove the casting of the time unit to <code>us</code> in the     <code>to_parquet()</code> function.</li> </ul>"},{"location":"tools/all.parquet.explanation.html#time_unit_conversion_when_reading_from_parquet","title":"Time unit conversion when reading from Parquet","text":"<ul> <li>Context: The pyarrow version prior to 15.0.0 did not correctly preserve     the time unit information when reading data back from Parquet files. That's     why casting the time unit to <code>ns</code> was necessary in the <code>from_parquet()</code>     function.</li> <li>Problem: Since the upgrade to pyarrow 15.0.0, casting the time unit to     <code>ns</code> is no longer necessary, as the new version of pyarrow correctly preserves     the time unit. See the Pyarrow issue for details:     https://github.com/apache/arrow/issues/33321 When reading Parquet files with a     time unit that is not in ['us', 'ns'], the <code>pyarrow.lib.ArrowInvalid</code>     exception could be raised. This could occur when Pyarrow attempts to cast the     time unit to a lower resolution. This behavior is tested in the     <code>test_parquet_files_with_mixed_time_units_2</code> test. In this case, the     alphabetical order of the files is important. The data from the first file     will be cast to the time unit of the rest of the files.</li> <li>Insight: The general approach is to preserve the time unit information     after reading data back from Parquet files. Currently, resolving this issue is     challenging because Parquet data is mixed with data from CSV files, which     convert the time unit to <code>ns</code> by default. Refer to CmampTask7331 for details.     https://github.com/cryptokaizen/cmamp/issues/7331</li> <li>Solution: Retain the casting of the time unit to <code>ns</code> in the     <code>from_parquet()</code> function.</li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html","title":"All.precommit.how to guide","text":""},{"location":"tools/all.precommit.how_to_guide.html#tools_for_code_quality","title":"Tools for code quality","text":""},{"location":"tools/all.precommit.how_to_guide.html#pre-commit","title":"<code>pre-commit</code>","text":"<ul> <li> <p>The documentation is https://pre-commit.com/</p> </li> <li> <p>The <code>.pre-commit-config.yaml</code> contains the configuration for <code>pre-commit</code></p> <p><code>yaml repos:     - repo: https://github.com/astral-sh/ruff-pre-commit         rev: v0.4.4         hooks:             - id: ruff     - repo: local         hooks:             - id: fixit-lint                 name: fixit lint                 entry: fixit lint                 language: system                 types: [python]             - id: pyrefly                 name: pyrefly lint                 entry: pyrefly lint                 language: system                 types: [python]</code></p> </li> <li> <p>To install:     <code>&gt; pre-commit install     &gt; pip install pre-commit ruff fixit pyrefly</code></p> <ul> <li>TODO(gp): Maybe we should install in the thin env?</li> </ul> </li> <li> <p>Run against all the files</p> <p>```bash</p> <p>pre-commit run --all-files ```</p> </li> <li> <p>Run against a subset of files     <code>bash     &gt; pre-commit run --files $(find helpers_root -type f)</code></p> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#ruff","title":"<code>ruff</code>","text":"<ul> <li> <p>Website: https://docs.astral.sh/ruff/</p> </li> <li> <p><code>ruff</code>:</p> <ul> <li>Is a fast Python linter and code formatter</li> <li>Supports over 700 linting rules from popular tools</li> <li>Is meant to be a single replacement for multiple tools (e.g., flake8,     pylint, isort, pyupgrade)</li> </ul> </li> </ul> <pre><code>&gt; ruff -h\n...\nCommands:\n    check    Run Ruff on the given files or directories\n    rule     Explain a rule (or all rules)\n    config   List or describe the available configuration options\n    linter   List all supported upstream linters\n    clean    Clear any caches in the current directory and any subdirectories\n    format   Run the Ruff formatter on the given files or directories\n    server   Run the language server\n    analyze  Run analysis over Python source code\n    version  Display Ruff's version\n    help     Print this message or the help of the given subcommand(s)\n...\n</code></pre>"},{"location":"tools/all.precommit.how_to_guide.html#config","title":"Config","text":"<ul> <li> <p>Often we want to exclude certain files in the repos, e.g., files under     <code>outcomes</code>:</p> <p>```bash</p> <p>ruff ... --exclude  '/outcomes/' --exclude '/import_check/example/' ```</p> </li> <li> <p>By default we use the following options in <code>pyproject.toml</code></p> <p>```text [tool.ruff] line-length = 81 target-version = \"py311\" fix = true exclude = [     \"/outcomes/\",     \"/import_check/example/\" ] output-format = \"concise\"</p> <p>[tool.ruff.lint]</p> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#e731_do_not_assign_a_lambda_expression_use_a_def","title":"E731 Do not assign a <code>lambda</code> expression, use a <code>def</code>","text":"<p>ignore = [\"E731\"] ```</p>"},{"location":"tools/all.precommit.how_to_guide.html#ruff_format","title":"<code>ruff format</code>","text":"<ul> <li>Format only:     <code>&gt; ruff format --line-length 80</code></li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#ruff_linter","title":"<code>ruff linter</code>","text":"<ul> <li>List all supported upstream linters     <code>bash     &gt; ruff linter    AIR Airflow    ERA eradicate     FAST FastAPI    YTT flake8-2020    ANN flake8-annotations     ASYNC flake8-async     ...</code></li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#ruff_analyze","title":"<code>ruff analyze</code>","text":"<ul> <li> <p>Can analyze the code base</p> <p>```text</p> <p>ruff analyze graph -h Generate a map of Python file dependencies or dependents</p> <p>Usage: ruff analyze graph [OPTIONS] [FILES]...</p> <p>Arguments:     [FILES]...  List of files or directories to include [default: .]</p> <p>Options:             --direction             The direction of the import map. By                                        default, generates a dependency map, i.e., a map from file to files that                                        it depends on. Use <code>--direction dependents</code> to generate a map from file                                        to files that depend on it [default: dependencies] [possible values:                                        dependencies, dependents]             --detect-string-imports            Attempt to detect imports from string literals             --preview                          Enable preview mode. Use <code>--no-preview</code> to disable     -h, --help                             Print help (see more with '--help') ```"},{"location":"tools/all.precommit.how_to_guide.html#linting","title":"Linting","text":""},{"location":"tools/all.precommit.how_to_guide.html#ruff_check","title":"<code>ruff check</code>","text":"<ul> <li> <p>Lint, auto-fix, and format code:</p> <p>```bash</p> <p>ruff check $DIR helpers/notebooks/cache.ipynb:cell 11:1:7: F821 Undefined name <code>dict_</code> helpers/notebooks/cache.ipynb:cell 12:3:7: F821 Undefined name <code>dict_</code> ... Found 62 errors. No fixes available (2 hidden fixes can be enabled with the <code>--unsafe-fixes</code> option). ```</p> </li> <li> <p>Interesting options are:</p> <p><code>text         --fix                              Apply fixes to resolve lint violations         --unsafe-fixes                     Include fixes that may not retain the original intent of the code         --ignore-noqa                      Ignore any `# noqa` comments         --output-format &lt;OUTPUT_FORMAT&gt;    Output serialization format for violations -o, --output-file &lt;OUTPUT_FILE&gt;        Specify file to write the linter output to (default: stdout)         --statistics                       Show counts for every rule with at least one violation         --add-noqa                         Enable automatic additions of `noqa` directives to failing lines         --show-settings                    See the settings Ruff will use to lint a given Python file</code></p> </li> <li> <p>Remove colors (useful for cfile)</p> <p>```bash</p> <p>ruff check . | less -R | cat | tee cfile vic ```</p> </li> <li> <p>To select only a subset of errors     <code># Skip the warnings     &gt; ruff check . --select E,F</code>     where:</p> <ul> <li>E: pycodestyle errors</li> <li>F: pyflakes errors</li> <li>W: warnings</li> <li>I: isort, etc.</li> </ul> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#ty","title":"<code>ty</code>","text":"<ul> <li> <p>It's best to run <code>ty</code> inside the dev container to get the type hints of the     installed packages     <code>``bash     docker&gt; sudo bash -c \"(source /venv/bin/activate; pip install --quiet ty)\"     docker&gt; ty check --output-format concise --color never --exclude '**/outcomes/**' --exclude '**/import_check/example/**' .     ...     error[unresolved-attribute] linters/utils.py:101:22: Type</code>list[str]<code>has no attribute</code>replace<code>error[unresolved-import] main_pytest.py:15:8: Cannot resolve imported module</code>junitparser<code>error[unresolved-import] tasks.py:122:10: Cannot resolve imported module</code>oms.lib_tasks_binance`     ...</p> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#to_format_the_output_for_a_cfile","title":"To format the output for a cfile:","text":"<p>docker&gt; ty check ... | cut -d' ' -f2- | tee cfile ```</p>"},{"location":"tools/all.precommit.how_to_guide.html#fixit","title":"<code>fixit</code>","text":"<ul> <li> <p>Website:     https://fixit.readthedocs.io/en/latest/index.html</p> </li> <li> <p><code>fixit</code></p> <ul> <li>Configurable linting framework with support for auto-fixes</li> <li>Custom \"local\" lint rules, and hierarchical configuration</li> <li>Built on LibCST.</li> </ul> </li> <li> <p>Fixit doesn't seem to support toml</p> </li> <li> <p>Run on <code>helpers</code> dir</p> <p>```bash</p> <p>fixit lint . ```</p> </li> <li> <p>Fix automatically     <code>bash     &gt; fixit fix --automatic</code></p> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#pyrefly","title":"<code>pyrefly</code>","text":"<ul> <li> <p>Website: https://pyrefly.org/</p> </li> <li> <p>Pyrefly is a high-performance static type checker for Python</p> <ul> <li>Type Checking: Analyzes Python code for type consistency before runtime</li> <li>Type Inference: Infers types for local variables and return values, even in     un-annotated code</li> <li>Configurable through <code>pyproject.toml</code><ul> <li>TODO: Is this true?</li> </ul> </li> </ul> <p>```bash</p> <p>pyrefly -h</p> <p>Usage: pyrefly [OPTIONS]  <p>Commands:     check        Full type checking on a file or a project     dump-config  Dump info about pyrefly's configuration. Use by replacing              <code>check</code> with <code>dump-config</code> in your pyrefly invocation     init         Initialize a new pyrefly config in the given directory, or              migrate an existing mypy or pyright config to pyrefly     lsp          Start an LSP server</p> <p>Options:     -j, --threads   Number of threads to use for parallelization.                          Setting the value to 1 implies sequential execution without any parallelism.                          Setting the value to 0 means to pick the number of threads automatically                          using default heuristics [default: 0]             --color       Controls whether colored output is used ``` <li> <p>It's best to run <code>pyrefly</code> inside the dev container to get the type hints of     the installed packages</p> <p><code>bash docker&gt; sudo bash -c \"(source /venv/bin/activate; pip install --quiet pyrefly)\"</code></p> <p><code>bash docker&gt; pyrefly check --color=never --output-format=min-text --project-excludes '**/outcomes/**' --project-excludes '**/import_check/example/**' --project-excludes '**/mkdocs.venv/**'</code></p> </li>"},{"location":"tools/all.precommit.how_to_guide.html#call_graph_and_dependencies","title":"Call graph and dependencies","text":""},{"location":"tools/all.precommit.how_to_guide.html#pycg_practical_call_graph_generator","title":"PyCG (Practical Call Graph Generator)","text":"<ul> <li>GitHub: https://github.com/vitsalis/PyCG <ul> <li>Read only (349 stars)</li> </ul> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#code2flow","title":"code2flow","text":"<ul> <li> <p>GitHub: https://github.com/scottrogowski/code2flow</p> </li> <li> <p>Install with:     <code>bash     &gt; sudo /bin/bash \\-c \"(source /venv/bin/activate; pip install code2flow)\"</code></p> </li> <li> <p>Run with:     <code>bash     &gt; code2flow helpers/hmarkdown*.py     &gt; open out.png</code></p> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#pycallgraph2","title":"pycallgraph2","text":"<ul> <li> <p>GitHub: https://github.com/daneads/pycallgraph2</p> <ul> <li>236 stars</li> </ul> </li> <li> <p>Install with:     <code>bash     docker&gt; sudo /bin/bash \\-c \"(source /venv/bin/activate; pip install pycallgraph2)\"</code></p> </li> <li> <p>Run with:     <code>&gt; pycallgraph graphviz \\-- helpers/hmarkdown*.py</code></p> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#py-call-graph","title":"py-call-graph","text":"<ul> <li>GitHub: https://github.com/lewiscowles1986/py-call-graph</li> <li>Doc:<ul> <li>https://pycallgraph.readthedocs.io/en/master/ </li> <li>https://pypi.org/project/python-call-graph/</li> </ul> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#pyan","title":"pyan","text":"<ul> <li>https://github.com/davidfraser/pyan </li> <li> <p>692 stars</p> </li> <li> <p>Install with:     <code>bash     &gt; sudo /bin/bash -c \"(source /venv/bin/activate; pip install pyan3==1.1.1)\"     # Top of the tree is broken (https://github.com/Technologicat/pyan/issues/72)     &gt; sudo /bin/bash -c \"(source /venv/bin/activate; pip install pyan3)\"</code></p> </li> <li> <p>Run with:     <code>bash     &gt; pyan ./helpers/hmarkdown_filtering.py --dot --uses --no-defines &gt; callgraph.dot     &gt; ./dev_scripts_helpers/documentation/dockerized_graphviz.py -i callgraph.dot -o c.png       &gt; open c.png</code></p> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#snakeviz","title":"<code>SnakeViz</code>","text":"<ul> <li>Works on runtime profiling (not static), gives function timing &amp; hierarchy</li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#dependencies","title":"Dependencies","text":""},{"location":"tools/all.precommit.how_to_guide.html#pylint_pyreverse","title":"<code>pylint pyreverse</code>","text":"<ul> <li>GitHub: https://github.com/pylint-dev/pylint </li> <li>Doc:<ul> <li>https://pylint.readthedocs.io/en/latest/</li> <li>https://pylint.readthedocs.io/en/latest/additional_tools/pyreverse/index.html</li> </ul> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#pydeps","title":"<code>Pydeps</code>","text":"<ul> <li>GitHub: https://github.com/thebjorn/pydeps </li> <li> <p>Doc: https://pydeps.readthedocs.io/en/latest/</p> </li> <li> <p>Run with     <code>&gt; pydeps helpers --noshow --show-dot -o import_graph.dot</code></p> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#snakefood","title":"<code>snakefood</code>","text":"<ul> <li>GitHub: https://github.com/blais/snakefood</li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#measuring_complexity","title":"Measuring complexity","text":""},{"location":"tools/all.precommit.how_to_guide.html#radon","title":"<code>radon</code>","text":"<ul> <li>Static analysis for complexity, can complement class structure graphs</li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#xenon","title":"<code>xenon</code>","text":"<ul> <li>Enforces complexity thresholds based on Radon</li> <li>Built on Radon</li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#lizard","title":"<code>lizard</code>","text":"<ul> <li>Measures cyclomatic complexity for many languages</li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#finding_dead_code","title":"Finding dead code","text":""},{"location":"tools/all.precommit.how_to_guide.html#vulture","title":"Vulture","text":"<ul> <li>Finds dead (unused) code</li> <li>GitHub: https://github.com/jendrikseipp/vulture</li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#detecting_copy-paste","title":"Detecting copy-paste","text":""},{"location":"tools/all.precommit.how_to_guide.html#pylint_symilar","title":"<code>pylint symilar</code>","text":"<ul> <li>Docs: https://pylint.readthedocs.io/en/latest/additional_tools/symilar/index.html</li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#lizard_1","title":"<code>lizard</code>","text":"<ul> <li> <p>Install with:     <code>bash     &gt; pip install lizard</code></p> </li> <li> <p>Run with:     ```bash     &gt; lizard -Eduplicate helpers/test/test_hmarkdown_coloring.py helpers/test/test_hmarkdown.py</p> <p>Duplicate block: -------------------------- ./dev_scripts_helpers/llms/llm_prompts.py:1129 \\~ 1151 ./dev_scripts_helpers/llms/llm_prompts.py:1151 \\~ 1177 ./dev_scripts_helpers/llms/llm_prompts.py:1177 \\~ 1219 ./dev_scripts_helpers/llms/llm_prompts.py:1219 \\~ 1256 ./dev_scripts_helpers/llms/llm_prompts.py:1256 \\~ 1280 ./dev_scripts_helpers/llms/llm_prompts.py:1280 \\~ 1312 ./dev_scripts_helpers/llms/llm_prompts.py:1377 \\~ 1425 ```</p> </li> </ul>"},{"location":"tools/all.precommit.how_to_guide.html#jscpd","title":"<code>jscpd</code>","text":"<ul> <li> <p>Install with:     <code>docker&gt; sudo npm install -g jscpd</code></p> </li> <li> <p>Run with:     <code>&gt; jscpd --languages python --reporters console,path/to/your/code     &gt; jscpd --format=python --reporters console helpers</code></p> </li> <li> <p>It doesn't seem to work</p> </li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html","title":"All.profiling.how to guide","text":""},{"location":"tools/all.profiling.how_to_guide.html#profiling","title":"Profiling","text":""},{"location":"tools/all.profiling.how_to_guide.html#profiling_1","title":"Profiling","text":""},{"location":"tools/all.profiling.how_to_guide.html#profiling_end-to-end_a_command_line","title":"Profiling end-to-end a command line","text":"<ul> <li> <p>You can use the time-tested Linux <code>time</code> command to profile both time and     memory</p> <p>```bash</p> <p>/usr/bin/time -v COMMAND 2&gt;&amp;1 | tee time.log</p> <p>Command being timed: \"...COMMAND...\" User time (seconds): 187.70 System time (seconds): 16.27 Percent of CPU this job got: 96% Elapsed (wall clock) time (h:mm:ss or m:ss): 3:31.38 Average shared text size (kbytes): 0 Average unshared data size (kbytes): 0 Average stack size (kbytes): 0 Average total size (kbytes): 0 Maximum resident set size (kbytes): 13083892 Average resident set size (kbytes): 0 Major (requiring I/O) page faults: 0 Minor (reclaiming a frame) page faults: 9911066 Voluntary context switches: 235772 Involuntary context switches: 724 Swaps: 0 File system inputs: 424 File system outputs: 274320 Socket messages sent: 0 Socket messages received: 0 Signals delivered: 0 Page size (bytes): 4096 Exit status: 0 ```</p> </li> <li> <p>Information about the spent time are:     <code>User time (seconds): 187.70     System time (seconds): 16.27     Percent of CPU this job got: 96%     Elapsed (wall clock) time (h:mm:ss or m:ss): 3:31.38</code></p> </li> <li> <p>The relevant part is the following line representing the amount of resident     memory (which is ~13GB) <code>Maximum resident set size (kbytes): 13083892</code></p> </li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#profiling_python_code_from_command_line","title":"Profiling Python code from command line","text":""},{"location":"tools/all.profiling.how_to_guide.html#cprofile","title":"cProfile","text":""},{"location":"tools/all.profiling.how_to_guide.html#install_in_a_docker_container","title":"Install in a Docker container","text":"<ul> <li>From <code>devops/docker_build/install_cprofile.sh</code> <code>bash     &gt; sudo apt-get install -y python3-dev     &gt; sudo apt install -y libgraphviz-dev     &gt; sudo apt-get install -y graphviz     &gt; pip install gprof2dot</code></li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#how_to_use_with_workflow","title":"How to use with workflow","text":"<ul> <li>There is a script that runs the flow <code>amp/dev_scripts/run_profiling.sh</code></li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#how_to_use_manually","title":"How to use manually","text":"<ul> <li>You need to run the code first with profiling enabled to collect the profiling     data in a binary file (often called <code>prof.bin</code>).     <code>bash     # Profile a Python script.     &gt; python -m cProfile -o prof.bin ${CMD}</code></li> <li> <p>To profile a unit test you can run:</p> <p>```bash</p> </li> <li> <p>Plotting the results</p> <p>```bash</p> <p>gprof2dot -f pstats profile | dot -Tpng -o output.png gprof2dot -n 10 -f pstats profile | dot -Tpng -o output.png gprof2dot -n 10 -f pstats profile -l \"extract_tables_from_forms\" | dot -Tpng -o output.png ```</p> <p>How to read a graph: https://nesi.github.io/perf-training/python-scatter/profiling-cprofile</p> </li> <li> <p>Gprof2dot has lots of interesting options to tweak the output, e.g.,     <code>bash     &gt; gprof2dot -h     ...     -n PERCENTAGE, --node-thres=PERCENTAGE     eliminate nodes below this threshold [default: 0.5]     -e PERCENTAGE, --edge-thres=PERCENTAGE     eliminate edges below this threshold [default: 0.1]     --node-label=MEASURE measurements to on show the node (can be specified     multiple times): self-time, self-time-percentage,     total-time or total-time-percentage [default: total-     time-percentage, self-time-percentage]     -z ROOT, --root=ROOT prune call graph to show only descendants of     specified     root function     -l LEAF, --leaf=LEAF prune call graph to show only ancestors of     specified     leaf function     --depth=DEPTH prune call graph to show only descendants or ancestors     until specified depth     --skew=THEME_SKEW skew the colorization curve. Values &amp;lt; 1.0 give     more     variety to lower percentages. Values &gt; 1.0 give less     variety to lower percentages     -p FILTER_PATHS, --path=FILTER_PATHS     Filter all modules not in a specified path     ...</code></p> </li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#profile_a_unit_test","title":"Profile a unit test.","text":"<p>python -m cProfile -o profile edgar/forms8/test/test_edgar_utils.py python -m cProfile -o profile -m pytest edgar/forms8/test/test_edgar_utils.py::TestExtractTablesFromForms::test_table_extraction_example_2 ```</p>"},{"location":"tools/all.profiling.how_to_guide.html#process_profpy","title":"process_prof.py","text":"<ul> <li>You can use the script <code>dev_scripts/process_prof.py</code> to automate some tasks:<ul> <li>Top-level statistics</li> <li>Plotting the call-graph</li> <li>Custom statics</li> </ul> </li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#line_profiler","title":"line_profiler","text":"<ul> <li> <p>CProfile allows to break down the execution time into function calls, while     kernprof allows to profile a function line by line.</p> </li> <li> <p>GitHub: https://github.com/pyutils/line_profiler</p> </li> <li> <p>Install with:     <code>bash     &gt; pip install line_profiler</code></p> </li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#how_to_use","title":"How to use","text":"<ul> <li> <p>Instrument the code to profile:</p> <p>```python import line_profiler profiler = line_profiler.LineProfiler()</p> </li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#print_the_results_at_the_end_of_the_run","title":"Print the results at the end of the run.","text":"<p>import atexit</p> <p>def exit_handler():         profiler.print_stats()</p> <p>atexit.register(exit_handler)</p> <p>@profiler def function():</p> <p>...     ```</p> <ul> <li>Through command line:     <code>bash     &gt; kernprof -o prof.lprof -l $cmd     ...     Wrote profile results to run_process_forecasts.py.lprof</code></li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#pytest-profiling","title":"pytest-profiling","text":"<ul> <li> <p>Webpage: https://pypi.org/project/pytest-profiling</p> </li> <li> <p>Install it with     <code>bash     &gt; pip install pytest-profiling</code></p> </li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#how_to_use_1","title":"How to use","text":"<pre><code> &gt; pytest --profile ./amp/core/dataflow_model/test/test_pnl_simulator.py::TestPnlSimulator2::test_perf1 -s\n</code></pre>"},{"location":"tools/all.profiling.how_to_guide.html#profiling_in_a_jupyter_notebook","title":"Profiling in a Jupyter notebook","text":"<ul> <li>You can find all of the examples below in action in the     <code>amp/core/notebooks/time_memory_profiling_example.ipynb</code> link.</li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#time_profilers","title":"Time profilers","text":"<ul> <li>In a notebook, execute cell with <code>%time</code> cell-magic:     <code>python %%time func()</code></li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#by_function","title":"By function","text":"<ul> <li> <p>We prefer cProfile for profiling and gprof2dot for visualization.</p> </li> <li> <p>The documentation does not state this, but <code>%prun</code> magic uses cProfile under     the hood, so we can use it in the notebook instead</p> <p>```python</p> </li> <li> <p>This will output something like this:     </p> </li> <li> <p>If you open the output image in the new tab, you can zoom in and look at the     graph in detail.</p> </li> <li> <p>Gprof2dot supports thresholds that make output more readable:</p> <p><code>python !gprof2dot -n 5 -e 5 -f pstats tmp.pstats | dot -Tpng -o output.png dspl.Image(filename=\"output.png\")</code></p> </li> <li> <p>This will filter the output into something like this:     </p> </li> </ul>"},{"location":"tools/all.profiling.how_to_guide.html#we_can_suppress_output_to_the_notebook_by_specifying_-q","title":"We can suppress output to the notebook by specifying \"-q\".","text":"<p>%%prun -D tmp.pstats func() !gprof2dot -f pstats tmp.pstats | dot -Tpng -o output.png dspl.Image(filename=\"output.png\") ```</p>"},{"location":"tools/all.profiling.how_to_guide.html#memory_profilers","title":"Memory profilers","text":"<ul> <li> <p>We prefer using memory-profiler.</p> </li> <li> <p>Peak memory</p> <p><code>bash %%memit func()</code></p> </li> <li> <p>Memory by line     <code>bash     %mprun -f func func()</code></p> </li> </ul>"},{"location":"tools/all.pycharm.how_to_guide.html","title":"All.pycharm.how to guide","text":""},{"location":"tools/all.pycharm.how_to_guide.html#pycharm","title":"Pycharm","text":""},{"location":"tools/all.pycharm.how_to_guide.html#pycharm_1","title":"PyCharm","text":""},{"location":"tools/all.pycharm.how_to_guide.html#current_situation","title":"Current situation","text":"<p>There are multiple ways to develop on a remote server using PyCharm</p> <ol> <li> <p>VNC approach         - PyCharm runs locally on the server using a \"virtual screen\"         - Your laptop interacts with a VNC server to get the GUI locally         - Pros:             - Everything works             - You can run anything like you are local on the server, since you are in                 practice just using a virtual screen         - Cons:             - Without enough bandwidth it's slow and not snappy enough</p> </li> <li> <p>X11 approach         - Same as VNC, but instead of sending bitmaps through VNC, a \"compressed\"             version of the GUI is sent to the local computer directly         - Pros:             - Maybe faster than VNC             - PyCharm window is like a native window on your laptop         - Cons:             - X11 is old crap developed long time again and not really supported any                 more             - One needs to tunnel X11 traffic, set things up, and so on</p> </li> <li> <p>PyCharm Gateway         - New client-server architecture for PyCharm             - A \"headless\" PyCharm runs on the server             - A GUI client PyCharm runs on your laptop         - Pros             - It's as fast as possible, probably as fast as running locally         - Cons             - Need a PyCharm pro license (not a problem, we have money)             - It's not super polished: kind of beta, but it will get better and better</p> </li> <li> <p>PyCharm Remote Set-up             - Edit locally and then PyCharm moves the files back and forth         - Pros             - Only requires ssh         - Cons             - You can't run / debug remotely</p> </li> <li> <p>Approach 1) seems to require lots of memory and CPU and it's not really fast.</p> </li> <li> <p>Approach 2) works but it's a pain to set-up and slow.</p> </li> <li> <p>We want to try with 3)</p> <ul> <li>TODO(gp): @Juraj pls a short tutorial on how to install</li> <li>TODO(gp): @Juraj understand if it works, if it's fast, and if it requires     less memory</li> </ul> </li> </ol>"},{"location":"tools/all.pycharm.how_to_guide.html#how_to_run_our_cmamp_container_directly_from_pycharm","title":"How to run our cmamp container directly from PyCharm","text":"<ul> <li>PyCharm allows to run commands directly inside a container<ul> <li>See     https://www.jetbrains.com/help/pycharm/using-docker-as-a-remote-interpreter.html</li> </ul> </li> <li> <p>In fact when we do <code>i docker_bash</code> we launch a container and run bash inside     it, but PyCharm can do the same thing</p> </li> <li> <p>TODO(gp): @Juraj Let's both try this. There are some notes below about it</p> </li> </ul>"},{"location":"tools/all.pycharm.how_to_guide.html#how_to_review_a_pr_inside_pycharm","title":"How to review a PR inside Pycharm","text":"<ul> <li> <p>CTRL + SHIFT + A -&gt; View Pull Request</p> </li> <li> <p></p> </li> </ul>"},{"location":"tools/all.pycharm.how_to_guide.html#how_to_edit_remote_code","title":"How to edit remote code","text":"<ul> <li> <p>You need to use a certain local directory (e.g.,     /Users/saggese/src/commodity_research1) and a remote directory (e.g.,     /wd/saggese/src/commodity_research1)</p> </li> <li> <p>They need to be synced at the same git branch (e.g., master or     AmpTask1112_Audit_amp_Docker_system_03)</p> </li> <li> <p>Set-up Deployment</p> </li> </ul> <p></p> <p></p> <ul> <li>The deployment options are</li> </ul> <p></p> <ul> <li>You can see what file is changed in the file transfer window:</li> </ul> <p></p> <p>pycharm</p> <ul> <li>Develop on one node, sync, run on the server</li> <li> <p>Run local application with venv</p> </li> <li> <p>Database</p> </li> <li> <p>Run application inside Docker</p> </li> <li> <p>Run application remotely inside Docker</p> </li> </ul>"},{"location":"tools/all.pycharm.how_to_guide.html#general_ssh_config","title":"General ssh config","text":"<ul> <li> <p>File | Settings | Tools | SSH Configurations</p> <ul> <li></li> </ul> </li> <li> <p>Once setup, ssh config can be used for all tools in PyCharm.</p> <ul> <li>Remote Interpreter</li> <li>DataGrip</li> <li>Deployment</li> <li>Etc.</li> </ul> </li> </ul>"},{"location":"tools/all.pycharm.how_to_guide.html#db_connection_via_ssh","title":"DB connection via ssh","text":"<p>Note: PyCharm Professional DataGrip is used as an example. There are numerous open source alternatives such as Beaver. Config below should apply to them also.</p> <ul> <li> <p>To add a new data source in DataGrip, go to the database section in the lower     left corner.</p> <ul> <li></li> </ul> </li> <li> <p>Then pick your desired data source from the dropdown in the upper right     corner.</p> <ul> <li></li> </ul> </li> <li> <p>You will be presented with a dummy config that needs to be replaced with     proper data as shown below.</p> <ul> <li></li> </ul> </li> <li> <p>Before that is done, be sure that proper ssh info is added in SSH/SSL section.</p> <ul> <li></li> </ul> </li> </ul>"},{"location":"tools/all.pycharm.how_to_guide.html#deployment_with_remote_repository_through_sync","title":"Deployment with remote repository (through sync)","text":"<p>Note: Before setting up deployment, pull the cmamp repo on EC2 instance and use the same name as on your local machine (example: cmamp1). Always try to keep both repos in sync via git. For more subtle and simpler changes use File | Reload All From Disk . This will upload changes to the remote repo.</p> <ul> <li> <p>Tools | Deployment | Configuration</p> <ul> <li></li> <li></li> </ul> </li> <li> <p>Tools | Deployment | Options</p> <ul> <li><ul> <li>Uncheck \"Skip external changes\" and check \"Delete remote files\"</li> </ul> </li> </ul> </li> <li> <p>Tools | Deployment | Automatic Upload</p> <ul> <li>Check it</li> </ul> </li> <li> <p>Tools | Deployment | Browse Remote Host</p> <ul> <li></li> </ul> </li> </ul>"},{"location":"tools/all.pycharm.how_to_guide.html#pudb_-_remote_debugging_-_todo","title":"PUDB - remote debugging - ToDo","text":""},{"location":"tools/all.pycharm.how_to_guide.html#how_to_run_tests_inside_a_container","title":"How to run tests inside a container","text":"<ul> <li> <p>https://www.jetbrains.com/help/pycharm/using-docker-compose-as-a-remote-interpreter.html#docker-compose-remote</p> </li> <li> <p>Note that the \"start SSH session...\" action is available only in PyCharm     Professional Edition, while the terminal itself is available in both     Professional and Community editions.</p> </li> </ul>"},{"location":"tools/all.pycharm.how_to_guide.html#installing_pycharm_professional","title":"Installing PyCharm Professional","text":""},{"location":"tools/all.pycharm.how_to_guide.html#windows","title":"Windows","text":"<ol> <li>Download the installer using this    link</li> <li>Run the installer and follow the wizard steps.</li> <li>To run PyCharm, find it in the Windows Start menu or use the desktop    shortcut.</li> </ol>"},{"location":"tools/all.pycharm.how_to_guide.html#macos","title":"macOS","text":"<p>There are separate disk images for Intel and Apple Silicon processors.</p> <ol> <li> <p>Download the image, based on your processor using this    link</p> </li> <li> <p>Mount the image and drag the PyCharm app to the Applications folder.</p> </li> <li> <p>Run the PyCharm app from the Applications directory, Launchpad, or Spotlight.</p> </li> </ol>"},{"location":"tools/all.pycharm.how_to_guide.html#linux","title":"Linux","text":"<p>**Using tar archives **</p> <ol> <li> <p>Download the tar archive using this    link</p> </li> <li> <p>Unpack the pycharm-*.tar.gz file to a different folder, if your current    Download folder doesn't support file execution:    ```</p> <p>tar xzf pycharm-*.tar.gz -C     ``` <li> <p>The recommended installation location according to the filesystem hierarchy     standard (FHS) is <code>/opt</code>. To install PyCharm into this directory, enter the     following command:     <code>&gt; sudo tar xzf pycharm-\\*.tar.gz -C /opt/</code></p> </li> <li> <p>Switch to the bin subdirectory:    ```</p> <p>cd /pycharm-*/bin    # E.g., cd /opt/pycharm-*/bin    ``` <li> <p>Run pycharm.sh from the bin subdirectory    ```</p> <p>sh pycharm.sh    ```</p> </li> <p>Using snap packages</p> <ol> <li> <p>For Ubuntu 16.04 and later, you can use snap packages to install PyCharm.    ```</p> <p>sudo snap install pycharm-professional --classic    # or sudo snap install pycharm-community --classic    ```</p> </li> <li> <p>Run in the Terminalu    ```</p> <p>pycharm-professional    # or pycharm-community    # or pycharm-educational    ```</p> </li> </ol>"},{"location":"tools/all.pycharm.how_to_guide.html#connecting_via_pycharm_gateway_ssh","title":"Connecting via PyCharm gateway (SSH)","text":"<p>The first thing you need to do is sign up for a free trial license or use it if it already have</p> <p>Then make sure you have a VPN connection to our VPC</p> <ol> <li>Click on Connect via SSH</li> <li>Into Username: write &lt;&gt; Example: richard <li>Into Host: write &lt;&gt; Example: 172.30.2.136 <li>Mark the Specify private key check box and locate the private key from the         zip which was sent in the onboarding process. Example: crypto.pub</li> <li>Leave Port: 22 as it is.</li> <li>Click on Check Connection and Continue.</li> <li>Select IDE version: PyCharm Py 213.6777.x</li> <li>Locate your directory. Example: /data/richard</li> <li>Click on Download and Start IDE.</li>"},{"location":"tools/all.pycharm.how_to_guide.html#connecting_via_vnc","title":"Connecting via VNC","text":"<ul> <li>Make sure you have a VPN connection.</li> </ul> <p>Installing VNC</p> <ul> <li>Install VNC using this link:<ul> <li>https://www.realvnc.com/en/connect/download/viewer/windows/</li> </ul> </li> </ul> <p>Sysadmin has sent you:</p> <ul> <li><code>os_password.txt</code></li> <li>Your username <code>$USER</code></li> <li>A key <code>crypto.pub</code> that looks like:     <code>-----BEGIN OPENSSH PRIVATE KEY----- # gitleaks:allow     b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn     NhAAAAAwEAAQAAAYEA0IQsLy1lL3bhPT+43sht2/m9tqZm8sEQrXMAVtfm4ji/LXMr7094 ...     hakqVTlQ2sr0YTAAAAHnNhZ2dlc2VAZ3BtYWMuZmlvcy1yb3V0ZXIuaG9tZQECAwQ=     -----END OPENSSH PRIVATE KEY-----</code></li> </ul> <p>Let's say you are connected via VNC.</p> <ol> <li>Login into the OS.</li> <li>Run pycharm.sh using terminal (should be there)    ``` <p>bash /opt/pycharm-community-2021.2.3/bin/pycharm.sh    ```</p> </li> </ol>"},{"location":"tools/all.pycharm.how_to_guide.html#configuration","title":"Configuration","text":""},{"location":"tools/all.pycharm.how_to_guide.html#reflow","title":"Reflow","text":"<ul> <li>Set the reflow to reindent<ul> <li></li> </ul> </li> </ul>"},{"location":"tools/all.pycharm.how_to_guide.html#some_recommended_plug-ins","title":"Some recommended plug-ins","text":"<ul> <li>Vim</li> <li>Grazie</li> <li>Wrap-to-column</li> <li>GitHub Copilot</li> </ul>"},{"location":"tools/all.reference_template.html","title":"All.reference template","text":"<p> <pre><code>&lt;title&gt;Reference Guide: &amp;lt;FileName&amp;gt;&lt;/title&gt;\n\n&lt;description&gt;\n    TODO: Write a short paragraph describing what the script does and its key use cases.\n&lt;/description&gt;\n\n&lt;section name=\"WhatItDoes\"&gt;\n</code></pre>"},{"location":"tools/all.reference_template.html#what_it_does","title":"What It Does","text":"<pre><code>    &lt;objective&gt;\n        &lt;bullet&gt;TODO: Explain core objective #1&lt;/bullet&gt;\n        &lt;bullet&gt;TODO: Explain core objective #2&lt;/bullet&gt;\n        &lt;bullet&gt;TODO: Explain core objective #3&lt;/bullet&gt;\n    &lt;/objective&gt;\n\n    &lt;sample_behavior&gt;\n        This script performs the following:\n        &lt;bullet&gt;Detects fenced code blocks (e.g., PlantUML, Mermaid, TikZ)&lt;/bullet&gt;\n        &lt;bullet&gt;Renders them using the appropriate tool&lt;/bullet&gt;\n        &lt;bullet&gt;Comments out the original code block&lt;/bullet&gt;\n        &lt;bullet&gt;Inserts an image reference like &lt;code&gt;![](image.png)&lt;/code&gt;&lt;/bullet&gt;\n    &lt;/sample_behavior&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"InputAndOutput\"&gt;\n</code></pre>"},{"location":"tools/all.reference_template.html#input_and_output","title":"Input And Output","text":"<pre><code>    &lt;inputs&gt;\n        TODO: List accepted input formats (e.g., .md, .tex)\n    &lt;/inputs&gt;\n    &lt;outputs&gt;\n        TODO: Describe the output file format and its expected content\n    &lt;/outputs&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"SupportedFileTypes\"&gt;\n</code></pre>"},{"location":"tools/all.reference_template.html#supported_file_types","title":"Supported File Types","text":"<pre><code>    &lt;sample_behavior&gt;\n    &lt;filetype name=\".md\"&gt;TODO: Markdown processing logic&lt;/filetype&gt;\n    &lt;filetype name=\".tex\"&gt;TODO: LaTeX file processing logic&lt;/filetype&gt;\n\n    &lt;use_cases&gt;\n        &lt;case description=\"Render Markdown to a new file\"&gt;\n            &lt;code_block language=\"bash\"&gt;\n</code></pre> <p>render_images.py -i ABC.md -o XYZ.md --action render --run_dockerized   render_images.py -i ABC.md --action render --run_dockerized   render_images.py -i ABC.md --action open --run_dockerized  </p> <pre><code>&lt;/section&gt;\n\n&lt;section name=\"FlagOptions\"&gt;\n</code></pre>"},{"location":"tools/all.reference_template.html#flag_options","title":"Flag Options","text":"<pre><code>    &lt;sample_behavior&gt;\n    &lt;code_block language=\"bash\"&gt;\n    &lt;flag name=\"-h, --help\"&gt;Show help message and exit&lt;/flag&gt;\n    &lt;flag name=\"-i, --in_file_name\"&gt;Input file path&lt;/flag&gt;\n    &lt;flag name=\"-o, --out_file_name\"&gt;Output file path&lt;/flag&gt;\n    &lt;flag name=\"--action {open,render}\"&gt;Action to execute&lt;/flag&gt;\n    &lt;flag name=\"--skip_action {open,render}\"&gt;Action to skip&lt;/flag&gt;\n    &lt;flag name=\"--all\"&gt;Run all available actions&lt;/flag&gt;\n    &lt;flag name=\"--dry_run\"&gt;Simulate run (update files but don't render images)&lt;/flag&gt;\n    &lt;flag name=\"--dockerized_force_rebuild\"&gt;Force Docker container rebuild&lt;/flag&gt;\n    &lt;flag name=\"--dockerized_use_sudo\"&gt;Use sudo inside Docker container&lt;/flag&gt;\n    &lt;flag name=\"-v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}\"&gt;Set logging level&lt;/flag&gt;\n    &lt;/code_block&gt;\n    &lt;/sample_behavior&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"Examples\"&gt;\n</code></pre>"},{"location":"tools/all.reference_template.html#examples","title":"Examples","text":"<pre><code>    &lt;example title=\"Render to a new Markdown file\"&gt;\n        &lt;code_block language=\"bash\"&gt;\n</code></pre> <p>render_images.py -i lesson.md -o lesson.rendered.md --action render --run_dockerized   render_images.py -i lesson.md --action render --run_dockerized   render_images.py -i lesson.md --action open --run_dockerized   render_images.py -i lesson.md -o /tmp/out.md --dry_run  </p> <pre><code>&lt;/section&gt;\n\n&lt;section name=\"ErrorsAndFixes\"&gt;\n</code></pre>"},{"location":"tools/all.reference_template.html#errors_and_fixes","title":"Errors And Fixes","text":"<pre><code>    &lt;error&gt;\n        &lt;issue&gt;TODO: Describe a common error&lt;/issue&gt;\n        &lt;cause&gt;TODO: Likely cause&lt;/cause&gt;\n        &lt;solution&gt;TODO: Suggested fix or workaround&lt;/solution&gt;\n    &lt;/error&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"Dependencies\"&gt;\n</code></pre>"},{"location":"tools/all.reference_template.html#dependencies","title":"Dependencies","text":"<pre><code>&lt;dependency&gt;\n    &lt;bullet&gt;TODO: Docker (for --run_dockerized support)&lt;/bullet&gt;\n    &lt;bullet&gt;TODO: Graphviz, PlantUML, or other rendering tools depending on code block types&lt;/bullet&gt;\n    &lt;bullet&gt;TODO: Any other external CLI utilities used in subprocess calls&lt;/bullet&gt;\n&lt;/dependency&gt;\n</code></pre>"},{"location":"tools/all.selenium.how_to_guide.html","title":"All.selenium.how to guide","text":""},{"location":"tools/all.selenium.how_to_guide.html#selenium","title":"Selenium","text":""},{"location":"tools/all.selenium.how_to_guide.html#what_is_selenium","title":"What is Selenium","text":"<ul> <li> <p><code>Selenium</code> is an open-source automation tool primarily used for testing web     applications. It allows developers to programmatically control web browsers,     making it valuable for data crawling and web scraping when interacting with     dynamic, <code>JavaScript</code> driven websites. With Selenium, you can simulate user     actions like clicking buttons, filling forms, and navigating pages, which     makes it particularly effective for extracting data from websites that do not     provide a direct <code>API</code> or have content loaded dynamically via <code>JavaScript</code></p> </li> <li> <p>Key Features for Data Crawling:</p> <ul> <li><code>Browser Automation</code>: <code>Selenium</code> supports multiple browsers (e.g., <code>Chrome</code>,     <code>Firefox</code>, <code>Safari</code>), enabling you to simulate a real user's interaction     with the webpage</li> <li><code>JavaScript Handling:</code>Selenium<code>can interact with</code>JavaScript` based     elements, ensuring data loaded dynamically is accessible</li> <li><code>Customizable Waiting Strategies</code>: <code>Selenium</code> offers both explicit and     implicit waits, allowing scripts to wait for specific elements or     conditions, essential for dealing with varying page load times</li> </ul> </li> </ul>"},{"location":"tools/all.selenium.how_to_guide.html#installation","title":"Installation","text":"<ul> <li>The following installation is based on <code>Chrome browser</code> and <code>Linux OS</code> for a     local machine environment and not our dev servrer. It can be used for any     other OS and browsers</li> <li> <p>Check your google chrome verion</p> <ul> <li>Open Google Chrome, type <code>chrome://settings/help</code> and press Enter. You\u2019ll     see the Chrome version listed under \"Google Chrome\" (e.g.,     <code>Version 118.0.5993.89</code>)</li> </ul> </li> <li> <p>There are 2 ways to install chrome driver</p> <ul> <li>Using Web Browser<ul> <li>Check what <code>ChromeDriver</code> version in compatible with your chrome and     download at     ChromeDriver Download</li> </ul> </li> <li> <p>Using Terminal</p> <ul> <li>Check what <code>ChromeDriver</code> version in compatibility with chrome     ChromeDriver Link</li> </ul> <p>```bash</p> <p>wget https://chromedriver.storage.googleapis.com//chromedriver_linux64.zip ``` <p>OR</p> <p>```bash</p> <p>curl -O https://chromedriver.storage.googleapis.com//chromedriver_linux64.zip ```     - Unzip the file <p>```bash</p> <p>unzip chromedriver_linux64.zip ```     - Move the file to given path and extract from here when needed</p> <p>```bash</p> <p>mv chromedriver // ``` <li> <p>Install the following modules</p> <ul> <li> <p>Within terminal</p> <p>```bash</p> <p>pip install selenium pip install webdriver-manager ```</p> </li> </ul> </li>"},{"location":"tools/all.selenium.how_to_guide.html#code_tutorial","title":"Code Tutorial","text":"<ol> <li> <p>Go to the notebook:    Selenium tutorial</p> </li> <li> <p>High-Level Overview</p> </li> </ol> <p>The uploaded notebook uses <code>Selenium WebDriver</code> to automate browser actions    for web scraping and testing two websites:    - Website 1: Test site -      Automates the process of logging in and out, ensuring the login      functionality works as expected    - Website 2:      Adidas shoes site -      Focuses on extracting show data from multiple pages within a specific      section of the site (Basketball)</p> <ol> <li>Key functionalities include:</li> <li>Setting up <code>ChromeDriver</code> with custom options for compatibility in      restricted environments</li> <li>Interacting with web elements like <code>buttons</code>, <code>input fields</code>, and      <code>pagination</code> controls</li> <li>Extracting and potentially saving scraped data using <code>pandas</code></li> <li> <p>Employing explicit waits (<code>WebDriverWait</code>) to handle dynamic content      loading effectively</p> </li> <li> <p>How to Find Elements Using <code>Selenium</code></p> </li> </ol> <p><code>Selenium</code> provides several ways to locate elements on a webpage. Below is an    explanation of each method, an example, and placeholders for images you can    insert:    - By <code>ID</code>:      - Description: Locates an element by its unique <code>id</code> attribute      - Example:        ```        from selenium.webdriver.common.by import By</p> <pre><code>   # Example: Find the login field element by its ID.\n   login_field = driver.find_element(By.ID, \"username\")\n   login_field.send_keys(\"xyz\")\n   ```\n\n   &lt;img src=\"figs/selenium/image1.png\"&gt;\n</code></pre> <ul> <li> <p>By <code>Name</code>:</p> <ul> <li>Description: Finds an element using its <code>Name</code> attribute.</li> <li>Example:    <code># Example: Find the password field by its name attribute.    password_field = driver.find_element(By.NAME, \"password\")    password_field.send_keys(\"xyz\")</code></li> </ul> <p>    - By <code>Class Name</code>  - Description: Locates elements by their <code>class</code> attribute  - Example:    <code># Example: Find a show name by its class name.    shoe = driver.find_element(By.CLASS_NAME, \"product-card-description_name__xHvJ2\")</code></p> <p>    - By <code>Tag Name</code>:  - Description: Targets elements based on their <code>HTML</code> tag (e.g., <code>div</code>,    <code>button</code>)  - Example:    <code># Example: Find shoe type by tag and print their text    type = driver.find_elements(By.TAG_NAME, \"p\")    print(type.text)</code></p> <p>    - By <code>CSS Selector</code>:  - Description: Leverages <code>CSS selectors</code> for flexible element    identification.  - Consider the following <code>html</code>:    <code>&lt;div class=\"product\" id=\"item-1\"&gt;      &lt;h2 class=\"product-title\"&gt;Running Shoes&lt;/h2&gt;      &lt;p class=\"product-price\"&gt;$50&lt;/p&gt;      &lt;button class=\"add-to-cart\"&gt;Add to Cart&lt;/button&gt;    &lt;/div&gt;</code>  - Key CSS Selectors with Examples    - Select by <code>Class</code>:      <code>button = driver.find_element(By.CSS_SELECTOR, \".add-to-cart\")      button.click()</code>    - Select by <code>ID</code>:      <code>product = driver.find_element(By.CSS_SELECTOR, \"#item-1\")      print(product.text)</code>    - Select by <code>Parent-Child</code>:      <code>price = driver.find_element(By.CSS_SELECTOR, \"#item-1 &gt; p\")      print(price.text)</code>    - Select by <code>Descendant</code>:      <code>button = driver.find_element(By.CSS_SELECTOR, \".product button\")      button.click()</code>    - Select by <code>Attribute</code>:      <code>title = driver.find_element(By.CSS_SELECTOR, \"[class='product-title']\")      print(title.text)</code>    - By <code>XPath</code>:  - Description: Uses <code>XML path</code> expressions to locate elements.  - Example:    <code># Example: Find a shoe block using an XPath query    show_block = driver.find_element(By.XPATH, \"/html/body/div[1]/div[2]/main/section[3]/article[2]/div\")</code></p> <p></p> </li> <li> <p>The above tutorial describes how to set up <code>Selenium</code>, find elements on a    webpage, and automate tasks like clicking, sending data and scraping data.    With different locator methods like <code>ID</code>, <code>Name</code>, and <code>XPath</code>, you can handle    most web pages. Just tweak the code to fit your target site, and you\u2019re good    to go!</p> </li> </ul>"},{"location":"tools/all.ssh.how_to_guide.html","title":"All.ssh.how to guide","text":""},{"location":"tools/all.ssh.how_to_guide.html#ssh","title":"Ssh","text":""},{"location":"tools/all.ssh.how_to_guide.html#what_is_ssh","title":"What is SSH?","text":"<p>From the Wikipedia</p> <p>Secure Shell (SSH) is a cryptographic network protocol for operating network services securely over an unsecured network.[1] Typical applications include remote command-line, login, and remote command execution, but any network service can be secured with SSH.</p> <p>More details here</p>"},{"location":"tools/all.ssh.how_to_guide.html#how_we_use_ssh_in_our_company","title":"How we use ssh in our company?","text":"<ul> <li>We use it to connect to any of our servers.</li> <li>Sometimes we use <code>scp</code> to copy files between hosts via <code>ssh</code>.<ul> <li>Don't know what is <code>scp</code>? read     here</li> </ul> </li> </ul>"},{"location":"tools/all.ssh.how_to_guide.html#public_key_for_authorization","title":"Public key for authorization?","text":"<ul> <li>We use <code>public key</code> authorization. This is the common way of secure     authorization for SSH connection.</li> <li>GitHub also can authorize you with <code>public key</code>, if you setup it in your GH     account. This is mean that you don't have to type your <code>login</code> and <code>password</code>     when you interact with GitHub via <code>git</code> e.g. <code>git clone</code>, <code>git pull</code>, etc.</li> </ul> <p>More details about <code>public key</code> here</p>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html","title":"All.sync gh projects.how to guide","text":""},{"location":"tools/all.sync_gh_projects.how_to_guide.html#github_project_sync_tool","title":"GitHub Project Sync Tool","text":"<p>This guide explains how the <code>sync_gh_projects.py</code> tool works, what it's designed to do, and the known limitations due to GitHub's Projects (Beta) API constraints. This tool has many limitations currently due to restricted GraphQL API support for ProjectV2, especially around views, layout, and ordering.</p>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#purpose","title":"Purpose","text":"<p>This tool synchronizes the <code>structure</code> of a GitHub Project from a source template project to a target destination project. It helps teams quickly align project configuration without manually replicating fields.</p>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#what_it_actually_syncs","title":"What It Actually Syncs","text":"<ul> <li><code>Fields</code>: These are the global columns in your project (e.g., Status,     Assignees, Labels). When created via the API, they are automatically visible     in all views. There is no API support to add fields specifically to one view     or to hide them from others</li> <li><code>Views</code>: These are saved layouts (e.g., \"Backlog\", \"Current sprint\"). However,     the API currently does not support creating views, so this script only     compares view names and logs a warning if any are missing in the destination</li> </ul>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#features","title":"Features:","text":"<ul> <li>Adds <code>missing global fields</code> from the template into the destination project</li> <li>Logs a warning about any <code>missing views</code> in the destination project</li> <li>Supports a <code>dry-run mode</code> (<code>--dry-run</code>) to preview intended changes without     modifying anything</li> <li>Supports a <code>verbose mode</code> (<code>--verbose</code>) to display detailed command logs</li> </ul>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#how_it_works","title":"How It Works","text":""},{"location":"tools/all.sync_gh_projects.how_to_guide.html#step-by-step","title":"Step-by-Step:","text":"<ol> <li>Lists all Projects V2 for the specified GitHub <code>--owner</code> using    <code>gh project list</code></li> <li>Locates the source (<code>--src-template</code>) and destination (<code>--dst-project</code>) by    title</li> <li>Queries each project's structure using GraphQL to fetch:</li> <li>Global <code>fields</code></li> <li> <p>Global <code>views</code> (names only)</p> </li> <li> <p>Compares structures:</p> </li> <li>If a field is in the source but not in the destination, it is added using      <code>addProjectV2Field</code>.</li> <li>If a view is in the source but not in the destination, it is not      created but logged with a warning.</li> </ol>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#limitations","title":"Limitations","text":"Capability Supported Notes Create missing global fields Yes Done via GraphQL <code>addProjectV2Field</code> mutation Detect and warn about missing views Yes Logs missing view names, but cannot recreate them Delete extra fields/views No Not implemented (non-destructive by design) Reorder fields or views No API does not expose position or ordering mutations View-specific field visibility No GraphQL only exposes global field presence View filters, sort orders, layout No Not accessible via GitHub's GraphQL API"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#current_state_technical_insights","title":"Current State &amp; Technical Insights","text":"<ul> <li><code>All fields in GitHub Projects are global</code> - They are shared across views.     While you can hide fields per view in the UI, this cannot be accessed or     modified via the API.</li> <li><code>There is no GraphQL mutation to create or configure views</code>     (<code>addProjectV2View</code> does not exist) - Hence, the tool only checks for view     names and alerts when missing.</li> <li><code>The script avoids destructive behavior</code> - It does not delete fields or views     that appear extra in the destination project.</li> <li><code>Newly added fields will be visible across all views</code> unless manually hidden     in the UI.</li> <li><code>You must still manually create views or configure filters, grouping, and sorting</code>,     as none of that is available through GitHub's API.</li> </ul>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#recommended_usage","title":"Recommended Usage","text":""},{"location":"tools/all.sync_gh_projects.how_to_guide.html#1_to_preview_what_changes_would_be_made","title":"1. To preview what changes would be made:","text":"<pre><code>sync_gh_projects.py \\\n    --owner causify-ai \\\n    --src-template \"[TEMPLATE] Causify Project\" \\\n    --dst-project \"Buildmeister\" \\\n    --dry-run --verbose\n</code></pre> <p>This will output:</p> <ul> <li>Fields in both source and destination</li> <li>Views in both source and destination</li> <li>Fields that would be created</li> <li>Views that are missing but cannot be added via script</li> </ul>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#2_to_apply_changes","title":"2. To apply changes:","text":"<pre><code>sync_gh_projects.py \\\n    --owner causify-ai \\\n    --src-template \"[TEMPLATE] Causify Project\" \\\n    --dst-project \"Buildmeister\"\n</code></pre> <p>This will:</p> <ul> <li>Add any missing fields that are present in the source template and absent in     the destination.</li> <li>Warn about any views missing in the destination.</li> <li>Not reorder, delete, or modify any existing structure.</li> </ul>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#example_scenario","title":"Example Scenario","text":""},{"location":"tools/all.sync_gh_projects.how_to_guide.html#source_project_template_causify_project","title":"Source Project <code>[TEMPLATE] Causify Project</code>","text":"<p><code>Fields</code>:</p> <ul> <li>Title, Assignees, Status, Labels, Milestone, Repository, Estimate</li> </ul> <p><code>Views</code>:</p> <ul> <li>Backlog, Current sprint, All issues</li> </ul>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#destination_project_buildmeister","title":"Destination Project <code>Buildmeister</code>","text":"<p><code>Fields</code>:</p> <ul> <li>Title, Assignees, Status, Labels, Milestone</li> </ul> <p><code>Views</code>:</p> <ul> <li>Backlog, Current sprint</li> </ul>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#what_the_script_will_do","title":"What the Script Will Do:","text":"<ul> <li>It will <code>add</code> the missing fields: <code>Repository</code>, <code>Estimate</code></li> <li>It will <code>log a warning</code> that the view <code>All issues</code> is missing</li> <li>It will <code>not</code> remove or alter any other structure</li> </ul>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#example_output_from_script","title":"Example output from script","text":"<ul> <li> <p>The source project is <code>[TEMPLATE] Causify Project</code> and dst project is     <code>Test_Shaunak</code></p> <ul> <li>The dst project does not contain <code>All issues</code> view which is a req according     to the template</li> <li>The dst project also does not have <code>Assignees</code> field in <code>Backlog</code> view</li> <li>The fields are calculated globally as git does not allow to see fileds for a     certain view yet</li> </ul> </li> <li> <p>The output is displayed below:     <code>text     15:11:14 - INFO  sync_gh_projects.py main:304                           Syncing '[TEMPLATE] Causify Project' \u2794 'Test_Shaunak'     15:11:14 - INFO  sync_gh_projects.py _get_structure:180                 Project #92 ([TEMPLATE] Causify Project) structure:     15:11:14 - INFO  sync_gh_projects.py _get_structure:181                 Fields: ['Title', 'Assignees', 'Status', 'Labels', 'Linked pull requests', 'Milestone', 'Repository', 'Reviewers', 'Parent issue', 'Sub-issues progress', 'Estimate', 'Sprint']     15:11:14 - INFO  sync_gh_projects.py _get_structure:182                 Views: ['All issues', 'Current sprint', 'Next sprint', 'Backlog', 'Team capacity ']     15:11:14 - WARN  sync_gh_projects.py _get_structure:183                 This script cannot detect per-view visibility, filters, grouping or ordering, since GitHubs GraphQL API does not expose them.     15:11:14 - INFO  sync_gh_projects.py _get_structure:180                 Project #95 (Test_Shaunak) structure:     15:11:14 - INFO  sync_gh_projects.py _get_structure:181                 Fields: ['Title', 'Assignees', 'Status', 'Labels', 'Linked pull requests', 'Milestone', 'Repository', 'Reviewers', 'Parent issue', 'Sub-issues progress', 'Estimate', 'Sprint']     15:11:14 - INFO  sync_gh_projects.py _get_structure:182                 Views: ['Current sprint', 'Next sprint', 'Backlog', 'Team capacity ']     15:11:14 - WARN  sync_gh_projects.py _get_structure:183                 This script cannot detect per-view visibility, filters, grouping or ordering, since GitHubs GraphQL API does not expose them.     15:11:14 - WARN  sync_gh_projects.py _sync_structure:245                View 'All issues' is missing in destination. GitHub API does not currently support view creation. Please add manually.     15:11:14 - INFO  sync_gh_projects.py _sync_structure:250                Structure sync complete.</code></p> <ul> <li>The script displayes the missing fields or views in the dst project when     compared to the source. <code>Github</code> has not introduced a way to automate view     creation yet.</li> </ul> </li> </ul>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#final_notes","title":"Final Notes","text":"<p>This tool is ideal for aligning the structure of GitHub Projects across teams or environments where a base template is used. It helps reduce manual setup effort and avoids configuration drift.</p> <p>Because GitHub's ProjectV2 API is still limited, this tool takes a conservative approach:</p> <ul> <li>It only creates fields</li> <li>It does not delete or modify</li> <li>It only checks for view presence by name</li> </ul>"},{"location":"tools/all.sync_gh_projects.how_to_guide.html#when_github_adds_support_for","title":"When GitHub adds support for:","text":"<ul> <li>View creation</li> <li>View filters and layout configuration</li> <li>Field ordering</li> </ul> <p>...this tool can be extended to do more complete synchronization.</p>"},{"location":"tools/all.tutorial_template.html","title":"All.tutorial template","text":"<p> <pre><code>&lt;title&gt;Tutorial Guide: script.py&lt;/title&gt;\n\n&lt;section name=\"Introduction\"&gt;\n</code></pre>"},{"location":"tools/all.tutorial_template.html#introduction","title":"Introduction","text":"<pre><code>    Provide a brief description of the task and what the script does.\n\n    &lt;bullet&gt;TODO: Define the task at a high level&lt;/bullet&gt;\n    &lt;bullet&gt;TODO: Identify the key motivation or use case&lt;/bullet&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"WhatYoullBuild\"&gt;\n</code></pre>"},{"location":"tools/all.tutorial_template.html#what_youll_build","title":"What You'll Build","text":"<pre><code>    Describe what the script produces or automates.\n    &lt;sample_behavior&gt;\n    &lt;bullet&gt;A Markdown file with diagram code blocks&lt;/bullet&gt;\n    &lt;bullet&gt;A script that processes and renders those diagrams&lt;/bullet&gt;\n    &lt;bullet&gt;A new output file with embedded images&lt;/bullet&gt;\n    &lt;/sample_behavior&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"BeforeYouBegin\"&gt;\n</code></pre>"},{"location":"tools/all.tutorial_template.html#before_you_begin","title":"Before You Begin","text":"<pre><code>    Describe all prerequisites or setup conditions required.\n\n    &lt;bullet&gt;Docker installed and running&lt;/bullet&gt;\n    &lt;bullet&gt;Linter up and running&lt;/bullet&gt;\n    &lt;bullet&gt;TODO: Any required environment variables or permissions&lt;/bullet&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"UsingTheScript\"&gt;\n</code></pre>"},{"location":"tools/all.tutorial_template.html#using_the_script","title":"Using the Script","text":"<pre><code>    Describe how to execute the script with examples.\n\n    &lt;sample_behavior&gt;\n    &lt;code_block language=\"bash\"&gt;\n</code></pre> <p>python render_images.py -i input.md -o output.md </p> <pre><code>    &lt;bullet&gt;TODO: Explain the different flags&lt;/bullet&gt;\n    &lt;bullet&gt;TODO: Mention optional flags or parameters&lt;/bullet&gt;\n    &lt;/sample_behavior&gt;\n\n&lt;/section&gt;\n\n&lt;section name=\"Output\"&gt;\n</code></pre>"},{"location":"tools/all.tutorial_template.html#output","title":"Output","text":"<pre><code>    Explain what output is expected and how to validate it.\n\n    &lt;bullet&gt;TODO: Describe where the output file is saved&lt;/bullet&gt;\n    &lt;bullet&gt;TODO: Mention format of embedded images or changes in the file&lt;/bullet&gt;\n\n&lt;/section&gt;\n</code></pre>"},{"location":"tools/all.veracrypt.how_to_guide.html","title":"All.veracrypt.how to guide","text":""},{"location":"tools/all.veracrypt.how_to_guide.html#how_to_use_veracrypt","title":"How to use VeraCrypt","text":""},{"location":"tools/all.veracrypt.how_to_guide.html#using_gui","title":"Using GUI","text":"<ol> <li> <p>Connect to VPN</p> </li> <li> <p>Connect to one of dev server via VNC that you want to have or already         have encrypted folder</p> </li> <li> <p>In CLI, type veracrypt or via application menu open veracrypt</p> </li> </ol>"},{"location":"tools/all.veracrypt.how_to_guide.html#creating_a_veracrypt_volume","title":"Creating a VeraCrypt Volume","text":"<ol> <li>Open up the VeraCrypt application and select \"Create Volume\".</li> <li> <p>Select \"Create an encrypted file container\" and click \"Next\".</p> </li> <li> <p>Then choose if you want hidden or standard volume</p> </li> <li> <p>Create a name and select a location to save the VeraCrypt volume that you    will be creating and select \"Next.\"</p> </li> <li>It is helpful if you create a volume name that ends with <code>.vc</code>. This will      associate the volume with the VeraCrypt application.</li> </ol> <p></p> <ol> <li>VeraCrypt provides some options for encryption. Select the desired         encryption algorithms and select \"Next\". VeraCrypt uses the AES algorithm by         default and is recommended for selection.         - In most cases a 500mb size volume is suitable. However this depends on the             user's need and should be adjusted accordingly.</li> </ol> <p></p> <ol> <li>Create the size of the volume that you would like to create. Click \"Next\".</li> <li>Set the volume to have appropriate size.</li> </ol> <p></p> <ol> <li>Create a password for your encrypted volume. Click \"Next\".</li> <li>The longer the password the better it is. It is important to not lose your      password. A lost password will make the data unrecoverable.</li> </ol> <p></p> <ol> <li>Select the file system and click \"Next\". The default is suitable in most    cases. Then you should move your mouse to get randomness in Pool but feel    free to click \"Format\".</li> <li>You do not need to move your cursor that much as this server is behind VPN.</li> </ol> <p> </p> <ol> <li>Your volume has been created. Click \"Exit\" or X button in top right corner.</li> </ol> <p></p>"},{"location":"tools/all.veracrypt.how_to_guide.html#mounting_a_veracrypt_volume","title":"Mounting a VeraCrypt Volume","text":"<ol> <li>Select the VeraCrypt volume that you would like to mount. Select the letter    drive location you would like to use. Click \"Mount\".</li> </ol> <ol> <li>Enter your password for the VeraCrypt volume. Click \"OK\".</li> </ol> <ol> <li>The drive is now unencrypted and ready for use at the Mount Directory    (<code>/media/veracrypt&lt;slotnumber&gt;</code>). Save files and documents as you would    normally do with a mounted drive.</li> </ol>"},{"location":"tools/all.veracrypt.how_to_guide.html#unmounting_a_veracrypt_volume","title":"Unmounting a VeraCrypt Volume","text":"<ul> <li>You can Encrypt files by Dismounting Volume, Select Slot and press Dismount</li> </ul>"},{"location":"tools/all.veracrypt.how_to_guide.html#using_cli","title":"Using CLI","text":""},{"location":"tools/all.veracrypt.how_to_guide.html#creating_a_veracrypt_volume_with_interactive_mode","title":"Creating a VeraCrypt volume with interactive mode","text":"<ul> <li> <p>If you need help use <code>veracrypt -h</code> there is everything you need</p> </li> <li> <p>If it asks this: (Enter your user password or administrator password:) use     your os_password or type sudo before the command.</p> </li> <li> <p>Be careful in not deleting the file that VeraCrypt uses to store the data     (e.g., <code>/data/saggese/.src_vc_DO_NOT_OVERWRITE</code>)</p> </li> <li> <p>Go to the server in your home and create a Veracrypt dir under your <code>$HOME</code>.     Since the directory typically stores code, we assume that it's in parallel to     <code>$HOME</code> like <code>/data/saggese/src_vc_encrypted</code></p> </li> </ul> <pre><code>&gt; sudo veracrypt -t -c\nVolume type:\n1) Normal\n2) Hidden\nSelect [1]: 1\n\nEnter volume path: /data/$USER/.src_vc_DO_NOT_OVERWRITE\n\n# An orange client is around 350M, so you can create 2GB worth of encrypted dir\n\nEnter volume size (sizeK/size[M]/sizeG): 2G\n\nEncryption Algorithm:\n1) AES\n2) Serpent\n3) Twofish\n4) Camellia\n5) Kuznyechik\n6) AES(Twofish)\n7) AES(Twofish(Serpent))\n8) Camellia(Kuznyechik)\n9) Camellia(Serpent)\n10) Kuznyechik(AES)\n11) Kuznyechik(Serpent(Camellia))\n12) Kuznyechik(Twofish)\n13) Serpent(AES)\n14) Serpent(Twofish(AES))\n15) Twofish(Serpent)\nSelect [1]: 1\n\nHash algorithm:\n1) SHA-512\n2) Whirlpool\n3) SHA-256\n4) Streebog\nSelect [1]: 1\n\nFilesystem:\n1) None\n2) FAT\n3) Linux Ext2\n4) Linux Ext3\n5) Linux Ext4\n6) NTFS\n7) exFAT\n8) Btrfs\nSelect [2]: 5\n\n# Avoid using \"FAT\" as it does not support symbolic links, meaning that\n# files such as `invoke.yaml`, `conftest.py` (hence `i docker_bash`,\n# `i run_fast_tests`) won't work, because in `lemonade` they are symbolic\n# links to the corresponding files in `amp`. Use Linux Ext4 (!)\n\nEnter password: Password123\nWARNING: Short passwords are easy to crack using brute force techniques!\n\nWe recommend choosing a password consisting of 20 or more characters.\nAre you sure you want to use a short password? (y=Yes/n=No) [No]: y\n\nRe-enter password: Password123\n\nEnter PIM: 1\n\nEnter keyfile path [none]: none\n\n# It interprets \"none\" as a literal path, instead just press Enter to proceed\n\nPlease type at least 320 randomly chosen characters and then press\nEnter: aksjdhqhhaskfjjhcb,anbowquehjkashdbncakh...\n\nDone: 100.000% Speed: 123 MiB/s Left: 0 s\n\nThe VeraCrypt volume has been successfully created.\n</code></pre>"},{"location":"tools/all.veracrypt.how_to_guide.html#create_a_veracrypt_volume_with_non-interactive_mode","title":"Create a VeraCrypt volume with non-interactive mode","text":"<pre><code>&gt; sudo veracrypt -t -c --volume-type=normal\n/data/$USER/.src_vc_DO_NOT_OVERWRITE \\\n--size=2G --encryption=aes --hash=sha-512 \\\n--filesystem=ext4 --pim=1 \\\n-k \"\" --random-source=/dev/urandom\n</code></pre> <ul> <li>You need to use the same PIM (personal iterations multipliers) that you used     to create the volume</li> </ul>"},{"location":"tools/all.veracrypt.how_to_guide.html#mounting_a_veracrypt_volume_1","title":"Mounting a VeraCrypt Volume","text":"<ul> <li> <p>It seems that this operation can be performed only by a \"sudoer\", refer to     https://github.com/veracrypt/VeraCrypt/issues/496.     Check with the infra team if there are any problems.</p> </li> <li> <p>To mount without user prompts:</p> </li> </ul> <pre><code>&gt; sudo veracrypt /data/$USER/.src_vc_DO_NOT_OVERWRITE\n/data/$USER/src_vc --pim=1 --protect-hidden=no -k \"\"\n</code></pre> <ul> <li>To mount an encrypted dir (e.g., <code>/data/richard/vc</code>):</li> </ul> <pre><code># Create the mount point if it doesn't exist\n&gt; mkdir /data/$USER/src_vc\n&gt; sudo veracrypt /data/$USER/.src_vc_DO_NOT_OVERWRITE\n/data/$USER/src_vc\nEnter password for /data/$USER/Desktop/vc: Password123\nEnter PIM for /data/$USER/Desktop/test.vc: 1\nEnter keyfile [none]: none (i.e. press Enter without typing anything)\nProtect hidden volume (if any)? (y=Yes/n=No) [No]: No\n\n# Change ownership of the `src_vc` folder. When using Linux Ext4 filesystem,\n# it creates a `lost+found` folder inside `src_vc` owned by `root` and then\n# the entire `src_vc` is owned by `root`. Meaning that you cannot create a\n# dir (e.g., `lemonade1` to clone the repo to) inside `src_vc` as long as\n# it is owned by `root`. Perhaps, you may need to ask the infra team to run\n# the cmd for you.\n&gt; sudo chown -R $USER:$USER /data/$USER/src_vc\n</code></pre> <ul> <li>To check what dirs are mounted by Veracrypt:</li> </ul> <pre><code>&gt; veracrypt -l\n1: /data/richard/Desktop/test.vc /dev/mapper/veracrypt1\n/data/richard/Desktop/private\n</code></pre>"},{"location":"tools/all.veracrypt.how_to_guide.html#unmounting_a_veracrypt_volume_1","title":"Unmounting a VeraCrypt Volume","text":"<ul> <li>Dismount a volume:</li> </ul> <pre><code>&gt; sudo veracrypt -d /data/$USER/src_vc\n</code></pre> <ul> <li>Umount all mounted volumes:</li> </ul> <pre><code>&gt; sudo veracrypt -d\n</code></pre>"},{"location":"tools/all.veracrypt.how_to_guide.html#how_to_use_a_git_client_under_an_encrypted_volume","title":"How to use a Git client under an encrypted volume","text":"<p>TODO(gp): Move somewhere else</p>"},{"location":"tools/all.veracrypt.how_to_guide.html#mount","title":"Mount","text":"<pre><code>&gt; sudo veracrypt -l\n1: /data/saggese/.src_vc_DO_NOT_OVERWRITE /dev/mapper/veracrypt1\n/data/saggese/src_vc\n\n&gt; sudo veracrypt -d /data/$USER/src_vc\n\n&gt; sudo veracrypt /data/$USER/.src_vc_DO_NOT_OVERWRITE\n/data/$USER/src_vc --pim=1 --protect-hidden=no -k \"\"\n\n# Create dir if it was not created already.\n&gt; mkdir ~/src_vc/lemonade1\n\n&gt; cd ~/src_vc/lemonade1/\n</code></pre>"},{"location":"tools/all.veracrypt.how_to_guide.html#one_time_set-up","title":"One time set-up","text":"<pre><code># Clone the code\n&gt; GIT_SSH_COMMAND=\"ssh -i ~/.ssh/id_rsa.gpsaggese.github\" git clone\n--recursive git@github.com:causify-ai/lemonade.git lemonade1\n\n# Init amp sub-module\n&gt; cd amp\n&gt; git submodule init\n&gt; git submodule update\n</code></pre> <ul> <li> <p>No need to build a thin environment. We need to re-use the helpers venv which     is located under <code>src</code>. For example:     <code>venv_dir=/data/grisha/src/venv/client_venv.helpers</code></p> </li> <li> <p>Assumption: the helpers virtual environment was already built under <code>src</code> when     setting up a different repo.</p> </li> </ul>"},{"location":"tools/all.veracrypt.how_to_guide.html#create_the_tmux_env","title":"Create the tmux env","text":"<pre><code># Create a soft link. The cmd creates a file `~/go_lemonade.py` which creates a tmux session.\n&gt; dev_scripts_lemonade/thin_client/tmux.py --create_global_link\n\n# Export encrypted dir path before creating a tmux session.\n&gt; export AM_SRC_DIR=\"/data/$USER/src_vc\"\n\n# Create the tmux session\n&gt; ~/go_lemonade.py --index 1\n\n# Verify that all tmux windows run without issues\n\n# Test Docker\n&gt; i docker_bash\n\n# Run tests\n&gt; i run_fast_tests\n</code></pre>"},{"location":"tools/all.visual_studio_code.how_to_guide.html","title":"All.visual studio code.how to guide","text":""},{"location":"tools/all.visual_studio_code.how_to_guide.html#visual_studio_code","title":"Visual Studio Code","text":""},{"location":"tools/all.visual_studio_code.how_to_guide.html#installation_of_vscode","title":"Installation of VSCode","text":""},{"location":"tools/all.visual_studio_code.how_to_guide.html#windows_linux_mac","title":"Windows, Linux, Mac","text":"<ul> <li>Download the installer using this link:     Download Visual Studio Code - Mac, Linux, Windows.</li> <li>Run the installer and follow the wizard steps.</li> <li>To run VSCode, find it in the Start menu or use the desktop shortcut.</li> <li>In the left navigation bar search for extensions ( or use <code>Ctrl+Shift+X</code> ) and     search for \"ms-vscode-remote.remote-ssh\" and then click on the install button.     </li> </ul>"},{"location":"tools/all.visual_studio_code.how_to_guide.html#connect_to_dev_server","title":"Connect to dev server","text":"<ul> <li>Connect to the VPN</li> <li>In bottom left corner click on this green button:     </li> <li>Then you will see these options on top of the screen, click on \"Open SSH     Configuration File\u2026\" and then click on the <code>user\\.ssh\\config</code> or     <code>user/.ssh/config</code>.     </li> <li>The config should look like this:     <ul> <li><code>HostName</code>: dev1 (or dev2) server IP.</li> <li><code>User</code>: your linux user name on the dev server.</li> <li><code>IdentityFile</code>: private key that you use to <code>SSH</code> to the dev server.</li> </ul> </li> <li>Save and close the config file and press the green button again, then for     connection click on \"Connect to Host...\". You should see the IP address of the     server, so just click on it and it will connect you in a new window.</li> <li>Open a preferred repo directory.<ul> <li>Click on the \"Source control\" button on the left:     </li> <li>Choose \"Open Folder\":     </li> <li>Choose the desired repo directory from the drop-down menu, e.g., <code>cmamp1</code>:     </li> </ul> </li> </ul>"},{"location":"tools/all.visual_studio_code.how_to_guide.html#how_to_run_a_vscode_debugger_within_a_remote_container","title":"How to run a VSCode debugger within a remote container","text":"<p>The goal is to successfully run a Visual Studio Code (VSCode) debugger on code that runs within a docker container located on a remote server.</p>"},{"location":"tools/all.visual_studio_code.how_to_guide.html#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>Mac OS or Linux-based OS</p> </li> <li> <p>Visual Studio Code installed on the local machine you are working from</p> </li> <li> <p>VSCode extensions installed:</p> <ul> <li>Python     (Installed on the remote machine)<ul> <li>VSCode installs some files inside <code>.vscode-*</code> directories on the remote     host to ensure full functionality of the extensions</li> </ul> </li> <li>Remote SSH</li> <li>Remote Explorer</li> <li>Remote Development     (Installed on the remote machine)</li> <li>Dev Containers</li> </ul> </li> <li> <p>Ability to access the remote server where the container will be located using     SSH</p> </li> <li> <p>Kaizen dev environment set-up on the remote machine to allow running <code>invoke</code>     targets</p> </li> <li> <p>A running Kaizen dev docker container created on the remote machine using     <code>invoke docker_bash</code></p> </li> </ul>"},{"location":"tools/all.visual_studio_code.how_to_guide.html#tips","title":"Tips","text":"<ul> <li>To open the command palette in VSCode use a keyboard shortcut <code>Cmd + Shift + P</code></li> </ul>"},{"location":"tools/all.visual_studio_code.how_to_guide.html#steps","title":"Steps","text":"<ol> <li> <p>Open the command palette and search for <code>Remote-SSH: Connect to host</code> action</p> </li> <li> <p>Click <code>+ Add new SSH Host</code>. The prompt will ask you to provide a full         command to log in to the server, e.g. \"ssh ubuntu@10.11.12.13\"</p> <pre><code>1.  Note: if you use `~/.ssh/config` to specify frequently used hosts,\n        VSCode will automatically fetch the list for you, this way you simply\n        choose the name of the host and the connection is established\n        automatically\n</code></pre> </li> <li> <p>Make sure you have a container running using <code>invoke docker_bash</code> on the         remote machine</p> </li> <li> <p>VSCode will establish a remote connection to the server and open a new         window. Make sure that is your active window.</p> </li> <li> <p>Open the command palette and search for         <code>Dev Containers: Attach To Running Container</code>. A list of all running         containers on your server will appear, choose your container.</p> </li> <li> <p>Choose the debugging panel from the sidebar and click on <code>Run and Debug</code></p> </li> </ol> <p></p> <ol> <li>Click \"Add Configuration\" an editor for \"launch.json\" should open</li> </ol> <p></p> <ol> <li>Paste the following JSON into the file and save it.</li> </ol> <pre><code>{\n        // Use IntelliSense to learn about possible attributes.\n        // Hover to view descriptions of existing attributes.\n        // For more information, visit:\n        https://go.microsoft.com/fwlink/?linkid=830387\n        \"version\": \"0.2.0\",\n        \"configurations\": [\n        {\n                \"name\": \"Python: Current File\",\n                \"type\": \"python\",\n                \"request\": \"launch\",\n                \"program\": \"${file}\",\n                \"console\": \"integratedTerminal\",\n                \"justMyCode\": true,\n                \"python\":\"/venv/bin/python\",\n                \"env\": {\n                        \"PATH\":\n                        \"/app/amp/documentation/scripts:/app/amp/dev_scripts/testing:/app/amp/dev_scripts/notebooks:/app/amp/dev_scripts/install:/app/amp/dev_scripts/infra:/app/amp/dev_scripts/git:/app/amp/dev_scripts/aws:/app/amp/dev_scripts:/app/amp:/app/dev_script_p1:/app:.:/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\",\n                        \"PYTHONPATH\":\"/app/amp:/app:\"\n                }\n        }\n        ]\n}\n</code></pre> <ol> <li>Now you can open any Python file within the environment. You can for example         set a breakpoint, hit the green \"play\" button and the debugger will run the         currently opened file</li> </ol> <p></p> <ol> <li>To run a script with a specific set of command line arguments, append         <code>\"args\"</code> key to the launch.json file (the location should be         <code>.vscode/launch.json</code>). The value is a list of command line arguments and         values. Example below:</li> </ol> <pre><code>\"args\": [\n        \"--file\",\n        \"/app/datapull/ccxt/notebooks/Master_universe.ipynb\",\n        \"--action\",\n        \"convert\"\n]\n</code></pre>"},{"location":"tools/all.visual_studio_code.how_to_guide.html#how_to_access_the_jupyter_server_running_on_the_remote_server_through_your_local_machine","title":"How to access the Jupyter server running on the remote server through your local machine","text":"<ol> <li>Run the Jupyter server</li> </ol> <pre><code>&gt; i docker_jupyter\n</code></pre> <pre><code>###&gt; devops/docker_run/run_jupyter_server.sh\n&gt; cmd=jupyter notebook --ip=* --port=10421 --allow-root --NotebookApp.token=''\n...\n</code></pre> <ol> <li>In VSCode, add port forwarding from port <code>10421</code> on the server to the same    port on your local machine. You can then access the Jupyter notebook running    on the remote machine through <code>http://localhost:10421</code> on your local machine.</li> </ol>"},{"location":"tools/all.visual_studio_code.how_to_guide.html#connecting_via_vnc","title":"Connecting via VNC","text":"<ul> <li>Make sure you have a VPN connection.</li> </ul>"},{"location":"tools/all.visual_studio_code.how_to_guide.html#installing_vnc","title":"Installing VNC","text":"<ul> <li>Install VNC using this link: https://www.realvnc.com/en/connect/download/viewer/windows/</li> <li> <p>Sysadmin has sent you:</p> <ul> <li><code>os_password.txt</code></li> <li>Your username <code>$USER</code></li> <li>A key <code>crypto.pub</code> that looks like:     <code>-----BEGIN OPENSSH PRIVATE KEY----- # gitleaks:allow     b3BlbnNzaC1rZXktdjEAAAAABG5vbmUAAAAEbm9uZQAAAAAAAAABAAABlwAAAAdzc2gtcn     NhAAAAAwEAAQAAAYEA0IQsLy1lL3bhPT+43sht2/m9tqZm8sEQrXMAVtfm4ji/LXMr7094     \u2026     hakqVTlQ2sr0YTAAAAHnNhZ2dlc2VAZ3BtYWMuZmlvcy1yb3V0ZXIuaG9tZQECAwQ=     -----END OPENSSH PRIVATE KEY-----</code></li> </ul> </li> <li> <p>Let's say you are connected via VNC.</p> <ul> <li>Login into the OS.</li> <li>Run <code>pycharm.sh</code> using terminal (should be there):     <code>&gt; bash /opt/pycharm-community-2021.2.3/bin/pycharm.sh</code></li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html","title":"All.conda environment obsolete.how to guide","text":""},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#conda_environment_obsolete","title":"Conda Environment (Obsolete)","text":"<p>THIS IS OBSOLETE AFTER DOCKER DEV CONTAINER</p>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#conda_flow","title":"Conda flow","text":""},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#optional_install_anaconda","title":"(optional) Install anaconda","text":"<ul> <li>For the AWS machine there is already a central conda, so there is no need for     users to install</li> <li>For a laptop you need to install it yourself<ul> <li>You need anaconda3</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#configure_anaconda","title":"Configure anaconda","text":"<ul> <li>Configure anaconda for your shell using:     <code>bash     &gt; conda init bash</code></li> <li>Anaconda3 adds a snippet of code in your <code>.bashrc</code></li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#create_conda_environment","title":"Create conda environment","text":"<ul> <li>This is needed to install all the packages that are required for development:     <code>bash     &gt; cd $DST_DIR     &gt; ./dev_scripts/create_conda.develop.sh</code></li> <li> <p>This script takes 5 mins to run</p> </li> <li> <p>You need to create an environment for every server you use (e.g., for the AWS     server <code>research</code>, for your laptop)</p> </li> <li>You can reuse the same environment for multiple Git clients</li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#check_conda_environment","title":"Check conda environment","text":"<ul> <li>Check that your conda environment exists:     <code>bash     &gt; conda info --envs     # conda environments:     #     base                     /anaconda3     develop            *  /home/&lt;USER&gt;/.conda/envs/develop</code></li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#configure_conda_environment","title":"Configure conda environment","text":"<ul> <li> <p>Every time you open a shell you need to activate the development environment     run:</p> <p>```bash</p> <p>source dev_scripts/setenv_....sh ```</p> </li> <li> <p>This script:</p> <ul> <li>Activates the conda environment</li> <li>Sets environment variables</li> <li>Makes sure things are working properly</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#delete_recreate_environment","title":"Delete / recreate environment","text":""},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#overwrite_a_conda_environment_with_create_condapy","title":"Overwrite a conda environment with <code>create_conda.py</code>","text":"<ul> <li>You can use the option <code>--delete_env_if_exists</code> to overwrite a conda env,     creating it from scratch</li> <li> <p>This is the typical approach</p> </li> <li> <p>There are some pre-packaged command lines to create the standard environments,     e.g., <code>./dev_scripts/create_conda.develop.sh</code></p> </li> <li> <p>The <code>create_conda.py</code> help as some useful examples of command lines, see the     help:     <code>bash     &gt; create_conda.py -h</code></p> </li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#manually_delete_a_conda_environment","title":"Manually delete a conda environment","text":"<ul> <li>You can delete a conda environment by simply deleting the corresponding     directory</li> <li>The conda command tries to be smart removing the packages and leaving the dir,     but IMO it doesn't always work</li> <li>You look at the environments with:     <code>bash     &gt; conda info --envs     # conda environments:     #     ...     develop               *  /Users/&lt;USER&gt;/.conda/envs/develop     ...</code></li> <li>Then you can delete with:     <code>bash     &gt; rm -rf /Users/&lt;USER&gt;/.conda/envs/develop</code></li> <li>It's a good idea to move it so you can resume it if something goes wrong:     <code>bash     &gt; mv /Users/&lt;USER&gt;/.conda/envs/develop &gt; /Users/&lt;USER&gt;/.conda/envs/develop.OLD</code><ul> <li>Note that <code>develop.OLD</code> might not work anymore since all the links are     broken by the move</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#to_delete_the_entire_conda_installation_advanced_users","title":"To delete the entire conda installation (advanced users)","text":"<ul> <li>This is a dangerous operation, since it deletes the executable <code>conda</code><ul> <li>You want to do this only when your environment is screwed up: a more expert     team member can help you diagnose it</li> </ul> </li> <li>If you want to delete your conda installation, find the base env     <code>bash     &gt; conda info --envs     base                     /anaconda3     ...</code></li> <li>Run <code>rm -rf /anaconda3</code></li> <li>A good idea is to move it so you can resume the state</li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#update_anaconda","title":"Update anaconda","text":"<ul> <li> <p>To update anaconda (i.e., the framework that manages conda packages and     <code>conda</code> executable)</p> <p>```bash</p> <p>conda activate base</p> </li> <li> <p>You can try to activate one environment     <code>bash     &gt; conda activate amp_develop     &gt; which python     /Users/saggese/.conda/envs/amp_develop/bin/python</code></p> </li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#remove_index_cache_lock_files_tarballs_unused_cache_packages_and_source","title":"Remove index cache, lock files, tarballs, unused cache packages, and source","text":""},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#cache","title":"cache.","text":"<p>conda clean --all conda update conda conda update anaconda conda -V conda 4.7.12 ```</p>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#configure_user_credentials","title":"Configure user credentials","text":"<ul> <li> <p>For now this topic is obsolete. All development with AWS is running on a     server side (or locally) in a docker container. Here you can find the     documentation     the link</p> </li> <li> <p>Update the user credential files in <code>amp/helpers/user_credentials.py</code></p> <ul> <li>Commit this so all your clients are configured</li> </ul> </li> <li>Typically you can just copy-paste a portion of the configuration of another     user</li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#be_patient","title":"Be patient","text":"<ul> <li> <p>The <code>create_conda.py</code> flow is designed to make our projects portable across:</p> <ul> <li>Platforms (e.g., macOS, Linux)</li> <li>Different installation of OSes (e.g., GP's laptop vs Paul's laptop) with all     the peculiar ways we install and manage servers and laptops</li> <li>Different versions of conda</li> <li>Different versions of python 3.x</li> <li>Different versions of python packages</li> </ul> </li> <li> <p>There is no easy way to make sure that <code>create_conda.py</code> works for everybody</p> <ul> <li>We can only make sure that Jenkins builds the environment correctly in its     set-up by following the process described above</li> <li>Try to follow the steps one by one, using a clean shell, cutting and pasting     commands</li> <li>If you hit a problem, be patient, ping GP / Paul, and we will extend the     script to handle the quirks of your set-up</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#conda_bloat","title":"Conda bloat","text":"<ul> <li> <p>\"Conda bloat\" refers to the situation when there are more packages in the     conda recipe than what strictly needed to allow us to make progress.</p> </li> <li> <p>The effects of conda bloat are:</p> <ul> <li>Longer time for <code>conda</code> to solve the dependencies, download the packages,     and update the environment</li> <li>Additional dependencies forcing <code>conda</code> to install older packages that can     create subtle issues in our environment (we want our code always to be     forward compatible, but we can't afford our code to be back compatible)</li> <li>People starting pinning down packages to get deterministic library behaviors</li> <li>In the worst case, not being able to install the environment at all due to     conflicting <code>conda</code> requirements</li> </ul> </li> <li> <p>On the one side, we want to minimize \"conda bloat\".</p> </li> <li>On the other side, we want to be able to experiment with packages.</li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#minimize_conda_bloat","title":"Minimize conda bloat","text":"<ul> <li>To minimize conda bloat, our process consists of adding a package to the conda     recipe when a new package is actually needed by code and to run unit tests<ul> <li>Having code using the library and running unit tests for that code should     happen together</li> <li>The rule is \"Important code should be tested, and if code is not important     it should not be in the repo at all\"</li> <li>Thus a corollary is that all code in the repo should be tested</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#conda_environment_lifecycle","title":"Conda environment lifecycle","text":""},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#experimental_conda_environment","title":"Experimental conda environment","text":"<ul> <li> <p>On the other side we want to be free to experiment with a package that can     save us tons of development time.</p> </li> <li> <p>The proposed approach is:</p> <ul> <li>Install the package in whatever way you want on top of your standard conda     environment (<code>conda</code>, <code>pip</code>, whatever)</li> <li>Be aware that some packages are not easy to add to our production system     (e.g. packages written by students trying to graduate, non-coding savvy PhDs     and professors)</li> <li>Create a new local experimental conda environment with:     <code>create_conda.py --env_name develop.nlp</code></li> <li>If you want to reproduce your environment or share it (e.g., among the NLP     team) you can branch <code>dev_scripts/install/requirements/develop.yaml</code> and     modify it, then you can create an environment programmatically with:     <code>create_conda.py --env_name develop.nlp --req_file dev_scripts/install/requirements/develop.nlp.yaml</code></li> </ul> </li> <li> <p>We can make this process more automated by generalizing the scripts we already     have.</p> </li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#releasing_a_new_conda_environment","title":"Releasing a new conda environment","text":"<ul> <li> <p>Once the new package is added to the official conda environment, we should:</p> <ul> <li>Test the new conda environment locally, by creating a fresh environment and     running all the tests</li> <li>Compare the list of packages installed before / after the change:     <code>git diff amp/dev_scripts/install/conda_envs/amp_develop.saggese.Darwin.gpmac.lan.txt</code><ul> <li>E.g., Check whether installed pandas went from 0.25 to 0.14.1</li> </ul> </li> <li>Use the <code>dev</code> build to make sure Jenkins is happy with it, especially when     we develop on a different OS than Linux</li> <li>Send an email to the team asking them to recreate the environment</li> </ul> </li> <li> <p>Typically GP takes care of getting all this fun stuff to work, but you are     welcome to try locally to minimize surprises.</p> </li> </ul>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#conda_maintenance_only_for_admins","title":"Conda maintenance (only for admins)","text":""},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#updating_conda_itself","title":"Updating conda itself","text":"<ul> <li>To update conda itself you can run:</li> </ul> <pre><code>&gt; conda activate base\n\n&gt; conda --version\n3.7.1\n\n&gt; conda update anaconda\n\n&gt; conda --version\n3.8.0\n</code></pre>"},{"location":"tools/dev_system/all.conda_environment_obsolete.how_to_guide.html#cleaning_conda_packages","title":"Cleaning conda packages","text":"<ul> <li>One can clean up the entire cache of packages with:</li> </ul> <pre><code>&gt; conda clean --yes --all\n</code></pre> <ul> <li>This operation:<ul> <li>Affects the conda system for all the users</li> <li>Is just about deleting cached artifacts so it should not have destructive     effects</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html","title":"All.create a runnable dir.how to guide","text":""},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#how_to_create_a_runnable_dir","title":"How to Create a Runnable Dir","text":""},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#definition","title":"Definition","text":"<ul> <li>A runnable dir is a directory containing code and a <code>devops</code> dir so that it     can build its own container storing all the dependencies to run and be tested</li> <li>A runnable dir can be<ul> <li>A super-repo (e.g. <code>//cmamp</code>, <code>//quant_dashboard</code>)<ul> <li>Follow     all.create_a_super_repo_with_helpers.how_to_guide.md     to create a runnable dir that is a super repo</li> </ul> </li> <li>A sub directory under a super-repo (e.g. <code>//cmamp/ck.infra</code>)</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#a_runnable_dir_as_sub_directory_under_a_super-repo","title":"A Runnable Dir as Sub Directory Under a Super-Repo","text":"<pre><code>&gt; export CSFY_RUNNABLE_DIR=\"ck.infra\"\n&gt; export CSFY_RUNNABLE_DIR_SUFFIX=\"cmamp_infra\"\n</code></pre>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#1_turn_the_repo_into_a_super-repo_with_helpers","title":"1) Turn the Repo Into a Super-Repo with Helpers","text":"<ul> <li>Follow     all.create_a_super_repo_with_helpers.how_to_guide.md     to turn the repo into a super-repo with helpers</li> <li>For example, for <code>//cmamp</code>, the resulting root directory should have a     structure like:     <code>bash     &gt; ls -1     ...     dev_scripts_cmamp     ...     helpers_root     ...</code></li> </ul>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#2_copy_and_customize_files_in_the_top_dir","title":"2) Copy and Customize Files in the Top Dir","text":"<ul> <li>Some files need to be copied from <code>helpers</code> to the runnable dir to configure     various tools (e.g., dev container workflow, <code>pytest</code>, <code>invoke</code>)     <code>bash     &gt; (cd helpers_root; git pull)     &gt; echo $CSFY_RUNNABLE_DIR     &gt; mkdir $CSFY_RUNNABLE_DIR     &gt; cp helpers_root/{changelog.txt,conftest.py,pytest.ini,invoke.yaml,repo_config.yaml,tasks.py} $CSFY_RUNNABLE_DIR</code><ul> <li><code>changelog.txt</code>: this is copied from the repo that builds the used container     or started from scratch for a new container     ```bash     &gt; vim $CSFY_RUNNABLE_DIR/changelog.txt     # cmamp-infra-1.0.0<ul> <li>2024-10-16</li> <li>First release ```</li> </ul> </li> <li><code>conftest.py</code>: needed to configure <code>pytest</code></li> <li><code>pytest.ini</code>: needed to configure <code>pytest</code> preferences</li> <li><code>invoke.yaml</code>: needed configure <code>invoke</code></li> <li><code>runnable_dir</code>: needed to be created as it is used for automated discovery     of runnable dirs for <code>pytest</code> runs     <code>bash     &gt; touch $CSFY_RUNNABLE_DIR/runnable_dir</code></li> <li><code>repo_config.yaml</code>: store information about this specific repo (e.g., name,     used container)<ul> <li>This needs to be modified ```bash <p>vim $CSFY_RUNNABLE_DIR/repo_config.yaml repo_info:     repo_name: cmamp ... docker_info:     docker_image_name: cmamp-infra ... runnable_dir_info:     use_helpers_as_nested_module: True     ...     dir_suffix: cmamp_infra ```</p> </li> </ul> </li> <li><code>tasks.py</code>: the <code>invoke</code> tasks available in this container<ul> <li>This can be modified if needed</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#3_copy_and_customize_files_in_devops","title":"3) Copy and Customize Files in <code>devops</code>","text":"<ul> <li>Copy the <code>devops</code> from <code>//helpers</code> as a template dir     <code>bash     &gt; (cd helpers_root; git pull)     &gt; cp -r helpers_root/devops $CSFY_RUNNABLE_DIR</code></li> <li>Follow the instructions in     <code>/docs/tools/dev_system/all.devops_docker.reference.md</code>     and     <code>/docs/tools/dev_system/all.devops_docker.how_to_guide.md</code>     to customize the files in order to build the Docker container<ul> <li>Typically, we might want to customize the following<ul> <li><code>$CSFY_RUNNABLE_DIR/devops/docker_build/dev.Dockerfile</code>: if we need to use     a base image with different Linux distro or version</li> <li><code>$CSFY_RUNNABLE_DIR/devops/docker_build/install_os_packages.sh</code>: if we     need to add or remove OS packages using the declarative approach<ul> <li>See     <code>/docs/tools/dev_system/all.devops_docker.reference.md#declarative-os-package-installation</code>     for details on the declarative OS package installation system</li> </ul> </li> <li><code>$CSFY_RUNNABLE_DIR/devops/docker_build/pyproject.toml</code>: if we need to add     or remove Python dependencies</li> </ul> </li> </ul> </li> <li>Always trim     <code>/devops/docker_build/pyproject.toml</code>     to only include the dependancies requred by the runnable dir</li> </ul>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#4_copy_and_customize_files_in_thin_client","title":"4) Copy and Customize Files in Thin_Client","text":"<ul> <li> <p>Create the <code>dev_scripts_{runnable_dir_suffix}</code> dir based off the template from     <code>helpers</code></p> <p>```bash</p> </li> <li> <p>The resulting <code>dev_script</code> should look like:</p> <p>```bash</p> <p>ls -1 $DST_DIR setenv.sh ```</p> </li> <li> <p>Replace file with symbolic links</p> <p>```bash</p> <p>echo $SRC_DIR ./helpers_root/dev_scripts_helpers/thin_client echo $DST_DIR ./ck.infra/dev_scripts_cmamp_infra/thin_client ./helpers_root/helpers/create_links.py --src_dir $SRC_DIR --dst_dir $DST_DIR --replace_links --use_relative_paths ```</p> </li> </ul>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#use_a_suffix_based_on_the_repo_name_and_runnable_dir_name_eg_cmamp_infra","title":"Use a suffix based on the repo name and runnable dir name, e.g., <code>cmamp_infra</code>.","text":"<p>SRC_DIR=\"./helpers_root/dev_scripts_helpers/thin_client\"; echo $SRC_DIR DST_DIR=\"${CSFY_RUNNABLE_DIR}/dev_scripts_${CSFY_RUNNABLE_DIR_SUFFIX}/thin_client\"; echo $DST_DIR mkdir -p $DST_DIR cp \"$SRC_DIR/setenv.sh\" $DST_DIR ```</p>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#5_replace_files_with_symbolic_links","title":"5) Replace Files with Symbolic Links","text":"<ul> <li> <p>Some common files can be replaced with symbolic links</p> <p>```bash</p> </li> <li> <p>Refer to     Managing common files     for explanation</p> </li> <li>Refer to     Managing symbolic links between directories     for how to use the commands</li> </ul>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#runnable_dir_is_ckinfra_in_this_case","title":"Runnable dir is \"ck.infra\" in this case.","text":"<p>./helpers_root/helpers/create_links.py --src_dir ./helpers_root --dst_dir $CSFY_RUNNABLE_DIR --replace_links --use_relative_paths ```</p>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#6_commit_changes","title":"6) Commit Changes","text":"<ul> <li>Commit changes     <code>bash     &gt; git add $CSFY_RUNNABLE_DIR     &gt; git commit -m \"Add runnable dir\"</code></li> </ul>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#7_build_a_container_for_a_runnable_dir","title":"7) Build a Container for a Runnable Dir","text":"<ul> <li> <p>Run the single-arch flow to test the flow</p> <p>```bash</p> <p>cd $CSFY_RUNNABLE_DIR source dev_scripts_${CSFY_RUNNABLE_DIR_SUFFIX}/thin_client/setenv.sh i docker_build_local_image --version 1.0.0 --container-dir-name $CSFY_RUNNABLE_DIR i docker_tag_local_image_as_dev --version 1.0.0 i docker_bash --skip-pull --version 1.0.0 ```</p> </li> <li> <p>Run the multi-arch flow     <code>bash     &gt; cd $CSFY_RUNNABLE_DIR     &gt; source dev_scripts_${CSFY_RUNNABLE_DIR_SUFFIX}/thin_client/setenv.sh     &gt; i docker_build_local_image --version 1.0.0 --container-dir-name $CSFY_RUNNABLE_DIR --multi-arch \"linux/amd64,linux/arm64\"     &gt; i docker_tag_local_image_as_dev --version 1.0.0     &gt; i docker_bash --skip-pull --version 1.0.0</code></p> </li> </ul>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#8_test_the_code","title":"8) Test the Code","text":"<ul> <li> <p>Run tests from the runnable dir (e.g. <code>cmamp/ck.infra</code>)</p> <p>```bash</p> </li> <li> <p>Run tests from the root dir (e.g. <code>cmamp</code>)     <code>bash     &gt; main_pytest.py run_fast_tests --dir ck.infra     &gt; main_pytest.py run_slow_tests --dir ck.infra</code></p> </li> </ul>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#if_the_version_of_the_locally_built_image_is_100","title":"If the version of the locally built image is 1.0.0.","text":"<p>cd $CSFY_RUNNABLE_DIR i run_fast_tests --version 1.0.0 --skip-pull i run_slow_tests --version 1.0.0 --skip-pull ```</p>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#9_add_the_dependency_lock_files_to_the_commit","title":"9) Add the Dependency Lock Files to the Commit","text":"<pre><code>&gt; cd $CSFY_RUNNABLE_DIR\n&gt; git add devops/docker_build/poetry.lock\n&gt; git add devops/docker_build/pip_list.txt\n&gt; git commit -m \"Update dependencies\"\n</code></pre>"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#10_release_the_docker_image","title":"10) Release the Docker Image","text":"<ul> <li> <p>Refer to the following docs for more info on image releases</p> <ul> <li>docs/tools/dev_system/all.devops_docker.how_to_guide.md#release-a-docker-image</li> </ul> </li> <li> <p>To create new registory to store the image, please contact Infra team.</p> </li> <li> <p>Release to ECR</p> <ul> <li>This is required for running the container from the dev/prod servers</li> </ul> <p>```bash</p> <p>i docker_push_dev_image --version  ``` - Or run the following command to build, test, and release the image <p>```bash</p> <p>i docker_release_dev_image --version  --container-dir-name $CSFY_RUNNABLE_DIR ``` <li> <p>Release to GHCR</p> <ul> <li>This is required for running the container from GH Actions CI/CD pipelines</li> <li>TODO(heanh): Can we create an invoke target for this? ```bash <p>docker login ghcr.io -u  -p  docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/:dev ghcr.io/causify-ai/:dev docker push ghcr.io/causify-ai/:dev ```"},{"location":"tools/dev_system/all.create_a_runnable_dir.how_to_guide.html#11_update_and_release_a_new_version_of_the_image","title":"11) Update and Release a New Version of the Image","text":"<p>We release a new version of the Docker image whenever we need to update its dependencies</p> <ol> <li> <p>Modify changelog</p> </li> <li> <p>Specify what was changed</p> </li> <li>Pick the release version according to semantic versioning convention</li> <li>For example for version 1.2.3:<ul> <li>1 is major, 2 is minor, 3 is patch</li> </ul> </li> </ol> <pre><code>&gt; cd $CSFY_RUNNABLE_DIR\n&gt; source dev_scripts_${CSFY_RUNNABLE_DIR_SUFFIX}/thin_client/setenv.sh\n&gt; vim changelog.txt\n\n# Cmamp-Infra-1.2.0\n- 2025-05-18\n- Add support for Kubernetes Kustomize\n- Upgrade `kubectl` to v1.31.0\n- Add `yq` command line tool\n</code></pre> <ol> <li> <p>Modify dependencies list</p> </li> <li> <p>Modify <code>$CSFY_RUNNABLE_DIR/devops/docker_build/os_packages/os_packages.txt</code> to     add or remove OS packages using the declarative approach</p> <ul> <li>See     <code>/docs/tools/dev_system/all.devops_docker.reference.md#declarative-os-package-installation</code>     for details on the declarative OS package installation system</li> </ul> </li> <li> <p>Modify <code>$CSFY_RUNNABLE_DIR/devops/docker_build/pyproject.toml</code> to add or     remove Python packages</p> </li> <li> <p>Build the image locally</p> </li> </ol> <pre><code># Build the Image.\n&gt; i docker_build_local_image --version 1.2.0 --container-dir-name $CSFY_RUNNABLE_DIR\n\n# Tag the Image as Dev.\n&gt; i docker_tag_local_image_as_dev --version 1.2.0\n</code></pre> <ol> <li>Add the dependency lock files to the commit</li> </ol> <pre><code>&gt; cd $CSFY_RUNNABLE_DIR\n&gt; git add devops/docker_build/poetry.lock\n&gt; git add devops/docker_build/pip_list.txt\n&gt; git commit -m \"Update dependencies\"\n</code></pre> <ol> <li>Make sure we can run the container</li> </ol> <pre><code>&gt; i docker_bash --skip-pull --stage local --version 1.2.0\n&gt; i docker_jupyter --skip-pull --stage local --version 1.2.0\n</code></pre> <ol> <li>Make sure all tests pass</li> </ol> <pre><code># Run Tests.\n&gt; i run_fast_tests --stage local --version 1.2.0\n&gt; i run_slow_tests --stage local --version 1.2.0\n</code></pre> <ol> <li>Release the images to remote registries</li> </ol>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html","title":"All.create a super repo with helpers.how to guide","text":""},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#how_to_create_a_super-repo_with_helpers","title":"How to Create a Super-Repo with <code>Helpers</code>","text":""},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#create_a_new_super_repo_in_the_desired_organization","title":"Create a New (Super) Repo in the Desired Organization","text":"<ul> <li>Create a repo within the     <code>causify-ai</code> organization</li> <li>Follow the     official guide<ul> <li>TODO(Grisha): consider using repository     templates</li> </ul> </li> <li>Recommended options:<ul> <li>Owner: <code>causify-ai</code></li> <li>Repository-name: provide a valid short name, e.g., <code>algo_trading</code></li> <li>Visibility: by default choose <code>Private</code></li> <li>Add a README file</li> <li><code>.gitignore template: None</code></li> <li>License: <code>General Public License v3.0</code></li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#add_helpers_sub-repo","title":"Add Helpers Sub-Repo","text":"<ul> <li> <p>Below there is an example for the <code>//helpers</code> repo, but it works for any repo     (e.g., <code>//orange</code>) including any other repo (e.g., <code>//cmamp</code>)</p> </li> <li> <p>Clone the super-repo locally</p> <p>```bash</p> <p>git clone --recursive git@github.com:causify-ai/{repo_name}.git ~/src/{repo_name}{index} ```</p> </li> <li> <p>Checkout to a new branch</p> <p>```bash</p> <p>git checkout -b repo_init ```</p> </li> <li> <p>Add a submodule</p> <p>```bash</p> <p>cd ~/src/repo_name1</p> </li> <li> <p>The command will create a <code>.gitmodules</code> file that we need to check-in:</p> <p><code>text [submodule \"helpers_root\"] path = helpers_root url = git@github.com:causify-ai/helpers.git</code></p> </li> <li> <p>Init the submodule and commit the <code>.gitmodules</code> file</p> <p>```bash</p> <p>git submodule init git submodule update ```</p> </li> <li> <p>Commit and push the changes     <code>bash     &gt; git add .gitmodules helpers_root     &gt; git commit -am \"Add helpers subrepo\" &amp;&amp; git push</code></p> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#in_general_form","title":"In general form.","text":""},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#git_submodule_add_submodule_url_submodule_path","title":"&gt; git submodule add {submodule_url} {submodule_path}","text":""},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#example_for_cmamp","title":"Example for <code>cmamp</code>.","text":"<p>git submodule add git@github.com:causify-ai/helpers.git helpers_root ```</p>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#copy_and_customize_files_in_the_top_dir","title":"Copy and Customize Files in the Top Dir","text":"<ul> <li> <p>Conceptually you need to copy and customize the files in:</p> <ol> <li><code>thin_client</code> (one can reuse the thin client across repos)</li> <li>The top dir (to run <code>pytest</code>, ...)</li> <li><code>devops</code> (to build the dev and prod containers)</li> <li><code>.github/workflows</code> (to run the GitHub regressions)</li> </ol> </li> <li> <p>After copying the files you can search for the string <code>xyz</code> to customize</p> </li> <li> <p>If there is a problem with phases 3) and 4) due to the thin environment not     being completely configured, you can keep moving and then re-run the command     later</p> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#copy_files_from_existing_repo","title":"Copy Files From Existing Repo","text":"<ul> <li> <p>Copy the files from a working repo (e.g., <code>//cmamp</code>)</p> <p>```bash</p> <p>export SRC_DIR=$HOME/src/cmamp1 ls $SRC_DIR</p> <p>cp $SRC_DIR/{pytest.ini,repo_config.yaml,tasks.py} . vi pytest.ini repo_config.yaml tasks.py ```</p> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#script_approach","title":"Script Approach","text":"<ul> <li>Customize the script     <code>/dev_scripts_helpers/thin_client/sync_super_repo.sh</code> <code>DST_PREFIX=\"umd_msml610\"</code></li> <li>Run the script which allows to <code>vimdiff</code> / <code>cp</code> files across a super-repo and     its <code>//helpers</code> dir</li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#manual_approach","title":"Manual Approach","text":"<ul> <li> <p>Some files need to be copied from <code>//helpers</code> to the root of the super-repo to     configure various tools (e.g., dev container workflow, <code>pytest</code>, <code>invoke</code>)</p> <ul> <li><code>pytest.ini</code>: configure <code>pytest</code> preferences</li> <li><code>repo_config.yaml</code>: stores information about this specific repo (e.g., name,     used container, runnable dir config)<ul> <li>This needs to be modified <code>yaml repo_info:     repo_name: cmamp ... docker_info:     docker_image_name: cmamp ... runnable_dir_info:     use_helpers_as_nested_module: 1     ...     dir_suffix: cmamp</code></li> </ul> </li> <li><code>tasks.py</code>: the <code>invoke</code> tasks available in this container<ul> <li>This can be modified if needed</li> </ul> </li> </ul> </li> <li> <p>You can copy all these files from <code>//helpers</code> <code>bash     &gt; cp helpers_root/{pytest.ini,repo_config.yaml,tasks.py} .     &gt; vim pytest.ini repo_config.yaml tasks.py</code></p> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#copy_and_customize_files_in_thin_client","title":"Copy and Customize Files in <code>Thin_Client</code>","text":""},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#copy_files_from_existing_repo_1","title":"Copy Files From Existing Repo","text":"<ul> <li> <p>Copy the files from a working repo (e.g., <code>//cmamp</code>)</p> <p>```bash</p> <p>export SRC_DIR=$HOME/src/cmamp1 ls $SRC_DIR</p> <p>cp $SRC_DIR/{pytest.ini,repo_config.yaml,tasks.py} . vi pytest.ini repo_config.yaml tasks.py ```</p> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#script_approach_1","title":"Script Approach","text":"<ul> <li>Customize the script     <code>/dev_scripts_helpers/thin_client/sync_super_repo.sh</code> <code>DST_PREFIX=\"umd_msml610\"</code></li> <li>Run the script which allows to <code>vimdiff</code> / <code>cp</code> files across a super-repo and     its <code>//helpers</code> dir</li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#manual_approach_1","title":"Manual Approach","text":"<ul> <li> <p>Create the <code>dev_scripts_{dir_name}</code> dir based off the template from <code>helpers</code></p> <p>```bash</p> </li> <li> <p>The resulting <code>dev_scripts_{dir_name}</code> should look like:</p> <p>```bash</p> <p>ls -1 $DST_DIR build.py requirements.txt setenv.sh tmux.py ```</p> </li> <li> <p>If we don't need to create a new thin env, then you can delete the files     <code>dev_scripts_{dir_name}/thin_client/build.py</code> and <code>requirements.txt</code></p> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#use_a_suffix_based_on_the_repo_name_eg_tutorials_sports_analytics","title":"Use a suffix based on the repo name, e.g., <code>tutorials</code>, <code>sports_analytics</code>.","text":"<p>SRC_DIR=\"./helpers_root/dev_scripts_helpers/thin_client\"; ls $SRC_DIR DST_SUFFIX=\"xyz\" DST_DIR=\"dev_scripts_${DST_SUFFIX}/thin_client\"; echo $DST_DIR mkdir -p $DST_DIR cp -r $SRC_DIR/{build.py,requirements.txt,setenv.sh,tmux.py} $DST_DIR ```</p>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#build_and_test_the_thin_environment","title":"Build and Test the Thin Environment","text":""},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#build_the_thin_environment","title":"Build the Thin Environment","text":"<ul> <li>Build the thin environment:     <code>&gt; $DST_DIR/build.py     ... ==&gt; `brew cleanup` has not been run in the last 30 days, running now...     ... Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.     ... Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).     14:37:37 - INFO  build.py _main:94                  # gh version=gh version 2.58.0 (2024-10-01)     https://github.com/cli/cli/releases/tag/v2.58.0     14:37:37 - INFO  build.py _main:100                 /Users/saggese/src/quant_dashboard1/dev_scripts_quant_dashboard/thin_client/build.py successful</code></li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#test_the_thin_environment","title":"Test the Thin Environment","text":"<ul> <li>Follow     the on-boarding guide</li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#create_the_tmux_links","title":"Create the Tmux Links","text":"<ul> <li>Follow     the on-boarding guide</li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#copy_and_customize_files_in_devops","title":"Copy and Customize Files in <code>Devops</code>","text":"<ul> <li>Copy the <code>devops</code> template dir     <code>bash     &gt; (cd helpers_root; git pull)     &gt; cp -r helpers_root/devops devops</code></li> <li> <p>If we don't need to build a container and just we can reuse, then we can     delete the corresponding <code>build</code> directory</p> <p>```bash</p> <p>rm -rf devops/docker_build ```</p> </li> <li> <p>Follow the instructions in     <code>/docs/tools/dev_system/all.devops_docker.reference.md</code>     and     <code>/docs/tools/dev_system/all.devops_docker.how_to_guide.md</code></p> </li> <li> <p>To customize OS package dependencies:</p> <ul> <li>The Docker build system uses a declarative approach for OS package     installation</li> <li>Edit     <code>/devops/docker_build/os_packages/os_packages.txt</code>     to control which packages will be installed during the container build<ul> <li>Disable a package: add <code>#</code> at the beginning of the line</li> <li>Enable a package: remove the <code>#</code> from the beginning of the line</li> </ul> </li> <li>See     <code>/docs/tools/dev_system/all.devops_docker.reference.md#declarative-os-package-installation</code>     for details on the declarative OS package installation system, including     important notes about dependencies between <code>helpers</code> modules and OS packages</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#create_symbolic_links","title":"Create Symbolic Links","text":"<ul> <li> <p>Check the difference between the super-repo and <code>helpers</code></p> <p>```bash</p> <p>helpers_root/dev_scripts_helpers/thin_client/sync_super_repo.sh ```</p> </li> <li> <p>Replace file with symbolic links:</p> <p>```bash</p> <p>echo $SRC_DIR ./helpers_root/dev_scripts_helpers/thin_client echo $DST_DIR ./dev_scripts_xyz/thin_client ./helpers_root/helpers/create_links.py --src_dir $SRC_DIR --dst_dir $DST_DIR --replace_links --use_relative_paths ```</p> </li> <li> <p>Refer to     Managing common files     for explanation</p> </li> <li>Refer to     Managing symbolic links between directories     for how to use the commands</li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#5_build_container_and_running_tests","title":"5) Build Container and Running Tests","text":""},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#build_a_container_for_a_super-repo","title":"Build a Container for a Super-Repo","text":"<ul> <li> <p>Run the single-arch flow:</p> <p>```bash</p> <p>i docker_build_local_image --version 1.0.0 &amp;&amp; i docker_tag_local_image_as_dev --version 1.0.0 i docker_bash --skip-pull i docker_jupyter ```</p> </li> <li> <p>Run the multi-arch flow:</p> <p>```bash</p> <p>i docker_build_local_image --version 1.0.0 --multi-arch \"linux/amd64,linux/arm64\" i docker_tag_local_image_as_dev --version 1.0.0 ```</p> </li> <li> <p>If you wish to push the dev image to a remote registry, contact the infra team     to add new registry with default settings</p> <ul> <li>Make sure the registry name matches the repo name for consistency</li> <li>By default we add new containers to Stockholm region (<code>eu-north-1</code>)</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#check_if_the_regressions_are_passing","title":"Check If the Regressions Are Passing","text":"<ul> <li> <p>Follow     the on-boarding doc     to confirm that everything is set up properly</p> </li> <li> <p>File a PR with the new files and merge the PR into <code>master</code></p> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#configure_regressions_via_github_actions","title":"Configure Regressions Via Github Actions","text":""},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#set_repository_secretsvariables","title":"Set Repository Secrets/Variables","text":"<ul> <li> <p>Some secrets/variables are shared in an organization wide storage</p> <ul> <li>E.g. for Causify at     https://github.com/organizations/causify-ai/settings/secrets/actions</li> <li>These values are shared across all repos in the organization so we don't     need to create them on a per-repo basis<ul> <li>The access method is the same as for per-repo variables - via actions     context <code>${{ secrets.MY_TOKEN }}</code> or <code>`${{ vars.MY_TOKEN }}</code></li> </ul> </li> <li>Once a <code>secret</code> is set it's read-only for everybody. To preview all of the     raw values that are currently used, visit     1password &gt; Shared vault &gt; Causify org GH actions secrets</li> </ul> </li> <li> <p>Before adding a new secret/variables for a repo, consider the following:</p> <ul> <li>If it's already present in the global storage for an organization, no action     is required</li> <li>If it's not, check if the newly added value is not needed in all of the     repos, if so, add it to the global storage to facilitate reusability<ul> <li>If you lack permissions for this operation, contact your TL</li> </ul> </li> </ul> </li> <li> <p>Should a repo need some additional secret values/variables, follow the     procedure below</p> </li> <li> <p>Login to 1password    https://causify.1password.com/home</p> </li> <li>Ask your TL if you don't have access to 1password</li> <li>Navigate to the <code>Shared Vault</code></li> <li>Search for <code>Github actions secrets JSON</code> secret</li> <li>Copy the JSON from 1password to a temporary local file <code>vars.json</code></li> <li>Run the script to set the secrets/variables    ```bash <p>cd ~/src/ ./helpers_root/dev_scripts_helpers/github/set_secrets_and_variables.py \\                 --file `vars.json' \\                 --repo '/'    ``` <li> <p>Make sure not to commit the raw <code>vars.json</code> file or the    <code>/dev_scripts_helpers/github/set_secrets_and_variables.py.log</code>    file</p> </li> <li> <p>Delete those files locally</p> </li>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#create_github_actions_workflow_files","title":"Create Github Actions Workflow Files","text":"<ol> <li>Create a directory <code>./github/workflows</code> in the super-repo</li> <li>Copy an example flow from helpers</li> <li>E.g. `helpers_root/.github/workflows/fast_tests.yml</li> <li>Modify it based on your needs<ul> <li>Find and replace mentions of <code>helpers</code> with the name of super repo for    consistency</li> <li>Replace <code>invoke run_fast_tests</code> with your desired action</li> </ul> </li> </ol>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#configure_gitleaks_scan","title":"Configure Gitleaks Scan","text":"<ul> <li> <p>Copy the configuration and workflow files</p> <p>```bash</p> <p>cp ./helpers_root/.github/gitleaks-rules.toml ./.github cp ./helpers_root/.github/workflows/gitleaks.yml ./.github/workflows ```</p> </li> <li> <p>Replace files with symbolic links</p> <p>```bash</p> <p>./helpers_root/helpers/create_links.py --src_dir ./helpers_root/.github --dst_dir ./.github --replace_links --use_relative_paths ```</p> </li> <li> <p>Note:</p> <ul> <li>Only the <code>gitleaks-rules.toml</code> file should be replaced with symbolic links</li> <li>The <code>gitleaks.yml</code> file should be copied as is because GitHub Actions does     not resolve symbolic links when parsing workflows in the <code>.github/workflows</code>     directory (See #CmampTask11429)</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.create_a_super_repo_with_helpers.how_to_guide.html#configure_github_repo","title":"Configure Github Repo","text":"<ul> <li> <p>Disclaimer: the following set-up requires paid GitHub version (Pro / Team     / Enterprise)</p> </li> <li> <p>Set-up branch protection rule for master</p> <ul> <li>Navigate to <code>https://github.com/&lt;your org&gt;/&lt;&lt;your-repo&gt;&gt;/settings/branches</code></li> <li>Click \"Add rule\"<ul> <li>Specify branch name pattern <code>master</code></li> <li>Check the following options:<ul> <li><code>Require a pull request before merging</code> (do not check the sub-options)</li> <li><code>Require status checks to pass before merging</code><ul> <li>Check the <code>Require branches to be up to date before merging</code>     sub-option</li> <li>In the <code>Status checks that are required</code> table specify the workflows     you want to pass before merging each PR<ul> <li>Depends on which workflows were set-up in the step above</li> <li>Usually its <code>run_fast_tests</code> and <code>run_slow_tests</code></li> </ul> </li> </ul> </li> <li><code>Require conversation resolution before merging</code></li> </ul> </li> <li>Click \"Save changes\" button</li> </ul> </li> </ul> </li> <li> <p>You need to sync the repo using the sync script</p> </li> </ul>"},{"location":"tools/dev_system/all.dealing_with_missing_dependencies.how_to_guide.html","title":"All.dealing with missing dependencies.how to guide","text":""},{"location":"tools/dev_system/all.dealing_with_missing_dependencies.how_to_guide.html#dealing_with_missing_dependencies_in_docker_images","title":"Dealing with missing dependencies in Docker images","text":"<ul> <li>Sometimes our scripts need to use dependencies that are not available in our     primary Docker image</li> <li>If the package is necessary for the entire project and not just for a specific     script, we can request to have it installed in the Docker image and release a     new version of the image</li> <li> <p>However, sometimes the packages are only needed for a specific script or part     of a project. Thus, we don't want to install them in the Docker image as it     would bloat the image</p> </li> <li> <p>Due to missing dependencies, the build process will fail when tests are     discovered or when the module is imported and used in another module</p> </li> <li>The following are some common workarounds to deal with the missing     dependencies without breaking the build</li> </ul>"},{"location":"tools/dev_system/all.dealing_with_missing_dependencies.how_to_guide.html#quick_hacks","title":"Quick hacks","text":"<ul> <li> <p>In test files,</p> <ul> <li>We can use <code>pytest.importorskip</code> to skip the tests if the package is not     available     <code>python pytest.importorskip(\"somepackage\")</code></li> </ul> </li> <li> <p>In non-test files,</p> <ul> <li>We do not need to do anything besides avoiding importing the module</li> <li>If the file is discovered by <code>pytest</code> and the image doesn't have the package     installed, it will be skipped with the <code>pytest.importorskip</code> in the test     file</li> </ul> </li> <li> <p>Why not installing the package on the fly within the script?</p> <ul> <li>While it is possible, it is NOT a good practice</li> <li>This approach is NOT great because every time somebody imports that module     (even pytest during test discovery), the package gets installed<ul> <li>Bad <code>python subprocess.call([\"sudo\", \"/venv/bin/pip\", \"install\", \"somepackage\"])</code></li> </ul> </li> <li>An exception is in Jupyter Notebooks, where it's acceptable to install     packages on the fly for prototyping, experimenting, or running analyses</li> <li>However, we should \"comment out\" those lines afterwards, since Jupyter     Notebooks are often converted to Python scripts (through jupytext), and we     don't want these installation commands running automatically</li> <li>Example (in a Jupyter Notebook cell)     <code>bash     !sudo sudo /venv/bin/pip install --quiet somepackage)\"</code></li> </ul> </li> <li> <p>If we want to implement different variations of the code depending on whether     the package is available or not, we can use the following approaches     <code>python _HAS_MOTO = henv.has_module(\"moto\") if _HAS_MOTO:     import moto     # Implementation that uses moto. else:     # Implementation that doesn't use moto.</code></p> </li> </ul>"},{"location":"tools/dev_system/all.dealing_with_missing_dependencies.how_to_guide.html#delay_the_evaluation","title":"Delay the evaluation","text":"<ul> <li> <p>Sometimes it's sufficient to delay when a module is imported to prevent errors     during script loading or testing</p> </li> <li> <p>Move the <code>import</code> statement inside the function or class definition so that     the module is only required when that part of the code is executed</p> <p><code>python def my_function(): import somepackage</code></p> <p><code>python class MyClass: import somepackage ...</code></p> </li> </ul>"},{"location":"tools/dev_system/all.dealing_with_missing_dependencies.how_to_guide.html#type_annotations","title":"Type annotations","text":"<ul> <li> <p>If the missing dependency is only needed for type annotations, we can postpone     the evaluation of the annotations to avoid errors during module loading or     pytest discovery</p> </li> <li> <p>One option is to use string literals for type hints</p> <p><code>python def my_function(param: \"SomeClass\") -&gt; \"SomeClass\":     ...</code></p> </li> <li> <p>Another option is to use the <code>__future__</code> module to enable postponed     evaluation of type annotations     <code>python from __future__ import annotations</code></p> </li> </ul>"},{"location":"tools/dev_system/all.dealing_with_missing_dependencies.how_to_guide.html#long_term_solution","title":"Long term solution","text":"<ul> <li>While quick hacks are useful temporary workarounds, they are not a long term     solution</li> </ul>"},{"location":"tools/dev_system/all.dealing_with_missing_dependencies.how_to_guide.html#add_the_dependency_to_the_docker_image","title":"Add the dependency to the Docker image","text":"<ul> <li>Obviously, if the package is needed for the entire project, we should add it     to the Docker image and release a new version of the image</li> </ul>"},{"location":"tools/dev_system/all.dealing_with_missing_dependencies.how_to_guide.html#create_dockerized_executable","title":"Create \"dockerized\" executable","text":"<ul> <li>We can create a \"dockerized\" executable that can be run in a Docker container     with all the dependencies pre-installed</li> <li>A \"dockerized\" executable is easier to set up compared to a runnable directory</li> <li>It is suitable for utility scripts that have a single and well defined purpose     and do not have a large number of dependencies</li> <li>The image is built on the fly</li> <li>Some examples of dockerized executables:<ul> <li><code>dockerized_prettier</code></li> <li><code>dockerized_mermaid</code></li> <li><code>dockerized_extract_notebook_images</code></li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.dealing_with_missing_dependencies.how_to_guide.html#create_runnable_directory","title":"Create \"runnable\" directory","text":"<ul> <li>We can create a \"runnable\" directory that contains all the code and a <code>devops</code>     directory so it can build its own container with all the dependencies needed     to run and be tested</li> <li>A \"runnable\" dir is more complex to set up than the previous approach</li> <li>It is good for larger projects or parts of a project with many interacting     components that have many dependencies</li> <li>The image can be built and stored in the container registry</li> <li>Some examples of runnable directories:<ul> <li><code>infra</code></li> <li><code>optimizer</code></li> <li><code>sports_analytics</code></li> <li><code>tutorial_langchain</code></li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html","title":"All.devops docker.how to guide","text":""},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#high_level_philosophy","title":"High level philosophy","text":"<ul> <li>We use Docker extensively and assume you are familiar with Docker concepts and     workflows</li> <li>A short tutorial about Docker is     /docs/tools/docker/all.docker.tutorial.md</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#separate_docker_containers","title":"Separate Docker containers","text":"<ul> <li> <p>We always want to separate things that don't need to run together in different     containers along repos or \"independently runnable / deployable directories\"</p> <ul> <li>E.g., <code>dev / prod cmamp</code>, <code>optimizer</code>, <code>im</code>, <code>oms</code></li> </ul> </li> <li> <p>The rationale is that when we put too many dependencies in a single container,     we start having huge containers that are difficult to deploy and are unstable     in terms of building even when using tools like <code>poetry</code>.</p> </li> <li>Each dir that can be \"deployed\" and run should have a <code>devops</code> dir to build /     QA / release containers with all the needed dependencies</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#pre-built_vs_build-on-the-fly_containers","title":"Pre-built vs build-on-the-fly containers","text":"<ol> <li> <p>Certain containers that need to be widely available to the team and deployed    go through the release process and are stored in ECR (AWS and DockerHub)</p> </li> <li> <p>Other containers that are lightweight and used only by one person can be    built on the fly using <code>docker compose</code> / <code>docker build</code>.</p> </li> <li> <p>E.g., the <code>infra</code> container, containers to run a simple tools</p> </li> <li> <p>Sometimes we install a dependency on the fly from inside the code, when it's    needed only rarely and it doesn't justify being added to the Docker container</p> </li> <li>E.g., when we need to profile Python code, we install the package directly      in the container</li> </ol>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#thin_client","title":"Thin client","text":"<ul> <li>To bootstrap the system we use a \"thin client\" which installs in a virtual env     the minimum set of packages to run (e.g., installs <code>invoke</code>, <code>docker</code>, etc).</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#amp_cmamp_container","title":"amp / cmamp container","text":"<ul> <li>The <code>dev</code> version is used to develop</li> <li> <p>The <code>prod</code> version is used for deployment as shortcut to creating a smaller     container with only the strictly needed dependencies</p> </li> <li> <p>In order to avoid shipping the monster <code>cmamp</code> dev / prod container, we want     to start building smaller containers with only the dependencies that specific     prod scripts need</p> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#stages","title":"Stages","text":"<ul> <li>A \"stage\" is a step (e.g., local, dev, prod) in our release workflow of Docker     images, code, or infrastructure.</li> <li>To run a Docker container in a certain stage use the <code>stage</code> parameter<ul> <li>E.g. <code>i docker_bash --stage=\"local\"</code> creates a bash session inside the local     Docker <code>amp</code> container</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#local","title":"Local","text":"<ul> <li>A <code>local</code> image is used to develop and test an update to the Docker container<ul> <li>E.g. after updating a package, installing a new package, etc.</li> </ul> </li> <li>Local images can only be accessed locally by a developer, i.e. the team     members can not / should not use local images</li> <li>In practice <code>local</code> images are like <code>dev</code> images but private to users and     servers</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#dev","title":"Dev","text":"<ul> <li>A <code>dev</code> image is used by team members to develop our system<ul> <li>E.g., add new functionalities to the <code>cmamp</code> or <code>helpers</code> codebase</li> </ul> </li> <li>The source code is mounted through a bind mount in Docker so that one can     change the code and execute it in Docker, without rebuilding the container</li> <li>A local image is tested, blessed, and released as <code>dev</code> so that users and CI     can pull it and use it knowing that it's sane</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#prod","title":"Prod","text":"<ul> <li>A <code>prod</code> image is used to run a system by final users<ul> <li>E.g., Linter inside <code>helpers</code>, some prod system inside Airflow</li> </ul> </li> <li>It is self-contained (i.e., it has no dependencies) since it contains     everything required to run a system<ul> <li>E.g., OS, Python packages code, code</li> </ul> </li> <li>A <code>prod</code> image is typically created from the <code>dev</code> image by copying the     released code inside the <code>prod</code> image</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#registries","title":"Registries","text":"<ul> <li> <p>We use several Docker image registries</p> <ul> <li>AWS ECR: images are used on the dev and prod servers</li> <li>GHCR (GitHub Container Repo): images are used by the CI systems</li> <li>DockerHub: images are used by developers on the public facing repo</li> </ul> </li> <li> <p>In practice we mirror the Docker images to make sure they are close to where     they need to be used and they minimize downloading costs</p> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#invoke_targets","title":"invoke targets","text":""},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#single-arch_flow","title":"Single-arch flow","text":"<ul> <li><code>docker_build_local_image</code>: build a \"local\" image, i.e., a release candidate     for the \"dev\" image</li> <li><code>docker_tag_local_image_as_dev</code>: promote \"local\" image to \"dev\" without     pushing it to the remote</li> <li><code>docker_push_dev_image</code>: push image to an image registry</li> <li><code>docker_release_dev_image</code>: build, test the \"dev\" image and then release it to     ECR</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#multi-arch_flow","title":"Multi-arch flow","text":"<ul> <li><code>docker_tag_push_multi_build_local_image_as_dev</code>: build a \"local\" multi-arch     image and tag it as \"dev\"</li> <li><code>docker_release_multi_build_dev_image</code>: same as <code>docker_release_dev_image</code> but     for multi-arch image</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#prod_flow","title":"Prod flow","text":"<ul> <li><code>docker_build_prod_image</code>: build a \"prod\" image from a dev image</li> <li><code>docker_push_prod_image</code>: push the \"prod\" image to ECR</li> <li><code>docker_push_prod_candidate_image</code>: push the \"prod\" candidate image to ECR</li> <li><code>docker_build_multi_arch_prod_image</code>: build a multi arch. versioned \"prod\"     image from a \"dev\" image</li> <li><code>docker_tag_push_multi_arch_prod_image</code>: mark the multi-arch versioned \"prod\"     image as \"prod\" and push them to the target registry</li> <li><code>docker_release_prod_image</code>: build, test, and release the \"prod\" image to ECR</li> <li><code>docker_release_multi_arch_prod_image</code>: build, test, and release to Docker     registries the multi-arch prod image</li> <li><code>docker_release_all</code>: release both the \"dev\" and \"prod\" image to ECR</li> <li><code>docker_rollback_dev_image</code>: rollback the version of the \"dev\" image</li> <li><code>docker_rollback_prod_image</code>: rollback the version of the \"prod\" image</li> <li><code>docker_create_candidate_image</code>: create a new \"prod\" candidate image</li> <li><code>docker_update_prod_task_definition</code>: update image in \"prod\" task definition     to the desired version</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#how_to_test_a_package_in_a_docker_container","title":"How to test a package in a Docker container","text":""},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#install_a_package_in_a_container","title":"Install a package in a container","text":"<ul> <li>You can install a Python package with the following command in Docker     <code>bash     docker&gt; sudo /bin/bash -c \"(source /venv/bin/activate; pip install yfinance)\"     docker&gt; python -c \"import finance\"</code></li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#hacky_approach_to_patch_up_a_container","title":"Hacky approach to patch up a container","text":"<ul> <li> <p>To patch up a container you can use the following instructions</p> <p>```bash</p> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#after_changing_the_container_create_a_new_version_of_the_container","title":"After changing the container, create a new version of the container.","text":"<p>docker commit d2916dd5f122 \\         623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpr</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#push_the_updated_container_to_the_repo","title":"Push the updated container to the repo.","text":"<p>docker push 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#now_you_can_pull_the_container_on_different_machines","title":"Now you can pull the container on different machines.","text":"<p>docker pull 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#to_use_docker_bash_you_might_need_to_retag_it_to_match_what_the_system_expects","title":"To use <code>docker_bash</code> you might need to retag it to match what the system expects","text":"<p>docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev_ccxtpro ```</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#release_a_docker_image","title":"Release a Docker image","text":"<ul> <li>All the <code>invoke</code> tasks to run the release flow are in     <code>/helpers/lib_tasks.py</code></li> <li>Depending on the type of changes sometimes one needs to rebuild only the     <code>prod</code> image, other times one needs to rebuild also the <code>dev</code> image</li> <li> <p>E.g.,</p> <ul> <li>If you change Docker build-related things (e.g., add a Python package), you     need to rebuild the <code>dev</code> image and then the <code>prod</code> image from the <code>dev</code>     image</li> <li>If you change the code for a production system, you need to create a new     <code>prod</code> image</li> </ul> </li> <li> <p>We try to use the same build/release flow, conventions, and code for all the     containers (e.g., <code>amp</code>, <code>cmamp</code>, <code>helpers</code>, <code>opt</code>)</p> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#overview_of_how_to_release_an_image","title":"Overview of how to release an image","text":"<ul> <li> <p>The release flow consists of the following phases</p> <ul> <li>Make changes to the image<ul> <li>E.g., add Python package through <code>poetry</code>, add an OS package, ...</li> </ul> </li> <li>Update the changelog</li> <li>Build a local image<ul> <li>Run specific tests (e.g., make sure that the new packages are installed)</li> <li>Run unit tests</li> <li>Run QA tests</li> </ul> </li> <li>Tag local image as dev image</li> <li>Push dev image to ECR, DockerHub, GHCR</li> </ul> </li> <li> <p>If there is also an associated prod image</p> <ul> <li>Build prod image from dev image<ul> <li>Run unit tests</li> <li>Run QA tests</li> </ul> </li> <li>Push prod image to ECR, DockerHub, GHCR</li> </ul> </li> <li> <p>The release flow is implemented in the <code>invoke</code> tasks:</p> <ul> <li>E.g., <code>docker_release_dev_image</code>, <code>docker_release_prod_image</code>, etc.</li> <li>These tasks are quite complex and perform many operations</li> <li>Currently, we don't run them directly \u2014 instead, we execute the individual     steps from the <code>invoke</code> tasks</li> <li>Once the release process is more stable, we plan to run them directly</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#how_to_add_a_python_package_to_dev_image","title":"How to add a Python package to dev image","text":"<ul> <li> <p>To add a new Python package to a Docker image you need to update <code>poetry</code>     files and release a new image:</p> <ul> <li>Add a new package to     <code>/devops/docker_build/pyproject.toml</code>     file to the <code>[tool.poetry.dependencies]</code> section</li> <li>E.g., to add <code>pytest-timeout</code> do:     <code>text     [tool.poetry.dependencies]     ...     pytest-timeout = \"*\"     ...</code></li> <li>In general we use the latest version of a package <code>*</code> whenever possible<ul> <li>If the latest package has some problems with our codebase, we freeze the     version of the problematic packages to a known-good version to get the     tests back to green until the problem is solved. We switch back to the     latest version once the problem is fixed</li> <li>If you need to put a constraint on the package version, follow the     official docs,     and explain in a comment why this is needed making reference to GitHub     issues</li> </ul> </li> </ul> </li> <li> <p>To verify that package is installed correctly:</p> <ul> <li>Build a local image<ul> <li>There are two options:<ul> <li>Update <code>poetry</code> and upgrade all packages to the latest versions     <code>bash     &gt; i docker_build_local_image --version {version} --poetry-mode=\"update\"</code></li> <li>Install packages from the current <code>poetry.lock</code> file without upgrading     the packages     <code>bash     &gt; i docker_build_local_image --version &lt;VERSION&gt; --poetry-mode=\"no_update\"</code></li> </ul> </li> </ul> </li> <li>Run a docker container based on the local image     <code>bash     &gt; i docker_bash --stage local --version &lt;VERSION&gt;</code></li> <li>Verify what package was installed with <code>pip show {package name}</code>, e.g.,     <code>bash     &gt; pip show pytest-rerunfailures     Name: pytest-rerunfailures     Version: 10.2     Summary: pytest plugin to re-run tests to eliminate flaky failures     ...     Location: /venv/lib/python3.8/site-packages     Requires: pytest, setuptools     Required-by:</code></li> <li>Run regressions for the local image, i.e.     <code>bash     &gt; i run_fast_tests --stage local --version &lt;VERSION&gt;     &gt; i run_slow_tests --stage local --version &lt;VERSION&gt;</code></li> </ul> </li> <li> <p>Update the changelog describing the new version</p> </li> <li>Send a PR with the updated <code>poetry</code> files and any other change needed to make     the tests pass</li> <li>Release the new image following the     Release a Docker image     section, use <code>--update-poetry</code> flag to resolve the dependencies</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#how_to_find_unused_packages","title":"How to find unused packages","text":"<ul> <li> <p>While installing Python packages we need to make sure that we do not install     packages that are not used</p> </li> <li> <p>You can use the import-based approach using     <code>pipreqs</code></p> <ul> <li>Under the hood it uses the regex below and <code>os.walk</code> for selected dir:     <code>python REGEXP = [     re.compile(r'^import (.+)$'),     re.compile(r'^from ((?!\\.+).*?) import (?:.*)$') ]</code></li> </ul> </li> <li> <p><code>pipreqs</code> has some limitations</p> <ul> <li>Not all packages that we use are necessarily imported, e.g. <code>awscli</code>,     <code>jupyter</code>, <code>pytest-cov</code>, etc. so <code>pipreqs</code> won't find these packages</li> <li>The import name is not always equal to the package actual name, see the     mapping here</li> </ul> </li> <li> <p>Run a bash session inside a Docker container</p> </li> <li>Install <code>pipreqs</code> with <code>sudo pip install pipreqs</code><ul> <li>We install it temporary within a Docker bash session in order to introduce     another dependency</li> <li>You need to re-install <code>pipreqs</code> every time you create a new Docker bash     session</li> </ul> </li> <li>To run for a root dir do:     <code>bash     docker&gt; pipreqs . --savepath ./tmp.requirements.txt</code></li> <li>The command above will generate <code>./tmp.requirements.txt</code> with the list of the     imported packages, e.g.,     <code>text     amp==1.1.4     async_solipsism==0.3     beautifulsoup4==4.11.1     botocore==1.24.37     cvxopt==1.3.0     cvxpy==1.2.0     dill==0.3.4     environs==9.5.0     ...</code></li> <li>You can grep for a package name to see where it is used, e.g.,     <code>bash     &gt; jackpy \"dill\"     helpers/hpickle.py:108:       import dill     ...</code></li> <li>See the official docs for the advanced     usage</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#how_to_build_a_local_image","title":"How to build a local image","text":"<ul> <li>The <code>local</code> image is built using     <code>/devops/docker_build/dev.Dockerfile</code></li> <li> <p>This Dockerfile runs various scripts to install:</p> <ul> <li>OS</li> <li>Python</li> <li><code>venv</code> + Python packages</li> <li>Jupyter extensions</li> <li>Application-specific packages (e.g., for Linter)</li> </ul> </li> <li> <p>To build a local image run:</p> <p>```bash</p> <p>i docker_build_local_image --version 1.0.0</p> </li> <li> <p>Once an image is built, it is tagged as <code>local-${user}-${version}</code>, e.g.,     <code>local-saggese-1.0.0</code>     ```     Successfully tagged 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.0.9</p> <p>docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.0.9 REPOSITORY                                         TAG                    IMAGE ID            CREATED                  SIZE 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp   local-gsaggese-1.0.9   cf16e3e3d1c7        Less than a second ago   2.75GB ```</p> </li> <li> <p>A local image is a candidate for becoming a <code>dev</code> image.     <code>&gt; i run_fast_tests --stage local --version 1.0.0</code></p> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#build_from_scratch_and_not_incrementally","title":"Build from scratch and not incrementally.","text":"<p>i docker_build_local_image --version 1.0.0 --no-cache</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#update_poetry_package_list","title":"Update poetry package list.","text":"<p>i docker_build_local_image --version 1.0.0 --poetry-mode=\"update\"</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#see_more_options","title":"See more options:","text":"<p>i docker_build_local_image -h ```</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#testing_the_local_image","title":"Testing the local image","text":"<ul> <li> <p>Compare the version of the packages in the different images</p> <p>```bash</p> <p>i docker_bash docker&gt; pip list | tee pip_packages.dev.txt</p> <p>i docker_bash --stage local --version 1.0.9 docker&gt; pip list | tee pip_packages.local.txt ```</p> </li> <li> <p>Or in one command:     <code>&gt; i docker_cmd --cmd \"pip list | tee pip_packages.dev.txt\"; i docker_cmd --stage=local --version=1.0.9 --cmd \"pip list | tee pip_packages.local.txt\"     &gt; vimdiff pip_packages.dev.txt pip_packages.local.txt</code></p> </li> <li> <p>You can move the local image on different servers for testing by pushing it on     ECR:     <code>&gt; i docker_login     &gt; i docker push 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.1.0</code></p> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#pass_the_local_image_to_another_user_for_testing","title":"Pass the local image to another user for testing","text":"<ul> <li>Push the local image built by a user to ECR registry</li> <li> <p>For e.g., if the image is built by user <code>gsaggese</code> <code>&gt; i docker_login     &gt; i docker push 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.1.0</code></p> </li> <li> <p>From user session who wants to test: pull the local image from ECR     <code>&gt; i docker pull 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.1.0</code></p> </li> <li> <p>Tag the local image from user <code>gsaggese</code>, who built the image, as     <code>local-currentuser-1.1.0</code> for user <code>currentuser</code> who wants to test it     <code>&gt; i docker tag 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-gsaggese-1.1.0 665840871993.dkr.ecr.us-east-1.amazonaws.com/amp:local-currentuser-1.1.0</code></p> </li> <li> <p>Run any kind of test using the local image. For e.g., to run fast tests     <code>&gt; i run_fast_tests --stage local --version 1.1.0</code></p> </li> <li> <p>Check something inside the container     <code>&gt; i docker_bash --stage local --version 1.1.0     docker &gt; pip freeze | grep pandas</code></p> </li> <li>After testing and making sure the regressions are green, make sure to tag the     image built by the initial user as <code>dev</code> and not the one tagged for the     <code>current-user</code></li> <li>This will make sure image is tagged for both <code>arm</code> and <code>x86</code> architecture on     the remote registries</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#tag_local_image_as_dev","title":"Tag <code>local</code> image as <code>dev</code>","text":"<ul> <li>A docker tag is just a way of referring to an image, in the same way Git tags     refer to a particular commit in your history</li> <li>Basically, tagging is creating a reference from one image (e.g.,     <code>local-saggese-1.0.0</code>) to another (<code>dev</code>)</li> <li>Once the <code>local</code> (e.g., <code>local-saggese-1.0.0</code>) image is tagged as <code>dev</code>, your     <code>dev</code> image becomes equal to <code>local-saggese-1.0.0</code></li> <li><code>dev</code> image is also tagged with <code>dev-${version}</code>, e.g., <code>dev-1.0.0</code> to     preserve history and allow for quick rollback</li> <li>Locally in Git repository a git tag <code>${repo_name}-${version}</code>, e.g.     <code>cmamp-1.0.0</code> is created in order to properly control sync between code and     container</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#push_image","title":"Push image","text":"<ul> <li>Pushing <code>dev</code> or <code>prod</code> image means sending them to our docker registries</li> <li>Once an image is pushed, it can be used by the team members by running     <code>i docker_pull</code></li> <li>Local git tag <code>${repo_name}-${version}</code>, e.g. <code>cmamp-1.0.0</code>, is pushed at this     stage to the remote repository to allow others to properly control sync     between code and container.</li> <li>To be able to push an image to the ECR one should have permissions to do so</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#end-to-end_flow_for_dev_image","title":"End-to-end flow for <code>dev</code> image","text":"<ul> <li> <p>Conceptually the flow consists of the following phases:</p> <ol> <li>Build a local image of docker</li> <li><code>i docker_build_local_image --version 1.0.0</code></li> <li>Run fast tests to verify that nothing is broken</li> <li><code>i run_fast_tests --stage local --version 1.0.0</code></li> <li>Run specific end-to-end tests by</li> <li>Tag <code>local</code> image as <code>dev</code></li> <li><code>i docker_tag_local_image_as_dev --version 1.0.0</code></li> <li>Push <code>dev</code> image to the docker registry</li> <li><code>i docker_push_dev_image --version 1.0.0</code></li> <li>The mentioned flow is executed by <code>Build dev image</code> GH action and that is a     preferred way to do an image release</li> </ol> </li> <li> <p>For specific cases that can not be done via GH action see commands below:</p> <p>```bash</p> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#to_run_the_official_flow_end-to-end","title":"To run the official flow end-to-end:","text":"<p>i docker_release_dev_image --version 1.0.0</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#to_see_the_options","title":"To see the options:","text":"<p>i docker_release_dev_image -h</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#run_from_scratch_and_not_incrementally","title":"Run from scratch and not incrementally:","text":"<p>i docker_release_dev_image --version 1.0.0 --no-cache</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#force_an_update_to_poetry_to_pick_up_new_packages","title":"Force an update to poetry to pick up new packages","text":"<p>i docker_release_dev_image --version 1.0.0 --update-poetry</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#skip_running_the_qa_tests","title":"Skip running the QA tests","text":"<p>i docker_release_dev_image --version 1.0.0 --no-qa-tests</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#skip_running_the_tests","title":"Skip running the tests","text":"<p>i docker_release_dev_image --version 1.0.0 --skip-tests</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#skip_end-to-end_tests","title":"Skip end-to-end tests","text":"<p>i docker_release_dev_image --version 1.0.0 --no-run-end-to-end-tests ```</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#multi-architecture_build","title":"Multi-architecture build","text":"<ul> <li> <p>We allow to build multi-architecture Docker image using     <code>docker_build_local_image</code></p> <p>```bash</p> <p>i docker_build_local_image --version  --multi-arch  <code>- To build for specific platforms specify the platform name:     - For `x86`: `linux/amd64`     - For `arm`: `linux/arm64` - To build for both `arm` and `x86` architectures:</code>     &gt; i docker_build_local_image --version  --multi-arch linux/amd64,linux/arm64     ``` <li> <p>Multi-arch images are built using <code>docker buildx</code> which does not generate any     local image by default</p> </li> <li>Images are pushed to the remote registry and pulled for testing and usage</li> <li>To tag the local image as dev and push it to the target registry: e.g.,     <code>aws_ecr.ck</code> or <code>dockerhub.causify</code> , use     <code>bash     &gt; i docker_tag_push_multi_build_local_image_as_dev --version &lt;VERSION&gt; --target-registry &lt;TARGET&gt;</code></li> <li>Once the image has been successfully pushed to both ECR and DockerHub     registries, the subsequent step involves pushing the <code>dev</code> image to GHCR     registry. However, this action currently requires manual execution due to     restricted access<ul> <li>Access to the <code>causify-ai</code> packages is limited. To gain access, kindly reach     out to GP, Samarth or Vlad</li> <li>To proceed, perform a Docker login using your GitHub username and PAT     (Personal Access Token):     <code>bash     &gt; docker login ghcr.io -u &lt;username&gt;</code></li> <li>Tag the <code>dev</code> image to the GHCR namespace:     <code>bash     &gt; docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev ghcr.io/causify-ai/cmamp:dev</code></li> <li>Push the tagged image to the GHCR registry:     <code>bash     &gt; docker push ghcr.io/causify-ai/cmamp:dev</code></li> </ul> </li>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#release_a_multi-architecture_dev_image","title":"Release a multi-architecture dev image","text":""},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#overview","title":"Overview","text":"<ul> <li>Update the <code>changelog.txt</code> file with a description of the new version</li> <li>Build \"local\" image remotely in the CK AWS ECR registry and pull once it is     built</li> <li>Run the <code>cmamp</code> regressions using a local image</li> <li>Run QA tests using a local image</li> <li>Tag the image as dev image and push it to the target Docker registries</li> <li>Tag the new <code>dev</code> image to GHCR namespace and push it to GHCR registry</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#pre-release_check-list","title":"Pre-release check-list","text":"<p>Prerequisites:</p> <ul> <li>The new image is built locally</li> </ul> <p>Check-list:</p> <ul> <li> <p>Make sure that the regressions are passing when being run using the local     image because we run the regressions as part of the official release flow,     i.e. via <code>docker_release_multi_build_dev_image()</code>.</p> <ul> <li><code>cmamp</code><ul> <li>[ ] Update the <code>changelog.txt</code> file</li> <li>[ ] Fast tests</li> <li>[ ] Slow tests</li> <li>[ ] Super-slow test</li> <li>[ ] QA tests</li> </ul> </li> </ul> </li> <li> <p>Running regressions in the <code>orange</code> repository is not a part of the official     image release flow so run them separately.</p> <ul> <li><code>orange</code><ul> <li>[ ] Update the <code>changelog.txt</code> file</li> <li>[ ] Fast tests</li> <li>[ ] Slow tests</li> <li>[ ] Super-slow test</li> </ul> </li> </ul> </li> <li> <p>Example:     <code>bash     &gt; i run_fast_tests --version 1.10.0 --stage local</code>     where <code>1.10.0</code> is the new version of the image with stage as local.</p> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#command_to_run_the_release_flow","title":"Command to run the release flow","text":"<ul> <li> <p>To run the release flow</p> <p>```bash</p> <p>i docker_release_multi_build_dev_image \\         --version  \\         --target-registries  ``` <p>where - TARGET_REGISTRIES: list of target registries to push the image to. E.g.,     - <code>aws_ecr.ck</code>: private CK AWS Docker registry     - <code>dockerhub.causify</code>: public Dockerhub registry - All the other options are the same as for the <code>docker_release_dev_image</code>     end-to-end flow.</p> <li> <p>E.g.,     <code>&gt; i docker_release_multi_build_dev_image \\             --version 1.6.1 \\             --target-registries aws_ecr.ck,dockerhub.causify</code></p> </li>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#post-release_check-list","title":"Post-release check-list","text":"<ul> <li>[ ] Make an integration with the <code>kaizenflow</code> repository in order to copy all             the changes from the <code>cmamp</code> repository</li> <li>[ ] Tag the new <code>dev</code> image to GHCR namespace and push it to GHCR registry</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#build_prod_image","title":"Build prod image","text":"<ul> <li> <p>The main differences between <code>dev</code> image and <code>prod</code> image are:</p> <ul> <li>The source code is accessed through a bind mount for <code>dev</code> image (so that it     can be easily modified) and copied inside the image for a <code>prod</code> image     (since we want to package the code in the container)</li> <li>Requirements to be installed are different:<ul> <li><code>dev</code> image requires packages to develop and run the code</li> <li><code>prod</code> image requires packages only to run the code</li> </ul> </li> </ul> </li> <li> <p>The recipe to build a <code>prod</code> image is in     <code>/devops/docker_build/prod.Dockerfile</code>.</p> </li> <li> <p>To build the <code>prod</code> image run:</p> <p>```bash</p> <p>i docker_build_prod_image --version 1.0.0</p> </li> <li> <p>To run a command inside the prod image     <code>&gt; docker run --rm -t --user $(id -u):$(id -g) --workdir=/app     665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:prod-1.0.3 \"ls -l /app\"</code></p> </li> <li>Example of a complex command:     <code>&gt; docker run --rm -t --workdir=/app 665840871993.dkr.ecr.us-east-1.amazonaws.com/cmamp:prod-1.0.3 \"python /app/datapull/ccxt/data/extract/download_realtime.py --to_datetime '20211204-194432' --from_datetime '20211204-193932' --dst_dir 'test/ccxt_test' --data_type 'ohlcv' --api_keys 'API_keys.json' --universe 'v03'\"</code></li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#check_the_options","title":"Check the options.","text":"<p>i docker_build_prod_image -h</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#to_build_from_scratch","title":"To build from scratch.","text":"<p>i docker_build_prod_image --version 1.0.0 --no-cache ```</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#qa_for_prod_image","title":"QA for prod image","text":"<ul> <li>In dev_scripts repo test:     <code>&gt; i lint --files \"linters/amp_black.py\"</code></li> <li>In amp repo make sure:     <code>&gt; i lint -f \"helpers/dbg.py\"</code></li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#end-to-end_flow_for_prod_image","title":"End-to-end flow for <code>prod</code> image","text":"<ol> <li>Build docker <code>prod</code> image    ```bash <p>i docker_build_prod_image --version 1.0.0    ```</p> </li> <li>Run all the tests to verify that nothing is broken    ```bash <p>i run_fast_tests --version 1.0.0 --stage prod i run_slow_tests --version 1.0.0 --stage prod i run_superslow_tests --version 1.0.0 --stage prod i run_qa_tests --version 1.0.0 --stage prod    ```</p> </li> <li> <p>Push <code>prod</code> image to the docker registry    ```bash</p> <p>i docker_push_prod_image --version 1.0.0    ```</p> </li> <li> <p>To run the flow end-to-end do:</p> <p>```bash</p> <p>i docker_release_prod_image --version 1.0.0 ```</p> </li> <li> <p>The same options are available as for <code>i docker_release_dev_image</code> and you can     check them with <code>i docker_release_prod_image -h</code></p> </li> </ol>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#release_a_multi-architecture_prod_image","title":"Release a multi-architecture prod image","text":""},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#overview_1","title":"Overview","text":"<ul> <li>Build <code>prod</code> versioned image remotely in the CK AWS ECR registry and pull once     it is built</li> <li>Run the <code>cmamp</code> regressions using a versioned <code>prod</code> image</li> <li>Tag the versioned <code>prod</code> image as <code>prod</code> image and push it to the Docker     registries</li> <li>Tag the new <code>prod</code> image to GHCR namespace and push it to GHCR registry</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#flow_for_both_dev_and_prod_images","title":"Flow for both dev and prod images","text":"<ul> <li>To run both flows end-to-end do:<ul> <li><code>i docker_release_all</code></li> </ul> </li> <li>Alternatively, one can run the release stages step-by-step</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#release_flow","title":"Release flow","text":""},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#cmamp","title":"cmamp","text":"<ul> <li>File a GitHub Issue for the release<ul> <li>E.g., \"Add package foobar to cmamp image\"</li> </ul> </li> <li>Create the corresponding branch     <code>&gt; i git_create_branch -i ${issue_number}</code></li> <li>Change the code as needed</li> <li>Update the changelog, e.g., <code>//cmamp/changelog.txt</code><ul> <li>Specify what was changed</li> <li>Pick the release version according to     semantic versioning convention<ul> <li>For example for version <code>1.2.3</code>:<ul> <li>1 is major, 2 is minor, 3 is patch</li> </ul> </li> <li>We keep major and minor versions for <code>dev</code> and <code>prod</code> image in sync, while     <code>prod</code> gets patches<ul> <li>E.g., go from <code>prod-1.1.0</code> to <code>prod-1.1.1</code> for a small bug fix</li> </ul> </li> <li>In this manner, it cannot happen we have <code>dev-1.1.0</code> and <code>prod-1.2.0</code> at     any point in time, but <code>dev-1.1.0</code> and <code>prod-1.1.2</code> are perfectly fine</li> </ul> </li> </ul> </li> <li> <p>Test the change using the local release flow</p> <p>```bash</p> <p>i docker_build_local_image -v ${version} ...</p> <p>i run_fast_slow_tests -s local -v ${version} ```</p> </li> <li> <p>Make sure that the goal of the Issue is achieved</p> <ul> <li>E.g., a new package is visible, the package version has been updated</li> </ul> </li> <li>Do a PR with the change including the updated <code>changelog.txt</code>, the poetry     files (e.g.,     <code>/devops/docker_build/poetry.toml</code>,     <code>/devops/docker_build/poetry.lock</code></li> <li> <p>Run the release flow manually (or rely on GH Action build workflow to create     the new image)</p> <p>```bash</p> </li> <li> <p>Tag and push the latest <code>dev</code> to GHCR registry manually</p> <ul> <li>Perform a Docker login using your GitHub username and PAT (Personal Access     Token):     <code>bash     &gt; docker login ghcr.io -u &lt;username&gt;</code></li> <li>Tag the <code>dev</code> image to the GHCR namespace:     <code>bash     &gt; docker tag 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev ghcr.io/causify-ai/cmamp:dev</code></li> <li>Push the tagged image to the GHCR registry:     <code>bash     &gt; docker push ghcr.io/causify-ai/cmamp:dev</code></li> </ul> </li> <li> <p>Send a message on the <code>all@</code> chat telling people that a new version of the     <code>XYZ</code> container has been released</p> </li> <li>Users need to do a <code>i docker_pull</code> to get the new container</li> <li>Users that don't update should see a message telling them that the code and     container are not in sync any more, e.g.,:     ```     -----------------------------------------------------------------------------     This code is not in sync with the container:     code_version='1.0.3' != container_version='amp-1.0.3'     -----------------------------------------------------------------------------     You need to:<ul> <li>merge origin/master into your branch with <code>invoke git_merge_master</code></li> <li>pull the latest container with <code>invoke docker_pull</code> ```</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#release_dev_image","title":"Release dev image","text":"<p>i docker_release_dev_image --version $version</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#pick_up_the_new_image_from_image_repo","title":"Pick up the new image from image repo","text":"<p>i docker_pull ```</p>"},{"location":"tools/dev_system/all.devops_docker.how_to_guide.html#qa_flow","title":"QA flow","text":"<ul> <li>The goal is to test that the container as a whole works</li> <li> <p>We want to run the container as a user would do</p> </li> <li> <p>Usually we run tests inside a container to verify that the code is correct</p> </li> <li>To test the container itself right now we test outside (in the thin client)     <code>&gt; pytest -m qa test --image_stage dev</code></li> </ul>"},{"location":"tools/dev_system/all.devops_docker.reference.html","title":"All.devops docker.reference","text":""},{"location":"tools/dev_system/all.devops_docker.reference.html#code_organization","title":"Code Organization","text":"<ul> <li>A repo or directory is \"runnable\" if it can build a Docker container and run     its code inside the container<ul> <li>E.g., <code>//helpers</code>, <code>//amp</code>, <code>//orange</code>, <code>optimizer</code></li> </ul> </li> <li> <p>Each directory that is runnable contains the files:</p> <ul> <li><code>changelog.txt</code>: store the changelog</li> <li><code>devops</code>: dir with all the Docker files needed to build and run a container</li> </ul> </li> <li> <p>There are several types of <code>devops</code> dirs</p> <ul> <li>A <code>devops</code> where a container is built, run, and released<ul> <li>E.g., <code>//amp</code>, <code>//helpers</code>, <code>//optimizer</code>, <code>//sports_analytics</code>,     <code>//tutorials</code>, <code>//demo</code></li> </ul> </li> <li>A <code>devops</code> where a container from a different dir is run<ul> <li>E.g., <code>//orange</code> reuses <code>cmamp</code> container</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.reference.html#devops_to_build_run_release_a_container","title":"Devops to Build, Run, Release a Container","text":"<ul> <li> <p>An example of the <code>devops</code> structure is:     <code>&gt; cd //amp     &gt; tree.sh -p devops     devops/     |-- compose/     |   `-- __init__.py     |-- debug/     |   `-- repo_compare.sh*     |-- docker_build/     |   |-- create_users.sh*     |   |-- dev.Dockerfile     |   |-- etc_sudoers     |   |-- fstab     |   |-- install_cprofile.sh     |   |-- install_dind.sh*     |   |-- install_jupyter_extensions.sh*     |   |-- install_os_packages.sh*     |   |-- install_python_packages.sh*     |   |-- poetry.lock     |   |-- poetry.toml     |   |-- prod.Dockerfile     |   `-- pyproject.toml     |-- docker_run/     |   |-- aws_credentials.sh     |   |-- bashrc     |   |-- entrypoint.sh*     |   |-- run_jupyter_server.sh*     |   |-- setenv.sh     |   `-- test_setup.sh*     |-- env/     |   `-- default.env     |-- notebooks/     ...     |-- test/     ...     `-- __init__.py</code></p> </li> <li> <p>The organization of a <code>devops</code> dir is:</p> <ul> <li><code>compose</code>: Docker compose files</li> <li><code>docker_build</code>: everything related to building a Docker image</li> <li><code>docker_run</code>: everything related to running a Docker image</li> <li><code>env</code>: Docker env files</li> </ul> </li> <li> <p>Optional dirs are:</p> <ul> <li><code>debug</code>: scripts to debug and maintain the dir</li> <li><code>notebooks</code>: notebooks used to analyze, monitor Docker container<ul> <li>E.g., <code>Master_buildmeister_dashboard.ipynb</code></li> </ul> </li> <li><code>test</code>: everything needed to test a Docker image</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.reference.html#detailed_description_of_files","title":"Detailed Description of Files","text":"<ul> <li> <p>In <code>devops/compose/</code></p> <ul> <li><code>devops/compose/docker-compose.yml</code>: file generated by the <code>invoke</code> system     based on templates, to run a Docker compose</li> </ul> </li> <li> <p>In <code>devops/docker_build/</code></p> <ul> <li><code>create_users.sh</code>: create the needed users inside the container</li> <li><code>dev.Dockerfile</code>: Dockerfile to build the dev version of the container</li> <li><code>etc_sudoers</code>: give sudo permission to all users inside the container</li> <li><code>fstab</code>: mount S3 and other dir in the container<ul> <li>TODO(gp): Not sure if it's needed anymore</li> </ul> </li> <li><code>install_cprofile.sh</code>: install the packages needed to profile Python code     inside the repo</li> <li><code>install_dind.sh</code>: install docker-in-docker<ul> <li>This workflow has been superseded by Docker sibling approach</li> </ul> </li> <li><code>install_jupyter_extensions.sh</code>: install Jupyter</li> <li><code>install_os_packages.sh</code>: install the OS packages using the declarative     approach</li> <li><code>os_packages/</code>: directory containing individual OS package install scripts     and configuration</li> <li><code>install_python_packages.sh</code>: install the Python packages with <code>poetry</code> and     <code>pip</code></li> <li><code>poetry.lock</code>: file generated by <code>poetry</code> after the list of packages has     been generated<ul> <li>This tracks what packages were installed by <code>poetry</code></li> </ul> </li> <li><code>poetry.toml</code>: configuration file for <code>poetry</code></li> <li><code>prod.Dockerfile</code>: Dockerfile to build the prod version of the container</li> <li><code>pyproject.toml</code>: list of package specification for <code>poetry</code></li> </ul> </li> <li> <p>In <code>devops/docker_run/</code></p> <ul> <li><code>aws_credentials.sh</code>: loads the AWS credentials in the corresponding env     vars<ul> <li>TODO(gp): This seems useless</li> </ul> </li> <li><code>bashrc</code>: basic configuration for bash shells inside a container</li> <li><code>entrypoint.sh</code>: configures the container on startup<ul> <li>E.g., run <code>setenv.sh</code>, start dind, setup Git to run inside the container</li> </ul> </li> <li><code>run_jupyter_server.sh</code>: start Jupyter notebook</li> <li><code>setenv.sh</code>: configure the virtual environment inside the container<ul> <li>E.g., update <code>$PATH</code> and <code>$PYTHONPATH</code></li> <li>This is equivalent to <code>setenv.sh</code> for the thin environment</li> </ul> </li> <li><code>test_setup.sh</code>: check that the major systems are configured properly<ul> <li>E.g., <code>AWS</code>, <code>gspread</code>, ...</li> <li>TODO(gp): Probably obsolete</li> </ul> </li> </ul> </li> <li> <p>In <code>env/</code></p> <ul> <li><code>default.env</code>: set some variables and their defaults<ul> <li>TODO(gp): This seems useless</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.reference.html#declarative_os_package_installation","title":"Declarative OS Package Installation","text":"<ul> <li>The OS package installation system uses a declarative approach where packages     are defined in configuration files rather than hardcoded in scripts</li> <li>This approach provides better modularity, maintainability, and allows easy     customization of which packages to install</li> </ul>"},{"location":"tools/dev_system/all.devops_docker.reference.html#structure","title":"Structure","text":"<ul> <li> <p><code>/devops/docker_build/install_os_packages.sh</code>:     main script that orchestrates the installation process</p> <ul> <li>Reads the list of packages to install from <code>os_packages/os_packages.txt</code></li> <li>Executes individual install scripts located in the <code>os_packages/</code> directory</li> <li>Each install script is responsible for installing a specific set of OS     packages (e.g., AWS CLI, GitHub CLI, GraphViz)</li> </ul> </li> <li> <p><code>devops/docker_build/os_packages/</code>: directory containing individual OS package     install scripts and configuration</p> <ul> <li>Install scripts follow the naming convention <code>install_os_&lt;package_name&gt;.sh</code></li> <li><code>os_packages.txt</code>: declarative list of install scripts to execute, one     script per line<ul> <li>Lines starting with <code>#</code> are treated as comments</li> <li>Empty lines are ignored</li> </ul> </li> <li>Available install scripts:<ul> <li><code>install_os_aws_cli.sh</code>: install AWS CLI V2 for multiple architectures</li> <li><code>install_os_github_cli.sh</code>: install GitHub CLI and configure repository</li> <li><code>install_os_graphviz.sh</code>: install GraphViz and related packages for     diagram generation</li> <li><code>install_os_publishing_tools.sh</code>: install publishing tools like PlantUML</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.reference.html#how_to_disable_or_enable_os_packages","title":"How to Disable or Enable OS Packages","text":"<ul> <li> <p>Edit     <code>/devops/docker_build/os_packages/os_packages.txt</code>     to control which packages are installed:</p> <ul> <li>Disable a package: add <code>#</code> at the beginning of the line</li> <li>Enable a package: remove the <code>#</code> from the beginning of the line ``` install_os_publishing_tools.sh</li> </ul> </li> <li> <p>Important: While it is technically possible to build the Docker image     without any OS packages listed in <code>os_packages.txt</code> (by commenting out all     entries), be aware that certain <code>helpers</code> modules depend on specific OS     packages to function correctly. Disabling required packages may cause runtime     errors when those modules attempt to execute system-level operations. For     example:</p> <ul> <li><code>hs3.py</code> functions like <code>copy_file_to_s3()</code> and     <code>copy_data_from_s3_to_local_dir()</code> use AWS CLI commands (<code>aws s3 cp</code>,     <code>aws s3 sync</code>) which require the <code>awscli</code> package (installed via     <code>install_os_aws_cli.sh</code>)</li> <li><code>lib_tasks_gh.py</code> functions like <code>gh_create_pr()</code>, <code>gh_workflow_list()</code>, and     <code>gh_issue_title()</code> use GitHub CLI commands (<code>gh pr create</code>,     <code>gh workflow list</code>, <code>gh issue view</code>) which require the <code>gh</code> package     (installed via <code>install_os_github_cli.sh</code>)</li> </ul> <p>Always verify that the functionality you need will work with your selected package configuration.</p> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.reference.html#install_os_aws_clish_-_disabled","title":"install_os_aws_cli.sh          &lt;- disabled","text":"<p>install_os_github_cli.sh install_os_graphviz.sh ```</p>"},{"location":"tools/dev_system/all.devops_docker.reference.html#how_to_add_a_new_os_package_install_script","title":"How to Add a New OS Package Install Script","text":"<ul> <li> <p>Create a new install script following the naming convention:</p> <ul> <li>File name: <code>devops/docker_build/os_packages/install_os_&lt;package_name&gt;.sh</code></li> <li>Make the script executable: <code>chmod +x install_os_&lt;package_name&gt;.sh</code></li> <li>The script should handle package installation and any necessary     configuration</li> </ul> </li> <li> <p>Add the script to the package list:</p> <ul> <li>Edit     <code>/devops/docker_build/os_packages/os_packages.txt</code></li> <li>Add a new line with the script name:     <code>install_os_publishing_tools.sh     install_os_aws_cli.sh     install_os_github_cli.sh     install_os_graphviz.sh     install_os_&lt;package_name&gt;.sh</code></li> </ul> </li> <li> <p>Example script structure:</p> <p>```bash</p>"},{"location":"tools/dev_system/all.devops_docker.reference.html#usrbinenv_bash","title":"!/usr/bin/env bash","text":""},{"location":"tools/dev_system/all.devops_docker.reference.html#_1","title":"All.devops docker.reference","text":""},{"location":"tools/dev_system/all.devops_docker.reference.html#install","title":"Install .","text":""},{"location":"tools/dev_system/all.devops_docker.reference.html#_2","title":"<p>apt-get install $APT_GET_OPTS","text":""},{"location":"tools/dev_system/all.devops_docker.reference.html#add_installation_commands_here","title":"Add installation commands here <p>echo \" VERSION=\"$( --version) ```  <li> <p>The <code>APT_GET_OPTS</code> environment variable is available and contains standard     apt-get options (<code>-y --no-install-recommends</code>)</p> </li>","text":""},{"location":"tools/dev_system/all.devops_docker.reference.html#devops_to_run_an_already_existing_container","title":"Devops to Run an Already Existing Container","text":"<ul> <li>To reuse a Docker container from a different dir / repo, <code>devops</code> doesn't need     the <code>docker_build</code> dir     <code>bash     &gt; tree.sh -p devops     devops/     |-- debug/     |   `-- repo_compare.sh*     |-- docker_build/     |   `-- prod.Dockerfile -&gt; ../../amp/devops/docker_build/prod.Dockerfile     |-- docker_run/     |   |-- aws_credentials.sh     ...     `-- env/             `-- default.env</code></li> </ul>"},{"location":"tools/dev_system/all.devops_docker.reference.html#docker_invoke_flow","title":"Docker Invoke Flow","text":""},{"location":"tools/dev_system/all.devops_docker.reference.html#docker_bash","title":"<code>docker_bash</code>","text":"<ul> <li> <p>When <code>invoke docker_bash</code> is called, it generates a Docker compose command     line like:     <code>&gt; IMAGE=623860924167.dkr.ecr.eu-north-1.amazonaws.com/sports_analytics:dev \\             docker compose \\                 --file /Users/saggese/src/sports_analytics1/devops/compose/docker-compose.yml \\                 --env-file devops/env/default.env \\                 run \\                 --rm \\                 --name saggese.sports_analytics.app.sports_analytics1.20240817_113504 \\                 --user $(id -u):$(id -g) \\                 app \\                 bash</code></p> </li> <li> <p>The Docker compose file is generated programmatically based on the command     line options     ```     version: '3'</p> <p>services:     base_app:         cap_add:             - SYS_ADMIN         environment:             - AM_AWS_ACCESS_KEY_ID=$AM_AWS_ACCESS_KEY_ID             ...             - AM_ENABLE_DIND=0             - AM_FORCE_TEST_FAIL=$AM_FORCE_TEST_FAIL             - AM_HOST_NAME=gpmac.local             - AM_HOST_OS_NAME=Darwin             - AM_HOST_USER_NAME=saggese             - AM_HOST_VERSION=22.3.0             - AM_REPO_CONFIG_CHECK=True             # Use inferred path for <code>repo_config.py</code>.             - AM_REPO_CONFIG_PATH=             - AM_TELEGRAM_TOKEN=$AM_TELEGRAM_TOKEN             ...             - CK_TELEGRAM_TOKEN=$CK_TELEGRAM_TOKEN             # TODO(Vlad): consider removing, locally we use our personal tokens from files and             # inside GitHub actions we use the <code>GH_TOKEN</code> environment variable.             - GH_ACTION_ACCESS_TOKEN=$GH_ACTION_ACCESS_TOKEN             # Inside GitHub Actions we use <code>GH_TOKEN</code> environment variable,             # see https://cli.github.com/manual/gh_auth_login.             - GH_TOKEN=$GH_ACTION_ACCESS_TOKEN             # This env var is used by GH Action to signal that we are inside the CI.             - CI=$CI         image: ${IMAGE}</p> <pre><code>    restart: \"no\"\n    volumes:\n        # TODO(gp): We should pass the value of $HOME from dev.Dockerfile to here.\n        # E.g., we might define $HOME in the env file.\n        - ~/.aws:/home/.aws\n        - ~/.config/gspread_pandas/:/home/.config/gspread_pandas/\n        - ~/.config/gh:/home/.config/gh\n\n        # Use sibling-container approach.\n        - /var/run/docker.sock:/var/run/docker.sock\n\n# Mount `amp` when it is used as supermodule.\napp:\n    extends:\n        base_app\n    volumes:\n        - ../../:/app\n\nlinter:\n    extends:\n        base_app\n    volumes:\n        - /Users/saggese/src/sports_analytics1:/src\n        - ../../:/app\n    working_dir: /src\n    environment:\n        - MYPYPATH\n\njupyter_server:\n    command: devops/docker_run/run_jupyter_server.sh\n    environment:\n        - PORT=${PORT}\n    extends:\n        app\n    network_mode: ${NETWORK_MODE:-bridge}\n    ports:\n        # TODO(gp): Rename `AM_PORT`.\n        - \"${PORT}:${PORT}\"\n\njupyter_server_test:\n    command: jupyter notebook -h 2&gt;&amp;1 &gt;/dev/null\n    environment:\n        - PORT=${PORT}\n    extends:\n        app\n    network_mode: ${NETWORK_MODE:-bridge}\n    ports:\n        - \"${PORT}:${PORT}\"\n</code></pre> <p>networks:     default:         name: main_network ```</p> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker.reference.html#docker_jupyter","title":"<code>docker_jupyter</code>","text":"<ul> <li>This invokes a different service</li> <li>To disable vim go to Settings -&gt; Enable Vim Mode</li> <li>We use the percent format for Jupytext</li> </ul>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html","title":"All.devops docker auto release.explanation","text":""},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#automated_docker_release_system_architecture","title":"Automated Docker Release System Architecture","text":""},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#summary","title":"Summary","text":"<ul> <li>This document explains the architecture and design decisions for the automated     Docker dev image release system</li> <li>The system automatically builds, tests, and releases Docker images on a     periodic schedule</li> <li>It consists of three phases:<ul> <li>Phase 1: Automated build and test</li> <li>Phase 2: Manual review - Team member reviews and merges the PR</li> <li>Phase 3: Automated release to registries</li> </ul> </li> <li>The system uses GitHub Actions, invoke tasks, and GitHub CLI helpers to     orchestrate the entire workflow</li> </ul>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#architecture_overview","title":"Architecture Overview","text":""},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#core_components","title":"Core Components","text":"<ul> <li> <p>Invoke Task (<code>docker_build_test_dev_image</code>): Core automation logic</p> <ul> <li>Bumps version (minor by default)</li> <li>Creates GitHub issue and branch</li> <li>Builds local Docker image</li> <li>Runs test suites (currently commented out)</li> <li>Updates changelog</li> <li>Commits and pushes changes</li> <li>Creates PR (ready for review, with reviewer)</li> <li>Tags and pushes <code>dev-{version}</code> to GHCR</li> </ul> </li> <li> <p>GitHub Actions Workflow     (<code>.github/workflows/dev_image_build_and_test.yml</code>): Orchestrates the entire     automated build and test process</p> <ul> <li>Triggered weekly by cron schedule or manually</li> <li>Calls <code>invoke docker_build_test_dev_image --assignee=&lt;username&gt;</code></li> </ul> </li> <li> <p>GitHub CLI Helpers: New team management functions</p> <ul> <li><code>gh_get_org_team_names()</code>: Fetch organization teams</li> <li><code>gh_get_team_member_names()</code>: Fetch team members</li> <li>Enhanced <code>gh_create_pr()</code>: Support for reviewer parameter</li> </ul> </li> <li> <p>Version Management (<code>bump_version()</code> in     <code>/helpers/hversion.py)</code>): New semantic versioning     function</p> <ul> <li>Bumps version numbers (major/minor/patch)</li> <li>Default: minor bump (e.g., 2.2.0 \u2192 2.3.0)</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#system_diagram","title":"System Diagram","text":"<pre><code>graph TB\n        subgraph \"Trigger\"\n                A[Cron Schedule]\n                B[Manual Dispatch]\n        end\n\n        subgraph \"GitHub Actions\"\n                D[Workflow Runner]\n                D --&gt; E[Setup Environment]\n                E --&gt; F[Checkout Code]\n                F --&gt; G[Docker Login]\n                G --&gt; H[Run Invoke Task]\n        end\n\n        subgraph \"Phase 1: Build, Test &amp; PR Creation\"\n                H --&gt; I[Bump Version]\n                I --&gt; J[Create Issue]\n                J --&gt; K[Create Branch]\n                K --&gt; L[Build Image]\n                L --&gt; M[Run Tests]\n                M --&gt; N[Update Changelog]\n                N --&gt; O[Commit Changes]\n                O --&gt; P[Push Branch]\n                P --&gt; Q[Create PR &amp; Request Review]\n                Q --&gt; R[Push to GHCR]\n        end\n\n        subgraph \"Phase 2: Manual Review\"\n                R --&gt; S[Team Review]\n                S --&gt; T[Merge PR]\n        end\n\n        subgraph \"Phase 3: Automated Release\"\n                T --&gt; U[Detect Merge + Label]\n                U --&gt; V[Pull Image from GHCR]\n                V --&gt; W[Re-tag for ECR &amp; GHCR]\n                W --&gt; X[Push to Registries]\n        end\n\n        A --&gt; D\n        B --&gt; D\n</code></pre>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#phase_1_automated_build_and_test_implemented","title":"Phase 1: Automated Build and Test (Implemented)","text":"<p>Triggered weekly by schedule, the workflow bumps the version, creates a GitHub issue and branch, builds a Docker image, runs tests (currently commented out), updates the changelog, commits and pushes changes, creates a PR with reviewer assigned, and pushes the versioned image (<code>dev-{version}</code>) to GHCR for verification before production release.</p>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#components","title":"Components","text":"<ol> <li>GitHub Actions Workflow    (<code>.github/workflows/dev_image_build_and_test.yml</code>) - Weekly scheduled    workflow (Monday at 6 AM UTC) with manual trigger and assignee support</li> <li>Invoke Target (<code>docker_build_test_dev_image()</code> in    <code>/helpers/lib_tasks_docker_release.py)</code>) -    Orchestrates the complete build, test, and release pipeline</li> <li>GitHub CLI Helpers (<code>helpers/lib_tasks_gh.py</code>) - New team management    functions (<code>gh_get_org_team_names()</code>, <code>gh_get_team_member_names()</code>) and    enhanced <code>gh_create_pr()</code> with reviewer support</li> <li>Version Management (<code>bump_version()</code> in    <code>/helpers/hversion.py)</code>) - Semantic version bumping    (minor by default: 2.2.0 \u2192 2.3.0)</li> </ol>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#phase_2_manual_review","title":"Phase 2: Manual Review","text":"<p>This is the critical human gate between automated build and automated release. After Phase 1 completes, the assigned team member reviews the PR, validates the changes (changelog, version, poetry.lock), optionally tests the image from GHCR, and merges to master. The PR is automatically created as \"Ready for review\" with a reviewer assigned. Quality gates: all status checks pass, properly formatted changelog, no merge conflicts, valid sequential version number.</p>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#phase_3_automated_release","title":"Phase 3: Automated Release","text":"<p>When the PR from Phase 1 is merged, an automated workflow (<code>.github/workflows/dev_image_release.yml</code>) detects the merge event with the \"Automated release\" label, pulls the verified image from GHCR, re-tags it for production registries (AWS ECR, etc.), and pushes to all target registries.</p>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#team-based_assignment","title":"Team-Based Assignment","text":"<p>Configuration:</p> <ul> <li>Add to <code>repo_config.yaml</code>:     <code>yaml     release_team: \"dev_system\" # GitHub team slug</code></li> </ul> <p>Implementation plan:</p> <ul> <li>Fetch team members using <code>gh_get_team_member_names()</code></li> <li>Assign issue to all team members (multi-assignee)</li> <li>Request PR review from team (not individual)</li> <li>Format: <code>--reviewer team:org/team-slug</code></li> </ul> <p>Benefits:</p> <ul> <li>Distributes responsibility across team</li> <li>Any team member can review and merge</li> <li>Reduces single point of failure</li> </ul>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#pr_labeling","title":"PR Labeling","text":"<p>Label name: <code>Automated release</code></p> <p>Purpose:</p> <ul> <li>Identifies PRs created by automated workflow</li> <li>Used as trigger for release workflow</li> <li>Enables filtering and automation</li> </ul> <p>Implementation:</p> <pre><code># In Gh_Create_Pr():\nif is_automated_release:\n        cmd += ' --label \"Automated release\"'\n</code></pre>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#invoke_target_docker_tag_push_dev_image_from_ghcr","title":"Invoke Target <code>docker_tag_push_dev_image_from_ghcr()</code>","text":"<p>Gets the version from changelog, pulls the versioned dev image from GHCR, re-tags it for target registries (GHCR and AWS ECR), pushes to all configured registries, and verifies the images. Supports dry-run mode for testing.</p>"},{"location":"tools/dev_system/all.devops_docker_auto_release.explanation.html#release_workflow_githubworkflowsdev_image_releaseyml","title":"Release Workflow (<code>.github/workflows/dev_image_release.yml</code>)","text":"<p>Trigger:</p> <ul> <li>Event: <code>pull_request</code></li> <li>Types: <code>[closed]</code></li> <li>Condition: <code>if: github.event.pull_request.merged == true</code></li> <li>Label filter: Check for <code>Automated release</code> label</li> <li>Also supports manual <code>workflow_dispatch</code></li> </ul> <p>Steps:</p> <ul> <li>Execute <code>invoke docker_tag_push_dev_image_from_ghcr --dry-run</code></li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html","title":"All.docker optimizer container.how to guide","text":""},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#optimizer_container","title":"Optimizer container","text":""},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#rationale","title":"Rationale","text":"<ul> <li>The high-level goal is to move towards containerized Python scripts running in     smaller containers instead of keep adding packages to <code>amp</code> / <code>cmamp</code>, which     makes the <code>amp</code> / <code>cmamp</code> container bloated and risky to build</li> <li>Along this design philosophy similar to microservices, we want to have a     Docker container, called <code>opt</code> with a Python script that uses some packages     that are not compatible with <code>amp</code> (specifically cvxopt, cvxpy)</li> <li>This is similar to what we do for the <code>helpers</code>, which is like a     containerized Python script for the Linter</li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#build_and_run_a_local_version_of_opt","title":"Build and run a local version of <code>opt</code>","text":"<ul> <li>You can build the container locally with:     <code>&gt; cd optimizer     &gt; i opt_docker_build_local_image --version 0.1.0</code></li> <li>This process takes around 5 mins and then you should have the container     <code>docker image ls 665840871993.dkr.ecr.us-east-1.amazonaws.com/opt:local-saggese-0.1.0     REPOSITORY                                         TAG                   IMAGE ID       CREATED         SIZE     665840871993.dkr.ecr.us-east-1.amazonaws.com/opt   local-saggese-0.1.0   bb7d60d6a7d0   7 seconds ago   1.23GB</code></li> <li>Run the container as:     <code>&gt; i opt_docker_bash --stage local --version 0.1.0</code></li> <li>To run a Jupyter notebook in the <code>opt</code> container:</li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#internals","title":"Internals","text":""},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#one_container_per_git_repo","title":"One container per Git repo","text":"<ul> <li> <p>A simple approach is to have each deployable unit (i.e., container)     corresponding to a Git repo</p> <ul> <li>The consequence would be:<ul> <li>A multiplication of repos</li> <li>No implicit sharing of code across different containers</li> <li>Some mechanism to share code (e.g., <code>helpers</code>) across repos (e.g., using     bind mount)</li> <li>Not playing nice with Git subrepo mechanism since Docker needs to see the     entire repo</li> </ul> </li> </ul> </li> <li> <p>So the code would be organized in 4 repos:     ```</p> <ul> <li>lemonade / lime         - helpers         - optimizer         - oms         - models in amp ```</li> <li>Where the dependency between containers are<ul> <li>Lemonade -&gt; amp</li> <li>Amp -&gt; optimizer, helpers</li> <li>Optimizer -&gt; helpers, core</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#multiple_containers_per_git_repo","title":"Multiple containers per Git repo","text":"<ul> <li>Another approach is to have <code>optimizer</code> as a directory inside <code>amp</code><ul> <li>This keeps <code>amp</code> and <code>optimizer</code> in a single repo</li> <li>To build / run optimizer code in its container one needs to <code>cd</code> in the dir</li> <li>The problem then becomes how to share <code>helpers</code></li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#mounting_only_optimizer_dir_inside_docker","title":"Mounting only <code>optimizer</code> dir inside Docker","text":"<ul> <li>From <code>devops/compose/docker-compose.yml</code> <code>42 volumes:     43  # Move one dir up to include the entire git repo (see AmpTask1017).     44  - ../../:/app     45 # Move one dir down to include the entire git repo (see AmpTask1017).     46 working_dir: /app</code></li> <li>From <code>devops/docker_build/dev.Dockerfile</code></li> <li>ENTRYPOINT [\"devops/docker_run/entrypoint.sh\"]</li> <li>The problem is that Git repo doesn't work anymore     <code>git --version: git version 2.30.2     fatal: not a git repository (or any parent up to mount point /)     Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).</code></li> <li> <p>A work around is to inject .git in /git of the container and then point git to     that     ```     environment:     ...</p> <ul> <li>GIT_DIR=/git</li> </ul> <p>volumes:     # Move one dir up to include the entire git repo (see AmpTask1017).     - ../../:/app     - ../../../../.git:/git     - ../../../../amp/helpers:/app/helpers ```</p> </li> <li> <p>Git works but it gets confused with the paths     <code>modified: .dockerignore             deleted: .github/gh_requirements.txt             deleted: .github/workflows/build_image.yml.DISABLED             deleted: .github/workflows/fast_tests.yml             deleted: .github/workflows/linter.yml.DISABLED             deleted: .github/workflows/slow_tests.yml             deleted: .github/workflows/superslow_tests.yml.DISABLED             deleted: .gitignore</code></p> </li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#mounting_the_supermodule_eg_lime_lemonade_amp_inside_docker","title":"Mounting the supermodule (e.g., lime, lemonade, amp) inside Docker","text":"<ul> <li>From <code>devops/compose/docker-compose.yml</code> <code>42 volumes:     43  # Move one dir up to include the entire git repo (see AmpTask1017).     44  - ../../../:/app     45 # Move one dir down to include the entire git repo (see AmpTask1017).     46 working_dir: /app/amp</code></li> <li>From <code>devops/docker_build/dev.Dockerfile</code></li> <li>ENTRYPOINT [\"optimizer/devops/docker_run/entrypoint.sh\"]</li> <li>This approach mounts 4 dirs up from devops/compose/docker-compose.yml, i.e.,     //lime</li> <li>The problem with this approach is that now repo_config.py is incorrect</li> <li><code>i opt_docker_build_local_image --version 0.4.0</code> <code>32 - ../../../helpers:/app/amp/optimizer/helpers     33     34 # Shared cache. This is specific of lime.     35 - /local/home/share/cache:/cache     36     37 # Mount `amp` when it is used as submodule. In this case we need to     38 # mount the super project in the container (to make git work with the     39 # supermodule) and then change dir to `amp`.     40 app:     41  extends:     42    base_app     43 volumes:     44  # Move one dir up to include the entire git repo (see AmpTask1017).     45  - ../../../../:/app     46 # Move one dir down to include the entire git repo (see AmpTask1017).     47 working_dir: /app/amp/optimizer     48 #entrypoint: /bin/bash -c \"ls helpers\"</code></li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#invariants","title":"Invariants","text":"<ul> <li>A deployable dir is a dir under a Git repo<ul> <li>It corresponds to a software component (code + library = Docker container)</li> <li>Anything that has a devops dir is \"deployable\"</li> </ul> </li> <li>Each Docker container is run from its corresponding dir, e.g.,<ul> <li>Amp container from the amp dir</li> <li>Amp container from the lemonade dir (this is just a shortcut since lemonade     has the same deps right now as amp)</li> </ul> </li> <li>Always mount the outermost Git repo under <code>/app</code></li> <li>Set the Docker working dir as the current dir</li> <li>Each deployable dir specifies all the needed information in <code>repo_config.py</code>     (which is the one in the current dir)<ul> <li>What container to run</li> <li>What functionality is supported on different servers (e.g., privileged way)</li> </ul> </li> <li>The <code>changelog.txt</code> file is in the deployable dir (e.g.,     optimizer/changelog.txt)</li> <li>Each</li> </ul> <p>One run the invoke commands from optimizer dir</p> <p>When the Docker container starts the current dir is optimizer</p> <p>helpers, core is mounted in the same dir</p> <p>You can't see code outside optimizer</p> <p>TODO(gp): running in amp under lemonade should use the local repo_config</p>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#release_and_ecr_flow","title":"Release and ECR flow","text":"<p>TODO(gp): Implement this</p>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#unit_testing_code_inside_opt_container","title":"Unit testing code inside <code>opt</code> container","text":"<ul> <li>Since we want to segregate the package dependencies in different containers,     tests that have a dependency from cvxopt /cvxpy can't be run inside the <code>amp</code>     container but need to be run inside <code>opt</code>.</li> <li>We want to:<ol> <li>(as always) write and run unit tests for the optimizer code in isolation,  i.e., test the code in the directory <code>optimizer</code> by itself</li> <li>Run all the tests for the entire repo (relying on both containers <code>amp</code> and  <code>optimizer</code> with a single command invocation)</li> <li>Be able to run tests belonging to only one of the containers to shorten the  debugging cycle</li> </ol> </li> <li>To achieve this we need to solve the 3 problems below.</li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#avoid_compiling_code_depending_from_cvxopt_when_running_amp","title":"Avoid compiling code depending from cvxopt when running amp","text":"<ul> <li> <p>We can't parse code (e.g., in <code>pytest</code>) that includes packages that are not     present in a container</p> <ul> <li>E.g., <code>pytest</code> running in <code>amp</code> should not parse code in <code>//amp/optimizer</code>     since it contains imports that will fail</li> </ul> </li> <li> <p>Solution 1</p> <ul> <li> <p>We use the pytest mechanism <code>cvx = pytest.importorskip(\"cvxpy\")</code> which is     conceptually equivalent to:     ```     try:         import cvxopt         has_cvxopt = True     except ImportError:         has_cvxopt = False</p> <p>if has_cvxopt:                 def utils1():              cvxopt\u2026 ```</p> </li> </ul> </li> <li> <p>Solution 2</p> <ul> <li>Test in eachfile for the existence of the needed packages and enclose the     code in an <code>if _has_package</code><ul> <li>Pros:<ul> <li>We can skip code based dynamically on a <code>try ... except ImportModule</code> to     check what packages are present</li> </ul> </li> <li>Cons:<ul> <li>Repeat the same piece of <code>try ... except</code> in many places<ul> <li>Solution: we can factor it out in a function</li> </ul> </li> <li>We need to enclose the code in a <code>if ...</code> that screws up the indentation     and makes the code weird</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Solution 3</p> <ul> <li>Exclude certain directories (e.g., <code>//amp/optimizer</code>) from <code>pytest</code><ul> <li>Pros:<ul> <li>We don't have to spread the <code>try ... except</code> and <code>if \\_has_package</code> in     the code</li> </ul> </li> <li>Cons:<ul> <li>The directory is relative to the top directory<ul> <li>Solution: we can use a regex to specify the dir without the full path</li> </ul> </li> <li>Which directories are included and excluded depends on where <code>pytest</code> is     run<ul> <li>E.g., running <code>pytest</code> in an <code>amp</code> container we need to skip the     <code>optimizer</code> dir, while <code>pytest</code> in an <code>optimizer</code> container should     skip everything but the <code>optimizer</code> dir</li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Solution 4</p> <ul> <li>Exclude certain directories or files based on which container we are running     in<ul> <li>Cons:<ul> <li>We need to have a way to determine in which container we are running<ul> <li>Solution: we can use the env vars we use for versioning ``` <p>echo $AM_CONTAINER_VERSION amp-1.0.3- ```</p> </li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li>Given the pros and cons, we decided to follow Solution 1 and Solution 3</li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#run_optimizer_tests_in_a_stand-alone_opt_container","title":"Run optimizer tests in a stand-alone <code>opt</code> container","text":"<ul> <li>To run the optimizer tests, you can create an <code>opt</code> container and then run     <code>pytest</code> <code>&gt; cd optimizer     &gt; i opt_docker_bash     docker&gt; pytest .</code></li> <li>We wrap this in an invoke target like <code>i opt_run_fast_tests</code></li> </ul> <p>Alternative solution</p> <ul> <li>We can use dind to run the <code>opt</code> container inside a <code>cmamp</code> one<ul> <li>Cons:<ul> <li>Dind complicates the system</li> <li>Dind is not supported everywhere (one needs privileged containers)</li> <li>Dind is slower since there are 2 levels of (relatively fast)     virtualization</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#run_optimizer_tests_as_part_of_running_unit_tests_for_cmamp","title":"Run optimizer tests as part of running unit tests for <code>cmamp</code>","text":"<ul> <li>We use the same mechanism as <code>run_fast_slow_superslow_tests</code> to pull together     different test lists</li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#call_a_dockerized_executable_from_a_container","title":"Call a Dockerized executable from a container","text":"<ul> <li>From     https://github.com/cryptokaizen/cmamp/issues/1357</li> <li> <p>We need to call something from <code>amp</code> to <code>opt</code> Docker</p> </li> <li> <p>Solution 1</p> <ul> <li>Inside the code we build the command line     <code>cmd = 'docker run -it ... '; system(cmd)</code><ul> <li>Cons:<ul> <li>There is code replicated between here and the invoke task (e.g., the     info about the container, ...)</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Solution 2</p> <ul> <li>Call the Dockerized executable using the <code>docker_cmd</code> invoke target     <code>cmd = \"invoke opt_docker_cmd -cmd '...'\"     system(cmd)</code><ul> <li>Pros:<ul> <li>All the Docker commands go through the same interface inside invoke</li> </ul> </li> <li>Cons<ul> <li>Bash interpolation in the command</li> <li>Another level of indirection: do a system call to call <code>invoke</code>,     <code>invoke</code> calls docker, docker does the work</li> <li><code>invoke</code> needs to be installed inside the calling container</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Solution 3</p> <ul> <li>Call opt_lib_tasks.py <code>opt_docker_cmd(cmd, ...)</code><ul> <li>Pros<ul> <li>Avoid doing a call to invoke</li> <li>Can deal with bash interpolation in Python</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>We should always use Solution 3, although in the code sometimes we use     Solution 1 and 2 (but we should replace in favor of Solution 3).</p> </li> </ul>"},{"location":"tools/dev_system/all.docker_optimizer_container.how_to_guide.html#_1","title":"All.docker optimizer container.how to guide","text":"<ul> <li>The interface to the Dockerized optimizer is in <code>run_optimizer</code> in     <code>//amp/oms/call_optimizer.py</code></li> <li>To run the examples     <code>&gt; cd //lime     &gt; i docker_bash     &gt; pytest ./amp/oms/test/test_call_optimizer.py::Test_run_dockerized_optimizer1</code></li> </ul>"},{"location":"tools/dev_system/all.python_package_upgrade_and_troubleshooting.how_to_guide.html","title":"All.python package upgrade and troubleshooting.how to guide","text":""},{"location":"tools/dev_system/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#python_package_upgrade_troubleshooting","title":"Python package upgrade &amp; troubleshooting","text":""},{"location":"tools/dev_system/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#description","title":"Description","text":"<ul> <li>Upgrading Python or its packages is a complex process requiring significant     attention and time</li> <li>This document aims to provide a step-by-step guide for upgrading Python or its     packages</li> <li>Several steps are outlined to be followed to avoid any potential issues</li> <li>The primary objective is to streamline the upgrade process, ensuring it     unfolds seamlessly while mitigating any potential issues that might arise     along the way</li> </ul>"},{"location":"tools/dev_system/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#building_the_local_image","title":"Building the local image","text":"<ul> <li>Upgrade the packages versions in the     /devops/docker_build/pyproject.toml</li> <li>After the upgrade, the first step is to build the local image</li> <li>The build command is described in the     /docs/work_tools/all.docker.how_to_guide.md#multi-architecture-build</li> </ul>"},{"location":"tools/dev_system/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#run_the_tests","title":"Run the tests","text":"<ul> <li>Create a new task-specific gdoc, e.g., CmTask7256_Upgrade_Pandas.docx</li> <li>Run all the tests from the orange repo against the new dev image<ul> <li>Fast</li> <li>Slow</li> <li>Superslow</li> <li>QA</li> <li>Example: <code>i run_fast_tests --stage local --version {new version}</code></li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#document_errors","title":"Document Errors","text":"<ul> <li>Collect all the failures in the GDOC:<ul> <li>Test name</li> <li>Traceback</li> </ul> </li> <li>Group errors by type, e.g.,     <code># Error type 1     ## Test name 1     traceback     ## Test name 2     traceback     # Error type 2     ## Test name 1     traceback     ## Test name 2     traceback     # Error type N ...</code></li> </ul>"},{"location":"tools/dev_system/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#identify_forward-compatible_fixes","title":"Identify forward-compatible fixes","text":"<p>Forward-compatible fix is a fix that works on both versions of a Python package, i.e. on the current one and the target one.</p> <ul> <li>Identify forward-compatible changes</li> <li>Move them to a separate GDOC section to isolate from the other changes</li> <li>For each forward-compatible fix file a separate GH issue and a separate PR to     master</li> </ul>"},{"location":"tools/dev_system/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#handle_non-forward_compatible_fixes","title":"Handle non-forward compatible fixes","text":"<ul> <li>File an issue for the error</li> <li>Describe the solution in the GDOC</li> <li>Apply changes to the main PR</li> <li>Run the tests for this error type and make sure they do not fail</li> <li>Repeat this process for all the types of errors</li> <li>Once resolved, run all the regressions locally on the new local image from the     orange repo and make sure the regressions are green</li> </ul>"},{"location":"tools/dev_system/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#image_release","title":"Image Release","text":"<ul> <li>Run release command described in the     /docs/work_tools/all.docker.how_to_guide.md#command-to-run-the-release-flow</li> <li>Follow the post-release check-list in the     /docs/work_tools/all.docker.how_to_guide.md#post-release-check-list</li> </ul>"},{"location":"tools/dev_system/all.python_package_upgrade_and_troubleshooting.how_to_guide.html#merge_base_pr","title":"Merge Base PR","text":"<ul> <li>Run the regressions from GitHub on the main PR</li> <li>Check that the GitHub Actions is picking up new image and all regressions     (fast, slow, superslow) are green on <code>cmamp</code>, <code>orange</code> and <code>kaizenflow</code></li> <li>Merge the main PR on all the repos (<code>cmamp</code>, <code>orange</code>, <code>kaizenflow</code>)</li> </ul>"},{"location":"tools/dev_system/all.replace_common_files_with_script_links.html","title":"All.replace common files with script links","text":""},{"location":"tools/dev_system/all.replace_common_files_with_script_links.html#managing_symbolic_links_between_directories","title":"Managing Symbolic Links Between Directories","text":""},{"location":"tools/dev_system/all.replace_common_files_with_script_links.html#summary","title":"Summary","text":"<ul> <li>This document describes two scripts, <code>create_links.py</code> and     <code>stage_linked_file.py</code> used to manage symbolic links between a     source directory and a destination directory</li> <li>These tools simplify workflows where you want to create read-only symbolic     links for files, stage modifications, and later restore the links</li> </ul>"},{"location":"tools/dev_system/all.replace_common_files_with_script_links.html#why_do_we_need_this_approach","title":"Why Do We Need This Approach?","text":"<ul> <li> <p>In our codebases, it is common to have duplicate files or files     that are identical between two directories. Maintaining these files manually     can lead to inefficiencies and errors:</p> <ul> <li>Synchronization: If changes are made in one location, they may not reflect     in the other, leading to inconsistencies</li> <li>Accidental Modifications: Directly modifying files that should remain     synchronized can result in unintended discrepancies</li> </ul> </li> <li> <p>With our approach:</p> <ul> <li>We avoid file duplication by creating links that point to the original files</li> <li>Links in the destination directory remain read-only, reducing the risk of     accidental changes</li> <li>If modifications are needed, the \"staging process\" ensures you can work     safely on copies without altering the original source files</li> <li>After the code has been developed, one can then convert copies of files, back     to links</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.replace_common_files_with_script_links.html#workflow_and_commands","title":"Workflow and Commands","text":"<ul> <li>Below is the step-by-step workflow for using these scripts</li> </ul>"},{"location":"tools/dev_system/all.replace_common_files_with_script_links.html#step_1_replace_files_with_symbolic_links","title":"Step 1: Replace Files with Symbolic Links","text":"<ul> <li> <p>Use <code>create_links.py</code> to replace files in <code>dst_dir</code> with read-only symbolic     links to the corresponding files in <code>src_dir</code></p> <p>Command: ```</p> <p>create_links.py --src_dir /path/to/src --dst_dir /path/to/dst --replace_links ```</p> </li> <li> <p>What it does:</p> <ul> <li>Scans all files in <code>src_dir</code> and checks for files with the same name and     content in <code>dst_dir</code></li> <li>For each match, the file in <code>dst_dir</code> is replaced with a symbolic link     pointing to the file in <code>src_dir</code></li> <li>Sets the symbolic link to <code>read-only</code> (permission 444) to prevent accidental     modifications</li> </ul> </li> <li> <p>Why it is important:</p> <ul> <li>This ensures that all common files are linked to a single source,     eliminating duplication and keeping the directories in sync</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.replace_common_files_with_script_links.html#step_2_stage_files_for_modification","title":"Step 2: Stage Files for Modification","text":"<ul> <li> <p>If you want to edit the files in <code>dst_dir</code> (which are currently symbolic     links), use <code>stage_linked_file.py</code> to stage them. Staging replaces the     symbolic links with writable copies of the original files</p> </li> <li> <p>Command:     <code>&gt; stage_linked_file.py --dst_dir /path/to/dst</code></p> </li> <li> <p>What it does:</p> <ul> <li>Finds all the symbolic links in <code>dst_dir</code></li> <li>Replaces each symbolic link with a writable copy of the file it points     to</li> <li>Sets file permissions to <code>644</code> (writable)</li> </ul> </li> <li> <p>Why it is important:</p> <ul> <li>It allows safe modifications to the files without directly editing the     original source files in <code>src_dir</code>. This ensures a clean and reversible     workflow</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.replace_common_files_with_script_links.html#step_3_restore_symbolic_links_after_modifications","title":"Step 3: Restore Symbolic Links After Modifications","text":"<ul> <li> <p>Once you\u2019ve finished modifying the files, you can restore the symbolic links     by running <code>create_links.py</code> again with the <code>--replace_links</code> flag</p> </li> <li> <p>Command:     <code>&gt; create_links.py --src_dir /path/to/src --dst_dir /path/to/dst --replace_links</code></p> </li> <li> <p>What it does:</p> <ul> <li>Compares the modified files in <code>dst_dir</code> to those in <code>src_dir</code></li> <li>Replaces the matching files with symbolic links, resetting them to     <code>read-only</code></li> </ul> </li> <li> <p>Why it is important:</p> <ul> <li>This step cleans up your workspace and restores the optimized     <code>symbolic link</code> structure, ensuring minimal disk usage and consistent file     management</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.replace_common_files_with_script_links.html#workflow_summary","title":"Workflow Summary","text":"<ul> <li> <p>Set up <code>symbolic links</code>:     <code>&gt; create_links.py --src_dir /path/to/src --dst_dir /path/to/dst --replace_links</code></p> </li> <li> <p>Stage <code>symbolic links</code> for modification:     <code>&gt; stage_linked_file.py --dst_dir /path/to/dst</code></p> </li> <li> <p>Modify files as required</p> </li> <li> <p>After modifications, restore the <code>symbolic links</code>:     <code>&gt; create_links.py --src_dir /path/to/src --dst_dir /path/to/dst --replace_links</code></p> </li> </ul>"},{"location":"tools/dev_system/all.replace_common_files_with_script_links.html#example_directory_structure","title":"Example Directory Structure","text":"<ul> <li> <p>Before running <code>create_links.py</code>     ```     src_dir/             file1.txt             subdir/                     file2.txt</p> <p>dst_dir/         file1.txt  (identical content)         subdir/                 file2.txt  (identical content) ```</p> </li> <li> <p>After running <code>create_links.py</code> <code>dst_dir/             file1.txt -&gt; src_dir/file1.txt  (symlink, read-only)             subdir/                     file2.txt -&gt; src_dir/subdir/file2.txt  (symlink, read-only)</code></p> </li> <li> <p>After running <code>stage_linked_file.py</code> <code>dst_dir/             file1.txt  (writable copy of src_dir/file1.txt)             subdir/                     file2.txt  (writable copy of src_dir/subdir/file2.txt)</code></p> </li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html","title":"All.runnable repo.reference","text":""},{"location":"tools/dev_system/all.runnable_repo.reference.html#summary","title":"Summary","text":"<ul> <li>This document describes the design principles around our approach to create     Git repos that contain code that can be:<ul> <li>Composed through different Git sub-repo</li> <li>Tested, built, run, and released (on a per-directory basis or not)</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#design_goals","title":"Design goals","text":"<ul> <li> <p>The development toolchain should support the following functionalities</p> </li> <li> <p>Standardize ways of building, testing, retrieving, and deploying containers</p> </li> <li>Bootstrap the development system using a \"thin environment\", which has the     minimum number of dependencies to allow development and deployment in exactly     the same way in different setups (e.g., server, personal laptop, CI/CD)</li> <li>Support composing code using a GitHub sub-repo approach</li> <li>Manage dependencies in a way that is uniform across platforms and OSes, using     Docker containers</li> <li>Separate the need to:<ul> <li>Build and deploy containers (by devops)</li> <li>Use containers to develop and test (by developers)</li> </ul> </li> <li>Ensure alignment between development environment, deployment, and CI/CD     systems (e.g., GitHub Actions)</li> <li>Carefully manage and control dependencies using Python managers (such as     <code>poetry</code>) and virtual environments</li> <li>Run end-to-end tests using <code>pytest</code> by automatically discover tests based on     dependencies and test lists, supporting the dependencies needed by different     directories</li> <li>Support automatically different stages for container development<ul> <li>E.g., <code>test</code> / <code>local</code>, <code>dev</code>, <code>prod</code></li> </ul> </li> <li>A system of makefile-like tools based on Python <code>invoke</code> package to create     complex workflows (e.g., for testing, building</li> <li>Make it easy to add the development tool chain to a \"new project\" by simply     adding a Git sub-repo to the main project and importing all the above     functionalities</li> <li>Have a simple way to maintain common files across different repos in sync     through links and automatically diff-ing files</li> <li>Code and containers can be versioned and kept in sync automatically since a     certain version of the code can require a certain version of the container to     run properly<ul> <li>Code is versioned through Git</li> <li>Each container has a <code>changelog.txt</code> that contains the current version and     the history</li> </ul> </li> <li>Built-in support for multi-architecture builds (e.g, for Intel <code>x86</code> and Arm)     across different OSes supporting containers (e.g., Linux, MacOS, Windows     Subsystem for Linux WSL)</li> <li>Support for both local and remote development using IDEs (e.g., PyCharm,     VSCode)</li> <li>Native support for both children-containers (i.e., Docker-in-Docker) and     sibling containers</li> <li>Support for developing, testing, and deploying multi-container applications</li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#our_previous_solution_to_the_development","title":"Our previous solution to the development","text":"<ul> <li> <p>Our previous approach was to create a single repo with different directories     containing different \"applications\" all running in a single Docker container     (aka <code>cmamp</code> or <code>dev</code> container)</p> </li> <li> <p>The main issues with this approach is that:</p> <ul> <li>The repo is enormous and monolithic</li> <li>There is not an easy way to have permission control over which parts of a     repo developers have access to</li> </ul> </li> <li> <p>These problems got progressively worse since we want to have all the dev chain     we are used to (thin environment, <code>invoke</code>, pytest, Docker), but since the dev     chain was bolted on <code>cmamp</code>, we kept adding to <code>cmamp</code> instead of creating     another repo</p> </li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#current_solution","title":"Current solution","text":"<ul> <li>The current solution follows the approach described below</li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#helper_sub-repo","title":"Helper sub-repo","text":"<ul> <li><code>//helpers</code> is the sub-repo that contains utilities shared by all the repos     and the development toolchain (e.g., thin environment, Docker, <code>setenv</code>,     <code>invoke</code> workflows)</li> <li> <p>Git repos can include <code>//helpers</code> and other ones as sub-repos</p> <p>```mermaid graph TD     runnable_repo[runnable repo 1]     runnable_repo_2[runnable repo 2]     runnable_repo_3[runnable repo 3]</p> <pre><code>runnable_repo --&gt; helpers_sub_repo_1[helpers sub-repo]\nrunnable_repo --&gt; others[...]\n\nrunnable_repo_2 --&gt; helpers_sub_repo_2[helpers sub-repo]\nrunnable_repo_2 --&gt; others_2[...]\n\nrunnable_repo_3 --&gt; helpers_sub_repo_3[helpers sub-repo]\nrunnable_repo_3 --&gt; others_3[...]\n\nhelper_repo[helper repo]\nhelpers_sub_repo_1 -.-&gt; helper_repo\nhelpers_sub_repo_2 -.-&gt; helper_repo\nhelpers_sub_repo_3 -.-&gt; helper_repo\n\nstyle helpers_sub_repo_1 fill:transparent, stroke-dasharray: 5 5\nstyle helpers_sub_repo_2 fill:transparent, stroke-dasharray: 5 5\nstyle helpers_sub_repo_3 fill:transparent, stroke-dasharray: 5 5\n</code></pre> <p>```</p> </li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#runnable_dir","title":"Runnable dir","text":"<ul> <li>A dir is runnable when it contains its own <code>devops</code> dir to have its own     container and dependencies to develop, run, and test<ul> <li>E.g., <code>//cmamp/optimizer</code>, <code>//cmamp/infra</code></li> </ul> </li> <li> <p>The code in a runnable directory or in the main repo requires a container that     is built using the code in the corresponding <code>devops</code> dir</p> <p>```mermaid graph TD     runnable_repo[\"runnable dir(runnable repo)\"]     runnable_repo --&gt; helpers_sub_repo[helpers sub-repo]     runnable_repo --&gt; devops[devops]     runnable_repo --&gt; runnable_dir[runnable dir]     runnable_repo --&gt; others[...]</p> <pre><code>runnable_dir  --&gt; runnable_dir_devops[devops]\n\nstyle helpers_sub_repo fill:transparent, stroke-dasharray: 5 5\n</code></pre> <p>```</p> </li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#runnable_repo","title":"Runnable repo","text":"<ul> <li>A runnable repo is a repo that contains a single runnable dir at the top</li> <li> <p>A repo can contain multiple runnable dirs in a hierarchical fashion</p> <p>```mermaid graph TD     runnable_repo[runnable repo]     runnable_repo --&gt; helpers_sub_repo[helpers sub-repo]     runnable_repo --&gt; devops[devops]     runnable_repo --&gt; runnable_dir_1[runnable dir 1]     runnable_repo --&gt; runnable_dir_2[runnable dir 2]     runnable_repo --&gt; runnable_dir_n[runnable dir n]     runnable_repo --&gt; others[...]</p> <pre><code>runnable_dir_1 --&gt; runnable_dir_1_devops[devops]\nrunnable_dir_2 --&gt; runnable_dir_2_devops[devops]\nrunnable_dir_n --&gt; runnable_dir_3_devops[devops]\n\nstyle helpers_sub_repo fill:transparent, stroke-dasharray: 5 5\n</code></pre> <p>```</p> </li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#thin_environment","title":"Thin environment","text":"<ul> <li>Each runnable dir requires a <code>dev_scripts</code> that contains code to build the     corresponding thin environment</li> <li>A thin environment can be shared across multiple repos and runnable     directories since typically the dependencies needed to start the development     containers are minimal and common</li> </ul> <pre><code>graph TD\n    thin_env[thin environment]\n\n    subgraph repo_1[\"runnable repo 1\"]\n        runnable_repo_1[runnable repo 1]\n        runnable_repo_1 --&gt; dev_scripts_repo1[dev_scripts_repo1]\n        runnable_repo_1 --&gt; others[...]\n        runnable_repo_1 --&gt; repo_1_runnable_dir\n        subgraph repo_1_runnable_dir[\"runnable dir\"]\n            repo_1_runnable_dirs[...]\n        end\n    end\n\n    subgraph repo_2[\"runnable repo 2\"]\n        runnable_repo_2[runnable repo 2]\n        runnable_repo_2 --&gt; dev_scripts_repo2[dev_scripts_repo2]\n        runnable_repo_2 --&gt; others_2[...]\n        runnable_repo_2 --&gt; repo_2_runnable_dir\n        subgraph repo_2_runnable_dir[\"runnable dir\"]\n            repo_2_runnable_dirs[...]\n        end\n    end\n\n    repo_1 -.-&gt; thin_env\n    repo_1_runnable_dir -.-&gt; thin_env\n    repo_2 -.-&gt; thin_env\n    repo_2_runnable_dir -.-&gt; thin_env\n</code></pre>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#setenv","title":"setenv","text":"<ul> <li>A <code>setenv</code> script is used to configure a shell outside and inside a Docker     container</li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#devops","title":"devops","text":"<ul> <li>A <code>devops</code> dir contains all the code needed to build and run a container for     both development, testing, and deployment</li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#managing_common_files","title":"Managing common files","text":"<ul> <li>When creating runnable repos or dirs, many files are copied from the template     files in <code>//helpers</code></li> <li>Some of these files are customized; however, many remain unchanged</li> <li>When changes are made to the files in <code>//helpers</code>, the corresponding files in     their respective runnable repos or dirs need to be identified and replaced     with the new copies</li> <li>This manual copying and comparison approach requires a lot of effort and     becomes increasingly difficult to maintain as the number of runnable repos and     dirs grows</li> <li> <p>There is no way to \"source control\" shared files because the files themselves     are used to create the repos</p> </li> <li> <p>The solution is to apply a symlink approach for all common files</p> </li> <li>All the template files are initially copied and customized based on the     requirements</li> <li>After customization, those that remain unchanged are deleted and replaced with     symbolic links to the corresponding template files in <code>//helpers</code></li> <li>If any changes are made in <code>//helpers</code>, those changes are automatically     propagated to all the linked files</li> <li>The symlinked files should not be directly editable. If a symlinked file needs     to be customized, it should first be staged for modification by replacing the     symlink with a copy of the file</li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#managing_repo_configurations","title":"Managing repo configurations","text":"<ul> <li>Centralizing configurations eliminates duplication, reduces maintenance     overhead, and ensures consistency across different components of the system</li> <li>When configuration parameters need to be modified, changes only need to be     made in one location rather than across multiple files</li> <li> <p>YAML file format is preferred for its better support for nested structures and     complex data types</p> </li> <li> <p>All repo configurations are stored in a central <code>repo_config.yaml</code> file</p> </li> <li>The configurations are used by both Python and shell scripts</li> <li>The configurations may contain repo identifiers, docker container information,     runnable repo or runnable directory configs, thin environments used....etc</li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#building_a_docker_container","title":"Building a Docker container","text":"<ul> <li>Code and tests need to be run inside a corresponding Docker container</li> <li>There is a shared toolchain to build, test, release, and deploy each Docker     container:<ul> <li>The containers have all the dependencies to run the code</li> <li>Each container is multi-architecture in order to be run on different     processor architectures (e.g., x86-64, arm64)</li> </ul> </li> <li>The same container is used to run the code in all set-ups (e.g., on a personal     laptop, on a server, in the CI)</li> <li>It is possible to have different dependencies for the dev and prod container<ul> <li>E.g., dependencies to run the unit tests are not needed in a prod container</li> </ul> </li> <li>The toolchain to build containers is managed through <code>invoke</code> targets<ul> <li>It supports versioning</li> </ul> </li> <li>The Python dependencies are managed through <code>poetry</code></li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#building_dev_container","title":"Building <code>dev</code> container","text":"<ul> <li>The <code>dev</code> container mounts the source code as a bind-mount directory so that     developers can use external tools to edit the code (e.g., vim, PyCharm,     VSCode)</li> <li>The <code>dev</code> container mounts the outermost repo so runnable dirs that are sub     directories can see and use <code>//helpers</code> submodule</li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#buiding_prod_container","title":"Buiding <code>prod</code> container","text":"<ul> <li>The <code>prod</code> container copies the code inside so that it contains OS     dependencies, Python packages, and code, without any external dependency</li> <li>The entire repo (not just the current runnable dir) is copied because<ul> <li>We should respect the structure of the repo</li> <li>We want to ensure setup in the <code>prod</code> image mirrors that of the <code>dev</code> image</li> <li>We are guaranteed that all the dependencies are verified</li> <li>It is difficult to understand what exactly is needed from the repo for a     runnable dir to work (e.g., we know we need <code>helpers</code>, but what else?)</li> </ul> </li> <li>The entire code can be encrypted for confidentiality reason</li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#running_a_docker_container","title":"Running a Docker container","text":"<ul> <li>Handy wrappers through the <code>invoke</code> toolchain are available to perform common     operations, e.g.,<ul> <li><code>invoke docker_bash</code> to start a shell inside a container</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#module_imports","title":"Module imports","text":"<ul> <li> <p>Runnable dirs (which are subdirs of a runnable repo) should still import from     the root of their parent repo</p> <ul> <li>The imports should respect the structure of the repo</li> <li>A runnable dir can be run independently from the rest of the code</li> <li>However, it should be able to import from other dirs in the same repo as     well</li> <li>For example,<ul> <li>In a set up where <code>//cmamp</code> is a runnable repo and <code>//cmamp/optimizer</code> is     a runnable dir</li> <li>All the imports in <code>//cmamp/optimizer</code> should start from <code>//cmamp</code> rather     than <code>//cmamp/optimizer</code></li> </ul> </li> </ul> </li> <li> <p>Only runnable dirs (which are runnable repos and can be submodules of other     repos), such as <code>//helpers</code>, should use relative imports</p> <ul> <li>For example,<ul> <li>In a setup where <code>//helpers</code> is a submodule of <code>//cmamp</code> (i.e.,     <code>//cmamp/helpers_root</code>)</li> <li>All imports in <code>//helpers</code> can start from <code>//helpers</code> rather than     <code>//cmamp</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#pytest","title":"pytest","text":"<ul> <li>To run all the tests in a repo, <code>pytest</code> needs to run inside a dev container</li> <li>For top-level repo, the needed container is the top-level container</li> <li> <p>For a runnable dir, the needed container is the one built in that specific     directory</p> <p><code>mermaid graph LR     start((start))     start --&gt; container     subgraph container[container]         direction LR         pytest((pytest))         pytest --&gt; dir_1[dir 1]         dir_1 --&gt; dir_1a[dir 1a]         dir_1a --&gt; dir_1a_others[...]         dir_1 --&gt; dir_1b[dir 1b]         dir_1 --&gt; dir_1_others[...]         pytest --&gt; dir_2[dir 2]         pytest --&gt; dir_others[...]     end</code></p> </li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#recursive_pytest","title":"Recursive pytest","text":"<ul> <li> <p>When we want to run all the tests in a repo containing multiple runnable dirs,     we need to iterate over the runnable dirs and run the corresponding tests in     the corresponding container, e.g.,</p> <p><code>bash for container in containers:  (cd container; i run_fast_tests)</code></p> <p><code>mermaid graph LR         start((start))         start --&gt; container_1         start --&gt; container_2         start --&gt; container_3         start --&gt; container_n         subgraph container_1[container 1]                 direction LR                 pytest_1((pytest)) --&gt; runnable_dir1[runnable dir 1]         end         subgraph container_2[container 2]                 direction LR                 pytest_2((pytest)) --&gt; runnable_dir2[runnable dir 2]         end         subgraph container_3[container 3]                 direction LR                 pytest_3((pytest)) --&gt; runnable_dir3[runnable dir 3]         end         subgraph container_n[container n]                 direction LR                 pytest_n((pytest)) --&gt; runnable_dir_n[runnable dir n]         end</code></p> </li> <li> <p>This function is performed automatically by the script <code>main_pytest.py</code> <code>bash     &gt; main_pytest.py --dir infra</code></p> </li> </ul>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#support_for_docker-in-docker_and_sibling-containers","title":"Support for docker-in-docker and sibling-containers","text":"<pre><code>graph TD\n        host[Host]\n        docker_engine[docker engine]\n        subgraph sibling_container[\"sibling containers\"]\n                container_1[container 1]\n                container_2[container 2]\n        end\n        subgraph children_container[\"children containers\"]\n                container_1a[container 1a]\n                container_1b[container 1b]\n        end\n        host --&gt; docker_engine\n        docker_engine --&gt; container_1\n        docker_engine --&gt; container_2\n        container_1 --&gt; container_1a\n        container_1 --&gt; container_1b\n</code></pre>"},{"location":"tools/dev_system/all.runnable_repo.reference.html#maintaining_code_across_different_sub-repos","title":"Maintaining code across different sub-repos","text":"<ul> <li>TODO(gp): Explain</li> </ul>"},{"location":"tools/dev_system/all.synchronize_gh_issue_labels.how_to_guide.html","title":"All.synchronize gh issue labels.how to guide","text":""},{"location":"tools/dev_system/all.synchronize_gh_issue_labels.how_to_guide.html#synchronize_gh_issue_labels","title":"Synchronize GH issue labels","text":""},{"location":"tools/dev_system/all.synchronize_gh_issue_labels.how_to_guide.html#overview","title":"Overview","text":"<ul> <li> <p>GitHub labels help categorize and prioritize issues in a repository. You can     find more information in the     <code>all.use_github.how_to_guide.md</code></p> </li> <li> <p>The     <code>/dev_scripts_helpers/github/sync_gh_issue_labels.py</code>     script synchronizes GitHub issue labels between a repository and a label     inventory manifest file</p> </li> <li> <p>The script includes options to back up existing labels before synchronization     to prevent data loss</p> </li> <li> <p>The labels are stored in the     <code>/dev_scripts_helpers/github/labels/gh_issues_labels.yml</code>     file, which acts as a manifest for synchronization. You can create your own     label file or use the default ones provided in the repository but you need to     use only YAML file</p> </li> </ul>"},{"location":"tools/dev_system/all.synchronize_gh_issue_labels.how_to_guide.html#prerequisites","title":"Prerequisites","text":""},{"location":"tools/dev_system/all.synchronize_gh_issue_labels.how_to_guide.html#label_file","title":"Label file","text":"<ul> <li> <p>A GitHub label includes the following attributes:</p> <ul> <li><code>name</code>: The label's name</li> <li><code>color</code>: The label's color in hex format (e.g., <code>#FF0000</code> for red)</li> <li><code>description</code>: A short description of the label</li> </ul> </li> <li> <p>The <code>dev_scripts_helpers/labels/gh_issues_labels.yml</code> file contains various     label types, as shown below:</p> </li> </ul> <pre><code># Priority Labels.\n- name: P0\n    description: High priority - needs immediate attention\n    color: \"#ff0000\"\n# Issue Types.\n- name: Blocking\n    description: This issue needs to be worked on immediately\n    color: \"#e11d21\"\n# Pull Request Guidelines.\n- name: PR for authors\n    description: The PR needs changes\n    color: \"#eb35f3\"\n# Issue Status.\n- name: Paused\n    description: An issue was started and then stopped\n    color: \"#cccccc\"\n# Contribution Types.\n- name: Good first issue\n    description: Simple tasks for newcomers\n    color: \"#7057ff\"\n</code></pre>"},{"location":"tools/dev_system/all.synchronize_gh_issue_labels.how_to_guide.html#permissions_and_access","title":"Permissions and access","text":"<ul> <li> <p>The script uses the <code>PyGithub</code> library to interact with the GitHub API. Ensure     you have it installed in your Python environment</p> </li> <li> <p>Ensure that your local Python environment contains a token with write access     to the repository for the script to function. To check whether you have the     token in the local environment, run the following command:</p> </li> </ul> <pre><code>&gt; echo $GITHUB_TOKEN\n</code></pre> <ul> <li>The script requires write access to the repository you want to update labels</li> </ul>"},{"location":"tools/dev_system/all.synchronize_gh_issue_labels.how_to_guide.html#how_to_synchronize_issue_labels","title":"How to synchronize issue labels","text":""},{"location":"tools/dev_system/all.synchronize_gh_issue_labels.how_to_guide.html#using_github","title":"Using GitHub","text":"<ul> <li> <p>Open the <code>Issues</code> section in the GitHub repository you want to update labels     in </p> </li> <li> <p>Click on <code>Labels</code> on the right side of the page to view all the labels in the     repository </p> </li> <li> <p>To edit or delete a label, click on the label and make the necessary changes</p> </li> <li> <p>To create a new label, click on <code>New Label</code>, enter its name, description, and     color, and click <code>Create Label</code> </p> </li> </ul>"},{"location":"tools/dev_system/all.synchronize_gh_issue_labels.how_to_guide.html#running_the_script_manually","title":"Running the script manually","text":"<ul> <li> <p>When you want to create, update, or delete a label, update the     <code>dev_scripts_helpers/labels/gh_issues_labels.yml</code> file accordingly. If working     with a different file, ensure to specify the correct path in the script</p> </li> <li> <p>Run the script to synchronize the labels in the GitHub repository using the     following command:</p> </li> </ul> <pre><code>&gt; ./dev_scripts_helpers/github/sync_gh_issue_labels.py \\\n        --input_file ./dev_scripts_helpers/github/labels/gh_issues_labels.yml \\\n        --owner causify-ai \\\n        --repo helpers \\\n        --token_env_var GITHUB_TOKEN \\\n        --backup \\\n        --dry_run\n12:26:27 - INFO  hdbg.py init_logger:1013                               Saving log to file '/home/peeyush/src/helpers1/dev_scripts_helpers/github/sync_gh_issue_labels.py.log'\n12:26:27 - INFO  hdbg.py init_logger:1018                               &gt; cmd='./dev_scripts_helpers/github/sync_gh_issue_labels.py --input_file ./dev_scripts_helpers/github/labels/gh_issues_labels.yml --owner causify-ai --repo helpers --token_env_var GITHUB_TOKEN --backup --dry_run'\n12:26:28 - INFO  sync_gh_issue_labels.py _main:205                      Labels backed up to /home/peeyush/src/helpers1/tmp.labels.causify-ai.helpers.yaml\nAre you sure you want to synchronize labels? [y/n] y\n12:26:32 - INFO  sync_gh_issue_labels.py _main:263                      Label 'P0' not changed\n12:26:32 - INFO  sync_gh_issue_labels.py _main:263                      Label 'P1' not changed\n...\n12:26:32 - INFO  sync_gh_issue_labels.py _main:263                      Label 'Readings' not changed\n12:26:32 - INFO  sync_gh_issue_labels.py _main:241                      Label 'Testing' will be created without --dry_run\n12:26:32 - INFO  sync_gh_issue_labels.py _main:263                      Label 'PR for authors' not changed\n...\n12:26:32 - INFO  sync_gh_issue_labels.py _main:263                      Label 'Bounty' not changed\n12:26:32 - INFO  sync_gh_issue_labels.py _main:264                      Label synchronization completed!\n</code></pre> <ul> <li>For more information on the script's arguments, run the following command:</li> </ul> <pre><code>&gt; ./dev_scripts_helpers/github/sync_gh_issue_labels.py --help\n</code></pre>"},{"location":"tools/dev_system/all.synchronize_gh_issue_labels.how_to_guide.html#using_invoke","title":"Using Invoke","text":"<p>TODO(*): Update this section once the invoke target is implemented</p> <ul> <li>You can run the script using the <code>invoke</code> command where <code>$FILENAME</code> is the     name of the YAML file containing the labels:</li> </ul> <pre><code>&gt; i sync_gh_issue_labels \\\n        --input_file $FILENAME \\\n        --owner causify-ai \\\n        --repo sports_analytics \\\n        --token_env_var GITHUB_TOKEN  \\\n        --backup\n</code></pre>"},{"location":"tools/dev_system/all.synchronize_gh_settings.how_to_guide.html","title":"All.synchronize gh settings.how to guide","text":""},{"location":"tools/dev_system/all.synchronize_gh_settings.how_to_guide.html#synchronize_github_repository_settings","title":"Synchronize GitHub repository settings","text":"<ul> <li>TODO(sandeep): Update the script path references once the dockerized     executable is implemented</li> </ul>"},{"location":"tools/dev_system/all.synchronize_gh_settings.how_to_guide.html#overview","title":"Overview","text":"<ul> <li> <p>The     <code>dockerized_sync_gh_repo_settings.py</code>     script helps to export and synchronize GitHub repository settings and branch     protection rules through a settings manifest file</p> </li> <li> <p>The script supports two operations:</p> <ul> <li><code>export</code>: write current repository settings to a YAML file</li> <li><code>sync</code>: apply the settings defined in a YAML file to a repository</li> </ul> </li> <li> <p>The script uses <code>PyGithub</code> library to interact with the GitHub API</p> </li> <li> <p>The script is a dockerized executable (i.e., runs in a Docker container to     ensure that all the dependencies are verified, e.g., <code>PyGitHub</code>)</p> </li> <li> <p>Every time you run sync the script saves a backup of the current settings so     you can roll back if needed</p> <ul> <li>The settings are backed up in the current working directory</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.synchronize_gh_settings.how_to_guide.html#prerequisites","title":"Prerequisites","text":""},{"location":"tools/dev_system/all.synchronize_gh_settings.how_to_guide.html#github_token","title":"GitHub token","text":"<ul> <li>You need a GitHub token with appropriate repository management permissions</li> <li>Store the token in an environment variable and verify it:     <code>bash     &gt; echo $GITHUB_TOKEN</code></li> </ul>"},{"location":"tools/dev_system/all.synchronize_gh_settings.how_to_guide.html#settings_manifest_file_structure","title":"Settings manifest file structure","text":"<ul> <li> <p>In each repo there are two parts of a configuration we need to set</p> </li> <li> <p>The \"repository settings\" section controls basic repository configuration:</p> <p><code>yaml repository_settings:     name: \"repo-name\"     description: \"Repository description\"     homepage: \"https://example.com\"     private: true     archived: false     has_issues: true     has_projects: true     has_wiki: true     allow_squash_merge: true     allow_merge_commit: true     allow_rebase_merge: true     delete_branch_on_merge: true     enable_automated_security_fixes: true     enable_vulnerability_alerts: true     topics:         - topic1         - topic2</code></p> </li> <li> <p>The \"branch protection\" section defines rules for protected branches:</p> <p><code>yaml branch_protection:     main:         enforce_admins: true         allow_force_pushes: false         allow_deletions: false         required_status_checks:             strict: true             contexts:                 - \"run tests\"         required_pull_request_reviews:             dismiss_stale_reviews: true             require_code_owner_reviews: true             required_approving_review_count: 2             dismissal_restrictions:                 users:                     - \"user1\"                 teams:                     - \"team1\"         restrictions:             users:                 - \"maintainer1\"                 - \"maintainer2\"             teams:                 - \"developers\"                 - \"admins\"</code></p> </li> <li> <p>You can use two reference files to help configure your repository:</p> <ol> <li><code>Common repository settings</code></li> <li>Find the file at    /dev_scripts_helpers/github/settings/common_repo_settings.yaml</li> <li>This file contains standard settings that we use across multiple    repositories</li> <li> <p>Start with this file when you want to maintain consistency with other    repositories</p> </li> <li> <p><code>Template settings</code></p> </li> <li>Find the file at    /dev_scripts_helpers/github/settings/template_repo_settings.yaml</li> <li>This template shows all available configuration options with example    values</li> <li>Use this file when you need to create a new settings file from scratch</li> <li>You can remove any settings parameters you don't need; the script will    handle missing entries</li> </ol> </li> </ul>"},{"location":"tools/dev_system/all.synchronize_gh_settings.how_to_guide.html#how_to_synchronize_repository_settings","title":"How to synchronize repository settings","text":"<ul> <li> <p>View more information about the script's arguments:</p> <p>```bash</p> <p>dockerized_sync_gh_repo_settings.py --help ```</p> <p>```bash</p> <p>dockerized_sync_gh_repo_settings.py export --help ```</p> <p>```bash</p> <p>dockerized_sync_gh_repo_settings.py sync --help ```</p> </li> <li> <p>Export repository settings to a YAML file:</p> <p>```bash</p> <p>dockerized_sync_gh_repo_settings.py export \\         --output_file settings.yaml \\         --owner your-org \\         --repo your-repo \\         --token_env_var GITHUB_TOKEN ```</p> </li> <li> <p>Synchronize settings from a YAML file to a repository:</p> <p>```bash</p> <p>dockerized_sync_gh_repo_settings.py sync \\         --input_file settings.yaml \\         --owner your-org \\         --repo your-repo \\         --token_env_var GITHUB_TOKEN ```</p> </li> <li> <p>Preview changes without applying them by adding <code>--dry_run</code> flag:</p> <p>```bash</p> <p>dockerized_sync_gh_repo_settings.py sync \\         --input_file settings.yaml \\         --owner your-org \\         --repo your-repo \\         --token_env_var GITHUB_TOKEN \\         --dry_run ```</p> </li> </ul>"},{"location":"tools/dev_system/all.synchronize_gh_settings.how_to_guide.html#troubleshooting","title":"Troubleshooting","text":"<ol> <li>For permission errors:</li> <li>Verify your GitHub token has sufficient permissions</li> <li> <p>Check your repository admin access</p> </li> <li> <p>If settings are not applied:</p> </li> <li>Run with <code>--dry_run</code> to preview changes</li> <li>Verify the manifest file structure</li> <li>Check all required fields are present</li> </ol>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html","title":"All.transition to cmamp2.how to guide","text":""},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#how_to_switch_to_cmamp-v20","title":"How to switch to cmamp-v2.0","text":"<ul> <li> <p><code>cmamp-2.0</code> brings major changes to our dev and deployment flow</p> </li> <li> <p>Changelog</p> <ul> <li>Made repos composable by using git subrepos instead of the mono-repo     approach used until now<ul> <li>E.g., <code>//helpers</code> is now a subrepo and not a normal dir</li> <li>In this way we can separate multiple applications in different composable     repos</li> </ul> </li> <li>Improved Docker flow where repos and dirs can run in different containers</li> <li>Improved Docker container building flow<ul> <li>Switched to Python 3.12</li> <li>Brought all the packages to the newest version, including pandas, numpy,     etc.</li> </ul> </li> <li>Improved thin client flow and tmux flow<ul> <li>It has been almost completely re-written in Python from Bash</li> </ul> </li> <li>Improved documentation</li> </ul> </li> <li> <p>(The whole thing is so fancy, we are writing a paper on our approach).</p> </li> <li> <p>Despite the fact that we tested the flows for more than one month across     different builds (Linux, Mac, GH), there will be certainly be problems for you     or the build systems, since this is a big change</p> </li> <li>You will find more or fewer issues if your flow is different than the official     one. Be patient, ask for help (kindly), and we will get all your problems     fixed.</li> </ul>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#patching_up_an_existing_client_on_dev1_or_dev2","title":"Patching up an existing client on dev1 or dev2","text":"<ul> <li> <p>You need to patch up your Git client or start from scratch</p> <p>```bash</p> <p>cd /data/saggese/src/cmamp1</p> <p>git checkout master</p> </li> </ul>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#its_going_to_pull_a_lot_of_stuff","title":"... It's going to pull a lot of stuff ...","text":""},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#our_old_friendly_dirs_are_empty","title":"Our old friendly dirs are empty.","text":"<p>ls helpers/ config/ ls: config/: No such file or directory ls: helpers/: No such file or directory</p>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#and_there_is_a_new_friend_but_its_still_empty","title":"And there is a new friend, but it's still empty.","text":"<p>ls helpers_root/</p> <p>git submodule init Submodule 'helpers_root' (git@github.com:causify-ai/helpers.git) registered for path 'helpers_root'</p> <p>git submodule update Cloning into '/data/saggese/src/cmamp1/helpers_root'... Warning: Permanently added 'github.com' (ED25519) to the list of known hosts. Submodule path 'helpers_root': checked out '4534c7c3e0157aebc72d7fdc61a297e69cd23cc8'</p>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#now_helpers_root_is_populated","title":"Now <code>helpers_root</code> is populated","text":"<p>ls helpers_root/ LICENSE             init.py         config_root         dev_scripts_helpers docs                invoke.yaml         pytest.ini          tasks.py README.md           changelog.txt       conftest.py         devops              helpers             mypy.ini            repo_config.py ```</p>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#clone_git_client_from_scratch","title":"Clone Git client from scratch","text":"<ul> <li> <p>If patching up your Git client doesn't work you can take a more destructive     road and create a new repo</p> </li> <li> <p>Push all your local branches remotely, since the Git clients might have issues</p> </li> <li> <p>I suggest to rename your Git client to <code>.old</code>, e.g.,     <code>&gt; mv /data/saggese/cmamp1 /data/saggese/cmamp1.old</code></p> </li> <li> <p>Then clone the Git from scratch     <code>&gt; git clone --recursive ...</code></p> </li> </ul>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#fix_all_env_vars","title":"Fix all env vars","text":"<ul> <li> <p>Your old bash and tmux sessions need to be patched up and then restarted</p> </li> <li> <p>To patch up the env vars you can run in every bash:     <code>&gt; cd /.../cmamp1     &gt; ./dev_scripts_cmamp/thin_client/setenv.sh</code></p> </li> <li> <p>If you are using <code>tmux</code> (and you should) you need to restart the dev env     completely</p> </li> </ul>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#restart_tmux_system","title":"Restart tmux system","text":"<ul> <li> <p>E.g., for a client <code>/data/saggese/src/cmamp1</code></p> </li> <li> <p>Create a new centralized script to control the different copies of the repos     <code>&gt; /data/saggese/src/cmamp1/dev_scripts_cmamp/thin_client/tmux.py --create_global_link     ##&gt; /data/saggese/src/cmamp1/dev_scripts_cmamp/thin_client/tmux.py     16:07:44 - INFO  hdbg.py init_logger:1010                               Saving log to file '{'/data/saggese/src/cmamp1/helpers_root/dev_scripts_helpers/thin_client/thin_client_utils.py.log'}'     16:07:44 - INFO  hdbg.py init_logger:1015                               &gt; cmd='/data/saggese/src/cmamp1/dev_scripts_cmamp/thin_client/tmux.py --create_global_link'     16:07:44 - INFO  thin_client_utils.py create_tmux_session:199           Creating the global link     ################################################################################     ln -sf /data/saggese/src/cmamp1/dev_scripts_cmamp/thin_client/tmux.py ~/go_cmamp.py     ################################################################################     16:07:44 - INFO  thin_client_utils.py create_tmux_session:203           Link created: exiting</code></p> </li> <li> <p>Start a new session for the client 1 of <code>cmamp</code> <code>&gt; /data/saggese/src/cmamp1/dev_scripts_cmamp/thin_client/tmux.py --index1</code></p> </li> </ul>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#check_that_the_new_docker_container_works","title":"Check that the new Docker container works","text":"<ul> <li> <p>Pull the new container</p> <p>```bash</p> <p>i docker_pull</p> </li> <li> <p>Test the new container     ```     &gt; i docker_bash     16:14:49 - INFO  hdbg.py init_logger:1015                               &gt; cmd='/data/saggese/src/venv/client_venv.helpers/bin/invoke docker_bash'     ## docker_bash:     16:14:49 - INFO  lib_tasks_docker.py docker_pull:246                    Pulling the latest version of Docker     ## docker_pull:     ## docker_login:     ## _docker_login_ecr:     16:14:49 - INFO  lib_tasks_docker.py _docker_login_ecr:350              Logging in to the target registry         ... WARNING! Using --password via the CLI is insecure. Use --password-stdin.         ... Login Succeeded     16:14:53 - INFO  lib_tasks_docker.py _docker_pull:233                   image='623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev'     docker pull 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev     dev: Pulling from cmamp     Digest: sha256:68a1c9dfe5ec8a22f2e86ed13642422c5ca7abd886deb40637314805aa35af9c     Status: Image is up to date for 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev     623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev</p> <p>What's Next?     View a summary of image vulnerabilities and recommendations \u2192 docker scout quickview 623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev IMAGE=623860924167.dkr.ecr.eu-north-1.amazonaws.com/cmamp:dev \\                 docker compose \\                 --file /data/saggese/src/cmamp1/devops/compose/docker-compose.yml \\                 --env-file devops/env/default.env \\                 run \\                 --rm \\                 --name saggese.cmamp.app.cmamp1.20241110_111449 \\                 --user $(id -u):$(id -g) \\                 app \\                 bash WARN[0000] The \"CSFY_FORCE_TEST_FAIL\" variable is not set. Defaulting to a blank string. WARN[0000] The \"CSFY_AWS_ACCESS_KEY_ID\" variable is not set. Defaulting to a blank string. WARN[0000] The \"CSFY_AWS_DEFAULT_REGION\" variable is not set. Defaulting to a blank string. WARN[0000] The \"CSFY_AWS_SECRET_ACCESS_KEY\" variable is not set. Defaulting to a blank string. WARN[0000] The \"CSFY_TELEGRAM_TOKEN\" variable is not set. Defaulting to a blank string. WARN[0000] /data/saggese/src/cmamp1/devops/compose/docker-compose.yml: <code>version</code> is obsolete WARN[0000] Found orphan containers ([compose-im_postgres3923-1 compose-im_postgres5173-1]) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up. USE_HELPERS_AS_NESTED_MODULE=1</p> </li> <li> <p>Note that starting a container should be much faster than what it used to</p> </li> <li> <p>Run <code>pytest</code> <code>bash     docker&gt; pytest</code></p> </li> </ul>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#this_should_pull_a_new_image_from_the_repo_and_should_take_a_bit_of_time","title":"This should pull a new image from the repo and should take a bit of time","text":"<p>```</p>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#devopsdocker_runentrypointsh","title":"&gt; devops/docker_run/entrypoint.sh","text":"<p>UID=501 GID=20 GIT_ROOT_DIR=/app</p> <p>source /app/helpers_root/dev_scripts_helpers/thin_client/thin_client_utils.sh ... AM_CONTAINER_VERSION='2.0.0' USE_HELPERS_AS_NESTED_MODULE=1</p>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#devopsdocker_rundocker_setenvsh","title":"&gt; devops/docker_run/docker_setenv.sh","text":"<p>GIT_ROOT_DIR=/app</p> <p>source /app/helpers_root/dev_scripts_helpers/thin_client/thin_client_utils.sh ...</p>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#activate_environment","title":"Activate environment ...","text":""},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#activate_environment_done","title":"Activate environment ... done","text":""},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#set_path","title":"Set PATH","text":"<p>PATH=.:./dataflow:./reconciliation:./oms:./ck_web_apps:./dataflow_amp:./devops:./core:./test:./.pytest_cache:./optimizer:./papers:./market_data:./research_amp:./ck.infra:./docker_common:./ck.marketing:./docs:./ck.alembic:./helpers_root:./dev_scripts:./dev_scripts_cmamp:./.github:./datapull:./sorrentum_sandbox:./im:./mkdocs:./.git:./pnl_web_app:./ck_marketing:./defi:./data_schema:./.idea:/app:/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</p>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#set_pythonpath","title":"Set PYTHONPATH","text":"<p>PYTHONPATH=/app:</p>"},{"location":"tools/dev_system/all.transition_to_cmamp2.how_to_guide.html#configure_env","title":"Configure env","text":"<p>git --version: git version 2.43.0 /app PATH=.:./dataflow:./reconciliation:./oms:./ck_web_apps:./dataflow_amp:./devops:./core:./test:./.pytest_cache:./optimizer:./papers:./market_data:./research_amp:./ck.infra:./docker_common:./ck.marketing:./docs:./ck.alembic:./helpers_root:./dev_scripts:./dev_scripts_cmamp:./.github:./datapull:./sorrentum_sandbox:./im:./mkdocs:./.git:./pnl_web_app:./ck_marketing:./defi:./data_schema:./.idea:/app:/venv/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin PYTHONPATH=/app/helpers_root:/app: entrypoint.sh: 'bash' ```</p>"},{"location":"tools/dev_system/all.transition_to_repo_config_yml.how_to_guide.html","title":"All.transition to repo config yml.how to guide","text":""},{"location":"tools/dev_system/all.transition_to_repo_config_yml.how_to_guide.html#how_to_switch_from_repo_configpy_to_repo_configyml","title":"How to switch from <code>repo_config.py</code> to <code>repo_config.yml</code>","text":"<ul> <li> <p>Design choice is discussed in     Managing repo configurations</p> </li> <li> <p>Changelog</p> <ul> <li>The <code>repo_config.py</code> is removed in favor of <code>repo_config.yml</code></li> <li>The <code>henv.execute_repo_config_code(\"func\")</code> layer here is also completely     removed in favor of direct access to the config either through the interface     from <code>hserver</code> (for server related config) or <code>repo_config_utils</code> (for repo     related config)</li> <li>Some config variables are also moved between <code>repo_config</code> and <code>hserver</code> for     better organization and reflection of their purposes</li> <li>The <code>IS_SUPER_REPO</code> variable is renamed to <code>USE_HELPERS_AS_NESTED_MODULE</code>     for better clarity</li> <li>Files with identical content are replaced with symbolic links to their     counterparts in the //helpers</li> </ul> </li> </ul>"},{"location":"tools/dev_system/all.transition_to_repo_config_yml.how_to_guide.html#patching_up_an_existing_repo","title":"Patching up an existing repo","text":"<ul> <li> <p>Copy the new <code>repo_config.yml</code> template from //helpers</p> <p>```bash cp ./helpers_root/repo_config.yaml .</p> </li> <li> <p>Remove the old <code>repo_config.py</code> file</p> <p>```bash</p> <p>rm repo_config.py ```</p> </li> <li> <p>Copy the updated <code>setenv.sh</code> script from //helpers</p> <p><code>bash cp ./helpers_root/dev_scripts_helpers/thin_client/setenv.sh ./dev_scripts_cmamp/thin_client/setenv.sh</code></p> </li> <li> <p>Replace <code>setenv.sh</code> file with symoblic link reference to the file in //helpers</p> <ul> <li>Except for when amp is the first level submodule (instead of //helpers). See     CmampTask11623<ul> <li>TODO (heanh): Generalize amp path resolution in setenv.sh when amp is the     first level submodule (CmampTask11623)</li> </ul> </li> </ul> <p><code>bash python3 ./helpers_root/helpers/create_links.py --src_dir ./helpers_root/dev_scripts_helpers/thin_client --dst_dir ./dev_scripts_cmamp/thin_client --replace_links --use_relative_paths</code></p> </li> <li> <p>Replace all references of <code>henv.execute_repo_config_code</code> and <code>rconf</code> to their     corresponding functions in either <code>hrecouti.get_repo_config()</code> or <code>hserver</code></p> <ul> <li>For example,<ul> <li><code>rconf.get_docker_base_image_name()</code> -&gt;     <code>hrecouti.get_repo_config().get_docker_base_image_name()</code></li> <li><code>henv.execute_repo_config_code(\"get_html_bucket_path()\")</code> -&gt;     <code>hrecouti.get_repo_config().get_html_bucket_path()</code></li> <li><code>henv.execute_repo_config_code(\"is_CK_S3_available()\")</code> -&gt;     <code>hserver.is_CK_S3_available()</code></li> </ul> </li> <li>While the following script helps automate the replacement, it's worth to     double check that all the references are correctly replaced.     <code>bash     &gt; ./helpers_root/dev_scripts_helpers/cleanup_scripts/HelpersTask88_Improve_repo_config.sh</code></li> <li>Import statements in the modified files will need to be added or updated as     well</li> </ul> </li> <li> <p>Rename <code>IS_SUPER_REPO</code> to <code>USE_HELPERS_AS_NESTED_MODULE</code></p> <ul> <li>While the following script helps automate the replacement, it's worth to     double check that all the references are correctly replaced.     <code>bash     &gt; ./helpers_root/dev_scripts_helpers/cleanup_scripts/HelpersTask135_Rename_IS_SUPER_REPO_var.sh</code></li> </ul> </li> <li> <p>Replace identical files with symbolic links to their counterparts in //helpers     <code>bash     python3 ./helpers_root/helpers/create_links.py --src_dir ./helpers_root/ --dst_dir . --replace_links --use_relative_paths</code></p> </li> </ul>"},{"location":"tools/dev_system/all.transition_to_repo_config_yml.how_to_guide.html#modify_the_values_to_match_the_setting_in_the_current_repo_configpy","title":"Modify the values to match the setting in the current <code>repo_config.py</code>","text":"<p>```</p>"},{"location":"tools/docker/all.develop_in_helpers.how_to_guide.html","title":"All.develop in helpers.how to guide","text":""},{"location":"tools/docker/all.develop_in_helpers.how_to_guide.html#using_the_helpers_docker_image","title":"Using the <code>helpers</code> Docker image","text":"<ul> <li>We use Docker to encapsulate dependencies and maintain a reproducible     environment</li> </ul>"},{"location":"tools/docker/all.develop_in_helpers.how_to_guide.html#set_up_and_activate_the_thin_environment","title":"Set up and activate the thin environment","text":"<ul> <li> <p>Build the thin environment to bootstrap the development system (e.g., for     using invoke). Run once for all clients; re-running is only needed when the     dependencies are changed.</p> <p>```bash</p> <p>./dev_scripts_helpers/thin_client/build.py ```</p> </li> <li> <p>To activate the thin environment, run</p> <p>```bash</p> <p>source dev_scripts_helpers/thin_client/setenv.sh ```</p> <p>or start a tmux session, which will activate it automatically:</p> <p>```bash</p> <p>dev_scripts_helpers/thin_client/tmux.py --index 1 ```</p> </li> </ul>"},{"location":"tools/docker/all.develop_in_helpers.how_to_guide.html#releasing_docker_image","title":"Releasing Docker image","text":"<ul> <li>This flow is used only by the maintainers of the Docker container</li> </ul>"},{"location":"tools/docker/all.develop_in_helpers.how_to_guide.html#pull_docker_images_from_ecr","title":"Pull docker images from ECR","text":"<ul> <li> <p>Pull images from ECR for development purposes:</p> <p>```bash</p> <p>i docker_pull -s dev ```</p> </li> <li> <p>For production purposes:</p> <p>```bash</p> <p>i docker_pull -s prod ```</p> </li> </ul>"},{"location":"tools/docker/all.develop_in_helpers.how_to_guide.html#build_and_release_a_new_image_only_for_maintainers","title":"Build and release a new image (only for maintainers)","text":"<ul> <li> <p>To build a local image (e.g. for testing the changes locally), run</p> <p>```bash</p> <p>i docker_build_local_image --version ${version} ```</p> </li> <li> <p>To bash into the local image, run</p> <p>```bash</p> <p>i docker_bash --stage local --version ${version} ```</p> </li> <li> <p>To run different kinds of tests on a local image, run</p> <p>```bash</p> <p>i run_fast_tests --stage local --version ${version} i run_slow_tests --stage local --version ${version} i run_superslow_tests --stage local --version ${version} ```</p> </li> <li> <p>To tag the local image as dev, run</p> <p>```bash</p> <p>i docker_tag_local_image_as_dev --version ${version} ```</p> </li> <li> <p>To push the dev image to ECR, run</p> <p>```bash</p> <p>i docker_push_dev_image --version ${version} ```</p> </li> <li> <p>To promote the image to prod, run     <code>bash     &gt; i docker_build_prod_image --version ${version}     &gt; i docker_push_prod_image --version ${version}</code></p> </li> </ul>"},{"location":"tools/docker/all.develop_in_helpers.how_to_guide.html#let_people_know","title":"Let people know","text":"<ul> <li> <p>Update the changelog: <code>changelog.txt</code></p> <ul> <li>The changelog should be updated only after the image is released; otherwise     the sanity checks will assert that the release's version is not higher than     the latest version recorded in the changelog.</li> <li>Specify what has changed</li> <li>Pick the release version accordingly - NB! The release version should     consist of 3 digits, e.g. \"1.1.0\" instead of \"1.1\" - We use     semantic versioning convention - For example, adding     a package to the image would mean bumping up version 1.0.0 to 1.0.1</li> </ul> </li> <li> <p>Do a PR with the change including the updated <code>changelog.txt</code></p> </li> <li>Send a message on Slack letting people (especially developers) know that a new     version of the container has been released</li> </ul>"},{"location":"tools/docker/all.develop_in_helpers.how_to_guide.html#poetry","title":"Poetry","text":"<ul> <li>This repo uses poetry for package management</li> <li>Use <code>poetry</code> to<ul> <li>Help resolve dependencies</li> <li>Hard-code all package versions (in <code>poetry.lock</code>) to get reproducible builds</li> <li>Avoid the need for manually updating requirements files</li> <li>Allow an easy distinction between prod &amp; dev dependencies</li> </ul> </li> </ul>"},{"location":"tools/docker/all.develop_in_helpers.how_to_guide.html#poetry_usage_examples","title":"Poetry usage examples","text":"<ul> <li> <p>To create a new virtual env (or reactivate it after it's created):</p> <p>```shell</p> <p>poetry shell ```</p> </li> <li> <p>To install the requirements locally:</p> <p>```shell</p> <p>poetry install ```</p> </li> <li> <p>To add a new production dependency or upgrade the dependency:</p> <ul> <li>Edit the <code>devops/docker_build/pyproject.toml</code> with the updated version of     the package or the addition of a new dependency</li> <li> <p>Build a new local image</p> <p>```bash</p> <p>i docker_build_local_image --version ${version} <code>`` - By default, it runs with</code>--poetry-mode=\"update\"<code>, which updates</code>poetry<code>and upgrades all packages to the latest versions - Alternatively, run with the flag</code>--poetry-mode=\"no_update\"<code>in order to     install packages from the current</code>poetry.lock` file without upgrading.     This is useful when the goal is to remove / add / update only a single     package without updating everything     - Bash into the container to check the version of the package</p> <p>```bash</p> <p>i docker_bash --stage local --version ${version} docker &gt; pip freeze | grep  ```     - Run the regressions on the new image and do the release only after the regressions are green <p>```bash</p> <p>i run_fast_tests --stage local --version ${version} i run_slow_tests --stage local --version ${version} i run_superslow_tests --stage local --version ${version} ```</p> <li> <p>To update the Python version inside the container:</p> <ul> <li>Update the Python version inside <code>devops/docker_build/pyproject.toml</code></li> <li>If the version mismatch occurs, follow this hacky approach:<ul> <li>Login into the current image</li> <li>Install the specific Python version in it</li> <li>Update poetry inside the container, so that the <code>pyproject.toml</code> and     <code>poetry.lock</code> files are in sync with the latest Python version</li> <li>Build a local image with <code>i docker_build_local_image --version ${version}</code></li> </ul> </li> </ul> </li>"},{"location":"tools/docker/all.dind_and_sibling_containers.how_to_guide.html","title":"All.dind and sibling containers.how to guide","text":""},{"location":"tools/docker/all.dind_and_sibling_containers.how_to_guide.html#docker-in-docker_dind","title":"Docker-in-docker (dind)","text":"<ul> <li>It is possible to install a Docker engine inside a Docker container so that     one can run Docker container (e.g., OMS or IM) inside an isolated <code>amp</code>     container.</li> <li>The problems with this approach are:<ul> <li>Dind requires to run the external container in privileged mode, which might     not be possible due to security concerns</li> <li>The Docker / build cache is not shared across parent and children     containers, so one needs to pull / build an image every time the outermost     container is restarted</li> </ul> </li> <li>An alternative approach is the \"sibling container\" approach</li> </ul>"},{"location":"tools/docker/all.dind_and_sibling_containers.how_to_guide.html#sibling_container_approach","title":"Sibling container approach","text":"<ul> <li>Refs:<ul> <li>Can I run Docker-in-Docker without using the --privileged flag - Stack Overflow</li> <li>https://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/</li> </ul> </li> <li>Often what's really needed is the ability to build / run a container from     another container (e.g., CI or unit test). This can be achieved by mounting     the Docker socket <code>/var/run/docker.sock</code> to the container, so that a container     can talk to Docker Engine.</li> <li>This approach allows reuse of the build cache across the sibling containers.</li> <li>The downside is less isolation from the external container, e.g., spawned     containers can be left hanging or can collide.</li> <li> <p>E.g.,     <code>``     # Run</code>docker ps` in a container, showing the containers running in the main     container     &gt; docker run -ti --rm \\                 -v /var/run/docker.sock:/var/run/docker.sock \\                 dindtest \\                 docker ps</p> </li> </ul>"},{"location":"tools/docker/all.dind_and_sibling_containers.how_to_guide.html#start_a_sibling_hello_world_container","title":"Start a sibling hello world container:","text":"<p>docker run -it --rm \\             -v /var/run/docker.sock:/var/run/docker.sock \\             dindtest \\             docker run -ti --rm hello-world ```</p>"},{"location":"tools/docker/all.dind_and_sibling_containers.how_to_guide.html#connecting_to_postgres_instance_using_sibling_containers","title":"Connecting to Postgres instance using sibling containers","text":"<ul> <li>We can start the Docker container with Postgres as a service from outside the     container.     <code>&gt; (cd oms;  i oms_docker_up -s local)     INFO: &gt; cmd='/local/home/gsaggese/src/venv/amp.client_venv/bin/invoke oms_docker_up -s local'     report_memory_usage=False report_cpu_usage=False     docker-compose \\     --file /local/home/gsaggese/src/sasm-lime4/amp/oms/devops/compose/docker-compose.yml \\     --env-file /local/home/gsaggese/src/sasm-lime4/amp/oms/devops/env/local.oms_db_config.env \\     up \\     oms_postgres     Creating compose_oms_postgres_1 ... done     Attaching to compose_oms_postgres_1     oms_postgres_1  |     oms_postgres_1  | PostgreSQL Database directory appears to contain a database; Skipping initialization     oms_postgres_1  |     oms_postgres_1  | 2022-05-19 22:57:15.659 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit     oms_postgres_1  | 2022-05-19 22:57:15.659 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432     oms_postgres_1  | 2022-05-19 22:57:15.659 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432     oms_postgres_1  | 2022-05-19 22:57:15.663 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"     oms_postgres_1  | 2022-05-19 22:57:15.670 UTC [25] LOG:  database system was shut down at 2022-05-19 22:56:50 UTC     oms_postgres_1  | 2022-05-19 22:57:15.674 UTC [1] LOG:  database system is ready to accept connections</code></li> <li>Note that Postgres needs to be</li> <li>Start a container able to</li> <li>From inside a container I launch postgres through the /var/...     <code>&gt; docker ps | grep postgres     CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES     83bba0818c74 postgres:13 \"docker-entrypoint.s...\" 6 minutes ago Up 6 minutes     0.0.0.0:5432-&gt;5432/tcp compose-oms_postgres-1</code></li> <li>Test connection to the DB from outside the container     <code>&gt; psql --host=cf-spm-dev4 --port=5432 --user aljsdalsd -d oms_postgres_db_local     Password for user aljsdalsd:     psql (9.5.25, server 13.5 (Debian 13.5-1.pgdg110+1))     WARNING: psql major version 9.5, server major version 13.                     Some psql features might not work.     Type \"help\" for help.     oms_postgres_db_local=#</code></li> <li>Test connection to the DB from inside the container     <code>&gt; psql --host=cf-spm-dev4 --port=5432 --user aljsdalsd -d oms_postgres_db_local     ...</code></li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html","title":"All.docker.tutorial","text":""},{"location":"tools/docker/all.docker.tutorial.html#docker","title":"Docker","text":""},{"location":"tools/docker/all.docker.tutorial.html#introduction","title":"Introduction","text":"<ul> <li>Docker is an open-source tool designed to make our life typically easier     (although it takes energy and time to master) when creating, building,     deploying, and running software applications.</li> <li>Docker can package an application and its dependencies in a virtual container     that can run on any Linux, Windows, or macOS computer.</li> <li>Our Docker containers have everything required (e.g. OS packages, Python     packages) inside to run certain applications/code.</li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#concepts","title":"Concepts","text":""},{"location":"tools/docker/all.docker.tutorial.html#docker_image","title":"Docker image","text":"<ul> <li>A Docker image is a read-only template with instructions for creating a Docker     container</li> <li>Typically a Docker image includes needed libraries and packages and their     versions</li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#dockerfile","title":"Dockerfile","text":"<ul> <li>A <code>Dockerfile</code> is a text document that contains all the commands to call on     the command line to assemble an image. E.g.     <code>//cmamp/devops/docker_build/dev.Dockerfile</code>.</li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#docker_container","title":"Docker container","text":"<ul> <li>A Docker container is a runnable instance of an image. One can run code inside     a Docker container having all requirements installed.</li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#docker_registry","title":"Docker registry","text":"<ul> <li>A Docker registry stores Docker images. In other words, Docker registry for     docker images is like GitHub for code.</li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#poetry","title":"Poetry","text":"<ul> <li>Poetry is a tool for managing Python packages and dependencies and allows to:<ul> <li>List packages you want to install with some constraints<ul> <li>E.g., <code>pandas</code> must be above 1.0 in <code>devops/docker_build/pyproject.toml</code></li> </ul> </li> <li>Given a list of packages you need to install to get the desired environment,     <code>poetry</code> \"optimizes\" the package versions and generate     <code>devops/docker_build/poetry.lock</code>, which contains the list of versions of     the packages to install</li> <li>If there is a new version of a package re-running <code>poetry</code> might give you an     updated list of packages to install</li> </ul> </li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#build_a_docker_image","title":"Build a Docker image","text":""},{"location":"tools/docker/all.docker.tutorial.html#general","title":"General","text":"<ul> <li> <p>A docker image is built from a <code>Dockerfile</code>. The image is then used to run a     Docker container.</p> <p></p> </li> <li> <p>There is <code>/devops</code> dir under a project's dir that contains Docker-related     files, e.g. <code>cmamp/devops</code></p> </li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#base_image","title":"Base image","text":"<ul> <li>A <code>Dockerfile</code> should start with specifying a base image.</li> <li>The base image is an image that a new image is built on top of. A new Docker     image will have all the packages/dependencies that are installed in the base     image.</li> <li>Use <code>FROM</code> statement to specify a base image, e.g.     <code>FROM ubuntu:20.4</code></li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#copy_files","title":"Copy files","text":"<ul> <li>Copy files that are required to build a Docker image to the Docker filesystem</li> <li>To copy a file from <code>/source_dir</code> (your filesystem) to <code>/dst_dir</code> (Docker     filesystem) do:     <code>COPY source_dir/file dst_dir</code></li> <li>E.g., the command below will copy <code>install_packages.sh</code> from     <code>devops/docker_build</code> to the Docker's root directory so that     <code>install_packages.sh</code> can be accessed by Docker     <code>COPY devops/docker_build/install_packages.sh .</code></li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#install_os_packages","title":"Install OS packages","text":"<ul> <li>Install OS packages that are needed for a Docker app, but that are not     installed for a base image</li> <li>Use <code>RUN</code> instruction to install a package, e.g.     <code>RUN apt-get install postgresql-client</code></li> <li>Alternatively you can package all installation instructions in a <code>.sh</code> file     and run it. Do not forget to copy a <code>.sh</code> file to the Docker filesystem so     that Docker can see it. E.g.,     <code>COPY devops/docker_build/install_packages.sh .     RUN /bin/sh -c \"./install_packages.sh\"</code></li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#install_python_packages","title":"Install Python packages","text":"<ul> <li>We prefer to install Python packages with <code>poetry</code></li> <li>Make sure that there is instruction to install <code>pip3</code> and <code>poetry</code>. You can     either put it in a <code>Dockerfile</code> or in a separate file like     <code>install_packages.sh</code>.     <code>RUN apt-get install python3-pip     RUN pip3 install poetry</code></li> <li>Copy poetry-related files to the Docker filesystem so that files can be     accessed by Docker     <code>COPY devops/docker_build/poetry.toml     COPY devops/docker_build/poetry.lock</code></li> <li>Install Python packages     <code>RUN poetry install</code></li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#build_an_image_from_a_dockerfile","title":"Build an image from a Dockerfile","text":"<ul> <li>To build an image from a <code>Dockerfile</code> run:     <code>&gt; docker build .</code></li> <li>The <code>Dockerfile</code> must be called <code>Dockerfile</code> and located in the root of the     build context</li> <li>You can point to any <code>Dockerfile</code> by using <code>-f</code>:     <code>&gt; docker build -f /path/to/dockerfile</code></li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#run_multi-container_docker_application","title":"Run multi-container Docker application","text":"<ul> <li>Docker Compose is a tool for defining and running multi-container Docker     applications</li> <li> <p>With Docker Compose you use a <code>YAML</code> file to configure your application's     services</p> <p></p> </li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#version","title":"Version","text":"<ul> <li>At the beginning of a <code>docker-compose.yaml</code> file specify the <code>docker-compose</code>     version. For more information see     the official documents <code>version: \"3.0\"</code></li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#images","title":"Images","text":"<ul> <li>You can either re-use a public image or build a new one from a <code>Dockerfile</code></li> <li>The <code>app</code> service below uses the image that is built from the <code>dev.Dockerfile</code> <code>app:         build:             context: .             dockerfile: dev.Dockerfile</code></li> <li>The <code>im_postgres_local</code> service below uses the public <code>postgres</code> image pulled     from the Docker hub registry <code>im_postgres_local:         image: postgres: 13</code></li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#bind_mount","title":"Bind mount","text":"<ul> <li>If you want to be able to share files between the host and a Docker container,     you should bind-mount a directory</li> <li>E.g. mount current directory to <code>/app</code> dir inside a Docker container:     <code>app:         volumes:             - .:/app</code></li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#environment_variables","title":"Environment variables","text":"<ul> <li>You can either use variables directly from the environment or pass them in a     <code>docker-compose.yaml</code> file</li> <li>It is supposed that <code>POSTGRES_VERSION</code> is already defined in the shell.     <code>db:         image: \"postgres:${POSTGRES_VERSION}\"</code></li> <li>Set environment variable in a service's container     <code>db:         environment:             - POSTGRES_VERSION=13             image: \"postgres:${POSTGRES_VERSION}\"</code></li> <li>Set environment variable with <code>.env</code> file     <code>db:         env_file:             - ./postgres_env.env             image: \"postgres:${POSTGRES_VERSION}\"</code></li> <li>File <code>postgres_env.env</code> <code>bash     &gt; cat ./postgres_env.env     POSTGRES_VERSION=13</code></li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#basic_commands","title":"Basic commands","text":"<ul> <li> <p>To check more advanced usage, please see     the official documentation.</p> </li> <li> <p>Let's assume that the <code>docker-compose.yaml</code> file is located in the current dir</p> <p>```bash</p> </li> </ul>"},{"location":"tools/docker/all.docker.tutorial.html#build_recreate_start_and_attach_to_containers_for_a_service","title":"Build, (re)create, start, and attach to containers for a service.","text":"<p>docker-compose up</p>"},{"location":"tools/docker/all.docker.tutorial.html#list_containers","title":"List containers","text":"<p>docker-compose ps</p>"},{"location":"tools/docker/all.docker.tutorial.html#stop_containers_created_with_up","title":"Stop containers created with <code>up</code>","text":"<p>docker-compose down ```</p>"},{"location":"tools/docker/all.dockerhub.how_to_guide.html","title":"All.dockerhub.how to guide","text":""},{"location":"tools/docker/all.dockerhub.how_to_guide.html#dockerhub","title":"Dockerhub","text":""},{"location":"tools/docker/all.dockerhub.how_to_guide.html#login_dockerhub","title":"Login Dockerhub","text":"<p>https://hub.docker.com/</p> <p>Username: sorrentum Email: gp@crypto-kaizen.com</p> <p>There are several public images</p> <ul> <li>Sorrentum/cmamp</li> </ul> <p>Used in DATA605:</p> <ul> <li>Sorrentum/sorrentum</li> <li>Sorrentum/defi</li> <li>Sorrentum/jupyter</li> </ul> <p>The page corresponding to the Sorrentum repo is https://hub.docker.com/u/sorrentum</p>"},{"location":"tools/docker/all.dockerhub.how_to_guide.html#login_through_cli","title":"Login through CLI","text":"<p>docker login --username sorrentum --password XYZ</p>"},{"location":"tools/docker/all.dockerhub.how_to_guide.html#list_all_the_images","title":"List all the images","text":"<ul> <li>Without authentication</li> </ul> <pre><code>&gt; curl -s \"https://hub.docker.com/v2/repositories/sorrentum/?page_size=100\" | jq '.results|.[]|.name'\n\"sorrentum\"\n\"cmamp\"\n\"jupyter\"\n\"defi\"\n</code></pre>"},{"location":"tools/docker/all.dockerhub.how_to_guide.html#rename_an_image","title":"Rename an image","text":"<p>docker pull yourusername/oldimagename:tag docker tag yourusername/oldimagename:tag yourusername/newimagename:tag docker push yourusername/newimagename:tag</p> <ul> <li>To delete the old image you need to go through the GUI</li> </ul>"},{"location":"tools/docker/all.dockerized_flow.explanation.html","title":"All.dockerized flow.explanation","text":""},{"location":"tools/docker/all.dockerized_flow.explanation.html#dockerized_executable_flow","title":"Dockerized Executable Flow","text":""},{"location":"tools/docker/all.dockerized_flow.explanation.html#the_concept_of_dockerized_executables","title":"The concept of \"dockerized\" executables","text":"<ul> <li> <p>The objective of \"dockerized\" executables is to execute software applications     (e.g., <code>prettier</code>, LaTeX, <code>pandoc</code>) within a Docker container with all the     needed dependencies</p> </li> <li> <p>This approach eliminates the need for installing these applications directly on     the host system, in a virtual environment, or within a development container</p> </li> <li> <p>In other words, instead of install and execute <code>prettier</code> on the host     <code>bash     &gt; install prettier     &gt; prettier ...cmd opts...</code>     you want to run it in a container with minimal changes to the system call:     <code>bash     &gt; dockerized_prettier ...cmd opts...</code></p> </li> </ul>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#templates","title":"Templates","text":"<ul> <li> <p>There are two template for dockerized scripts:</p> <ul> <li><code>/dev_scripts_helpers/dockerize/dockerized_template.py</code></li> <li><code>/dev_scripts_helpers/dockerize/dockerized_template.sh</code></li> </ul> </li> <li> <p>As always, prefer to use the Python approach, instead of shell scripts</p> </li> </ul>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#examples_of_dockerized_executables","title":"Examples of Dockerized Executables","text":"<ul> <li> <p>We support several dockerized executables</p> <ul> <li><code>prettier</code></li> <li><code>pandoc</code></li> <li><code>markdown-toc</code></li> <li><code>plantuml</code></li> <li><code>mermaid</code></li> <li>Latex</li> <li><code>llm_transform</code> (which relies on <code>helpers/hllm.py</code>)</li> </ul> </li> <li> <p>Examples of dockerized Python scripts are:</p> <ul> <li><code>/dev_scripts_helpers/llms/llm_transform.py</code><ul> <li>Run a Python script using <code>helpers</code> in a container with <code>openai</code> packages</li> </ul> </li> <li><code>/dev_scripts_helpers/documentation/dockerized_prettier.py</code><ul> <li>Run <code>prettier</code> in a container</li> </ul> </li> <li><code>/dev_scripts_helpers/documentation/convert_docx_to_markdown.py</code><ul> <li>Run <code>pandoc</code> in a container</li> </ul> </li> </ul> </li> <li> <p>You can find all the Python dockerized executable with:     <code>bash     &gt; ffind.py dockerized | grep py     ./dev_scripts_helpers/dockerize/dockerized_template.py     ./dev_scripts_helpers/documentation/dockerized_graphviz.py     ./dev_scripts_helpers/documentation/dockerized_latex.py     ./dev_scripts_helpers/documentation/dockerized_mermaid.py     ./dev_scripts_helpers/documentation/dockerized_pandoc.py     ./dev_scripts_helpers/documentation/dockerized_prettier.py     ./dev_scripts_helpers/documentation/dockerized_tikz_to_bitmap.py     ./dev_scripts_helpers/documentation/test/test_dockerized_pandoc.py     ./dev_scripts_helpers/documentation/test/test_dockerized_prettier.py     ./dev_scripts_helpers/github/dockerized_invite_gh_contributors.py     ./dev_scripts_helpers/github/dockerized_sync_gh_issue_labels.py     ...     ./linters/dockerized_ty.py</code></p> </li> <li> <p>Examples of dockerized shell scripts are:</p> <ul> <li><code>/dev_scripts_helpers/documentation/lint_latex.sh</code></li> <li><code>/dev_scripts_helpers/documentation/latexdockercmd.sh</code></li> <li><code>/dev_scripts_helpers/documentation/run_latex.sh</code></li> <li>TODO(gp): Convert the scripts in Python and remove these</li> </ul> </li> </ul>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#directory_and_module_structure","title":"Directory and Module Structure","text":""},{"location":"tools/docker/all.dockerized_flow.explanation.html#core_infrastructure_modules","title":"Core Infrastructure Modules","text":"<ul> <li> <p><code>helpers/hdocker.py</code></p> <ul> <li>Core Docker infrastructure module providing low-level Docker operations:<ul> <li>Container lifecycle management (build, run, remove)</li> <li>Image building and caching with content-based hashing</li> <li>Path conversion between host and container filesystems</li> <li>Mount point configuration for different container types (children vs     sibling)</li> <li>Architecture compatibility checking</li> </ul> </li> <li>Contains generic utilities that work with any Docker container</li> <li>Examples: <code>build_container_image()</code>,     <code>convert_caller_to_callee_docker_path()</code>, <code>get_docker_mount_info()</code></li> </ul> </li> <li> <p><code>helpers/hdockerized_executables.py</code></p> <ul> <li>High-level wrappers for specific external tools (prettier, pandoc, latex,     mermaid, etc.)</li> <li>Uses <code>hdocker.py</code> functions to run tools inside Docker containers</li> <li>Handles tool-specific requirements:<ul> <li>Custom Dockerfiles for each tool</li> <li>Command-line argument parsing and conversion</li> <li>Input/output file path mapping</li> <li>Tool-specific configuration options</li> </ul> </li> <li>Contains functions like <code>run_dockerized_prettier()</code>,     <code>run_dockerized_pandoc()</code>, <code>run_dockerized_latex()</code></li> </ul> </li> </ul>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#script_architecture_pattern","title":"Script Architecture Pattern","text":"<p>The dockerized executable pattern follows a three-layer architecture:</p> <ol> <li>Entry Point Scripts (<code>XYZ.py</code>)</li> <li>User-facing command-line interface</li> <li>Handles argument parsing and validation</li> <li>Calls the appropriate dockerized function from <code>hdockerized_executables.py</code></li> <li> <p>Examples: </p> <ul> <li><code>dev_scripts_helpers/llms/llm_transform.py</code></li> <li><code>dev_scripts_helpers/documentation/dockerized_prettier.py</code></li> </ul> </li> <li> <p>Dockerized Wrapper Functions</p> </li> <li>Located in <code>helpers/hdockerized_executables.py</code></li> <li>Manages Docker container execution</li> <li>Converts file paths between host and container contexts</li> <li> <p>Examples: <code>run_dockerized_prettier()</code>, <code>run_dockerized_pandoc()</code></p> </li> <li> <p>Core Business Logic (Optional)</p> </li> <li>Complex functionality can be separated into dedicated helper modules</li> <li>Examples: <ul> <li>Image extraction logic in <code>helpers/hjupyter.py</code></li> <li>LLM operations in <code>helpers/hllm.py</code></li> </ul> </li> </ol>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#file_naming_conventions","title":"File Naming Conventions","text":"<ul> <li><code>dockerized_XYZ.py</code>: Scripts that run inside containers (deprecated pattern)</li> <li><code>XYZ.py</code>: User entry points that invoke dockerized functions</li> <li><code>run_dockerized_XYZ()</code>: Functions in <code>hdockerized_executables.py</code> that manage container execution</li> </ul>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#running_a_dockerized_executable","title":"Running a Dockerized executable","text":""},{"location":"tools/docker/all.dockerized_flow.explanation.html#children-_vs_sibling-container","title":"Children- vs Sibling-container","text":"<ul> <li> <p>There are several scenarios when one needs to run a dockerized executable     inside another docker container</p> <ul> <li>E.g., run an executable which has system and package dependencies:<ul> <li>in the thin environment, outside of a dev container</li> <li>inside the dev container while developing</li> <li>as part of unit testing (both CI and not CI)</li> </ul> </li> </ul> </li> <li> <p>In this case we need to use one of the following approaches:</p> <ul> <li>Children-container (aka Docker-in-Docker)<ul> <li>Run a Docker container inside another container</li> <li>It requires elevated privileges</li> </ul> </li> <li>Sibling-container:<ul> <li>More efficient and secure compared to children container approach</li> <li>It comes with greater usage restrictions</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#bind_mounting_a_directory_for_dockerized_executables","title":"Bind Mounting a Directory for Dockerized Executables","text":"<ul> <li>Files that needs to be processed by dockerized executables needs to be visible     and referred to in the file system of the dockerized executable</li> <li> <p>So we need to convert them to paths that are valid inside the filesystem of the     new Docker container</p> </li> <li> <p>Problem: There are multiple scenarios to consider</p> <ul> <li> <p>Files can be specified as absolute or relative path in the caller file system</p> </li> <li> <p>We can run a Dockerized executable:</p> <ul> <li>On the host; or</li> <li>Inside a Docker container</li> </ul> </li> <li> <p>The dockerized executables can be run:</p> <ul> <li>As children-container (aka docker-in-docker, dind); or</li> <li>As sibling-container</li> </ul> </li> </ul> </li> <li> <p>Let's consider the problems and how to solve them</p> </li> <li> <p>Children-container</p> <ul> <li>In this case bind mounting a directory does not pose any issues, since the     dockerized executable uses the same filesystem as the hosting container</li> </ul> </li> <li> <p>Sibling-container</p> <ul> <li>The mounted directory must be accessible from the host system</li> <li>For instance, when a local directory is mounted within the container at     <code>/src</code> (which is shared with the host):<ul> <li>The reference name within the container is <code>/src</code>, but the corresponding     name outside on the host system is different</li> <li>This introduces dependencies that can complicate the development     environment</li> <li>For example, the local directory <code>/tmp</code> on the host is not visible from     the development container</li> </ul> </li> </ul> </li> <li> <p>Assumption:</p> <ul> <li><code>src_root</code> and <code>dst_root</code> are the dirs used to bind mount the dockerized     executable</li> <li>For both children-container and sibling-containers we assume that the bind     mount point is from the Git root of the outermost repo to <code>dst_root=/src</code> of     the container</li> <li>In the case of nested containers, the \"style\" of container (i.e., children-     or sibling-) is the same. E.g., we assume that if the external container is     children-container (or sibling), also the internal one is children-container     (or sibling)</li> <li>In the case of sibling-container, we need to use the dir from the host     filesystem to mount a directory</li> <li>In the case of children-container, to mount a directory we can use the dir     from the caller filesystem</li> </ul> </li> <li> <p>Let's consider the 4 scenarios and how filesystems of the caller and called     dockerized executable are mapped onto each other</p> <ol> <li>Caller=host, callee=children-container  ```  caller=host</li> </ol> <p>callee=docker  - src_root=//host/users/.../git_root1  - dst_root=//docker/src</p> <p>exec.py -i foo/bar exec.py -i /users/.../git_root1/foo/bar (cd foo; exec.py -i bar)</p> <p>//host/users/.../git_root1/foo/bar -&gt; /foo/bar -&gt; //docker/src/foo/bar  ```</p> <ol> <li>Caller=host, callee=sibling-container  ```  caller=host</li> </ol> <p>callee=docker  - src_root=//host/users/.../git_root1  - dst_root=//docker/src</p> <p>exec.py -i foo/bar exec.py -i /users/.../src/foo/bar (cd foo; exec.py -i bar)</p> <p>//host/users/.../git_root1/foo/bar -&gt; /foo/bar -&gt; //docker/src/foo/bar  ```</p> <ol> <li>Caller=children-container, callee=children-container  ```  caller=docker1</li> <li>src_root=//host/users/.../git_root1</li> <li>dst_root=//docker1/src</li> </ol> <p>callee=docker2  - src_root=//docker1/src (which corresponds to the Git root)  - dst_root=//docker2/src</p> <p>exec.py -i foo/bar exec.py -i /src/foo/bar</p> <p>//docker1//src/foo/bar -&gt; /foo/bar -&gt; //docker2/src/foo/bar  ```</p> <ol> <li>Caller=sibling-container, callee=sibling-container  ```  caller=docker1</li> <li>src_root=//host/users/.../git_root1</li> <li>dst_root=//docker1/src</li> </ol> <p>callee=docker2  - src_root=//host/users/.../git_root1  - dst_root=//docker2/src</p> <p>exec.py -i foo/bar exec.py -i /src/foo/bar</p> <p>//docker1//src/foo/bar -&gt; /foo/bar -&gt; //docker2/src/foo/bar  ```</p> </li> <li> <p>The algorithm is:</p> <ul> <li>Normalize the input path to the caller filesystem (i.e., host or docker1)</li> <li>Compute the path as relative to the mount point of the caller</li> <li>Use the mount point of the caller container</li> </ul> </li> </ul>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#testing_a_dockerized_executable","title":"Testing a dockerized executable","text":"<ul> <li>Testing a dockerized executable can be complex, since in our development     system <code>pytest</code> is executed within a container environment.</li> <li>Thus the dockerized executable needs to be run inside the container running     <code>pytest</code>, rather than executing outside of Docker as it typically would when     called by a user.</li> <li> <p>The layers in this setup are</p> <ul> <li><code>host</code><ul> <li><code>dev container</code><ul> <li><code>pytest</code><ul> <li><code>dockerized executable</code></li> </ul> </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Existing Approaches:</p> <ul> <li>Approach 1:<ul> <li> <p>Overwrite the entrypoint to wait for an injected file, then run the     container's main command:</p> <p>```bash</p> </li> </ul> </li> </ul> </li> <li> <p>Preferred Approach: Simulated Usage Testing</p> <p>Instead of modifying the build context or patching the Dockerfile, we prefer to simulate real-world usage by testing the dockerized executable exactly as it will be used in production. This approach involves: - Using the <code>hdocker</code> Module Directly:</p> <pre><code>Run the container using the helper function (e.g.,\nrun_dockerized_notebook_image_extractor()) as-is, without any additional\nfile injection or Dockerfile modifications.\n</code></pre> <ul> <li> <p>Realistic Environment Simulation:</p> <p>The container is executed with its standard entrypoint and configuration, mimicking the actual user invocation.     - Output Verification:</p> <p>After execution, assert the presence and correctness of output files or log messages. If needed, you can verify the file system or container logs to confirm that the expected actions were performed.</p> </li> </ul> <p>Benefits - Simplifies the testing setup by reducing pytest configuration complexity - Ensures that tests mirror the actual behavior of the dockerized executable     in production - Avoids the overhead of additional layers or entrypoint modifications</p> <p>Example: <code>/dev_scripts_helpers/notebooks/test/test_extract_notebook_images.py</code></p> </li> </ul>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#binbash","title":"!/bin/bash","text":""},{"location":"tools/docker/all.dockerized_flow.explanation.html#wait_until_a_specific_file_is_copied_into_the_container","title":"Wait until a specific file is copied into the container","text":"<p>while [ ! -f \"/path/in/container/ready_file\" ]; do     echo \"Waiting for files...\"     sleep 1 done</p>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#run_the_containers_main_command","title":"Run the container's main command","text":"<p>exec \"$@\" ```         - Then write files in the running container     - Approach 2:         - Inject files into the Docker image by creating an additional layer using a Dockerfile: - Build an image with the test files injected. - Execute the test inside the container processing the input file. - Pause the container. - Transfer the output file from the container to the host system. - Terminate the container.</p>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#examples","title":"Examples","text":""},{"location":"tools/docker/all.dockerized_flow.explanation.html#example_1_notebook_image_extraction","title":"Example 1: Notebook Image Extraction","text":"<ul> <li> <p>Executable Script:</p> <ul> <li>Location:<code>/dev_scripts_helpers/notebooks/extract_notebook_images.py</code></li> <li>Role: Acts as the entry point for users. It parses command-line arguments     and orchestrates the workflow by invoking Docker operations from the     helpers/<code>hdocker</code> module</li> </ul> </li> <li> <p>Core Functionalities:</p> <ul> <li>Location: <code>helpers/hjupyter</code></li> <li>Role: Contains the logic for extracting images from Jupyter notebooks. This     module implements the actual image extraction process used by the executable     script</li> </ul> </li> <li> <p>Docker Management:</p> <ul> <li>Location: <code>helpers/hdocker</code></li> <li>Role: Encapsulates all Docker-related operations such as building the     container, defining Docker commands, and running the container. This module     provides the function (e.g., <code>run_dockerized_notebook_image_extractor()</code>)     that the executable script calls to execute the image extraction inside a     Docker container</li> </ul> </li> <li> <p>Testing:</p> <ul> <li>Location:     <code>/dev_scripts_helpers/notebooks/test/test_extract_notebook_images.py</code></li> <li>Role: Contains tests for the dockerized executable. The tests simulate     real-world usage by invoking the Docker container using the standard process     defined in <code>helpers/hdocker</code>, and then asserting that the expected output     (such as extracted images) is produced</li> </ul> </li> </ul>"},{"location":"tools/docker/all.dockerized_flow.explanation.html#example_2_llm_transform","title":"Example 2: llm_transform","text":"<p>The example illustrates how the dockerized executable for llm_transform manages file path conversions and container invocation in two different scenarios. Here's a breakdown:</p> <ul> <li> <p>Overview</p> <p>The tool (<code>llm_transform.py</code>) transforms a markdown file based on a given type (here, md_rewrite) and supports two configurations controlled by: - <code>is_caller_host</code>: Determines if the command is initiated from the host. - <code>use_sibling_container_for_callee</code>: Indicates if a sibling container is used     to execute the transformation.</p> </li> <li> <p>How File Paths Are Managed</p> <p>When you run the command, the tool converts file paths from the caller's context (either the host or within a container) to the paths that will be valid inside the target container. For instance: - Input File Conversion: The caller's <code>tmp.llm_transform.in.txt</code> is mapped to     <code>/app/tmp.llm_transform.in.txt</code> in the container - Output File Conversion: Similarly, <code>tmp.llm_transform.out.txt</code> is mapped to     <code>/app/tmp.llm_transform.out.txt</code></p> </li> <li> <p>Script and Directory Conversion: The script located at     <code>/dev_scripts_helpers/llms/dockerized_llm_transform.py</code>     is converted to <code>/app/dev_scripts_helpers/llms/dockerized_llm_transform.py</code> so that it     can be accessed inside the container.</p> </li> <li> <p>Docker Run Command Construction</p> <p>After converting the file paths, the system constructs a docker run command. - For the first scenario (when <code>is_caller_host = True</code>):     - Mounting: The host directory (eg: <code>/Users/saggese/src/helpers1</code>) is         mounted into the container at <code>/app</code>     - Environment and Working Directory: Environment variables (like         <code>OPENAI_API_KEY</code>) are passed, and the working directory is set to <code>/app</code>     - Command Execution:</p> <pre><code>The container then executes the transformed script\n(`/app/dev_scripts_helpers/llms/dockerized_llm_transform.py`) with the converted input\nand output paths and additional flags (e.g.,\n`transformation type -t md_rewrite and verbosity -v DEBUG`).\n</code></pre> <ul> <li>For the second scenario (when <code>is_caller_host = False</code>):<ul> <li>The conversion adjusts for the fact that the command is executed inside     the container.</li> <li>The mount paths and target paths differ (e.g., the container might mount     <code>/app</code> to <code>/src</code>), but the concept remains the same: ensure that the file     paths used in the command correspond correctly to those inside the     container.</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.extract_notebook_images.how_to_guide.html","title":"All.extract notebook images.how to guide","text":"<p>This script extracts images from a Jupyter notebook annotated with tags to determine which cells and how they need to be saved:</p> <pre><code># Extract images from notebook and save them to `screenshots` directory:\n&gt; dev_scripts_helpers/notebooks/extract_notebook_images.py \\\n        --in_notebook_filename ./dev_scripts_helpers/notebooks/test/outcomes/Test_run_dockerized_notebook_image_extractor1.test_run_dockerized_notebook_image_extractor/input/test_notebook_image_extractor.ipynb \\\n        --out_image_dir screenshots\n</code></pre> <p>The notebook contains tags inside the cells in the format below:</p> <pre><code># start_extract(&lt;mode&gt;)=&lt;output_filename&gt;\n...\n# end_extract\n</code></pre> <p>Example:</p> <ol> <li>To extract only the input code:         ```python</li> </ol>"},{"location":"tools/documentation_toolchain/all.extract_notebook_images.how_to_guide.html#start_extractonly_inputinput_codepy","title":"start_extract(only_input)=input_code.py","text":"<p>def test_func():         return \"Test\"</p>"},{"location":"tools/documentation_toolchain/all.extract_notebook_images.how_to_guide.html#end_extract","title":"end_extract","text":"<pre><code>    ```\n</code></pre> <ol> <li>To extract only the output of code:         ```python</li> </ol>"},{"location":"tools/documentation_toolchain/all.extract_notebook_images.how_to_guide.html#start_extractonly_outputoutputpng","title":"start_extract(only_output)=output.png","text":"<p>print(\"This is the output\")</p>"},{"location":"tools/documentation_toolchain/all.extract_notebook_images.how_to_guide.html#end_extract_1","title":"end_extract","text":"<pre><code>    ```\n</code></pre> <ol> <li>To extract both code and output:         # start_extract(all)=full_output.html         <code>python print(\"This is both code and output\")</code>         # end_extract         ```</li> </ol>"},{"location":"tools/documentation_toolchain/all.extract_notes.reference.html","title":"All.extract notes.reference","text":""},{"location":"tools/documentation_toolchain/all.extract_notes.reference.html#_1","title":"All.extract notes.reference","text":"<ul> <li> <p>The documentation is:</p> <ul> <li><code>docs/tools/documentation_toolchain/all.extract_notebook_images.how_to_guide.md</code></li> <li><code>docs/tools/documentation_toolchain/all.extract_notebook_images.reference.md</code></li> </ul> </li> <li> <p>This is implemented as a dockerized executables and thus there are two scripts:</p> <ul> <li><code>dev_scripts_helpers/notebooks/extract_notebook_images.py</code><ul> <li>Parses the command line options</li> <li>Create a Docker image with all the dependencies and run it</li> </ul> </li> <li><code>dev_scripts_helpers/notebooks/dockerized_extract_notebook_images.py</code><ul> <li>Instantiate <code>_NotebookImageExtractor</code></li> <li>Run the image extraction</li> </ul> </li> </ul> </li> <li> <p>The testing of the scripts are:</p> <ul> <li><code>dev_scripts_helpers/notebooks/test/test_extract_notebook_images.py</code><ul> <li>Test the end-to-end script with a handcrafted notebook     <code>dev_scripts_helpers/notebooks/test/outcomes/Test_run_dockerized_notebook_image_extractor1.test_run_dockerized_notebook_image_extractor/input/test_notebook_image_extractor.ipynb</code></li> </ul> </li> <li><code>dev_scripts_helpers/notebooks/test/test_dockerized_extract_notebook_images.py</code></li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html","title":"All.latex toolchain.how to guide","text":""},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html#definitions","title":"Definitions","text":"<ul> <li> <p>\"Notes\" are <code>txt</code> files in extended markdown that include:</p> <ul> <li>Latex</li> <li>Block comments</li> <li>Calls to external tools, such as <code>mermaid</code>, <code>plantuml</code>, <code>tikz</code>, ...</li> </ul> </li> <li> <p>Notes files can be converted to:</p> <ul> <li>PDF (through a conversion to an intermediate Latex file)</li> <li>Slides (through beamer)</li> <li>HTML</li> <li>Questions / answers (through Anki)</li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html#typical_use","title":"Typical Use","text":""},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html#files","title":"Files","text":"<pre><code>&gt; ls -1 dev_scripts_helpers/documentation\n</code></pre> <ul> <li>In the directory <code>//helpers/dev_scripts_helpers/documentation</code><ul> <li><code>convert_docx_to_markdown.py</code><ul> <li>Convert Docx file to markdown using Dockerized <code>pandoc</code> and save the figs     in a directory</li> </ul> </li> <li><code>dockerized_pandoc.py</code><ul> <li>Run <code>pandoc</code> inside a Docker container, building a container if needed</li> <li>Not tested directly but through <code>run_dockerized_pandoc()</code> in     <code>/helpers/test/test_hdocker.py</code></li> </ul> </li> <li><code>dockerized_prettier.py</code><ul> <li>Run <code>prettier</code> inside a Docker container to ensure consistent formatting     across different environments, building a container if needed</li> <li>Not tested directly but through <code>run_dockerized_prettier()</code> in     <code>/helpers/test/test_hdocker.py</code></li> </ul> </li> <li><code>generate_latex_sty.py</code><ul> <li>One-off script to generate the latex file with abbreviations</li> </ul> </li> <li><code>generate_script_catalog.py</code><ul> <li>Generate a markdown file with the docstring for any script in the repo</li> <li>TODO(gp): Unclear what to do with this. This can be a way to create an     index of all the scripts, if we use some consistent docstring</li> </ul> </li> <li><code>lint_txt.py</code><ul> <li>Lint \"notes\" files.</li> <li>Tested by     <code>/dev_scripts_helpers/documentation/test/test_lint_txt.py</code></li> </ul> </li> <li><code>latex_abbrevs.sty</code><ul> <li>Latex macros</li> </ul> </li> <li><code>latexdockercmd.sh</code><ul> <li>Wrapper for Latex docker container</li> <li>TODO(gp): probably obsolete</li> </ul> </li> <li><code>lint_latex.sh</code><ul> <li>Dockerized Linter for Latex using <code>prettier</code></li> <li>TODO(gp): This is the new flow, but it needs to be converted in Python</li> </ul> </li> <li><code>lint_latex2.sh</code><ul> <li>Dockerized Linter for Latex using <code>latexindent.pl</code></li> <li>TODO(gp): This is the old flow</li> </ul> </li> <li><code>notes_to_pdf.py</code><ul> <li>Convert a txt file into a PDF / HTML / slides using <code>pandoc</code></li> </ul> </li> <li><code>open_md_in_browser.sh</code><ul> <li>Render a markdown using <code>pandoc</code> (installed locally) and then open it in a     browser</li> </ul> </li> <li><code>open_md_on_github.sh</code><ul> <li>Open a markdown filename on GitHub</li> </ul> </li> <li><code>pandoc.latex</code><ul> <li><code>latex</code> template used by <code>notes_to_pdf.py</code></li> </ul> </li> <li><code>preprocess_notes.py</code><ul> <li>Convert a notes text file into markdown suitable for <code>notes_to_pdf.py</code></li> </ul> </li> <li><code>notes_to_pdf.py</code><ul> <li>Convert a <code>txt</code> file storing nodes into a PDF / HTML / beamer slides using     <code>pandoc</code></li> </ul> </li> <li><code>open_md_on_github.sh</code><ul> <li>Render a markdown using Pandoc and then open it in a browser.</li> </ul> </li> <li><code>open_md_on_github.sh</code><ul> <li>Open a markdown filename on GitHub.</li> </ul> </li> <li><code>process_md_headers.py</code><ul> <li>Extract headers from a Markdown file and generate a Vim cfile</li> </ul> </li> <li><code>publish_notes.py</code><ul> <li>Publish all notes to a Google dir.</li> </ul> </li> <li><code>render_images.py</code><ul> <li>Replace sections of image code with rendered images, commenting out the     original code, if needed.</li> </ul> </li> <li><code>replace_latex.py</code>, <code>replace_latex.sh</code><ul> <li>Scripts for one-off processing of latex files</li> </ul> </li> <li><code>run_latex.sh</code><ul> <li>Dockerized latex flow</li> <li>TODO(gp): Convert to Python</li> </ul> </li> <li><code>run_pandoc.py</code><ul> <li>Run pandoc on stdin/file to stdout/file.</li> </ul> </li> <li><code>test_lint_latex.sh</code><ul> <li>Run latex Linter and check if the file was modified</li> </ul> </li> <li><code>transform_notes.py</code><ul> <li>Perform one of several transformations on a text file, e.g.,<ol> <li><code>toc</code>: create table of context from the current file, with 1 level</li> <li><code>format</code>: format the current file with 3 levels</li> <li><code>increase</code>: increase level</li> </ol> </li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html#editing_txt_files","title":"Editing <code>Txt</code> Files","text":""},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html#generate_the_summary_of_the_headers","title":"Generate the Summary of the Headers","text":"<ul> <li>In <code>vim</code> <code>bash     :!helpers_root/dev_scripts_helpers/documentation/process_md_headers.py -i % -m 1     Probability 1     Random variables 735     Mathematical expectation of RVs 1161     Interesting RVs 1803     Probability inequalities 2124     Statistical Inference 2194     Statistical test 3707</code></li> <li> <p>This script also generates a <code>vim</code> <code>cfile</code> that can be navigated with <code>vic</code></p> </li> <li> <p>To get the summary up to 2 levels:     <code>bash     :!helpers_root/dev_scripts_helpers/documentation/process_md_headers.py -i % -m 2</code></p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html#format_a_chunk_of_txt_file","title":"Format a Chunk of <code>Txt</code> File","text":"<ul> <li>In vim     <code>bash     :'&lt;,'&gt;!helpers_root/dev_scripts_helpers/llms/llm_transform.py -i - -o - -t md_format</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html#list_possible_llm_transforms","title":"List Possible LLM Transforms","text":"<ul> <li>Use <code>llm_transform.py -t list</code> <code>bash     code_comment     code_docstring     code_type_hints     code_unit_test     code_1_unit_test     md_rewrite     md_format     slide_improve     slide_colorize</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html#convert_notes_to_slides","title":"Convert Notes to Slides","text":"<ul> <li>Convert notes to slides:     <code>bash     &gt; notes_to_pdf.py --input lectures_source/Lesson1-Intro.txt --output tmp.pdf -t slides</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html#latex_toolchain","title":"Latex Toolchain","text":""},{"location":"tools/documentation_toolchain/all.latex_toolchain.how_to_guide.html#running_and_linting_latex_files","title":"Running and Linting Latex Files","text":"<ul> <li>We organize each project is in a directory (e.g., under <code>//papers</code>)</li> <li> <p>Under each dir there are several scripts that assign some variables and then     call the main scripts to perform the actual work by calling     <code>run_notes_to_pdf.py</code></p> <ul> <li><code>run_latex.sh</code></li> <li><code>lint_latex.sh</code></li> </ul> </li> <li> <p>Scripts are \"dockerized\" scripts, which build a Docker container with     dependencies and then run use it to process the data</p> </li> <li> <p>To run the Latex flow we assume (as usual) that user runs from the top of the     tree</p> </li> <li> <p>To create the PDF from the Latex files:</p> <p>```bash</p> <p>papers/DataFlow_stream_computing_framework/run_latex.sh ... ```</p> </li> <li> <p>To lint the Latex file:     <code>&gt; papers/DataFlow_stream_computing_framework/lint_latex.sh     ...     &gt; docker run --rm -it --workdir /Users/saggese/src/cmamp1 --mount type=bind,source=/Users/saggese/src/cmamp1,target=/Users/saggese/src/cmamp1 lint_latex:latest sh -c ''\\''./tmp.lint_latex.sh'\\''' papers/DataFlow_stream_computing_framework/DataFlow_stream_computing_framework.tex     papers/DataFlow_stream_computing_framework/DataFlow_stream_computing_framework.tex 320ms (unchanged)</code></p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_to_pdf.explanation.html","title":"All.notes to pdf.explanation","text":""},{"location":"tools/documentation_toolchain/all.notes_to_pdf.explanation.html#flow_explanation","title":"Flow Explanation","text":"<ul> <li>This document walks through the architecture of the <code>notes_to_pdf.py</code> script.</li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_to_pdf.explanation.html#goal","title":"Goal","text":"<ul> <li>Convert a lightweight, annotated plain-text notes file (usually <code>*.txt</code>) into     a share-ready document.<ul> <li>Formats include PDF, HTML, or Beamer slide deck</li> </ul> </li> <li>Ensure the following:<ul> <li>Honor rich Markdown features and custom shorthand.</li> <li>Inline auto-generated diagrams:<ul> <li>PlantUML</li> <li>Mermaid</li> <li>TikZ</li> <li>Graphviz</li> <li>LaTeX tables</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_to_pdf.explanation.html#architecture_diagram","title":"Architecture diagram","text":"<pre><code>%%{init: {'theme':'default'}}%%\nC4Context\n\nSystem_Ext(user, \"User\", \"Author preparing course notes\")\nSystem(doc, \"Documentation Toolchain\", \"Python &amp; LaTeX\")\n\nSystem_Boundary(doc_boundary, \"\") {\n    Container(notes2pdf, \"notes_to_pdf.py\", \"Python CLI\", \"Orchestrates conversion: clean -&gt; images -&gt; Pandoc -&gt; LaTeX\")\n    Container(render, \"render_images.py\", \"Python module\", \"Renders diagram blocks to images. Caches results.\")\n    Container(preproc, \"preprocess_notes.py\", \"Python module\", \"Cleans notes &amp; expands macros (pre-Pandoc)\")\n    Container(style, \"latex_abbrevs.sty\", \"LaTeX style\", \"Provides LaTeX math &amp; formatting shortcuts\")\n}\n\nRel(user, notes2pdf, \"Invokes via CLI\")\nRel(notes2pdf, preproc, \"Uses for Note Pre-processing\")\nRel(notes2pdf, render, \"Calls to Render Images\")\nRel(notes2pdf, style, \"Injects LaTeX Style (.sty)\")\n</code></pre>"},{"location":"tools/documentation_toolchain/all.notes_to_pdf.explanation.html#steps","title":"Steps","text":"<ol> <li> <p>Clean-up &amp; augmentation</p> </li> <li> <p>Performed by: <code>preprocess_notes.py</code></p> </li> <li> <p>Key ideas:</p> <ul> <li>Normalizes headers</li> <li>Expands arrow shorthand (<code>-&gt;</code> to <code>\\rightarrow</code>)</li> <li>Deals with comments</li> <li>Inserts Pandoc YAML front-matter</li> <li>Inserts optional navigation slides</li> </ul> </li> <li> <p>Diagram extraction</p> </li> <li> <p>Performed by: <code>render_images.py</code></p> </li> <li> <p>Key ideas:</p> <ul> <li>Scans code blocks (e.g., ` plantuml)</li> <li>Renders diagrams via Docker containers</li> <li>Replaces the code with <code>![](figs/...)</code> include</li> <li>Comments out the original block</li> <li>Uses a SHA-256 cache to skip unchanged diagrams</li> </ul> </li> <li> <p>Orchestration</p> </li> <li> <p>Performed by: <code>notes_to_pdf.py</code></p> </li> <li> <p>Key ideas:</p> <ul> <li>Calls Stage 1 and Stage 2, then Pandoc, then (for PDF) LaTeX</li> <li>Flags control each sub-action to allow skipping, debugging, or re-running     steps individually</li> </ul> </li> <li> <p>Document synthesis</p> </li> <li> <p>Performed by: Pandoc + LaTeX</p> </li> <li>Key ideas:<ul> <li>Pandoc converts Markdown to LaTeX (or HTML / Beamer)</li> <li><code>latex_abbrevs.sty</code> is copied next to the generated <code>.tex</code> file</li> <li>Ensures vector/matrix macros (<code>\\vv{}</code>, <code>\\mat{}</code>), deep lists, and color     helpers compile correctly</li> </ul> </li> </ol>"},{"location":"tools/documentation_toolchain/all.notes_to_pdf.explanation.html#dependencies","title":"Dependencies","text":""},{"location":"tools/documentation_toolchain/all.notes_to_pdf.explanation.html#preprocess_notespy","title":"<code>preprocess_notes.py</code>","text":"<ul> <li>Input: Raw notes.</li> <li>Output: Pandoc\u2011ready Markdown.</li> <li>Handles<ul> <li>Formatting banner frames.</li> <li>Question formatting.</li> <li>Colour commands (<code>\\red{}</code> -&gt; <code>\\textcolor{red}{...}</code>).</li> <li>TOC injection.</li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_to_pdf.explanation.html#render_imagespy","title":"<code>render_images.py</code>","text":"<ul> <li>Docker\u2011wrapper around PlantUML, Mermaid CLI, TikZ, Graphviz to convert image     description in a file, replacing the text with the picture.     <code>figs/&lt;basename&gt;.&lt;index&gt;.png</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_to_pdf.explanation.html#latex_abbrevssty","title":"<code>latex_abbrevs.sty</code>","text":"<ul> <li>Custom style for Latex documents, including:<ul> <li>Bold-underlined vectors (<code>\\vv{x}</code>)</li> <li>Matrices</li> <li>Colour presets</li> <li>9-level <code>enumitem</code> lists</li> <li>Symbol shorthands</li> </ul> </li> <li>Copied automatically; you rarely touch this unless you need new macros.</li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html","title":"All.notes toolchain.how to guide","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#documentation_toolchain","title":"Documentation Toolchain","text":"<ul> <li> <p>This is a high\u2011level guide to the workflows that turn raw notes, slides, Latex     into polished PDFs, slide decks, etc.</p> </li> <li> <p>There are several documentation workflows available:</p> <ul> <li>latex<ul> <li>Standard Latex code to convert into PDF files</li> <li>E.g., [<code>//cmamp/papers</code>]</li> <li>E.g., [<code>//cmamp/papers/KaizenFlow</code>]</li> </ul> </li> <li>markdown<ul> <li>Documentation using Causify markdown extensions<ul> <li>E.g., <code>//helpers/docs</code>, <code>//cmamp/docs</code>, <code>//tutorials/docs</code></li> <li>E.g., https://github.com/causify-ai/helpers/tree/master/docs</li> </ul> </li> <li>It is automatically rendered with <code>mkdocs</code> and published on GitHub<ul> <li>E.g., https://causify-ai.github.io/helpers</li> </ul> </li> </ul> </li> <li>notes (aka .txt)<ul> <li><code>Pandoc</code> markdown with Causify extensions</li> <li>Processed by <code>preprocess_notes.py</code> to be converted in standard <code>Pandoc</code>     markdown</li> <li>Can be converted into PDFs and in Anki Q/A</li> <li>E.g., <code>//notes/notes/...</code> <code>&gt; vi /Users/saggese/src/notes1/notes/math.machine_learning.txt</code></li> </ul> </li> <li>slides<ul> <li>Same as <code>notes</code> but we use a different extension <code>.slides</code> to clarify that     they are rendered as slides</li> <li>E.g.,     <code>&gt; ls /Users/saggese/src/notes1/MSML610     &gt; vi /Users/saggese/src/notes1/lectures_source/Lesson00-Class.txt</code></li> <li>E.g., <code>//notes/DATA605</code></li> </ul> </li> <li>books<ul> <li>Extended <code>Pandoc</code> markdown that can be rendered with <code>Pandoc</code></li> <li>E.g., <code>//notes/books/book.programming_with_ai</code> <code>&gt; vi books/programming_with_ai/docs/coding-benchmark.md</code></li> </ul> </li> <li>jupyter books</li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#causify_extended_markdown","title":"Causify Extended Markdown","text":"<ul> <li>We refer to it to \"Causify markdown\" as some extension we use on top of     <code>Pandoc</code> markdown</li> <li> <p>The goal is invariants in formatting that we enforce so that the code looks     nicely formatted for a human</p> <ul> <li>The <code>linter</code> formats and enforces some these rules</li> </ul> </li> <li> <p>E.g.,</p> <ul> <li>Cross-repo links<ul> <li>E.g., [<code>//helpers/docs/code_guidelines/all.coding_style_guidelines.reference.md</code>]</li> </ul> </li> <li>Color  <ul> <li>E.g., <code>\\red{...}</code></li> </ul> </li> <li>Primitives we use in the slides  </li> <li>Indented triple fences</li> <li>Triple fences Plugins for tools  </li> <li>Comments  <ul> <li>E.g., C-like comments     <code>// Comment</code>     ```text     *     ...     */     ````</li> </ul> </li> <li>Framing of titles <code>text     # ##############     # Hello     # ##############</code></li> <li>Automatically update <code>&gt; notes_to_pdf.py -h</code></li> </ul> </li> <li> <p>Causify extended markdown is rendered for different backends (e.g., slides,     mkdocs, ...) by converting the source markdown into something that can be     rendered by the target tool (e.g., <code>mkdocs</code>, <code>Pandoc</code>), e.g.,</p> <ul> <li>[<code>//helpers/dev_scripts_helpers/documentation/mkdocs/preprocess_mkdocs.py</code>]</li> <li>[<code>//helpers/dev_scripts_helpers/documentation/preprocess_notes.py</code>]</li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#list_of_tools","title":"List of Tools","text":"<ul> <li>TODO(gp): Use the invoke to describe the list</li> </ul> <pre><code>&gt; ls -1 dev_scripts_helpers/documentation/\nconvert_docx_to_markdown.py\ndockerized_graphviz.py\ndockerized_latex.py\ndockerized_mermaid.py\ndockerized_pandoc.py\ndockerized_prettier.py\ndockerized_tikz_to_bitmap.py\nextract_headers_from_markdown.py\ngenerate_latex_sty.py\ngenerate_readme_index.py\ngenerate_script_catalog.py\nlatex_abbrevs.sty\nlatexdockercmd.sh\nlint_txt.py\nmkdocs\nnotes_to_pdf.py\nOLD\nopen_md_in_browser.sh\nopen_md_on_github.sh\npandoc.latex\npreprocess_notes.py\npublish_notes.py\nrender_images.py\nreplace_latex.py\nreplace_latex.sh\nrun_latex.sh\nrun_pandoc.py\ntest\ntransform_notes.py\n</code></pre>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#notes_to_pdfpy","title":"<code>Notes_To_Pdf.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does","title":"What It Does","text":"<ul> <li> <p>Convert plain\u2011text notes into polished PDF, HTML, or Beamer slides     with a single command:</p> <p>```bash</p> <p>notes_to_pdf.py --input  --output  --type [pdf|html|slides] ``` <li> <p>The interface is:     ```     &gt; notes_to_pdf.py -h     usage: notes_to_pdf.py [-h] -i INPUT -o OUTPUT --type {pdf,html,slides}                          [--filter_by_header FILTER_BY_HEADER]                          [--filter_by_lines FILTER_BY_LINES] [--script SCRIPT]                          [--preview_actions]                          [--toc_type {none,pandoc_native,navigation}]                          [--no_run_latex_again] [--debug_on_error]                          [--gdrive_dir GDRIVE_DIR] [--use_host_tools]                          [--action {cleanup_before,preprocess_notes,render_images,run_pandoc,copy_to_gdrive,open,cleanup_after} | --skip_action {cleanup_before,preprocess_notes,render_images,run_pandoc,copy_to_gdrive,open,cleanup_after}]                          [--all] [--dockerized_force_rebuild]                          [--dockerized_use_sudo]                          [-v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}]</p> <p>Convert a txt file into a PDF / HTML / slides using <code>pandoc</code>.</p> </li>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#from_scratch_with_toc","title":"From scratch with TOC:","text":"<p>notes_to_pdf.py -a pdf --input ...</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#for_interactive_mode","title":"For interactive mode:","text":"<p>notes_to_pdf.py -a pdf --no_cleanup_before --no_cleanup --input ...</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#check_that_can_be_compiled","title":"Check that can be compiled:","text":"<p>notes_to_pdf.py -a pdf --no_toc --no_open_pdf --input ...</p> <p>notes_to_pdf.py     --input notes/IN_PROGRESS/math.The_hundred_page_ML_book.Burkov.2019.txt     -t pdf     --no_cleanup --no_cleanup_before --no_run_latex_again --no_open</p> <p>options:     -h, --help            show this help message and exit     -i INPUT, --input INPUT     -o OUTPUT, --output OUTPUT                                                 Output file     --type {pdf,html,slides}                                                 Type of output to generate     --filter_by_header FILTER_BY_HEADER                                                 Filter by header     --filter_by_lines FILTER_BY_LINES                                                 Filter by lines (e.g., <code>0:10</code>, <code>1:None</code>, <code>None:10</code>)     --script SCRIPT       Bash script to generate with all the executed sub-                                                 commands     --preview_actions     Print the actions and exit     --toc_type {none,pandoc_native,navigation}     --no_run_latex_again     --debug_on_error     --gdrive_dir GDRIVE_DIR                                                 Directory where to save the output to share on Google                                                 Drive     --use_host_tools      Use the host tools instead of the dockerized ones     --action {cleanup_before,preprocess_notes,render_images,run_pandoc,copy_to_gdrive,open,cleanup_after}                                                 Actions to execute     --skip_action {cleanup_before,preprocess_notes,render_images,run_pandoc,copy_to_gdrive,open,cleanup_after}                                                 Actions to skip     --all                 Run all the actions (cleanup_before preprocess_notes                                                 render_images run_pandoc open cleanup_after)     --dockerized_force_rebuild                                                 Force to rebuild the Docker container     --dockerized_use_sudo                                                 Use sudo inside the container     -v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}                                                 Set the logging level ```</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples","title":"Examples","text":"<ul> <li>Compile to Beamer slides <code>&gt; notes_to_pdf.py -i lesson.txt -o lesson.pdf --type slides</code></li> <li>Produce a stand\u2011alone HTML page     <code>&gt; notes_to_pdf.py -i cheatsheet.txt -o cheatsheet.html --type html</code></li> <li>Build a PDF article (LaTeX)     <code>&gt; notes_to_pdf.py -i paper.txt -o paper.pdf --type pdf</code></li> <li> <p>Skip the final viewer open step     <code>&gt; ... --skip_action open`</code></p> </li> <li> <p>Tip: Run with <code>--preview_actions</code> to print the exact steps without     executing them.</p> </li> <li> <p>Slides with navigation breadcrumbs, keeping intermediate files for inspection     // TODO(indro): <code>--toc_type navigation</code> fails because of the preprocess step.</p> <p>```bash</p> <p>notes_to_pdf.py \\         --input lectures_source/Lesson5-Theory_Statistical_learning.txt \\         --output Lesson5.pdf \\         --type slides \\         --toc_type navigation \\         --debug_on_error \\         --skip_action cleanup_after ```</p> </li> <li> <p>Focus on a subsection, compiling only from line\u00a0362 to EOF for a fast     iteration when debugging slides</p> <p>```bash</p> <p>notes_to_pdf.py \\         --input Lesson8-Reasoning_over_time.txt \\         --output Focus.pdf \\         --type slides \\         --filter_by_lines 362:None \\         --skip_action cleanup_after ```</p> </li> <li> <p>Plain PDF article     <code>bash     &gt; notes_to_pdf.py -i book_notes.txt -o book_notes.pdf --type pdf</code></p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#render_imagespy","title":"<code>Render_Images.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_1","title":"What It Does","text":"<ul> <li> <p>This script auto renders figures by:</p> <ul> <li>Detecting fenced code blocks (PlantUML, Mermaid, TikZ, Graphviz, ...)</li> <li>Rendering them into images calling the appropriate tool</li> <li>Commenting them out the block</li> <li>Inlining a <code>![](img)</code> markup</li> </ul> </li> <li> <p>Render the images in a text file     <code>bash     &gt; render_images.py -i lectures_source/Lesson9-Causal_inference.txt \\             -o lesson9.images.txt --run_dockerized</code></p> </li> </ul> <p>The supported File types and code blocks are:</p> <ul> <li>File extension: <code>.md</code>, <code>.txt</code><ul> <li>Rendering syntax allowed:<ul> <li><code>plantuml</code></li> <li><code>mermaid</code></li> <li><code>graphviz</code></li> <li><code>tikz</code></li> <li><code>latex</code></li> </ul> </li> <li>Output embeds as: <code>&lt;img src=\"figs/xxx.png\"&gt;</code></li> </ul> </li> <li>File extension: <code>.tex</code><ul> <li>Rendering syntax allowed:<ul> <li>Same tags (TikZ &amp; LaTeX especially)</li> </ul> </li> <li>Output embeds as: <code>\\includegraphics{...}</code></li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples_1","title":"Examples","text":"<ul> <li> <p>Render to a new file</p> <p>```bash</p> <p>render_images.py -i lesson.md -o lesson.rendered.md --action render --run_dockerized ```</p> </li> <li> <p>Render in\u2011place (Markdown or LaTeX)</p> <p>```bash</p> <p>render_images.py -i lesson.md --action render --run_dockerized ```</p> </li> <li> <p>HTML preview of already\u2011rendered images</p> <p>```bash</p> <p>render_images.py -i lesson.md --action open --run_dockerized ```</p> </li> <li> <p>Dry\u2011run (test parsing / comments only)     <code>bash     &gt; render_images.py -i lesson.md -o /tmp/out.md --dry_run</code></p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#interface","title":"Interface","text":"<ul> <li> <p>The interface</p> <p>```bash</p> <p>render_images.py -h usage: render_images.py [-h] -i IN_FILE_NAME [-o OUT_FILE_NAME]                                                 [--action {open,render} | --skip_action {open,render}]                                                 [--all] [--dry_run] [--dockerized_force_rebuild]                                                 [--dockerized_use_sudo]                                                 [-v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}]</p> <p>Replace sections of image code with rendered images, commenting out the original code, if needed.</p> <p>See <code>docs/work_tools/documentation_toolchain/all.render_images.explanation.md</code>.</p> <p>Usage:</p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#create_a_new_markdown_file_with_rendered_images","title":"Create a new Markdown file with rendered images:","text":"<p>render_images.py -i ABC.md -o XYZ.md --action render --run_dockerized</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#render_images_in_place_in_the_original_markdown_file","title":"Render images in place in the original Markdown file:","text":"<p>render_images.py -i ABC.md --action render --run_dockerized</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#render_images_in_place_in_the_original_latex_file","title":"Render images in place in the original LaTeX file:","text":"<p>render_images.py -i ABC.tex --action render --run_dockerized</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#open_rendered_images_from_a_markdown_file_in_html_to_preview","title":"Open rendered images from a Markdown file in HTML to preview:","text":"<p>render_images.py -i ABC.md --action open --run_dockerized</p> <p>options:     -h, --help            show this help message and exit     -i IN_FILE_NAME, --in_file_name IN_FILE_NAME                                                 Path to the input file     -o OUT_FILE_NAME, --out_file_name OUT_FILE_NAME                                                 Path to the output file     --action {open,render}                                                 Actions to execute     --skip_action {open,render}                                                 Actions to skip     --all                 Run all the actions ()     --dry_run             Update the file but do not render images     --dockerized_force_rebuild                                                 Force to rebuild the Docker container     --dockerized_use_sudo                                                 Use sudo inside the container     -v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}                                                 Set the logging level ```</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#lint_notespy","title":"<code>Lint_Notes.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_2","title":"What It Does","text":"<ul> <li> <p>Tidy up notes in different formats (selected with the file extension or     <code>--type</code>):</p> <ul> <li>Markdown</li> <li>LaTeX</li> <li>Txt notes</li> </ul> </li> <li> <p>Various preprocessing and postprocessing steps:</p> <ul> <li>Refreshing the Table\u202fof\u202fContents</li> <li>Normalising Google Docs artifacts</li> <li>Running Prettier</li> <li>Fixing bullet/heading quirks</li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#_1","title":"All.notes toolchain.how to guide","text":"<pre><code>    \"preprocess\",\n    \"prettier\",\n    \"postprocess\",\n    \"frame_chapters\",\n    \"refresh_toc\",\n</code></pre>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples_2","title":"Examples","text":"<ul> <li> <p>Basic usage</p> <p>```bash</p> <p>lint_txt.py -i input.md -o output.md ```</p> </li> <li> <p>Process specific actions only     <code>&gt; lint_txt.py -i input.md -o output.md --action preprocess,prettier</code></p> </li> <li> <p>Prettify with Dockerized Prettier and TOC rebuild</p> <p>```bash</p> <p>lint_txt.py -i Lesson10.md \\         --use_dockerized_prettier \\         --use_dockerized_markdown_toc ```</p> </li> <li> <p>Custom print width and selective actions</p> <p>```bash</p> <p>lint_txt.py -i draft.txt -o tidy.txt -w 100 \\         --action preprocess,prettier,postprocess ```</p> </li> <li> <p>Use in vim for inline formatting     <code>verbatim     :%!lint_txt.py</code></p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#interface_1","title":"Interface","text":"<pre><code>&gt; lint_txt.py -h\nusage: lint_txt.py [-h] -i IN_FILE_NAME [-o OUT_FILE_NAME] [--type TYPE] [-w PRINT_WIDTH] [--use_dockerized_prettier] [--use_dockerized_markdown_toc]\n                     [--action {preprocess,prettier,postprocess,frame_chapters,refresh_toc} | --skip_action {preprocess,prettier,postprocess,frame_chapters,refresh_toc}] [--all]\n                     [--dockerized_force_rebuild] [--dockerized_use_sudo] [-v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}]\n\nSee instructions at docs/tools/documentation_toolchain/all.notes_toolchain.how_to_guide.md\n\nLint \"notes\" files.\n\n&gt; lint_txt.py -i foo.md -o bar.md     --use_dockerized_prettier     --use_dockerized_markdown_toc\n\n- It can be used in vim to prettify a part of the text using stdin / stdout.\n</code></pre> <p>:%!lint_txt.py</p> <pre><code>\noptions:\n    -h, --help            show this help message and exit\n    -i IN_FILE_NAME, --in_file_name IN_FILE_NAME\n                                                Input file or `-` for stdin\n    -o OUT_FILE_NAME, --out_file_name OUT_FILE_NAME\n                                                Output file or `-` for stdout\n    --type TYPE\n    -w PRINT_WIDTH, --print-width PRINT_WIDTH\n    --use_dockerized_prettier\n    --use_dockerized_markdown_toc\n    --action {preprocess,prettier,postprocess,frame_chapters,refresh_toc}\n                                                Actions to execute\n    --skip_action {preprocess,prettier,postprocess,frame_chapters,refresh_toc}\n                                                Actions to skip\n    --all                 Run all the actions (preprocess prettier postprocess frame_chapters refresh_toc)\n    --dockerized_force_rebuild\n                                                Force to rebuild the Docker container\n    --dockerized_use_sudo\n                                                Use sudo inside the container\n    -v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}\n                                                Set the logging level\n</code></pre> <p>// TODO</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#extract_notebook_imagespy","title":"<code>Extract_Notebook_Images.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_3","title":"What It Does","text":"<ul> <li>Spins up a docker container and dumps every <code>png/svg</code> output cell into a     folder.</li> <li>You can then publish or reuse the static plots/diagrams already rendered in a     Jupyter notebook.</li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#example","title":"Example","text":"<ul> <li>Minimal call:     <code>bash     &gt; extract_notebook_images.py \\             --in_notebook_filename notebooks/Lesson8.ipynb \\             --out_image_dir notebooks/screenshots</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#interface_2","title":"Interface","text":"<p>// TODO</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#llm_transformpy","title":"<code>Llm_Transform.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_4","title":"What It Does","text":"<ul> <li> <p>Apply a GPT\u2011style transformation (rewrite, summarise, critique code, convert     to slides, etc.) to any text file without leaving the terminal / editor.</p> </li> <li> <p>Note: You need to have an <code>OPENAI_API_KEY</code> and an internet connection.</p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples_3","title":"Examples","text":"<ul> <li> <p>TODO</p> <p><code>bash llm_transform.py -i draft.txt -o polished.txt -p rewrite_clearer</code></p> </li> <li> <p>Finding available prompts</p> <p><code>bash llm_transform.py -p list -i - -o -</code></p> </li> <li> <p>Turn a code file into a review checklist</p> <p>```bash</p> <p>llm_transform.py -i foo.py -o cfile -p code_review vim cfile ```</p> </li> <li> <p>Color\u2011accent the bold bullets for slides</p> <p>```bash</p> <p>llm_transform.py -i deck.md -o - -p slide_colorize | tee deck.color.md ```</p> </li> <li> <p>Inline use in Vim, visual\u2011select a block, then:</p> <p><code>vim :'&lt;,'&gt;!llm_transform.py -p summarize -i - -o -</code></p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#interface_3","title":"Interface","text":"<p>// TODO</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#run_pandocpy","title":"<code>Run_Pandoc.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_5","title":"What It Does","text":"<ul> <li>Reads Markdown from stdin or <code>--input</code> file.</li> <li>Dispatches to a named action (currently only <code>convert_md_to_latex</code>).</li> <li>Pushes the Pandoc output to stdout or the <code>--output</code> file.</li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#example_1","title":"Example","text":"<ul> <li>Convert a Markdown file to LaTeX     <code>&gt; run_pandoc.py -i note.md -o note.tex</code></li> <li>Same, but stream from <code>stdin</code> to <code>stdout</code> <code>&gt; cat note.md | run_pandoc.py -i - -o -</code></li> <li> <p>Inside Vim (visual range)     <code>:&lt;,'&gt;!run_pandoc.py -i - -o - -v CRITICAL</code></p> </li> <li> <p>Tip: pass <code>-v CRITICAL</code> to silence helper logging when piping into     editors.</p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#interface_4","title":"Interface","text":"<ul> <li><code>-i / --input</code><ul> <li>Default: <code>-</code></li> <li>Meaning: Source file or <code>-</code> for STDIN</li> </ul> </li> <li><code>-o / --output</code><ul> <li>Default: <code>-</code></li> <li>Meaning: Destination file or <code>-</code> for STDOUT</li> </ul> </li> <li><code>--action</code><ul> <li>Default: <code>convert_md_to_latex</code></li> <li>Meaning: Transformation to apply. Future-proofed for more actions</li> </ul> </li> <li><code>-v / --log_level</code><ul> <li>Default: <code>INFO</code></li> <li>Meaning: Standard helper-library verbosity</li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#transform_notespy","title":"<code>Transform_Notes.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_6","title":"What It Does","text":"<ul> <li>Accepts a text/Markdown stream (file or <code>-</code>).</li> <li>Applies a named action (<code>-a/--action</code>).</li> <li>Writes the result to the given output (in\u2011place, file, or <code>-</code>).</li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples_4","title":"Examples","text":"<ul> <li> <p>Run <code>-a list</code> to print a list of the valid</p> <ul> <li><code>toc</code><ul> <li>Generate a bullet TOC (top-level by default)</li> <li>Typical Vim one-liner: <code>:!transform_notes.py -a toc -i % -l 1</code></li> </ul> </li> <li><code>format_headers</code><ul> <li>Re-flow / indent headers (up to <code>--max_lev</code>)</li> <li>Typical Vim one-liner: <code>:%!transform_notes.py -a format -i - --max_lev 3</code></li> </ul> </li> <li><code>increase_headers_level</code><ul> <li>Bump all headers down one level</li> <li>Typical Vim one-liner: <code>:%!transform_notes.py -a increase -i -</code></li> </ul> </li> <li><code>md_list_to_latex</code><ul> <li>Convert a Markdown list to LaTeX <code>\\begin{itemize}</code></li> <li>Typical Vim one-liner: <code>:%!transform_notes.py -a md_list_to_latex -i -</code></li> </ul> </li> <li><code>md_*</code> family<ul> <li>Formatting clean-ups (bold bullets, colorize bold text, etc.)</li> <li>Additional Information: See <code>-a list</code> for more details</li> </ul> </li> </ul> </li> <li> <p>Re\u2011flow &amp; clean a file in place</p> <p>```bash</p> <p>transform_notes.py -a md_format -i notes/lecture.txt ```</p> </li> <li> <p>Generate a 2\u2011level TOC to STDOUT</p> <p>```bash</p> <p>transform_notes.py -a toc -i notes/lecture.md -o - -l 2 ```</p> </li> <li> <p>Tidy ChatGPT\u2011generated Markdown (visual mode in Vim)     <code>:'&lt;,'&gt;!transform_notes.py -i - -o - -a md_fix_chatgpt_output</code></p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#interface_5","title":"Interface","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#extract_headers_from_markdownpy","title":"<code>Extract_Headers_From_Markdown.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_7","title":"What It Does","text":"<ul> <li>Turn a Markdown document into either:<ul> <li>A plain list of headers</li> <li>A nested header map</li> <li>A *Vim quick\u2011fix** (<code>cfile</code>) that lets you jump between sections with     <code>:cnext</code>.</li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples_5","title":"Examples","text":"<ul> <li> <p>Human\u2011readable map (levels 1\u20113) to <code>stdout</code></p> <p>```bash</p> <p>extract_headers_from_markdown.py -i README.md -o - --mode list --max-level 3 ```</p> </li> <li> <p>Build a quick\u2011fix file and open Vim on it     <code>bash     &gt; extract_headers_from_markdown.py -i README.md -o headers.cfile --mode cfile     &gt; vim -c \"cfile headers.cfile\"</code></p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#dockerized_tikz_to_bitmappy","title":"<code>Dockerized_Tikz_To_Bitmap.Py</code>","text":"<ul> <li>Convert a <code>.tex</code> file containing TikZ code into a <code>.png</code> image using a Dockerized toolchain consisting of pdflatex and ImageMagick.</li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples_6","title":"Examples","text":"<ul> <li> <p>Plain 300\u00a0DPI conversion</p> <p>```bash</p> <p>dockerized_tikz_to_bitmap.py -i figure.tikz -o figure.png ```</p> </li> <li> <p>Custom ImageMagick options (e.g. 600\u00a0DPI)     <code>bash     &gt; dockerized_tikz_to_bitmap.py -i fig.tikz -o fig.png -- -density 600 -quality 90</code></p> <ul> <li>Any extra tokens after <code>--</code> are passed verbatim to <code>convert</code></li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#dockerized_graphvizpy","title":"<code>Dockerized_Graphviz.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_8","title":"What It Does","text":"<ul> <li> <p>Converts a Graphviz <code>.dot</code> file into a <code>.png</code> image using a Dockerized     container.</p> <p><code>bash graphviz_wrapper.py --input input.dot --output output.png</code></p> </li> <li> <p>This script serves as a thin wrapper around Dockerized Graphviz for consistent     rendering across systems.</p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#interface_6","title":"Interface","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples_7","title":"Examples","text":"<ul> <li>Convert DOT to PNG     <code>&gt; graphviz_wrapper.py -i diagram.dot -o diagram.png</code></li> <li>Rebuild Docker image     <code>&gt; graphviz_wrapper.py -i diagram.dot -o diagram.png --dockerized_force_rebuild</code></li> <li>Use <code>sudo</code> for Docker     <code>bash     &gt; graphviz_wrapper.py -i diagram.dot -o diagram.png --dockerized_use_sudo</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#dockerized_latexpy","title":"<code>Dockerized_Latex.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_9","title":"What It Does","text":"<ul> <li>Compiles a LaTeX <code>.tex</code> file into a PDF using <code>pdflatex</code> inside a Docker     container.</li> <li>Automatically rebuilds the Docker image if needed.</li> <li>Supports optional rerun of LaTeX for proper references or table of contents     generation     <code>bash     &gt; latex_wrapper.py --input doc.tex --output doc.pdf</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples_8","title":"Examples","text":"<ul> <li>Compile <code>.tex</code> to <code>.pdf</code> <code>&gt; latex_wrapper.py -i report.tex -o report.pdf</code></li> <li>Rebuild Docker image     <code>&gt; latex_wrapper.py -i report.tex -o report.pdf --dockerized_force_rebuild</code></li> <li>Use <code>sudo</code> for Docker     <code>&gt; latex_wrapper.py -i report.tex -o report.pdf --dockerized_use_sudo</code></li> <li>Run LaTeX twice     <code>&gt; latex_wrapper.py -i paper.tex -o paper.pdf --run_latex_again</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#dockerized_mermaidpy","title":"<code>Dockerized_Mermaid.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_10","title":"What It Does","text":"<ul> <li>Renders Mermaid <code>.mmd</code> or <code>.md</code> diagrams into image files using a Dockerized     container.</li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples_9","title":"Examples","text":"<ul> <li> <p>TODO</p> <p>```bash</p> <p>mermaid_wrapper.py --input flowchart.mmd --output flowchart.png ```</p> </li> <li> <p>Automatically sets output to match input name if <code>--output</code> is omitted</p> </li> <li> <p>Mermaid diagram     <code>&gt; mermaid_wrapper.py -i diagram.mmd -o diagram.png</code></p> </li> <li>Use input as output (default)     <code>&gt; mermaid_wrapper.py -i diagram.mmd</code></li> <li>Rebuild container     <code>&gt; mermaid_wrapper.py -i diagram.mmd -o diagram.png --dockerized_force_rebuild</code></li> <li>Use <code>sudo</code> for Docker     <code>&gt; mermaid_wrapper.py -i diagram.mmd -o diagram.png --dockerized_use_sudo</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#dockerized_pandocpy","title":"<code>Dockerized_Pandoc.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_11","title":"What It Does","text":"<ul> <li>Converts documents using <code>pandoc</code> inside a Docker container</li> <li>Supports output to Beamer slides, PDFs, and more with custom CLI flags.</li> </ul> <pre><code>&gt; pandoc_wrapper.py --input notes.md --output slides.pdf -- docker_args...\n</code></pre> <ul> <li>Internally builds a Docker container and passes the full <code>pandoc</code> command     string.</li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#example_2","title":"Example","text":"<ul> <li>Convert Markdown to PDF     <code>&gt; pandoc_wrapper.py --input notes.md --output notes.pdf --container_type pandoc_latex</code></li> <li>Convert to Beamer slides     <code>&gt; pandoc_wrapper.py --input slides.md --output slides.pdf --container_type pandoc_latex -- -t beamer</code></li> <li>Rebuild Docker image     <code>&gt; pandoc_wrapper.py --input notes.md --output notes.pdf --dockerized_force_rebuild</code></li> <li>Run with sudo     <code>&gt; pandoc_wrapper.py --input notes.md --output notes.pdf --dockerized_use_sudo</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#dockerized_prettierpy","title":"<code>Dockerized_Prettier.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_12","title":"What It Does","text":"<ul> <li>Formats text files (<code>.md</code>, <code>.txt</code>, <code>.tex</code>, etc.) using Prettier within a     Docker container</li> <li>Avoids environment-specific issues and ensures consistent formatting.</li> <li> <p>Supports full Prettier CLI flexibility via passthrough of additional options.</p> <p><code>bash dockerized_prettier.py --parser markdown --write test.md</code></p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#examples_10","title":"Examples","text":"<ul> <li>Format a Markdown file     <code>&gt; dockerized_prettier.py --parser markdown --write test.md</code></li> <li>Use <code>sudo</code> for Docker execution     <code>&gt; dockerized_prettier.py --use_sudo --parser markdown --write test.md</code></li> <li>Rebuild the Docker image     <code>&gt; dockerized_prettier.py --dockerized_force_rebuild --parser markdown --write test.md</code></li> <li>Change indentation and wrap style     <code>dockerized_prettier.py --parser markdown --tab-width 4 --prose-wrap always --write test.md</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#interface_7","title":"Interface","text":"<ul> <li> <p>Interface     ```     &gt; dockerized_prettier.py -h     usage: dockerized_prettier.py [-h] -i IN_FILE_NAME [-o OUT_FILE_NAME]                                                                 [--dockerized_force_rebuild]                                                                 [--dockerized_use_sudo]                                                                 [-v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}]</p> <p>Run <code>prettier</code> inside a Docker container to ensure consistent formatting across different environments.</p> <p>This script builds the container dynamically if necessary and formats the specified file using the provided <code>prettier</code> options.</p> <p>Examples</p> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#basic_usage","title":"Basic usage:","text":"<p>dockerized_prettier.py --parser markdown --prose-wrap always --write     --tab-width 2 test.md</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#use_sudo_for_docker_commands","title":"Use sudo for Docker commands:","text":"<p>dockerized_prettier.py --use_sudo --parser markdown --prose-wrap always     --write --tab-width 2 test.md</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#set_logging_verbosity","title":"Set logging verbosity:","text":"<p>dockerized_prettier.py -v DEBUG --parser markdown --prose-wrap always     --write --tab-width 2 test.md </p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#process_a_file","title":"Process a file:","text":"<p>cat test.md - a     - b                 - c dockerized_prettier.py --parser markdown --prose-wrap always     --write --tab-width 2 test.md</p> <p>options:     -h, --help            show this help message and exit     -i IN_FILE_NAME, --in_file_name IN_FILE_NAME                                                 Input file or <code>-</code> for stdin     -o OUT_FILE_NAME, --out_file_name OUT_FILE_NAME                                                 Output file or <code>-</code> for stdout     --dockerized_force_rebuild                                                 Force to rebuild the Docker container     --dockerized_use_sudo                                                 Use sudo inside the container     -v {TRACE,DEBUG,INFO,WARNING,ERROR,CRITICAL}                                                 Set the logging level ```</p>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#save_screenshotpy","title":"<code>Save_Screenshot.Py</code>","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#what_it_does_13","title":"What It Does","text":"<ol> <li>Prompts you to select a screen region (<code>\u2318\u202f+\u202fCtrl\u202f+\u202f4</code>).</li> <li>Saves it as <code>screenshot.YYYY\u2011MM\u2011DD_HH\u2011MM\u2011SS.png</code> (or your chosen name).</li> <li>Prints and copies the Markdown embed <code>&lt;img src=\"path/to/file.png\"&gt;</code>.</li> </ol>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#useful_tools","title":"Useful Tools","text":""},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#mermaid","title":"Mermaid","text":"<ul> <li> <p>To render on-line: https://mermaid.live</p> </li> <li> <p>Resources:</p> <ul> <li>https://mermaid.js.org/syntax/examples.html</li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#graphviz","title":"Graphviz","text":"<ul> <li> <p>To render on-line:     https://dreampuf.github.io/GraphvizOnline</p> </li> <li> <p>Resources:</p> <ul> <li>https://graphviz.org/gallery/</li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#markdown","title":"Markdown","text":"<ul> <li>To render on-line:     https://markdownlivepreview.com/</li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#pandoc","title":"Pandoc","text":"<ul> <li>To render on-line: https://pandoc.org/try/</li> </ul>"},{"location":"tools/documentation_toolchain/all.notes_toolchain.how_to_guide.html#tikz","title":"Tikz","text":"<ul> <li> <p>To render on-line use Overleaf</p> </li> <li> <p>Resources</p> <ul> <li>https://www.overleaf.com/learn/latex/TikZ_package</li> <li>https://texample.net/</li> <li>https://www.integral-domain.org/lwilliams/Resources/tikzsnippets.php</li> <li>https://tikz.pablopie.xyz/</li> <li>https://tikzit.github.io/</li> <li>https://latexdraw.com/</li> </ul> </li> </ul>"},{"location":"tools/documentation_toolchain/all.render_images.explanation.html","title":"All.render images.explanation","text":""},{"location":"tools/documentation_toolchain/all.render_images.explanation.html#render_imagespy_tool","title":"<code>render_images.py</code> tool","text":"<ul> <li>The <code>render_images.py</code> tool replaces image code in Markdown/LaTeX files (e.g.,     <code>plantUML</code> or <code>mermaid</code> code for diagrams) with rendered images.</li> <li>Location: <code>dev_scripts_helpers/documentation/render_images.py</code></li> <li>Typical usage to render images in a Markdown file:     <code>bash     &gt; render_images.py -i knowledge_graph/vendors/README.md --action render --run_dockerized</code></li> </ul>"},{"location":"tools/documentation_toolchain/all.render_images.explanation.html#how_to_use","title":"How to use","text":"<ol> <li> <p>If you don't include the option <code>--run_dockerized</code>, make sure <code>plantuml</code> and    <code>mermaid</code> are installed on your machine. The easiest way is to use the Docker    container. All the packages typically needed for development are installed in    the container.</p> </li> <li> <p>How to use:    ```bash</p> <p>render_images.py -h    ```</p> </li> <li> <p>We try to let the rendering engine do its job of deciding where to put stuff     even if sometimes it's not perfect. Otherwise, with any update of the text we     need to iterate on making it look nice: we don't want to do that.</p> </li> <li> <p><code>.md</code> files should be linted by our tools.</p> </li> <li> <p>If you want to use <code>open</code> action, make sure that your machine is able to open    <code>.html</code> files in the browser.</p> </li> </ol>"},{"location":"tools/git/all.git.how_to_guide.html","title":"All.git.how to guide","text":""},{"location":"tools/git/all.git.how_to_guide.html#git","title":"Git","text":""},{"location":"tools/git/all.git.how_to_guide.html#git_workflow_and_best_practices","title":"Git workflow and best practices","text":""},{"location":"tools/git/all.git.how_to_guide.html#before_you_start","title":"Before you start","text":"<ul> <li>GitHub is the place where we keep our code</li> <li><code>git</code> is the tool (program) for version control</li> <li>We interact with GitHub via <code>git</code></li> <li>Use public key for authorization<ul> <li>You can add a new public key here     GH -&gt; Personal settings -&gt; SSH keys</li> <li>More details about what is public key you can find in     all.ssh.how_to_guide.md</li> </ul> </li> </ul>"},{"location":"tools/git/all.git.how_to_guide.html#readings","title":"Readings","text":"<ul> <li>Read at least the first 3 chapters of     Git book</li> <li>Read about     Git Submodules<ul> <li>We use Git submodules to compose and share code about repos</li> </ul> </li> </ul>"},{"location":"tools/git/all.git.how_to_guide.html#workflow","title":"Workflow","text":"<ul> <li> <p>Run <code>git fetch</code>     ```     # Fetch all the data from origin.     &gt; git fetch</p> </li> <li> <p>Checkout master and pull</p> <ul> <li> <p>You want to branch from the latest version of master to avoid a merge:     <code>``     # Checkout the</code>master` branch.     &gt; git checkout master</p> </li> </ul> </li> <li> <p>Name a branch after its corresponding issue</p> <ul> <li>The canonical name for a new feature branch is obtained by running     <code>i gh_issue_title</code>:     ```     &gt; i gh_issue_title -i 274     INFO: &gt; cmd='/Users/saggese/src/venv/amp.client_venv/bin/invoke gh_issue_title -i 274'     report_memory_usage=False report_cpu_usage=False     ## gh_issue_title: issue_id='274', repo_short_name='current'     ## gh_login:     07:35:54 - INFO  lib_tasks_gh.py gh_login:48                            account='sorrentum'     export GIT_SSH_COMMAND='ssh -i /Users/saggese/.ssh/id_rsa.sorrentum.github'     gh auth login --with-token </li> </ul> </li> </ul>"},{"location":"tools/git/all.git.how_to_guide.html#list_all_the_branches","title":"List all the branches.","text":"<p>git branch -r origin/HEAD -&gt; origin/master origin/PTask274 ... ```</p>"},{"location":"tools/git/all.git.how_to_guide.html#make_sure_your_local_master_is_in_sync_with_the_remote","title":"Make sure your local <code>master</code> is in sync with the remote.","text":"<p>git pull --rebase <code>- Alternatively, and especially if you have local changes to move to a new branch, run</code> git checkout master i git_pull ```</p>"},{"location":"tools/git/all.git_hooks.how_to_guide.html","title":"All.git hooks.how to guide","text":""},{"location":"tools/git/all.git_hooks.how_to_guide.html#setup","title":"Setup","text":""},{"location":"tools/git/all.git_hooks.how_to_guide.html#configuration","title":"Configuration","text":"<ul> <li> <p>Git hooks are installed by default when the user activates the thin     environment via the <code>setenv.sh</code> script</p> </li> <li> <p>Although not recommended, users can explicitly disable the hooks for the     entire repo by adding the following configuration in the <code>repo_config.yaml</code>:     <code>yaml     repo_info:         ...         # Enable git-commit hooks.         enable_git_commit_hook: False     ...</code></p> </li> </ul>"},{"location":"tools/git/all.git_hooks.how_to_guide.html#installation","title":"Installation","text":"<ul> <li> <p>To manually install the hooks, run</p> <p>```bash</p> <p>dev_scripts_helpers/git/git_hooks/install_hooks.py --action install ```</p> </li> <li> <p>To manually remove the hooks, run     <code>bash     &gt; dev_scripts_helpers/git/git_hooks/install_hooks.py --action remove</code></p> </li> </ul>"},{"location":"tools/git/all.git_hooks.how_to_guide.html#usage","title":"Usage","text":""},{"location":"tools/git/all.git_hooks.how_to_guide.html#commit_changes","title":"Commit changes","text":"<ul> <li>Once installed, the hooks will be run automatically when a user tries to     commit changes</li> <li> <p>If the pre-commit and the commit-msg checks pass, the commit will be created</p> <p>```bash</p> <p>git commit -m \"Lint\"</p> </li> <li> <p>The pre-commit checks that were run and passed to the commit are also     automatically added to the commit messages</p> <p>```bash</p> <p>git log 02f2c08 commit 02f2c08b1677627a95a43c21246219462f9ae339 (HEAD -&gt; CmampTask11073_Document_git_hooks, origin/CmampTask11073_Document_git_hooks) Author: Dummy dummy@gmail.com Date:   Mon Apr 28 22:36:50 2025 -0400</p> <pre><code>    Lint\n\n    Pre-commit checks:\n    All checks passed \u2705\n</code></pre> <p>```</p> </li> </ul>"},{"location":"tools/git/all.git_hooks.how_to_guide.html#running_git_pre-commit_hook","title":"Running git pre-commit hook ...","text":""},{"location":"tools/git/all.git_hooks.how_to_guide.html#check_master","title":"check_master","text":"<p>git rev-parse --abbrev-ref HEAD Branch is 'CmampTask11073_Document_git_hooks' 'check_master' passed</p>"},{"location":"tools/git/all.git_hooks.how_to_guide.html#check_author","title":"check_author","text":"<p>git config user.name git config --show-origin user.name git config user.email git config --show-origin user.email user_email='dummy@gmail.com' 'check_author' passed</p>"},{"location":"tools/git/all.git_hooks.how_to_guide.html#check_file_size","title":"check_file_size","text":"<p>max file size=512 KB 'check_file_size' passed</p>"},{"location":"tools/git/all.git_hooks.how_to_guide.html#check_python_compile","title":"check_python_compile","text":"<p>Compiling 'dev_scripts_helpers/git/git_hooks/install_hooks.py'... 'check_python_compile' passed</p>"},{"location":"tools/git/all.git_hooks.how_to_guide.html#check_gitleaks","title":"check_gitleaks","text":"<p>'check_gitleaks' passed</p>"},{"location":"tools/git/all.git_hooks.how_to_guide.html#all_pre-commit_hooks_passed_committing","title":"All pre-commit hooks passed: committing","text":"<p>Run git commit-msg hook ...</p>"},{"location":"tools/git/all.git_hooks.how_to_guide.html#commit-msg_hook_passed_committing","title":"commit-msg hook passed: committing","text":"<p>[CmampTask11073_Document_git_hooks 02f2c08] Lint 1 file changed, 11 insertions(+), 9 deletions(-) ```</p>"},{"location":"tools/git/all.git_hooks.how_to_guide.html#skip_the_checks","title":"Skip the checks","text":"<ul> <li>To skip the checks for a specific commit, use the <code>--no-verify</code> option     <code>bash     &gt; git commit -m \"Enable copilot review by default\" --no-verify</code></li> </ul>"},{"location":"tools/git/all.git_hooks.reference.html","title":"All.git hooks.reference","text":""},{"location":"tools/git/all.git_hooks.reference.html#git_hooks","title":"Git hooks","text":""},{"location":"tools/git/all.git_hooks.reference.html#overview","title":"Overview","text":"<ul> <li> <p>A Git hook is a script that Git automatically executes before or after certain     events such as committing code, merging branches, or pushing changes</p> </li> <li> <p><code>dev_scripts_helpers/git/git_hooks</code>: contains our custom Git hooks</p> <ul> <li><code>./install_hooks.py</code>: can be used to install or remove the hooks</li> <li><code>./pre-commit.py</code>: runs before a commit is created</li> <li><code>./commit-msg.py</code>: checks and/or edits the commit message</li> </ul> </li> </ul>"},{"location":"tools/git/all.git_hooks.reference.html#pre-commit_hook","title":"Pre-commit hook","text":"<ul> <li>The <code>pre-commit.py</code> script enforces a set of invariants before allowing a     <code>git commit</code> to succeed</li> <li>It ensures that essential checks are passed, such as verifying the branch,     author information, file size limits, forbidden words, Python file     compilations, secret leaks...etc.</li> </ul>"},{"location":"tools/git/all.git_hooks.reference.html#commit-msg_hook","title":"Commit-msg hook","text":"<ul> <li>The <code>commit-msg.py</code> script enforces rules related to git commit messages     before allowing a commit to succeed</li> <li>It checks that commit messages follow required conventions</li> <li>It also adds the pre-commit checks that were run and passed to the commit     messages</li> </ul>"},{"location":"tools/git/all.gitleaks.how_to_guide.html","title":"All.gitleaks.how to guide","text":""},{"location":"tools/git/all.gitleaks.how_to_guide.html#gitleaks_how_to_guide","title":"Gitleaks How to Guide","text":""},{"location":"tools/git/all.gitleaks.how_to_guide.html#how_to_run_gitleaks_locally","title":"How to run Gitleaks locally","text":"<p>The easiest way to run Gitleaks locally is with Docker</p> <ul> <li> <p>First, pull the image:</p> <p>```bash</p> <p>docker pull zricethezav/gitleaks:latest ```</p> </li> <li> <p>Scan git repositories for secrets (including git history)</p> <p>```bash</p> <p>docker run -v $(pwd):/path zricethezav/gitleaks:latest git /path -v -c /path/.github/gitleaks-rules.toml         \u25cb         \u2502\u2572         \u2502 \u25cb         \u25cb \u2591         \u2591    gitleaks</p> <p>7:52PM INF 1540 commits scanned. 7:52PM INF scanned ~51542901 bytes (51.54 MB) in 46.4s 7:52PM INF no leaks found ```</p> </li> <li> <p>Scan directories or files for secrets</p> <p>```bash</p> <p>docker run -v $(pwd):/path zricethezav/gitleaks:latest dir /path -v -c /path/.github/gitleaks-rules.toml         \u25cb         \u2502\u2572         \u2502 \u25cb         \u25cb \u2591         \u2591    gitleaks</p> <p>8:00PM INF scanned ~23901530 bytes (23.90 MB) in 26.1s 8:00PM INF no leaks found ```</p> </li> </ul>"},{"location":"tools/git/all.gitleaks.how_to_guide.html#how_to_deal_with_false_positives","title":"How to deal with false positives","text":"<ul> <li>Some secrets are detected by Gitleaks that are not actually secrets but false     positives</li> <li> <p>To deal with false positives, we can use the following methods:</p> </li> <li> <p>Use inline comments to allow specific lines of code</p> </li> </ul> <pre><code>&gt; aws_access_key_id=*** # gitleaks:allow\n</code></pre> <ol> <li>Add the fingerprint to the <code>.gitleaksignore</code> file</li> </ol> <pre><code>&gt; cat .gitleaksignore\n&gt; ck.infra/infra/terraform/environments/preprod/ap-northeast-1/terraform.tfvars:rule3:429\n&gt; 93f292c3dfa2649ef91f8925b623e79546fa992e:README.md:aws-access-token:121\n</code></pre> <ol> <li>Adjust the <code>allowlists</code> or <code>rules</code> in the <code>gitleaks-rules.toml</code> file to    reduce false positives</li> </ol> <pre><code>&gt; cat .github/gitleaks-rules.toml\ntitle = \"gitleaks config\"\n\n[[allowlists]]\ndescription = \"global allow lists\"\npaths = [\n        # Gitleaks rules file.\n        '''(^|.*/)gitleaks-rules\\.toml$''',\n        # Compressed files.\n        '''.*\\.(csv|txt)\\.gz$''',\n        # Zip files.\n        '''.*\\.zip$''',\n        # Gitleaks ignore file.\n        '''.gitleaksignore$''',\n]\n...\n[[rules]]\n        id = \"rule2\"\n        description = '''AWS API Key'''\n        regex = '''AKIA[0-9A-Z]{16}'''\n        tags = [\"secret\"]\n</code></pre>"},{"location":"tools/git/all.gitleaks.how_to_guide.html#how_to_remove_leaks_from_previous_commits_in_pr","title":"How to remove leaks from previous commits in PR","text":"<ul> <li>When leaks are committed to a PR, they remain in the commit history even after     removal</li> <li>The Gitleaks GitHub Actions will fail due to these leaks in the commit history</li> <li> <p>We can remove leaks from the commit history using the following methods:</p> </li> <li> <p>Option 1: Soft reset and combine all commits in the PR into a single commit</p> <ul> <li>Use this method if you want to discard all commit history in the PR</li> </ul> <p>```bash</p>"},{"location":"tools/git/all.gitleaks.how_to_guide.html#checkout_the_pr_branch","title":"Checkout the PR branch","text":"<p>git checkout"},{"location":"tools/git/all.gitleaks.how_to_guide.html#reset_to_the_base_commit_but_keep_changes_staged","title":"Reset to the base commit but keep changes staged","text":"<p>git reset --soft $(git merge-base HEAD master)</p>"},{"location":"tools/git/all.gitleaks.how_to_guide.html#create_a_new_commit_with_all_changes","title":"Create a new commit with all changes","text":"<p>git commit -m \"\""},{"location":"tools/git/all.gitleaks.how_to_guide.html#force_push_to_the_remote_branch","title":"Force push to the remote branch","text":"<p>git push -f ```</p> <li> <p>Option 2: Rebase and squash only the commits that contain the leaks</p> <ul> <li>Use this method if you want to retain some commit history in the PR</li> <li>See this guide for     more details on how to rebase and squash commits</li> </ul> </li>"},{"location":"tools/git/all.gitleaks.reference.html","title":"All.gitleaks.reference","text":""},{"location":"tools/git/all.gitleaks.reference.html#gitleaks","title":"Gitleaks","text":"<ul> <li>Gitleaks is a SAST tool used for detecting and preventing hardcoded secrets     such as passwords, API keys, and tokens in a directory or git repository</li> <li>It detects secrets not only in the codebase but also in the git commit history</li> <li>Gitleaks can be integrated into our workflow to prevent secrets from being     accidentally added to our codebase</li> <li>It is integrated into our dev system via:<ul> <li>Git hooks (Refer to     /docs/tools/git/all.git_hooks.reference.md))</li> <li>GitHub actions (Refer to     /docs/build_system/all.gitleaks_workflow.explanation.md</li> </ul> </li> </ul>"},{"location":"tools/git/all.gitleaks.reference.html#configuration","title":"Configuration","text":"<ul> <li>The rules for Gitleaks are specified in <code>.github/gitleaks-rules.toml</code></li> <li>To avoid redundancy, the file is symlinked to a common file in <code>//helpers</code>     located at     <code>/dev_scripts_helpers/git/gitleaks/gitleaks-rules.toml</code>     that can be shared across all repos</li> </ul>"},{"location":"tools/git/all.gitleaks.reference.html#allowlists","title":"<code>allowlists</code>","text":"<ul> <li>We can use allowlists to make exceptions to certain files or lines of code     based on paths or regex patterns</li> <li>If we want to exclude \"specific\" commits, lines, or rules, use the inline     <code># gitleaks:allow</code> comment or the <code>.gitleaksignore</code> file instead     <code>toml     [[allowlists]]     description = \"global allow lists\"     paths = [         '''.github/gitleaks-rules.toml''',     ]</code></li> </ul>"},{"location":"tools/git/all.gitleaks.reference.html#rule_extension","title":"Rule extension","text":"<ul> <li>We can extend our custom ruleset to include the default ruleset provided by     Gitleaks with:     <code>toml     [extend]     useDefault = true</code></li> </ul>"},{"location":"tools/git/all.gitleaks.reference.html#rules","title":"<code>rules</code>","text":"<ul> <li>Each rule is defined with:<ul> <li><code>id</code> - A unique identifier for each rule</li> <li><code>description</code> - Short human readable description of the rule</li> <li><code>regex</code> - Golang regular expression used to detect secrets</li> <li><code>tags</code> - Array of strings used for metadata and reporting purposes</li> </ul> </li> <li>For example:     <code>toml     [[rules]]         id = \"rule2\"         description = '''AWS API Key'''         regex = '''AKIA[0-9A-Z]{16}'''         tags = [\"secret\"]</code></li> </ul>"},{"location":"tools/git/all.gitleaks.reference.html#gitleaksignore","title":"<code>.gitleaksignore</code>","text":"<ul> <li>To prevent specific lines of code from being scanned by Gitleaks, we can use     the <code>.gitleaksignore</code> file</li> <li>It uses \"fingerprints\" to define the leaks</li> <li>Gitleaks generates these fingerprints when it detects a leak, which can then     be added to the file</li> <li>Examples of fingerprints:     <code>bash     &gt; ck.infra/infra/terraform/environments/preprod/ap-northeast-1/terraform.tfvars:rule3:429     &gt; 93f292c3dfa2649ef91f8925b623e79546fa992e:README.md:aws-access-token:121</code></li> <li>The <code>.gitleaksignore</code> file can be placed at the root directory of a repo</li> </ul>"},{"location":"tools/git/all.gitleaks.reference.html#additional_resource","title":"Additional Resource","text":"<ul> <li>The official GitHub page for Gitleaks -     https://github.com/gitleaks/gitleaks https://github.com/gitleaks/gitleaks-action/tree/master</li> <li>The custom ruleset was based on -     https://github.com/mazen160/secrets-patterns-db</li> </ul>"},{"location":"tools/git/all.integrate_helpers.how_to_guide.html","title":"All.integrate helpers.how to guide","text":"<p>This mainly follows the process described in //cmamp/docs/coding/all.integrate_repos.how_to_guide.md</p> <ul> <li> <p>Create a branch in both clients (e.g., <code>/Users/saggese/src/cmamp1</code> and     <code>/Users/saggese/src/helpers1</code>)</p> <p>```bash</p> <p>cd /Users/saggese/src/cmamp1 git pull i git_branch_create -b CmampTask10048_Integrate_helpers_20240922</p> <p>cd /Users/saggese/src/helpers1 git pull i git_branch_create -b CmampTask10048_Integrate_helpers_20240922 ```</p> </li> <li> <p>Diff the dirs     <code>&gt; diff_to_vimdiff.py --dir1 /Users/saggese/src/cmamp1/helpers --dir2 ~/src/helpers1/helpers     &gt; diff_to_vimdiff.py --dir1 /Users/saggese/src/cmamp1/helpers --dir2 /Users/saggese/src/cmamp2/helpers_root/helpers/</code></p> </li> </ul>"},{"location":"tools/git/all.invite_gh_contributors.explanation.html","title":"All.invite gh contributors.explanation","text":""},{"location":"tools/git/all.invite_gh_contributors.explanation.html#invite_gh_contributorspy_explanation","title":"<code>invite_gh_contributors.py</code> Explanation","text":"<p>This document is about how this script works and flows.</p>"},{"location":"tools/git/all.invite_gh_contributors.explanation.html#public_interface","title":"Public interface","text":"<pre><code># Google Sheet Source\ndev_scripts_helpers/github/invite_gh_contributors.py \\\n        --drive_url &lt;google\u2011sheet\u2011url&gt;  \\\n        --gh_token  &lt;github\u2011pat&gt;       \\\n        --org_name  &lt;github\u2011org&gt;       \\\n        --repo_name &lt;repo&gt;             \\\n        [--log_level 20]\n\n# CSV source (mutually exclusive with --drive_url)\ndev_scripts_helpers/github/invite_gh_contributors.py \\\n        --csv_file  &lt;/path/to/users.csv&gt; \\\n        --gh_token  &lt;github\u2011pat&gt;         \\\n        --org_name  &lt;github\u2011org&gt;         \\\n        --repo_name &lt;repo&gt;               \\\n        [--log_level 20]\n</code></pre> <ul> <li><code>drive_url</code>/<code>csv_file</code>: Spreadsheet containing a <code>GitHub user</code> column.</li> <li><code>gh_token</code>: PAT with <code>repo</code> scope (or fine\u2011grained \"Repository     administration\").</li> <li><code>org_name</code> / <code>repo_name</code>: identify thtarget repository.</li> <li><code>log_level</code>: standard Python numeric levels (10\u00a0= DEBUG, 20\u00a0=\u00a0INFO).</li> </ul>"},{"location":"tools/git/all.invite_gh_contributors.explanation.html#execution_flow","title":"Execution flow","text":"<pre><code>flowchart TD\n        A[parse CLI args] --&gt; B[init logging]\n        B --&gt; C[extract_usernames_from_gsheet/csv]\n        C --&gt;|\"list[str]\"| D[send_invitations]\n        D --&gt; E{already collaborator?}\n        E -- yes --&gt; F[skip + log]\n        E -- no --&gt; G[_invite wrapper]\n        G --&gt;|add_to_collaborators| H[GitHub API]\n        subgraph Rate-limit\n                direction TB\n                G\n                note(\"@ratelimit.limits 50 calls / 24h \u2192 sleep_and_retry if exceeded\")\n        end\n</code></pre>"},{"location":"tools/git/all.invite_gh_contributors.explanation.html#key_implementation_choices","title":"Key implementation choices","text":"<ul> <li>Dependency auto\u2011install: the small <code>pip install</code> loop avoids a separate     requirements file when the script runs in fresh environments, at the cost of     start\u2011up time.</li> <li>Service\u2011account auth: credentials path is hard\u2011coded but can be fed via     env\u2011var if desired; the helper supports both.</li> <li>Idempotence: <code>repo.has_in_collaborators()</code> prevents duplicate invites     counting toward the daily quota.</li> <li>Sleep strategy: we rely entirely on <code>ratelimit.sleep_and_retry</code>, so the     process may block for hours. Even if the process is terminated, the     idempotence measure will prevent the added names from contributing to the     quota</li> </ul>"},{"location":"tools/git/all.invite_gh_contributors.how_to_guide.html","title":"All.invite gh contributors.how to guide","text":""},{"location":"tools/git/all.invite_gh_contributors.how_to_guide.html#inviting_github_collaborators_from_google_sheets","title":"Inviting GitHub Collaborators from Google\u00a0Sheets","text":""},{"location":"tools/git/all.invite_gh_contributors.how_to_guide.html#prerequisites","title":"Prerequisites","text":"<ul> <li> <p>GitHub personal\u2011access token (PAT) with the classic <code>repo</code> scope, or a     fine\u2011grained token granting Repository\u00a0administration for the target     repository.</p> <p><code>bash export GH_PAT=github_pat_...</code></p> </li> <li> <p>Google service\u2011account JSON key</p> <ul> <li>Create a service account in Google\u00a0Cloud.</li> <li>Enable the Drive\u00a0API on that project.</li> <li>Download the key as JSON and place it at <code>/app/DATA605/google_secret.json</code>     (or change the path in the script).</li> <li>Share the spreadsheet with the service\u2011account address.</li> </ul> </li> </ul>"},{"location":"tools/git/all.invite_gh_contributors.how_to_guide.html#running_the_script","title":"Running the script","text":"<pre><code>dev_scripts_helpers/github/invite_gh_contributors.py \\\n        --drive_url \"https://docs.google.com/spreadsheets/d/...\" \\  # or  --csv_file  &lt;/path/to/users.csv&gt;\n        --gh_token  \"$GH_PAT\" \\\n        --org_name  causify-ai \\\n        --repo_name tutorials \\\n        --log_level 20          # INFO\n</code></pre> <ul> <li><code>--drive_url</code>/<code>--csv_file</code>: full URL of the Google\u00a0Sheet containing a     column named <code>GitHub user</code>.</li> <li><code>--gh_token</code>: your PAT (or set <code>GH_PAT</code> and pass <code>--gh_token \"$GH_PAT\"</code>).</li> <li><code>--org_name</code> / <code>--repo_name</code>: where the invitations will be sent.</li> <li>Increase <code>--log_level</code> to <code>10</code> for verbose debug logs.</li> </ul>"},{"location":"tools/helpers/all.asyncio.explanation.html","title":"All.asyncio.explanation","text":""},{"location":"tools/helpers/all.asyncio.explanation.html#asyncio_best_practices","title":"Asyncio Best Practices","text":"<p>Asynchronous programming is a powerful paradigm in Python that allows you to write non-blocking, concurrent code that can greatly improve the efficiency of your applications when I/O and computation can be overlapped.</p> <p>The official documentation is https://docs.python.org/3/library/asyncio.html This document provides an overview of how asyncio works and offers best practices to avoid common pitfalls.</p>"},{"location":"tools/helpers/all.asyncio.explanation.html#nomenclature_in_asyncio","title":"Nomenclature in asyncio","text":"<ul> <li> <p>Event Loop</p> <ul> <li>Asyncio operates on an event loop that manages the execution of asynchronous     tasks. Whenever one wants to execute the asynchronous tasks we do     <code>asyncio.run()</code> or <code>asyncio.run_until_complete()</code>: both of these methods     will start the event loop and it will be set to running yielding control to     it.</li> <li>In asyncio, you can obtain the event loop using either     <code>asyncio.get_event_loop()</code> or <code>asyncio.get_running_loop()</code>. In the latest     version of asyncio, both methods now serve the same purpose. However, it is     recommended to use <code>get_running_loop</code> because <code>get_event_loop</code> has been     deprecated since version 3.12. This change ensures consistency and future     compatibility with asyncio.</li> </ul> </li> <li> <p>Coroutines (aka \"async functions\" defined with <code>async def</code>)</p> <ul> <li>These functions can be paused and resumed without blocking other tasks.</li> </ul> </li> <li> <p><code>await</code></p> <ul> <li>The <code>await</code> keyword is used only within coroutines to pause execution until     an asynchronous operation (e.g., I/O) is completed. While waiting, the event     loop can execute other tasks.</li> </ul> </li> </ul>"},{"location":"tools/helpers/all.asyncio.explanation.html#common_pitfalls_and_solutions","title":"Common Pitfalls and Solutions","text":""},{"location":"tools/helpers/all.asyncio.explanation.html#1_avoid_running_multiple_event_loops","title":"1. Avoid Running Multiple Event Loops","text":"<p>One common error is <code>Event loop is already running</code>. When you use <code>asyncio.run()</code> or <code>run_until_complete()</code>, the event loop is started. Attempting to start a new event loop while one is already running will result in this error.</p> <p>To avoid this:</p> <ul> <li> <p>Solution 1: use <code>nest_asyncio</code></p> <ul> <li><code>nest_asyncio</code> is a library that allows you to create nested event loops.     While this may seem like a solution but may lead to complex issues. This was     mainly developed to run <code>asyncio</code> in Jupyter/ipython which already runs an     event loop in backend. This library also does not support     <code>asyncio_solipsism</code> so there is another trade-off.</li> <li>Here's how nest_asyncio works:<ul> <li>It saves the current event loop, if any, that is running in the     environment.</li> <li>It sets up a new event loop specifically for running asyncio code.</li> <li>You can run your asyncio code within this nested event loop.</li> <li>When you're done running asyncio code, <code>nest_asyncio</code> restores the     original event loop, ensuring compatibility with the environment.</li> </ul> </li> </ul> </li> <li> <p>Solution 2: use threads</p> <ul> <li>Instead of starting a new event loop, run that specific part of your code in     a separate thread to prevent conflicts. This solves the issue but using     thread has its own complications such as race conditions which can be     difficult to debug</li> </ul> </li> <li> <p>Solution 3: embrace \"async all the way up\" approach</p> <ul> <li>Use <code>await</code> instead of nested call to <code>asyncio.run</code> and make your methods     asynchronous using <code>async def</code> all the way</li> </ul> </li> </ul>"},{"location":"tools/helpers/all.asyncio.explanation.html#example_code","title":"Example Code","text":"<p>Consider the following coroutines</p> <ul> <li><code>A</code> that sleeps and then calls <code>B</code></li> <li><code>B</code> which calls <code>C</code></li> <li> <p><code>C</code> sleeps</p> <p>```mermaid graph TD         A[async def A] --&gt; B[def B]         B --&gt;C[ async def C]</p> <pre><code>    style A fill:#FFA07A, stroke:#FF6347\n    style B fill:#98FB98, stroke:#2E8B57\n    style C fill:#ADD8E6, stroke:#4682B4\n</code></pre> <p><code></code> import asyncio import helpers.hasyncio as hasynci</p> </li> <li> <p>The code above won't work and will give     <code>Error: \"Event loop is already running\"</code></p> <p>This error arises because, when <code>run()</code> is invoked, it initializes the event loop on the current thread. Subsequently, if <code>run()</code> is called within the function <code>B()</code>, the system checks for the running state of the event loop. If the event loop is already in progress, the system raises the 'Event loop is already running' error.\"</p> </li> <li> <p>Adding     ```     import nest_asyncio</p> <p>nest_asyncio.apply() ```</p> <p>the code above will work</p> </li> <li> <p>If <code>nest_asyncio</code> is present the following code does not work     <code>with hasynci.solipsism_context() as event_loop:         hasynci.run(A(), event_loop, close_event_loop=False)</code>     failing with the error     <code>Error: \"Event loop is already running\"</code></p> </li> </ul>"},{"location":"tools/helpers/all.asyncio.explanation.html#corresponds_to_submit_twap_in_cmamptask5842","title":"Corresponds to <code>submit twap</code> in CmampTask5842","text":"<p>async def A():         print(\"IN A\")         await asyncio.sleep(2)         print(\"ENTER B\")         B()</p>"},{"location":"tools/helpers/all.asyncio.explanation.html#corresponds_to_get_fill_per_order","title":"Corresponds to <code>get_fill_per_order</code>","text":"<p>async def C():         print(\"IN C\")         await asyncio.sleep(2)         print(\"EXIT C\")</p>"},{"location":"tools/helpers/all.asyncio.explanation.html#get_fill","title":"get_fill","text":"<p>def B():         print(\"IN B\")         cor = C()         asyncio.get_running_loop().run_until_complete(cor)         print(\"EXIT B\")</p>"},{"location":"tools/helpers/all.asyncio.explanation.html#call_a","title":"Call A.","text":"<p>hasynci.run(A(), asyncio.get_event_loop(), close_event_loop=False) ```</p>"},{"location":"tools/helpers/all.asyncio.explanation.html#2_timesleep_vs_asynciosleep","title":"2. time.sleep() vs asyncio.sleep()","text":"<p>One common error is to use <code>time.sleep()</code> with asynchronous methods.</p> <p>This blocks the execution of that method and the event loop cannot proceed to task until the sleep period is over. This negates the primary purpose of the asyncio and doesn't allow us to simulate systems with <code>solipsism</code>.</p> <p>We should almost never use this, and use <code>asyncio.sleep</code> instead.</p>"},{"location":"tools/helpers/all.hcache.explanation.html","title":"All.hcache.explanation","text":""},{"location":"tools/helpers/all.hcache.explanation.html#cache","title":"Cache","text":"<ul> <li> <p>This document explains the design and flow of a caching system implemented in     <code>/helpers/hcache.py</code>.</p> </li> <li> <p><code>hcache</code> provides a dual-layer cache (memory and disk) with tagging, global     and function-specific isolation, and deterministic modes for complex projects,     improving performance and persistency across sessions.</p> </li> </ul>"},{"location":"tools/helpers/all.hcache.explanation.html#overview","title":"Overview","text":"<ul> <li>In performance-sensitive systems, repeated evaluations of the same expensive     function can degrade efficiency (for example, calling an expensive function     twice with the same arguments returns instantly on the second call due to     caching).</li> <li> <p>The <code>hcache</code> module addresses this through a robust, dual-layer caching     mechanism that reduces recomputation, enables persistency across sessions, and     improves responsiveness in both scripts and interactive notebooks.</p> </li> <li> <p>Unlike lightweight alternatives like <code>hcache_simple</code>, <code>hcache</code> is suited for     complex use cases where cache configuration, inspection, tagging, and sharing     are necessary. It supports memory- and disk-based layers, function-level     control, and tagged caches for environment separation (e.g., test vs prod).</p> </li> </ul>"},{"location":"tools/helpers/all.hcache.explanation.html#usage_example","title":"Usage example","text":"<pre><code>from helpers.hcache import cache\n\n@cache(use_mem_cache=True, use_disk_cache=True, tag=\"unit_tests\")\ndef expensive_compute(x, y):\n        # heavy work...\n        return x * y\n\n# First call: computes and caches\nres1 = expensive_compute(3, 4)\n# Second call: instant return from memory\nres2 = expensive_compute(3, 4)\n</code></pre>"},{"location":"tools/helpers/all.hcache.explanation.html#core_concepts","title":"Core concepts","text":"<ul> <li> <p>Source Code Tracking: Detects changes in the wrapped function's bytecode     pointer to invalidate stale cache entries.</p> </li> <li> <p>Two-Level Cache: Cascading lookup in memory first (via <code>joblib.Memory</code>     over <code>tmpfs</code>), then on disk (via <code>joblib.Memory</code> at specified directory).</p> </li> <li> <p>Lookup and Store Flow: On function call, check memory \u2192 check disk \u2192     execute function if miss \u2192 store result in both layers.</p> </li> <li> <p>Global Cache: Default backend shared across all cached functions in a Git     repo, located at <code>$GIT_ROOT/tmp.cache.{mem,disk}</code>.</p> <ul> <li>Tagged Global Cache: Namespaces cache per <code>tag</code> parameter (e.g.,     <code>unit_tests</code> vs default) to isolate environments.</li> </ul> </li> <li> <p>Function-Specific Cache: Customizable cache directories for individual     functions managed via <code>.set_cache_directory()</code>, <code>.get_cache_directory()</code>, and     <code>.clear_function_cache()</code>.</p> </li> <li> <p>Deterministic Modes: <code>enable_read_only</code> and <code>check_only_if_present</code>     options enforce strict cache-only or read-only behaviors, supporting testing     and debugging.</p> </li> </ul>"},{"location":"tools/helpers/all.hcache.explanation.html#how_the_cache_works","title":"How the <code>Cache</code> works","text":"<ul> <li> <p><code>Cache</code> tracks changes in the source code of the wrapped function</p> <ul> <li>For performance reasons, it checks the code only one time unless the pointer     to the function is changed, e.g. in notebooks</li> </ul> </li> <li> <p>By default, it uses two levels of caching:</p> <ul> <li><code>Memory</code> level</li> <li><code>Disk</code> level</li> </ul> </li> <li> <p>When a call is made to the wrapped function:</p> <ul> <li>Firstly the <code>Memory</code> level is being checked</li> <li>If there's no hit in the <code>Memory</code>, the <code>Disk</code> level is checked</li> <li>If there's no hit in <code>Disk</code> level, the wrapped function is called</li> <li>The result is then stored in both <code>Disk</code> and <code>Memory</code> levels</li> </ul> </li> <li> <p><code>Cache</code> is equipped with a <code>get_last_cache_accessed()</code> method to understand if     the call hit the cache and on which level</p> </li> </ul>"},{"location":"tools/helpers/all.hcache.explanation.html#disk_level","title":"Disk level","text":"<ul> <li><code>Disk</code> level is implemented via     joblib.Memory</li> </ul>"},{"location":"tools/helpers/all.hcache.explanation.html#memory_level","title":"Memory level","text":"<ul> <li> <p>Initially, the idea was to use     functools.lru_cache     for memory cache</p> </li> <li> <p>Pros:</p> <ol> <li>Standard library implementation</li> <li>Quietly fast in-memory implementation</li> </ol> </li> <li> <p>Cons:</p> <ol> <li>Only hashable arguments are supported</li> <li>No access to cache, it's not possible to check if an item is in cache or  not</li> <li>It does not work properly in notebooks</li> </ol> </li> <li> <p>Because Cons outweighed Pros, we decided to implement <code>Memory</code> level as     joblib.Memory     over <code>tmpfs</code></p> </li> <li>In this way we reuse the same code for <code>Disk</code> level cache but over a RAM-based     disk<ul> <li>This implementation overcomes the Cons listed above, although it is slightly     slower than the pure <code>functools.lru_cache</code> approach</li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hcache.explanation.html#global_cache","title":"Global cache","text":"<ul> <li>By default, all cached functions save their cached values in the default     global cache</li> <li>The cache is \"global\" in the sense that:<ul> <li>It is unique per-user and per Git client</li> <li>It serves all the functions of a Git client</li> </ul> </li> <li>The cached data stored in a folder <code>$GIT_ROOT/tmp.cache.{mem,disk}.[tag]</code></li> <li> <p>This global cache is being managed via global functions named     <code>*_global_cache</code>, e.g., <code>set_global_cache()</code></p> </li> <li> <p>TODO(gp): maybe a better name is</p> <ul> <li>Global -&gt; local_cache, client_cache</li> <li>Function_specific -&gt; global or shared</li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hcache.explanation.html#tagged_global_cache","title":"Tagged global cache","text":"<ul> <li>A global cache can be specific of different applications (e.g., for unit tests     vs normal code)<ul> <li>It is controlled through the <code>tag</code> parameter</li> <li>The global cache corresponds to <code>tag = None</code></li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hcache.explanation.html#function-specific_cache","title":"Function-specific cache","text":"<ul> <li>It is possible to create function-specific caches, e.g., to share the result     of a function across clients and users</li> <li>In this case the client needs to set <code>disk_cache_directory</code> and / or     <code>mem_cache_directory</code> parameters in the decorator or in the <code>Cached</code> class     constructor</li> <li>If cache is set for the function, it can be managed with     <code>.set_cache_directory()</code>, <code>.get_cache_directory()</code>, <code>.destroy_cache()</code> and     <code>.clear_function_cache()</code> methods.</li> </ul>"},{"location":"tools/helpers/all.hcache.explanation.html#design_rationale_and_trade-offs","title":"Design rationale and trade-offs","text":"<p>We chose these approaches to balance performance, persistence, and configurability.</p> <p>Below is a summary of our key design choices and their trade-offs.</p> Choice Trade-off <code>joblib.Memory</code> over <code>lru_cache</code> More flexible and persistent, but slightly slower <code>copy.deepcopy()</code> on retrieval Ensures immutability but adds overhead Global + function-specific backends Powerful but adds complexity for setup and cleanup Verbose cache control (<code>enable_read_only</code>, <code>check_only_if_present</code>) Good for testing and debugging; may be overkill for casual use"},{"location":"tools/helpers/all.hcache.explanation.html#common_misunderstandings","title":"Common misunderstandings","text":"<ul> <li>Caches aren't auto-cleaned: Old entries may accumulate; manual clearing is     needed via <code>clear_global_cache()</code> or <code>clear_function_cache()</code>.</li> <li>Not all functions are cache-safe: Side effects, non-determinism (e.g.,     random values or timestamps), and unhashable arguments can lead to incorrect     or missing cache behavior.</li> <li>Function code changes may invalidate cache: <code>joblib</code> hashes the function     code, so altering it causes cache misses unless overridden intentionally via     <code>update_func_code_without_invalidating_cache()</code>.</li> </ul>"},{"location":"tools/helpers/all.hcache.explanation.html#execution_flow_diagram","title":"Execution flow diagram","text":"<p>Figure: Execution flow of the caching mechanism, showing decorator setup, lookup order, and advanced features.</p> <pre><code>flowchart LR\n    %% Decorator System\n    subgraph DecoratorSystem[\"Decorator System\"]\n        dec[\"@cache(use_mem_cache, use_disk_cache, disk_cache_path)\"]\n        CachedClass[\"CachedClass\"]\n        dec --&gt; CachedClass\n    end\n\n    %% Cached Class Components\n    subgraph CachedClassComponents[\"Cached Class Components\"]\n        memCache[\"_create_function_memory_cache()\"]\n        diskCache[\"_create_function_disk_cache()\"]\n        CachedClass --&gt; memCache\n        CachedClass --&gt; diskCache\n\n        memCache --&gt; globalType[\"get_global_cache_type(tag)\"]\n        diskCache --&gt; globalType\n        globalType --&gt; backendFactory[\"_create_global_cache_backend()\"]\n\n        backendFactory --&gt; globalPath[\"_get_global_cache_path()\"]\n        backendFactory --&gt; globalName[\"_get_global_cache_name()\"]\n        backendFactory --&gt; clearGlobal[\"clear_global_cache()\"]\n        backendFactory --&gt; infoGlobal[\"get_global_cache_info()\"]\n        backendFactory --&gt; clearFunc[\"clear_function_cache()\"]\n        backendFactory --&gt; funcPath[\"get_function_cache_path()\"]\n        backendFactory --&gt; hasFunc[\"has_function_cache()\"]\n    end\n\n    %% Cache Types &amp; Paths\n    subgraph CacheTypesAndPaths[\"Cache Types &amp; Paths\"]\n        memoryPath[\"Memory Cache Path\\n/tmp/jbill/mem\"]\n        diskPath[\"Disk Cache Path\\n~/.cache/hj/cache.disk\"]\n        funcSpecific[\"Function-specific Path\"]\n        globalPath --&gt; memoryPath\n        globalName --&gt; diskPath\n        funcPath   --&gt; funcSpecific\n    end\n\n    %% Joblib Integration\n    subgraph JoblibIntegration[\"Joblib Integration\"]\n        MemCache[\"MEMORY_CACHE\\njoblib.Memory\"]\n        DiskCache2[\"DISK_CACHE\\njoblib.Memory\"]\n        MemCache  --&gt; MemFunction[\"joblib.MemoryFunc\"]\n        DiskCache2 --&gt; MemFunction\n        MemFunction --&gt; StoreBackend[\"Store Backend\"]\n        StoreBackend --&gt; CodeTracking[\"Function Code Tracking\"]\n    end\n\n    %% Function Wrapper\n    subgraph FunctionWrapper[\"Function Wrapper\"]\n        Wrapper[\"call() \u2013 Main caching logic\"]\n        CachedClass --&gt; Wrapper\n        Wrapper --&gt; GetResult[\"get_memorized_result()\"]\n        Wrapper --&gt; StoreVersion[\"store_cached_version()\"]\n        Wrapper --&gt; GlobalMgmt[\"_IS_CACHE_ENABLED = true\"]\n    end\n\n    %% Advanced Features\n    subgraph AdvancedFeatures[\"Advanced Features\"]\n        CodeTracking --&gt; ReadOnly[\"enable_read_only()\"]\n        CodeTracking --&gt; CheckOnly[\"enable_check_only_if_present()\"]\n        CodeTracking --&gt; S3Support[\"S3 Backend Support\"]\n        CodeTracking --&gt; LastAccessed[\"_get_last_cache_accessed()\"]\n        CodeTracking --&gt; UpdateCode[\"update_func_code_without_invalidating_cache()\"]\n    end\n</code></pre>"},{"location":"tools/helpers/all.hcache_simple.explanation.html","title":"All.hcache simple.explanation","text":""},{"location":"tools/helpers/all.hcache_simple.explanation.html#cache_simple","title":"Cache Simple","text":"<ul> <li> <p>This document explains the design and flow of a caching system implemented in     <code>/helpers/hcache_simple.py</code>.</p> </li> <li> <p><code>hcache_simple</code> is a lightweight, decorator-based module designed for     individual function caching, offering basic in\u2011memory and disk storage (via     JSON or pickle) with manual management and simple performance tracking.</p> </li> <li>In contrast to <code>hcache</code> which is a robust, global caching solution that     supports tagged caches, automatic invalidation, and shared cache directories     across multiple functions and users, using advanced tools</li> <li>Use <code>hcache</code> for robust, global caching in complex projects, and     <code>hcache_simple</code> for lightweight, function-specific caching in simpler     applications or notebooks.</li> </ul>"},{"location":"tools/helpers/all.hcache_simple.explanation.html#overview","title":"Overview","text":"<ul> <li> <p>The caching system supports two storage layers:</p> <ul> <li><code>Memory Cache</code>: A Python dictionary storing function results for rapid     retrieval</li> <li><code>Disk Cache</code>: Persistent storage using either <code>JSON</code> or <code>pickle</code> files,     ensuring that cached results persist across sessions</li> </ul> </li> <li> <p>Additionally, the system monitors cache performance and allows users to     configure caching behavior via <code>user</code> and <code>system</code> properties</p> </li> </ul>"},{"location":"tools/helpers/all.hcache_simple.explanation.html#design_rationale_and_trade-offs","title":"Design Rationale and Trade-offs","text":"<ul> <li> <p>Memory vs Disk: Memory cache provides fast access but is volatile and     non-persistent. Disk cache persists across sessions but comes with I/O     overhead. The design allows combining both for flexibility.</p> </li> <li> <p>Pickle vs JSON: Pickle supports a wider range of Python-native types (like     objects, sets, etc.), while JSON is more portable and human-readable but     limited to basic types. The user can choose based on their use case.</p> </li> <li> <p>Property Storage: Properties are stored in separate <code>user</code> and <code>system</code>     pickle files to separate runtime configuration (user behavior) from     infrastructure-level settings (e.g., storage format).</p> </li> <li> <p>Performance Tracking is Optional: Monitoring is off by default to avoid     runtime overhead and is opt-in via <code>enable_cache_perf</code>.</p> </li> </ul>"},{"location":"tools/helpers/all.hcache_simple.explanation.html#memory_cache","title":"Memory Cache","text":"<ul> <li> <p>Implementation:</p> <ul> <li>A global dictionary <code>_CACHE</code> stores cached data</li> <li>A different nested dictionary is used for each function name</li> <li>The key is derived from the function's positional arguments (converted to     string format), and the value is the function result</li> </ul> </li> <li> <p>Flow example:</p> <ul> <li>When a decorated function is called with arguments <code>(3,)</code>, the system:<ul> <li>Checks if <code>_CACHE</code> contains the key <code>'{\"args\": [3], \"kwargs\": {}}'</code></li> <li>Returns the cached value if found <code>(cache hit)</code></li> <li>Otherwise, calls the function to compute the result, stores it in     <code>_CACHE</code>, and then returns it</li> </ul> </li> </ul> </li> <li> <p>Interface:</p> <ul> <li><code>get_cache(func_name)</code> returns the <code>in-memory cache</code> for a given function</li> <li><code>reset_mem_cache(func_name)</code> clears the <code>in-memory cache</code> for the function</li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hcache_simple.explanation.html#disk_cache","title":"Disk Cache","text":"<ul> <li> <p>File naming convention:</p> <ul> <li>Disk cache files are named using the pattern     <code>cache.&lt;func_name&gt;.&lt;extension&gt;</code>, where the extension depends on the cache     type:<ul> <li><code>JSON</code>: <code>.json</code></li> <li><code>Pickle</code>: <code>.pkl</code></li> </ul> </li> </ul> </li> <li> <p>Flow example:</p> <ul> <li>When a cache is flushed to disk:<ul> <li>The system determines the file name by checking the system cache property     <code>type</code> (set to <code>json</code> or <code>pickle</code>)</li> <li>The memory cache (a small dictionary of keys and values) is written to the     file using the appropriate format</li> <li>On subsequent runs, if the memory cache is empty, the system will load     cached results from disk</li> </ul> </li> </ul> </li> <li> <p>Interface:</p> <ul> <li><code>flush_cache_to_disk</code> writes the current memory cache to the disk file</li> <li><code>force_cache_from_disk</code> loads the disk cache and updates the in-memory cache</li> <li><code>reset_disk_cache</code> is intended to remove disk cache files<ul> <li>It is marked with an <code>assert 0</code> to disable this functionality</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hcache_simple.explanation.html#cache_performance_monitoring","title":"Cache Performance Monitoring","text":"<ul> <li> <p>This tracks how effective the caching is by recording total <code>calls</code>,     <code>cache hits</code>, and <code>cache misses</code></p> </li> <li> <p>Implementation:</p> <ul> <li>A global dictionary <code>_CACHE_PERF</code> stores performance data per function</li> <li>When enabled (via <code>enable_cache_perf</code>), it keeps counters:<ul> <li><code>tot</code>: Total number of calls</li> <li><code>hits</code>: Number of times the cache returned a value</li> <li><code>misses</code>: Number of times the function had to be called due to a cache     miss</li> </ul> </li> </ul> </li> <li> <p>Flow example:</p> <ul> <li>For a function call:<ul> <li>The system increments <code>tot</code></li> <li>If the <code>value</code> exists in the cache, <code>hits</code> is incremented</li> <li>Otherwise, <code>miss</code> is incremented and the function is executed</li> <li>The stats can then be printed with <code>get_cache_perf_stats</code> which returns a     summary string</li> </ul> </li> </ul> </li> <li> <p>Interface:</p> <ul> <li><code>enable_cache_perf</code>: to enable the cache performance</li> <li><code>disable_cache_perf</code>: to disable the cache performance</li> <li><code>get_cache_perf_stats</code> prints performance metrics</li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hcache_simple.explanation.html#cache_properties_user_and_system","title":"Cache Properties: User and System","text":"<ul> <li> <p>There are two types of properties:</p> <ul> <li><code>User Properties</code>: Configurable by the user to alter caching behavior. For     example:<ul> <li><code>abort_on_cache_miss</code>: Whether to raise an error if a cache miss occurs</li> <li><code>report_on_cache_miss</code>: Whether to return a special value (\"cache_miss\")     on a cache miss</li> <li><code>force_refresh</code>: Whether to bypass the cache and refresh the value</li> </ul> </li> <li><code>System Properties</code>: These include internal settings such as the cache type     (e.g., \"json\" or \"pickle\")</li> </ul> </li> <li> <p>Persistent storage:</p> <ul> <li>Cache properties are stored on disk as pickle files:<ul> <li>User properties in <code>cache_property.user.pkl</code></li> <li>System properties in <code>cache_property.system.pkl</code></li> </ul> </li> </ul> </li> <li> <p>Flow example:</p> <ul> <li>When a function is decorated, the system sets its system property (e.g., the     cache type) using <code>set_cache_property(system, func_name, type, cache_type)</code></li> <li>Later, when retrieving a cached value, it checks user properties (like     <code>force_refresh</code>) to decide whether to use the cached value or to recompute     the result</li> </ul> </li> <li> <p>Interface:</p> <ul> <li><code>set_cache_property(type, func_name, property_name, value)</code>: set a property</li> <li><code>get_cache_property(type, func_name, property_name)</code>: get the value of a     property</li> <li><code>reset_cache_property(type)</code>: reset Properties</li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hcache_simple.explanation.html#decorator","title":"Decorator","text":"<ul> <li> <p>Purpose:</p> <ul> <li>The decorator simplifies caching by wrapping any function so that its     results are automatically stored and retrieved from cache</li> </ul> </li> <li> <p>Decorator flow:</p> <ul> <li>Initialization:<ul> <li>When the function is decorated with, for example,     <code>@simple_cache(cache_type=\"json\")</code>, the decorator sets the system property     for the cache type</li> </ul> </li> <li>Wrapper execution:<ul> <li>Key generation: The wrapper generates a <code>cache key</code> from both arguments     and keyword arguments.<ul> <li>Exclude Keys: The wrapper excludes certain keys from the cache key by     using the <code>exclude_keys</code> argument in the decorator. These keys are     omitted from <code>kwargs</code> when forming the cache key.</li> </ul> </li> <li>Cache lookup:<ul> <li>If the key exists in the memory cache (and no force refresh is     requested), it returns the cached value</li> <li>If the key is missing (or a force refresh is requested), it calls the     original function</li> </ul> </li> <li>Performance stats:<ul> <li>The system updates performance statistics for every call (incrementing     total, hits, or misses accordingly)</li> </ul> </li> <li>Cache update:<ul> <li>After computing the value, the result is stored in the memory cache (and     optionally written through to disk if <code>write_through</code> is set)</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Flow Example:</p> <ul> <li>Suppose we have a function defined as follows:     <code>python @simple_cache(cache_type=\"json\")     def multiply_by_two(x):     return x * 2</code></li> <li>First call:<ul> <li>When you call <code>multiply_by_two(4)</code>, the cache key is generated (in this     case, <code>{\"args\": [4], \"kwargs\": {}}</code> as a string)</li> <li>Since the key is not in the cache, the function is executed, returning     <code>8</code>, which is then stored in the <code>memory cache</code>.</li> </ul> </li> <li>Subsequent call:<ul> <li>Calling <code>multiply_by_two(4)</code> again finds the key in the cache and     immediately returns <code>8</code> without executing the function again.</li> </ul> </li> <li>Force Refresh:<ul> <li>If the user property <code>force_refresh</code> is enabled for this function, even if     the key exists in the cache, the function is executed again, and the cache     is updated.</li> </ul> </li> </ul> </li> <li> <p>Configuration examples:</p> <ul> <li>Set Force Refresh:<ul> <li>With this property set, each call to <code>multiply_by_two(4)</code> will recompute     the result and update the cache.</li> </ul> </li> <li>Enable <code>write_through</code>:<ul> <li>When using <code>@simple_cache(write_through=True)</code>, the decorator will flush     the memory cache to disk immediately after updating.</li> </ul> </li> <li>Exclude certain keys from cache key:<ul> <li>Suppose we have a function that uses an OpenAI client to fetch     completions, but the actual output depends only on the prompt. The     <code>client</code> object should be excluded from the cache key because it varies     per session:     <code>python @simple_cache(exclude_keys=[\"client\"]) def get_summary(prompt: str, client: Any): return client.complete(prompt=prompt)</code></li> <li>Without <code>exclude_keys=[\"client\"]</code>, each call with a different <code>client</code>     instance (even for the same prompt) would result in a cache miss. This     exclusion ensures the cache key is based only on the <code>prompt</code>, improving     hit rates.</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hcache_simple.explanation.html#common_misunderstandings","title":"Common Misunderstandings","text":"<ul> <li> <p>Cached Function Must Be Deterministic: The cache assumes the same inputs     always produce the same outputs. Functions with side effects or     non-deterministic behavior (e.g., randomness, time-based logic) may yield     inconsistent results.</p> </li> <li> <p><code>force_refresh</code> Must Be Reset: Once <code>force_refresh</code> is set, every call     recomputes the result. Users must manually unset this flag if they want to     resume normal caching.</p> </li> <li> <p>Disk Cache Is Persistent: Cache files on disk are not automatically     cleaned up or rotated. Old or unused caches may accumulate over time.</p> </li> <li> <p>write_through Only Applies to Disk: Setting <code>write_through=True</code> will     update the disk immediately after each cache write, which is useful for     persistency but may impact performance.</p> </li> </ul>"},{"location":"tools/helpers/all.hcache_simple.explanation.html#execution_flow_diagram","title":"Execution Flow Diagram","text":"<pre><code>flowchart TD\n        %% Decorator Setup %%\n        subgraph \"Decorator Setup\"\n                A1[Function Decorated with @simple_cache]\n                A2[Set System Property: type: json/pickle]\n                A3[Wrap Function with Caching Wrapper]\n                A1 --&gt; A2\n                A2 --&gt; A3\n        end\n\n        %% Function Call Flow %%\n        subgraph \"Function Call Flow\"\n                B1[Function Called with Args, Keyword Arguments]\n                B2[Generate Cache Key]\n                B3[Update Performance Totals]\n                B4{force_refresh Enabled?}\n                B5{Key in Memory Cache?}\n                B6[Cache Hit: Return Cached Value]\n                B7[Cache Miss: Evaluate Properties]\n                B8[Call Original Function]\n                B9[Store Result in Memory Cache]\n                B10{write_through Enabled?}\n                B11[Flush Memory Cache to Disk]\n                B12[Return Result]\n\n                A3 --&gt; B1\n                B1 --&gt; B2\n                B2 --&gt; B3\n                B3 --&gt; B4\n                B4 -- Yes --&gt; B8\n                B4 -- No --&gt; B5\n                B5 -- Yes --&gt; B6\n                B5 -- No --&gt; B7\n                B7 --&gt; B8\n                B8 --&gt; B9\n                B9 --&gt; B10\n                B10 -- Yes --&gt; B11\n                B10 -- No --&gt; B12\n                B11 --&gt; B12\n        end\n\n        %% Disk Cache Operations %%\n        subgraph \"Disk Cache Operations\"\n                C1[Flush Cache to Disk]\n                C2[Force Cache from Disk]\n        end\n\n        %% Cache Properties &amp; Performance %%\n        subgraph \"Properties &amp; Performance\"\n                D1[User Properties:&lt;br&gt;force_refresh, abort_on_cache_miss,&lt;br&gt;report_on_cache_miss]\n                D2[System Properties:&lt;br&gt;cache type: json/pickle]\n                D3[Performance Stats:&lt;br&gt;tot, hits, misses]\n        end\n\n        %% Connections for Reference %%\n        B12 --- C1\n        C2 --- B9\n        D1 --- B4\n        D2 --- A2\n        D3 --- B3\n</code></pre>"},{"location":"tools/helpers/all.hllm.explanation.html","title":"All.hllm.explanation","text":""},{"location":"tools/helpers/all.hllm.explanation.html#hllm_explanation","title":"hllm \u2013 Explanation","text":""},{"location":"tools/helpers/all.hllm.explanation.html#overview","title":"Overview","text":"<ul> <li><code>hllm.get_completion()</code> is a helper function that sends prompts to LLMs     (OpenAI or OpenRouter) and returns their responses</li> <li> <p>It includes a custom caching mechanism to reduce redundant requests and allow     unit testing</p> </li> <li> <p>An example call to the LLM interface is</p> <p>```python from helpers import hllm</p> </li> </ul> <p>response = hllm.get_completion(         user_prompt=\"What is machine learning?\",         system_prompt=\"You are a helpful assistant.\",         model=\"gpt-4o-mini\",         cache_mode=\"DISABLE_CACHE\",         cache_file=\"cache.get_completion.json\",         temperature=0.5 )     ```</p> <ul> <li>The parameter used are:<ul> <li><code>user_prompt</code>: User's message to the LLM.</li> <li><code>system_prompt</code>: Context-setting instruction for the assistant.</li> <li><code>model</code>: OpenAI or OpenRouter model to use.</li> <li><code>cache_mode</code>: One of:<ul> <li><code>\"DISABLE_CACHE\"</code>: No caching.</li> <li><code>\"HIT_CACHE_OR_ABORT\"</code>: Only use cached response; raise error if not     found.</li> <li><code>\"NORMAL\"</code>: Use cache if available, else make an API call.</li> <li><code>\"REFRESH_CACHE\"</code>: Always make an API call and update the cache.</li> </ul> </li> <li><code>cache_file</code>: Path to JSON cache file.</li> <li><code>temperature</code>: Sampling temperature (0-2).</li> <li><code>**create_kwargs</code>: Additional arguments passed to the API.</li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hllm.explanation.html#caching_logic","title":"Caching Logic","text":"<ul> <li> <p>The caching mechanism uses a hashed key generated from:</p> <ul> <li><code>user_prompt</code></li> <li><code>system_prompt</code></li> <li><code>model</code></li> <li><code>temperature</code></li> <li>Additional parameters</li> </ul> </li> <li> <p>The caching layer</p> <ul> <li>Reads from <code>cache_file</code> on load</li> <li>Writes to disk when cache is updated</li> <li>Tracks metadata:<ul> <li><code>hits</code> : Number of times the cache returned a value.</li> <li><code>misses</code>: Number of times the function looked for response in cache but     didn't find</li> <li>Timestamps : Stores both <code>created_at</code> and <code>last_updated</code> timestamps.</li> </ul> </li> </ul> </li> <li> <p>The cache storage format:     <code>json     {         \"version\": \"1.0\",         \"metadata\": {             \"created_at\": \"...\",             \"last_updated\": \"...\",             \"hits\": 10,             \"misses\": 5         },         \"entries\": {             \"&lt;hash_key&gt;\": {                 \"request\": { ... },                 \"response\": { ... }             }         }     }</code></p> </li> </ul>"},{"location":"tools/helpers/all.hllm.explanation.html#cost_calculation","title":"Cost Calculation","text":""},{"location":"tools/helpers/all.hllm.explanation.html#openai_models","title":"OpenAI Models","text":"<ul> <li>For OpenAI models like <code>gpt-4o</code> or <code>gpt-3.5-turbo</code>, the cost is computed using     hardcoded pricing from OpenAI's pricing page</li> <li> <p>Each model has a per-token price for both the prompt and completion     components:</p> <p><code>python pricing = {     \"gpt-3.5-turbo\": {\"prompt\": 0.5, \"completion\": 1.5},     \"gpt-4o-mini\": {\"prompt\": 0.15, \"completion\": 0.60},     ... }</code></p> </li> <li> <p>The formula used is:     <code>text     cost = (prompt_tokens / 1e6) * prompt_price + (completion_tokens / 1e6) * completion_price</code></p> </li> </ul>"},{"location":"tools/helpers/all.hllm.explanation.html#openrouter_models","title":"OpenRouter Models","text":"<ul> <li>For OpenRouter models the price is extracted through the API<ol> <li>If the local file <code>tmp.openrouter_models_info.csv</code> does not exist:</li> <li>It queries <code>https://openrouter.ai/api/v1/models</code></li> <li>Saves the returned model pricing and metadata to a CSV file</li> <li>The cost is computed using:  <code>python cost = prompt_tokens * prompt_price + completion_tokens * completion_price</code></li> <li>Note: OpenRouter prices are already in per-token format.</li> </ol> </li> </ul>"},{"location":"tools/helpers/all.hllm.explanation.html#runtime_behavior","title":"Runtime Behavior","text":"<ul> <li>The cost is printed if <code>print_cost=True</code></li> <li>It's also stored in the cache JSON under <code>response[\"cost\"]</code></li> <li>All costs (if <code>start_logging_costs()</code> is called) accumulate in a global     counter accessible via <code>get_current_cost()</code></li> </ul>"},{"location":"tools/helpers/all.hllm.explanation.html#entire_flow","title":"Entire Flow","text":"<p>This section summarizes how <code>get_completion()</code> operates internally.</p> <ol> <li>Input Handling</li> <li>Accepts <code>user_prompt</code>, <code>system_prompt</code>, <code>model</code>, <code>cache_mode</code>, and other      parameters.</li> <li> <p>Builds OpenAI-compatible <code>messages</code> list.</p> </li> <li> <p>Cache Key Creation</p> </li> <li> <p>Calls <code>_CompletionCache.hash_key_generator()</code> using model, prompts, and      parameters to produce a unique hash key.</p> </li> <li> <p>Cache Behavior</p> </li> <li>If <code>cache_mode == \"HIT_CACHE_OR_ABORT\"</code>:<ul> <li>Looks up the response using hash key.</li> <li>If not found, raises an error.</li> </ul> </li> <li>If <code>cache_mode == \"NORMAL\"</code>:<ul> <li>Returns cached response if available.</li> <li>Otherwise proceeds to make an API call and saves the new response to    cache.</li> </ul> </li> <li>If <code>cache_mode == \"REFRESH_CACHE\"</code>:<ul> <li>Always makes a fresh API call and updates cache.</li> </ul> </li> <li> <p>If <code>cache_mode == \"DISABLE_CACHE\"</code>:</p> <ul> <li>Makes a fresh API call and does not touch cache.</li> </ul> </li> <li> <p>API Call Execution</p> </li> <li>Calls OpenAI API synchronously (with or without streaming).</li> <li> <p>Collects the completion output and underlying response object.</p> </li> <li> <p>Cost Estimation</p> </li> <li>Uses <code>_calculate_cost()</code> to compute token usage cost.</li> <li> <p>If enabled, accumulates cost using <code>start_logging_costs()</code>.</p> </li> <li> <p>Response Caching</p> </li> <li>Saves response to cache via <code>_CompletionCache.save_response_to_cache()</code> (if      applicable).</li> <li> <p>Cost and response metadata are saved in JSON.</p> </li> <li> <p>Return</p> </li> <li> <p>Returns only the final text content from the model.</p> </li> <li> <p>All caching operations are handled by the <code>_CompletionCache</code> class.</p> </li> </ol>"},{"location":"tools/helpers/all.hllm.explanation.html#how_testing_works","title":"How Testing Works","text":"<ul> <li>During unit tests, the cache:<ul> <li>Is set in <code>HIT_CACHE_OR_ABORT</code> mode to avoid real API calls (tests raise     error if the required response is not cached)</li> <li>Uses for cache a file that is checked into the repo</li> <li>Expected prompts and responses are cached beforehand or as tests are     executed</li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hllm.explanation.html#cache_refreshing","title":"Cache Refreshing","text":"<ul> <li> <p>When testing, it might be necessary to regenerate cached responses, e.g.,</p> <ul> <li>Prompts or expected completions have changed.</li> <li>You are adding a new test using a LLM prompt</li> <li>You want to ensure the cache reflects the latest LLM output.</li> </ul> </li> <li> <p>When testing, it might be necessary to regenerate cached responses after     making prompt or model changes. You can trigger cache refreshing using:</p> <p>```bash</p> <p>pytest --update_llm_cache <code>`` - This sets the global</code>UPDATE_LLM_CACHE<code>flag (defined in your conftest or     test setup for now later it will be moved to</code>hllm.py<code>). - Internally, this sets</code>cache_mode=\"REFRESH_CACHE\"<code>when calling</code>get_completion()<code>.     - All API calls will be re-executed even if cached versions exist.     - The cache file (e.g.,</code>cache.get_completion.json<code>) is updated with new         responses.     - Metadata like</code>last_updated<code>,</code>hits<code>, and</code>misses` are also updated.</p> </li> <li> <p>Note that the cache might contain old prompts that are not needed anymore</p> <ul> <li>If you want to generate a cache with all and only what is needed, you can     delete the unit test cache and then run all the tests with     <code>--update_llm_cache</code></li> </ul> </li> <li> <p>Once the cache is refreshed, the cache should be reviewed and committed the     updated cache file to version control</p> </li> </ul>"},{"location":"tools/helpers/all.hllm.explanation.html#proposal_for_integrating_hcache_simplesimple_cache","title":"Proposal for integrating <code>hcache_simple.simple_cache()</code>","text":"<ul> <li> <p>Consolidating the bespoke <code>_CompletionCache</code> logic in <code>hllm.get_completion()</code>     by adopting the <code>simple_cache()</code> decorator from <code>helpers.hcache_simple</code>.</p> </li> <li> <p>Instead of wrapping the top\u2010level <code>get_completion()</code>, apply caching directly     to the internal <code>_call_api_sync</code> function where the actual OpenAI/OpenRouter     API call is made:</p> <p>```python from helpers import hcache_simple as hcacsimp</p> </li> </ul> <p>@hcacsimp.simple_cache(         write_through=True,         exclude_keys=[\"client\", \"cache_mode\"], ) def _call_api_sync_cached(         cache_mode: str,         client: openai.OpenAI,         messages: List[Dict[str, str]],         model: str,         temperature: float,         create_kwargs, ) -&gt; Dict[str, Any]:         hdbg.dassert_in(cache_mode, (\"REFRESH_CACHE\", \"HIT_CACHE_OR_ABORT\", \"NORMAL\"))         return _call_api_sync(                 client=client,                 messages=messages,                 model=model,                 temperature=temperature,                 create_kwargs,         )     ```</p> <ul> <li> <p>In <code>get_completion()</code>, map the existing <code>cache_mode</code> values to the     <code>simple_cache()</code>'s parameters:</p> <ul> <li><code>REFRESH_CACHE</code> \u2192 <code>force_refresh=True</code></li> <li><code>HIT_CACHE_OR_ABORT</code> \u2192 <code>abort_on_cache_miss=True</code></li> </ul> </li> <li> <p>Benefits</p> <ul> <li>Leverages <code>hcache_simple</code>'s tested in\u2010memory and disk caching.</li> <li>Eliminates custom JSON serialization and metadata management.</li> <li>Simplifies <code>hllm.py</code> by removing the <code>_CompletionCache</code> class.</li> </ul> </li> <li> <p>Trade-offs</p> <ul> <li>The legacy prompt normalization (lowercasing and whitespace trimming) must     be handled upstream if still required.</li> <li>Cache metadata format and statistics will follow <code>hcache_simple</code> conventions     rather than the custom JSON schema.</li> </ul> </li> </ul>"},{"location":"tools/helpers/all.hplayback.how_to_guide.html","title":"All.hplayback.how to guide","text":""},{"location":"tools/helpers/all.hplayback.how_to_guide.html#playback","title":"Playback","text":"<ul> <li> <p><code>Playback</code> is a way to automatically generate a unit test for a given function     by capturing the inputs applied to the function by the external world</p> </li> <li> <p>The working principle is:</p> <ol> <li>Instrument the target function <code>f()</code> to test with a <code>Playback</code> object or  with a decorator <code>@playback</code></li> <li>Run the function <code>f()</code> using the external code to drive its inputs</li> <li>E.g., while the function is executed as part of a more complex system, or    in a notebook</li> <li>The playback framework:</li> <li>Captures the inputs and the output of the function <code>f()</code></li> <li>Generates Python code to apply the stimuli to <code>f()</code> and to check its    output against the expected output</li> <li>Modify the code automatically generated by <code>Playback</code> to create handcrafted  unit tests</li> </ol> </li> </ul>"},{"location":"tools/helpers/all.hplayback.how_to_guide.html#code_and_tests","title":"Code and tests","text":"<ul> <li>The code for <code>Playback</code> is located at <code>helpers/hplayback.py</code></li> <li>Unit tests for <code>Playback</code> with useful usage examples are located at     <code>helpers/test/test_playback.py</code></li> </ul>"},{"location":"tools/helpers/all.hplayback.how_to_guide.html#using_playback","title":"Using playback","text":""},{"location":"tools/helpers/all.hplayback.how_to_guide.html#quick_start","title":"Quick start","text":"<ul> <li>Given a function to test like:</li> </ul> <pre><code>def function_under_test(...) -&gt; ...:\n        ...\n        &lt;code&gt;\n        ...\n        res = ...\n        return res\n</code></pre> <pre><code>def function_under_test(...) -&gt; ...:\n        import helpers.hplayback as hplayb\n        playback = hplayb.Playback(\"assert_equal\")\n\n        ...\n        &lt;code&gt;\n        ...\n\n        res = ...\n        code = playback.run(res)\n        print(code)\n        return res\n</code></pre>"},{"location":"tools/helpers/all.hplayback.how_to_guide.html#example_1_testing_get_sum","title":"Example 1: testing <code>get_sum()</code>","text":"<ul> <li> <p>Assume that we want unit test a function <code>get_sum()</code></p> <p><code>python def get_sum(a: List[int], b: List[int]) -&gt; Any:     c = a + b     return c</code></p> </li> <li> <p>Assume that typically <code>get_sum()</code> gets its inputs from a complex pipeline</p> <p><code>python def complex_data_pipeline() -&gt; Tuple[List[int], List[int]]:    # Incredibly complex pipeline generating:    a = [1, 2, 3]    b = [4, 5, 6]    return a, b</code></p> </li> <li> <p>The function is called with:</p> <p><code>python a, b = complex_data_pipeline() c = get_sum(a, b)</code></p> </li> <li> <p>We don't want to compute by hand the inputs <code>a, b</code>, but we can reuse     <code>complex_data_pipeline</code> to create a realistic workload for the function under     test</p> </li> <li> <p>Instrument the code with <code>Playback</code>:</p> <p>```python import helpers.playback as hplayb</p> </li> </ul> <p>def get_sum(a: List[int], b: List[int]) -&gt; Any:         playback = hplayb.Playback(\"assert_equal\")         c = a + b         code = playback.run(res)         print(code)         return c     ```</p> <ul> <li> <p>Create the playback object</p> <p><code>python playback = hplayb.Playback(\"assert_equal\")</code></p> <p>which specifies: - The unit test mode: \"check_string\" or \"assert_equal\" - The function name that is being tested: in our case, \"get_sum\" - The function parameters that were created earlier</p> </li> <li> <p>Run it with:</p> <p><code>python a, b = complex_data_pipeline() c = get_sum(a, b)</code></p> </li> <li> <p>Run the playback passing the expected outcome as a parameter</p> <p><code>python code = playback.run(res)</code></p> </li> <li> <p>The output <code>code</code> will contain a string with the unit test for <code>get_sum()</code></p> <p>```python import helpers.unit_test as hut</p> </li> </ul> <p>class TestGetSum(hut.TestCase):         def test1(self) -&gt; None:                 # Initialize function parameters.                 a = [1, 2, 3]                 b = [4, 5, 6]                 # Get the actual function output.                 act = get_sum(a, b)                 # Create the expected function output.                 exp = [1, 2, 3, 4, 5, 6]                 # Check whether the expected value equals the actual value.                 self.assertEqual(act, exp)     ```</p>"},{"location":"tools/helpers/all.hplayback.how_to_guide.html#example_2_testing_render_images_from_render_imagespy","title":"Example 2: testing <code>_render_images()</code> from <code>render_images.py</code>","text":"<ul> <li> <p>Copy real <code>im_architecture.md</code> to a test location</p> </li> <li> <p>Add playback into the code:</p> <p><code>python ... import helpers.playback as hplayb ... def _render_images(     in_lines: List[str], out_file: str, dst_ext: str, dry_run: bool ) -&gt; List[str]:     # Generate test.     playback = hplayb.Playback(\"check_string\")     print(prnt.frame(playback.run(None)))     ... ...</code></p> </li> <li> <p>Run <code>render_images.py -i im_architecture.md</code></p> </li> <li> <p>The following output is prompted:</p> <p>```python</p> </li> </ul> <p>class TestRenderImages(hut.TestCase):         def test1(self) -&gt; None:                 # Define input variables.                 in_lines = [\"\", ..., \"\", \"&gt; GP:: Not urgent\", \"\"]                 out_file = \"im_architecture.md\"                 dst_ext = \"png\"                 dry_run = False                 # Call function to test.                 act = _render_images(in_lines=in_lines, out_file=out_file, dst_ext=dst_ext, dry_run=dry_run)                 act = str(act)                 # Check output.                 self.check_string(act)     ```</p> <ul> <li><code>in_lines</code> value is too long to keep it in test - needed to be replaced with     previously generated file. Also some cosmetic changes are needed and code is     ready to paste to the existing test:     <code>python def test_render_images_playback1(self) -&gt; None:         \"\"\"Test real usage for im_architecture.md.test\"\"\"         # Define input variables.         file_name = \"im_architecture.md.test\"         in_file = os.path.join(self.get_input_dir(), file_name)         in_lines = io_.from_file(in_file).split(\"\\n\")         out_file = os.path.join(self.get_scratch_space(), file_name)         dst_ext = \"png\"         dry_run = True         # Call function to test.         act = rmd._render_images(                 in_lines=in_lines, out_file=out_file, dst_ext=dst_ext, dry_run=dry_run         )         act = \"\\n\".join(act)         # Check output.         self.check_string(act)</code></li> </ul>"},{"location":"tools/helpers/all.hplayback.how_to_guide.html#test_created_for_main_render_images","title":"Test created for main._render_images","text":"<p>import helpers.unit_test as hut import jsonpickle import pandas as pd</p>"},{"location":"tools/helpers/all.qgrid.how_to_guide.html","title":"All.qgrid.how to guide","text":""},{"location":"tools/helpers/all.qgrid.how_to_guide.html#qgrid","title":"Qgrid","text":""},{"location":"tools/helpers/all.qgrid.how_to_guide.html#some_documentation","title":"Some documentation","text":"<ul> <li>GitHub: https://github.com/quantopian/qgrid</li> <li>Official documentation: https://qgrid.readthedocs.io/en/latest</li> <li>Interesting demo:     https://mybinder.org/v2/gh/quantopian/qgrid-notebooks/master?filepath=index.ipynb</li> </ul>"},{"location":"tools/helpers/all.qgrid.how_to_guide.html#testing","title":"Testing","text":"<ul> <li> <p>Pandas &gt;1.0 needs qgrid at least 1.3.0 which is installed in our</p> <p><code>python print(qgrid.__version__) 1.3.0</code></p> </li> <li> <p>Go to the gallery notebook as in:     http://localhost:10002/notebooks/amp/core/notebooks/gallery_examples.ipynb#Qgrid</p> </li> <li> <p>Make sure you can see the output of the cells</p> </li> <li> <p>If not you might need to apply the fix as per     https://github.com/quantopian/qgrid/issues/253</p> <ul> <li>Stop the notebook server</li> <li>Execute</li> </ul> <p>```bash</p> <p>jupyter nbextension enable --py --sys-prefix qgrid /Users/saggese/.conda/envs/develop/bin/jupytext jupyter nbextension enable --py --sys-prefix widgetsnbextensionEnabling notebook extension qgrid/extension...             - Validating: OK</p> <p>jupyter nbextension enable --py --sys-prefix widgetsnbextension /Users/saggese/.conda/envs/develop/bin/jupytext Enabling notebook extension jupyter-js-widgets/extension...             - Validating: OK ``` - Restart the notebook - Test again with the gallery notebook</p> </li> </ul>"},{"location":"tools/helpers/all.str_to_df.how_to_guide.html","title":"All.str to df.how to guide","text":""},{"location":"tools/helpers/all.str_to_df.how_to_guide.html#converting_a_string_into_a_pandas_df","title":"Converting a string into a Pandas df","text":""},{"location":"tools/helpers/all.str_to_df.how_to_guide.html#rationale","title":"Rationale","text":"<p>Standard ways to construct a Pandas dataframe from data (e.g., from a dict or a list of lists) are extremely cumbersome, especially for longer dataframes with many rows. Our goal is to introduce a tool that allows to create a Pandas dataframe from its string representation, which is much more readable.</p> <p>Compare the data needed to create a df in a standard way:</p> <pre><code>data = {\n        1030828978: [0.20136, 0.20158, 0.20162, 0.20128, 0.20134, 0.20110],\n        1464553467: [\n                1838.812,\n                1839.690,\n                1839.936,\n                1839.430,\n                1840.016,\n                1839.615,\n        ]\n}\ntimestamps = pd.date_range(\n        start=\"2023-08-15 11:45:00\",\n        end=\"2023-08-15 12:10:00\",\n        tz=\"America/New_York\",\n        freq=\"5T\",\n)\n</code></pre> <p>To the data needed to create a df with our tool:</p> <pre><code>df_as_str = \"\"\"\n                               1030828978  1464553467\nend_timestamp\n\"2023-08-15 11:45:00-04:00\"     0.20136    1838.812\n\"2023-08-15 11:50:00-04:00\"     0.20158    1839.690\n\"2023-08-15 11:55:00-04:00\"     0.20162    1839.936\n\"2023-08-15 12:00:00-04:00\"     0.20128    1839.430\n\"2023-08-15 12:05:00-04:00\"     0.20134    1840.016\n\"2023-08-15 12:10:00-04:00\"     0.20110    1839.615\"\"\"\n</code></pre>"},{"location":"tools/helpers/all.str_to_df.how_to_guide.html#how_to_use","title":"How to use","text":"<ul> <li>The function is <code>str_to_df()</code>, located in <code>helpers/hpandas.py</code></li> <li>To use the function, one needs to provide a string representation of the df     and the mappings between the columns and the desired types of column names and     values</li> <li>For example, given the string representation of a df in the previous section     and the following mappings:</li> </ul> <pre><code>col_to_type = {\n        \"__index__\": pd.Timestamp,\n        \"1030828978\": float,\n        \"1464553467\": float,\n}\ncol_to_name_type = {\n        \"1030828978\": int,\n        \"1464553467\": int,\n}\n</code></pre> <p><code>str_to_df()</code> will output the following dataframe:</p> <p></p> <ul> <li>More usage examples can be found, e.g.:<ul> <li>In the dedicated unit tests in     <code>helpers/test/test_hpandas.py::Test_str_to_df</code></li> <li>In practical use cases in <code>oms/order/test/test_order_converter.py</code></li> </ul> </li> </ul>"},{"location":"tools/helpers/all.str_to_df.how_to_guide.html#notes","title":"Notes","text":"<ul> <li>The string representation used as an input to the function should be largely     the same as the output of <code>hpandas.df_to_str()</code>. Note that it is not the same     as the output of <code>str()</code>, which is much less controlled and therefore messier     than of <code>df_to_str()</code> (e.g., the column names are often split into different     rows far away from each other, etc)</li> <li>There are several differences between the raw output of <code>df_to_str()</code> and the     required format of the input to <code>str_to_df()</code>:<ul> <li>If the number of rows is over a certain number, then <code>df_to_str()</code> cuts     them, leaving only the head and the tail of the df, and everything in     between is replaced by the \"[...]\" placeholder. In <code>str_to_df()</code>, the     placeholder \"[...]\" is simply removed and the data it stands for cannot be     restored</li> <li>In the input to <code>str_to_df()</code>, values that contain spaces (e.g., a timestamp     like \"2000-01-01 09:00:00\") need to be enclosed in double quotation marks.     If there are values with spaces in the input to <code>df_to_str()</code>, then it will     convert them into strings but will not automatically put quotation marks     around them, which means that if we attempt to convert the output string     back into a df with <code>str_to_df()</code> without adding the quotation marks     manually, these values will be split into two different columns, possibly     also breaking the whole process due to the column number mismatch</li> </ul> </li> </ul>"},{"location":"tools/linter/all.amp_fix_md_links.explanation.html","title":"All.amp fix md links.explanation","text":""},{"location":"tools/linter/all.amp_fix_md_links.explanation.html#fixing_link_formatting_in_markdown_files","title":"Fixing link formatting in Markdown files","text":""},{"location":"tools/linter/all.amp_fix_md_links.explanation.html#problem","title":"Problem","text":"<ul> <li>Our Markdown documentation typically contains many links to various files;     however, the formatting of the links is not always uniform<ul> <li>E.g., a link to a figure can follow the HTML standard:     <code>&lt;img src=\"PATH_TO_FIG\"&gt;</code></li> <li>Or it can follow the Markdown standard: <code>![](PATH_TO_FIG)</code></li> </ul> </li> <li>Links may also become obsolete, for example, if a file gets renamed, moved     elsewhere or deleted but the Markdown is not updated</li> <li>We want the links to have consistent formatting and not to point to     non-existent files</li> </ul>"},{"location":"tools/linter/all.amp_fix_md_links.explanation.html#solution","title":"Solution","text":"<ul> <li>Add a Linter step <code>amp_fix_md_links</code> that<ul> <li>Regularizes the format of file links and figure pointers</li> <li>Issues a warning if a link is pointing to a file that does not exist</li> </ul> </li> </ul>"},{"location":"tools/linter/all.amp_fix_md_links.explanation.html#formatting_conventions","title":"Formatting conventions","text":"<ul> <li>We adopt the following conventions regarding the formatting of links in     Markdown:<ul> <li>The format of a link is     <code>[/DIR_NAME/FILE_NAME](/DIR_NAME/FILE_NAME)</code><ul> <li>E.g., <code>[/dir1/dir2/file.py](/dir1/dir2/file.py)</code></li> <li>The link text is the same as the link<ul> <li>Unless it is a regular text (not a file path), e.g.,     <code>[here](/dir1/dir2/file.py)</code></li> </ul> </li> <li>The link is an absolute path to the file (not a relative path and not a     URL)</li> </ul> </li> <li>A bare file path <code>/DIR_NAME/FILE_NAME</code> is converted into a     link, which should conform to the conventions described above<ul> <li>E.g., <code>/dir1/dir2/file{dot}py</code> is converted into     <code>[/dir1/dir2/file.py](/dir1/dir2/file.py)</code></li> </ul> </li> <li>The format of a figure pointer is     <code>&lt;img src=\"figs/DIR_NAME/FILE_NAME\"&gt;</code><ul> <li>E.g., <code>&lt;img src=\"figs/all.docs.md/file.png\"&gt;</code></li> <li><code>DIR_NAME</code> is the same as the name of the Markdown file where the figure     pointer is located</li> <li>An additional warning is issued if the actual path to the figure does not     conform to this format</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/linter/all.developing_linter.how_to_guide.html","title":"All.developing linter.how to guide","text":""},{"location":"tools/linter/all.developing_linter.how_to_guide.html#linter","title":"Linter","text":"<ul> <li>Linter enforces coding rules we aim to follow</li> <li>It is a set of Python scripts that check and possibly modify the code to     improve its readability and style, detect bugs and other technical issues</li> </ul>"},{"location":"tools/linter/all.developing_linter.how_to_guide.html#structure_of_a_linter_script","title":"Structure of a Linter script","text":"<ul> <li>Each Linter script contains a <code>linters.action.Action</code> class<ul> <li>It has the following implemented interface methods:<ul> <li><code>_execute(self, file_name: str, pedantic: int) -&gt; List[str])</code> --- returns     a list of lines where the script detected a failure, along with the     failure description</li> <li><code>check_if_possible(self) -&gt; bool</code> --- returns True if it is possible to     execute an action in the current environment</li> </ul> </li> <li>It accepts a list of files as an argument for the <code>run()</code> method</li> <li>Executing a script is running the <code>_execute()</code> method on all the files from     the list of files passed to <code>run()</code></li> </ul> </li> <li>Normally, a Linter script checks whether the input files conform to certain     conventions and (optionally) modifies them if they do not</li> <li>For an example of how a Linter script should look like, refer to any of the     existing ones in <code>linters/</code></li> </ul>"},{"location":"tools/linter/all.developing_linter.how_to_guide.html#how_to_introduce_a_new_linter_step","title":"How to introduce a new Linter step","text":"<ul> <li> <p>Create a Python script with the new functionality in <code>linters/</code></p> <ul> <li>Regarding the style and structure of the code, follow the example of the     existing Linter scripts</li> </ul> </li> <li> <p>Add the new Action to the list of actions in     <code>/linters/base.py</code></p> <ul> <li>Add to <code>_MODIFYING_ACTIONS</code> if it is supposed to modify the files it runs on     (e.g., fix mistakes, remove or add code)</li> <li>Add to <code>_NON_MODIFYING_ACTIONS</code> if it only checks the files and reports the     results (e.g., warns about incorrect formatting)</li> <li>Specify the name of the action (usually the name of the script without the     \"amp_\" prefix and without the extension), a short description of what it     does and a reference to the action class</li> </ul> </li> <li>Unit test the new code in <code>linters/test/</code><ul> <li>Add a specific test file for the new code (follow the example of the     existing test files)</li> <li>(Optional) Add some lines that you expect to trigger and not to trigger the     new Linter step to the string in     <code>Test_linter_py1._get_horrible_python_code1()</code> in     <code>/linters/test/test_amp_dev_scripts.py</code>,     and verify that your expectations are correct by updating the golden     outcomes of the tests in this test file</li> </ul> </li> <li>Add the new Linter script with a short description to the list below (in     alphabetical order)</li> </ul>"},{"location":"tools/linter/all.developing_linter.how_to_guide.html#list_of_linter_scripts","title":"List of Linter scripts","text":""},{"location":"tools/linter/all.developing_linter.how_to_guide.html#bringing_the_files_in_accordance_with_our_rules_modifying","title":"Bringing the files in accordance with our rules (modifying)","text":"<ul> <li> <p><code>add_python_init_files.py</code></p> <ul> <li>Adds missing <code>__init__.py</code> files to directories</li> </ul> </li> <li> <p><code>amp_add_class_frames.py</code></p> <ul> <li>Adds frames with class names before the classes are initialized</li> </ul> </li> <li> <p><code>amp_add_toc_to_notebook.py</code></p> <ul> <li>Adds a table of contents to the first cell of an IPython notebook</li> </ul> </li> <li> <p><code>amp_autoflake.py</code></p> <ul> <li>A wrapper around <code>autoflake</code></li> <li>Removes unused imports and variables</li> </ul> </li> <li> <p><code>amp_black.py</code></p> <ul> <li>A wrapper around <code>black</code></li> </ul> </li> <li> <p><code>amp_check_md_toc_headers.py</code></p> <ul> <li>Checks that there is no content before TOC and ensures header levels are     following hierarchical order without skipping levels.</li> </ul> </li> <li> <p><code>amp_class_method_order.py</code></p> <ul> <li>Sorts methods in classes so that they are in the order of<ul> <li>Init</li> <li>Magic</li> <li>Public static</li> <li>Public regular</li> <li>Private static</li> <li>Private regular</li> </ul> </li> </ul> </li> <li> <p><code>amp_doc_formatter.py</code></p> <ul> <li>A wrapper around <code>docformatter</code> and     <code>pydocstyle</code></li> </ul> </li> <li> <p><code>amp_fix_comments.py</code></p> <ul> <li>Reflows, capitalizes and adds punctuation to comment lines</li> <li>NB! Currently disabled due to instability.</li> </ul> </li> <li> <p><code>amp_fix_md_links.py</code></p> <ul> <li>Regularizes the format of links to files (incl. figure pointers) in Markdown     and checks if the files referenced by the links exist</li> </ul> </li> <li> <p><code>amp_fix_whitespaces.py</code></p> <ul> <li>Standardizes the use of whitespace characters (spaces, tabs, newlines, etc.)</li> </ul> </li> <li> <p><code>amp_format_separating_line.py</code></p> <ul> <li>Normalizes separating lines in the code</li> </ul> </li> <li> <p><code>amp_isort.py</code></p> <ul> <li>A wrapper around <code>isort</code></li> </ul> </li> <li> <p><code>amp_lint_md.py</code></p> <ul> <li>A wrapper around <code>prettier</code>.</li> <li>Cleans up Markdown files and updates the table of contents</li> </ul> </li> <li> <p><code>amp_normalize_import.py</code></p> <ul> <li>Normalizes imports in the code and in the docstring according to our     conventions</li> </ul> </li> <li> <p><code>amp_processjupytext.py</code></p> <ul> <li>A wrapper around <code>jupytext</code></li> <li>Keeps paired <code>.ipynb</code> and <code>.py</code> files synchronized</li> </ul> </li> <li> <p><code>amp_remove_empty_lines_in_function.py</code></p> <ul> <li>Cleans up empty lines inside functions and methods</li> </ul> </li> </ul>"},{"location":"tools/linter/all.developing_linter.how_to_guide.html#checking_if_the_files_are_in_accordance_with_our_rules_non-modifying","title":"Checking if the files are in accordance with our rules (non-modifying)","text":"<ul> <li> <p><code>amp_check_file_size.py</code></p> <ul> <li>Checks that files do not exceed the maximum size</li> </ul> </li> <li> <p><code>amp_check_filename.py</code></p> <ul> <li>Checks that test files and notebooks are located in <code>test</code>/<code>notebooks</code> dirs     respectively</li> <li>Checks that the names of non-Master notebooks contain a reference to a task</li> </ul> </li> <li> <p><code>amp_check_import.py</code></p> <ul> <li>Checks that the <code>from ... import ...</code> pattern is used only for the <code>typing</code>     package</li> <li>Confirms that the short import length in <code>import ... as ...</code> is no longer     than 8 symbols</li> </ul> </li> <li> <p><code>amp_check_md_reference.py</code></p> <ul> <li>Checks if the markdown file is referenced in README.md</li> </ul> </li> <li> <p><code>amp_check_merge_conflict.py</code></p> <ul> <li>Checks the file for git merge conflict markers (e.g. \"&lt;&lt;&lt;&lt;&lt;&lt;&lt; \")</li> </ul> </li> <li> <p><code>amp_check_shebang.py</code></p> <ul> <li>Check that all and only executable Python files start with a shebang</li> <li>NB! Currently disabled, see DevToolsTask97.</li> </ul> </li> <li> <p><code>amp_flake8.py</code></p> <ul> <li>A wrapper around <code>flake8</code></li> </ul> </li> <li> <p><code>amp_mypy.py</code></p> <ul> <li>A wrapper around <code>mypy</code></li> </ul> </li> <li> <p><code>amp_pylint.py</code></p> <ul> <li>A wrapper around <code>pylint</code></li> </ul> </li> <li> <p><code>amp_warn_incorrectly_formatted_todo.py</code></p> <ul> <li>Checks that TODO comments follow the <code># TODO(assignee): task.</code> format</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.add_toc_to_notebook.how_to_guide.html","title":"All.add toc to notebook.how to guide","text":""},{"location":"tools/notebooks/all.add_toc_to_notebook.how_to_guide.html#adding_a_table_of_contents_to_a_notebook","title":"Adding a table of contents to a notebook","text":""},{"location":"tools/notebooks/all.add_toc_to_notebook.how_to_guide.html#problem","title":"Problem","text":"<ul> <li>The Jupyter Notebook's extension that adds a table of contents (TOC) to a     notebook only works on the Jupyter server. When the notebook is published as     HTML or rendered on GitHub, the TOC added by this extension is not displayed</li> <li>We want to be able to add a TOC to a notebook in a way that is also visible in     these environments</li> </ul>"},{"location":"tools/notebooks/all.add_toc_to_notebook.how_to_guide.html#solution","title":"Solution","text":"<ul> <li>Create a custom script that adds a TOC to a notebook by parsing its contents</li> <li>The script:     /dev_scripts/notebooks/add_toc_to_notebook.py</li> </ul>"},{"location":"tools/notebooks/all.add_toc_to_notebook.how_to_guide.html#how_it_works","title":"How it works","text":"<ul> <li>Parse the JSON representation of the notebook's contents</li> <li>Identify cells that contain headings<ul> <li>Headings are located in Markdown-type cells, start at the beginning of a     line and consist of one or more hash signs, followed by at least one     whitespace and then the heading text</li> <li>E.g., \"# First heading\"</li> </ul> </li> <li>Add HTML anchors to the cells where headings were found<ul> <li>E.g., <code>html &lt;a name=\"first-heading\"&gt;&lt;/a&gt;</code></li> </ul> </li> <li>Compose a TOC from the extracted headings, using anchors to create internal     links to the notebook cells</li> <li> <p>Add the TOC to the first cell of the notebook</p> <ul> <li>If there is already a TOC in that cell, replace it with the new one to     ensure the TOC is up-to-date</li> </ul> </li> <li> <p>The flow of the script resembles     this 3rd-party solution.     The principal difference is that we are not using the <code>nbformat</code> library and     instead parse the JSON representation of the notebook's code directly</p> </li> <li>The reason is that we prioritize transparency in parsing and prefer not to     rely on external libraries that may unexpectedly change their behavior in the     future</li> </ul>"},{"location":"tools/notebooks/all.add_toc_to_notebook.how_to_guide.html#how_to_use","title":"How to use","text":"<ul> <li>Run on one or more notebooks:</li> </ul> <pre><code>&gt; dev_scripts/notebooks/add_toc_to_notebook.py \\\n        --input_files dir1/file1.ipynb dir2/file2.ipynb\n</code></pre> <ul> <li>Run on a directory to process all the notebooks in it:</li> </ul> <pre><code>&gt; dev_scripts/notebooks/add_toc_to_notebook.py \\\n        --input_dir dir1/\n</code></pre>"},{"location":"tools/notebooks/all.add_toc_to_notebook.how_to_guide.html#limitations","title":"Limitations","text":"<ul> <li>If a notebook with the TOC added by the script is rendered on GitHub, the TOC     is displayed but its links are broken (they do not lead to the cells with the     corresponding headings)</li> <li>This is due to a known issue with how GitHub renders notebooks, see, for     example, discussions     here     and     here</li> <li>If the notebook is published as HTML, the TOC is displayed and its links work     as expected</li> </ul>"},{"location":"tools/notebooks/all.gsheet_into_pandas.how_to_guide.html","title":"All.gsheet into pandas.how to guide","text":""},{"location":"tools/notebooks/all.gsheet_into_pandas.how_to_guide.html#connecting_google_sheets_to_pandas","title":"Connecting Google Sheets to Pandas","text":"<ul> <li>There are two layers of the API<ul> <li>gspread<ul> <li>This allows to connect to Google Sheets API</li> </ul> </li> <li>gspread-pandas<ul> <li>This allows to interact with Google Sheets through Pandas DataFrames,     using <code>gspread</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.gsheet_into_pandas.how_to_guide.html#installation","title":"Installation","text":"<ul> <li> <p>To check that the library is installed in the dev container, you can run:</p> <ul> <li> <p>In a notebook</p> <p>```bash import gspread print(gspread.version) 5.11.3</p> <p>import gspread_pandas print(gspread_pandas.version) 3.2.3 <code>- In the dev container</code>bash docker&gt; python -c \"import gspread; print(gspread.version); import gspread_pandas; print(gspread_pandas.version)\" 5.10.0 ```</p> </li> </ul> </li> <li> <p>If the library is not installed in the Dev container, you can install it in the     notebook with:     <code>bash     notebook&gt; !pip install gspread-pandas</code></p> </li> <li> <p>Or in the Docker container with:</p> <p><code>bash docker&gt; sudo /bin/bash -c \"(source /venv/bin/activate; pip install gspread)\"</code></p> </li> </ul>"},{"location":"tools/notebooks/all.gsheet_into_pandas.how_to_guide.html#authentication","title":"Authentication","text":"<ul> <li>Check if you already have the authentication token     <code>bash     &gt; cat ~/.config/gspread_pandas/google_secret.json     {         \"type\": \"service_account\", # gitleaks:allow         \"project_id\": \"gspread-gp\",         \"private_key_id\": \"...         \"private_key\": \"-----BEGIN PRIVATE KEY-----\\n...         \"client_email\": \"gp-gspread@...\",         \"client_id\": \"101087234904396404157\",         \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",         \"token_uri\": \"https://oauth2.googleapis.com/token\",         \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",         \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/gp-gspread%40gspread-gp.iam.gserviceaccount.com\",         \"universe_domain\": \"googleapis.com\"     }</code></li> <li>Note that this needs to be visible from Docker as <code>.config</code> is automatically     mounted     <code>bash     docker&gt; cat ~/.config/gspread_pandas/google_secret.json</code></li> </ul>"},{"location":"tools/notebooks/all.gsheet_into_pandas.how_to_guide.html#official_reference","title":"Official reference","text":"<ul> <li> <p>Since <code>gspread-pandas</code> leverages <code>gspread</code>, you can follow the instructions     for gspread at https://docs.gspread.org/en/v6.0.0/oauth2.html</p> </li> <li> <p>More details are in</p> <ul> <li><code>gspread</code>: https://docs.gspread.org/en/latest/oauth2.html</li> <li><code>gspread-pandas</code>:     https://gspread-pandas.readthedocs.io/en/latest/configuration.html</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.gsheet_into_pandas.how_to_guide.html#authentication_in_short","title":"Authentication in short","text":"<ul> <li>There are two ways to authenticate:     1) OAuth Client ID     2) Service account key</li> <li> <p>It's best to access Google API using a \"Service Account\", which is used for a     bots</p> </li> <li> <p>Go to Google Developers Console and create a new project or select one you     already have</p> <ul> <li>E.g., name \"gp-gspread\" and ID \"gp-gspread-426713\"</li> </ul> </li> <li>Search for \"Google Drive API\" and click on \"Enable API\"</li> <li>Search for \"Google Sheets API\" and click on \"Enable API\"</li> <li>Go to Credentials<ul> <li>Create credentials -&gt; Service account key</li> </ul> </li> <li>Service account details<ul> <li>Service account name: gspread</li> <li>Service account ID: gspread</li> <li>Email address: gspread@gp-gspread-426713.iam.gserviceaccount.com</li> <li>Role: owner</li> </ul> </li> <li> <p>Click on <code>gspread</code></p> <ul> <li>Keys -&gt; Create new key -&gt; JSON</li> </ul> </li> <li> <p>A file is downloaded     <code>&gt; more ~/Downloads/gspread-gp-94afb83adb02.json     {         \"type\": \"service_account\", # gitleaks:allow         \"project_id\": \"gspread-gp\",         \"private_key_id\": \"94afb...5258ac\",         \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvg...FtmcXiHuZ46EMouxnQCEqrT5\\n-----END PRIVATE KEY-----\\n\",         \"client_email\": \"gp-gspread@...\",         \"client_id\": \"101087234904396404157\",         \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",         \"token_uri\": \"https://oauth2.googleapis.com/token\",         \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",         \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/gp-gspread%40gspread-gp.iam.gserviceaccount.com\",         \"universe_domain\": \"googleapis.com\"     }</code></p> </li> <li> <p>Move the key in the right place     <code>&gt; mv ~/Downloads/gspread-gp-94afb83adb02.json ~/.config/gspread_pandas/google_secret.json</code></p> </li> <li> <p>Check that the key is visible In the Docker container     <code>docker&gt; more ~/.config/gspread_pandas/google_secret.json     ...</code></p> </li> <li> <p>Go to your spreadsheet and share it with a <code>client_email</code> from the step above     If you don\u2019t do this, you\u2019ll get a <code>gspread.exceptions.SpreadsheetNotFound</code>     exception when trying to access this spreadsheet from your application or a     script</p> </li> </ul>"},{"location":"tools/notebooks/all.gsheet_into_pandas.how_to_guide.html#testing_gspread-pandas","title":"Testing gspread-pandas","text":"<ul> <li> <p>The notebook with the usage example is located at     <code>amp/core/notebooks/gsheet_into_pandas_example.ipynb</code>.</p> </li> <li> <p>Don't feel stupid if you need multiple iterations to get this stuff     working</p> <ul> <li>Clicking on GUI is always a recipe for low productivity</li> <li>Go command line and vim!</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html","title":"All.jupyter notebook.how to guide","text":""},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#jupyter_notebook","title":"Jupyter Notebook","text":""},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#jupyter_notebook_best_practices","title":"Jupyter notebook best practices","text":""},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#when_to_use_a_jupyter_notebook","title":"When to use a Jupyter notebook","text":"<ul> <li>A notebook can be used for various goals:<ul> <li>Tutorial / gallery<ul> <li>Show how some code works (e.g., functions in <code>signal_processing.py</code> or     <code>data_encyclopedia.ipynb</code>)</li> <li>The code should always work</li> <li>We might want to add unit tests for it</li> </ul> </li> <li>Prototyping / one-off<ul> <li>E.g.,<ul> <li>We prototype some code, before it becomes library code</li> <li>We did some one-off analysis</li> </ul> </li> </ul> </li> <li>Analysis<ul> <li>Aka \"master\" notebooks</li> <li>The notebook should always work so we need to treat it as part of the code     base</li> <li>We might want to add unit tests for it</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#general_structure_of_a_notebook","title":"General structure of a notebook","text":""},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#description","title":"Description","text":"<ul> <li> <p>At the top of the notebook add a description section explaining a notebook's     goal and what it does, e.g.,     ```     # Description</p> <p>This notebook was used for prototyping / debugging code that was moved in the file <code>abc.py</code> ```</p> </li> <li> <p>Convert section headers and cells with description text to a Markdown format     by selecting a cell and then at Jupyter interface do     <code>Cell -&gt; Cell Type -&gt; Markdown</code></p> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#imports","title":"Imports","text":"<ul> <li>Add a code section importing the needed libraries<ul> <li>Autoreload modules to keep Jupyter and local code updated in real-time</li> <li>Standard imports, e.g. <code>os</code></li> <li>Third-party imports, e.g. <code>pandas</code></li> <li>Local imports from our lib</li> </ul> </li> <li> <p>It's better to put all the imports in one cell and separate different import     types by 1 empty line, e.g.:     ```     # Imports</p> <p>%load_ext autoreload %autoreload 2</p> <p>import logging import os</p> <p>import matplotlib.pyplot as plt import pandas as pd</p> <p>import helpers.dbg as dbg import helpers.env as env import helpers.printing as prnt import core.explore as exp import core.signal_processing as sigp ... ```</p> </li> <li> <p>In this way executing one cell is enough to configure the notebook</p> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#configuration","title":"Configuration","text":"<ul> <li> <p>You can configure the notebooks with some utils, logging, and report info on     how the notebook was executed (e.g., Git commit, libs, etc.) by using the     following cell:     ```     # Configure logger.     hdbg.init_logger(verbosity=logging.INFO)     _LOG = logging.getLogger(name)</p> </li> <li> <p>The output of the cell looks like:     <code>INFO: &gt; cmd='/venv/lib/python3.8/site-packages/ipykernel_launcher.py -f /home/.local/share/jupyter/runtime/kernel-a48f60fd-0f96-48b4-82d8-879385f2be91.json'     WARNING: Running in Jupyter     DEBUG Effective logging level=10     DEBUG Shut up 1 modules: asyncio     DEBUG &gt; (cd . &amp;&amp; cd \"$(git rev-parse --show-toplevel)/..\" &amp;&amp; (git rev-parse --is-inside-work-tree | grep -q true)) 2&gt;&amp;1     DEBUG &gt; (git rev-parse --show-toplevel) 2&gt;&amp;1     ...     # Packages     python: 3.8.10     cvxopt: 1.3.0     cvxpy: 1.2.2     gluonnlp: ?     gluonts: 0.6.7     joblib: 1.2.0     mxnet: 1.9.1     numpy: 1.23.4     pandas: 1.5.1     pyarrow: 10.0.0     scipy: 1.9.3     seaborn: 0.12.1     sklearn: 1.1.3     statsmodels: 0.13.5</code></p> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#print_system_signature","title":"Print system signature.","text":"<p>_LOG.info(\"%s\", henv.get_system_signature()[0])</p>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#configure_the_notebook_style","title":"Configure the notebook style.","text":"<p>hprint.config_notebook() ```</p>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#make_the_notebook_flow_clear","title":"Make the notebook flow clear","text":"<ul> <li>Each notebook needs to follow a clear and logical flow, e.g:<ul> <li>Load data</li> <li>Compute stats</li> <li>Clean data</li> <li>Compute stats</li> <li>Do analysis</li> <li>Show results</li> </ul> </li> <li>The flow should be highlighted using headings in markdown:     <code># Level 1     ## Level 2     ### Level 3</code></li> <li>Use the extension for navigating the notebook (see our suggestions for Jupyter     plug-ins)</li> <li>Keep related code and analysis close together so:<ul> <li>Readers can understand the logical flow</li> <li>One could \"easily\" split the notebook in parts (e.g., when it becomes too     big)</li> <li>You can collapse the cells and don't scroll back and forth too much</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#general_best_practices","title":"General best practices","text":""},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#update_calls_only_for_mastergallery_notebooks","title":"Update calls only for Master/Gallery notebooks","text":""},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#convention","title":"Convention:","text":"<ul> <li>We do our best to update the calls in the Master/Gallery notebooks but we     don't guarantee that the fix is correct</li> <li>For other notebooks we either do the fix (e.g., changing a name of a function)     and tweak the call to enforce the old behavior, or even not do anything if     there are too many changes</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#rationale","title":"Rationale:","text":"<ul> <li>We have dozens of ad-hoc research notebooks</li> <li>When a piece of code is updated (e.g., <code>ImClient</code>) the change should be     propagated everywhere in the code base, including the notebooks</li> <li>This results in excessive amount of maintenance work which we want to avoid</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#keep_code_that_belongs_together_in_one_cell","title":"Keep code that belongs together in one cell","text":"<ul> <li>It's often useful to keep in a cell computation that needs to be always     executed together<ul> <li>E.g., compute something and then print results</li> </ul> </li> <li>In this way a single cell execution computes all data together</li> <li>Often computation starts in multiple cells, e.g., to inline debugging, and     once we are more confident that it works correctly we can merge it in a cell     (or even better in a function)</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#write_beautiful_code_even_in_notebooks","title":"Write beautiful code, even in notebooks","text":"<ul> <li>Follow the conventions and suggestions for     Python code style</li> <li>When prototyping with a notebook, the code can be of lower quality than code     in a final library, but still needs to be readable and robust</li> <li>In our opinion it's just better to always do write robust and readable code:     it doesn't buy much time to cut corners</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#show_how_data_is_transformed_as_you_go","title":"Show how data is transformed as you go","text":"<ul> <li>Print a few lines of data structures (e.g., <code>df.head(3)</code>) so one can see how     data is transformed through the cells</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#use_keyboard_shortcuts","title":"Use keyboard shortcuts","text":"<ul> <li>Learn the default keyboard shortcuts to edit efficiently</li> <li>You can use the vim plug-in (see below) and become 3x more ninja</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#strive_for_simplicity","title":"Strive for simplicity","text":"<ul> <li>Always make the notebook easy to be understood and run by somebody else</li> <li>Explain what happens</li> <li>Organize the code in a logical way</li> <li>Use decent variable names</li> <li>Comment the results, when possible / needed</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#dependencies_among_cells","title":"Dependencies among cells","text":"<ul> <li>Try to avoid dependencies between cells</li> <li>Even better avoid any dependency between cells, e.g.:<ul> <li>Put all the imports in one cell at the beginning, so with one cell execution     you can make sure that all the imports are done</li> <li>Compare this approach with the case where the imports are randomly sprinkled     in the notebook, then you need to go execute them one by one if you     re-initialize the notebook</li> </ul> </li> <li>For the same reason group functions in one cell that you can easily re-execute</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#re-execute_from_scratch","title":"Re-execute from scratch","text":"<ul> <li>Once in a while (e.g., once a day)</li> <li>Commit your changes</li> <li>Make sure you can re-execute everything from the top with     <code>Kernel -&gt; Restart &amp; Clean output</code> and then <code>Kernel -&gt; Run all</code></li> <li>Visually verify that the results didn't change, so that there is no weird     state or dependency in the code</li> <li>Before a commit (and definitively before a PR) do a clean run</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#add_comments_for_complex_cells","title":"Add comments for complex cells","text":"<ul> <li>When a cell is too long, explain in a comment what a cell does, e.g.,     <code>## Count stocks with all nans.     num_nans = np.isnan(rets).sum(axis=0)     num_nans /= rets.shape[0]     num_nans = num_nans.sort_values(ascending=False)     num_stocks_with_no_nans = (num_nans == 0.0).sum()     print(\"num_stocks_with_no_nans=%s\" % hprint.perc(num_stocks_with_no_nans, rets.shape[1]))</code></li> <li>Another approach is to factor out the code in functions with clear names and     simplify the flow</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#do_not_cut_paste_code","title":"Do not cut &amp; paste code","text":"<ul> <li>Cutting + paste + modify is NEVER a good idea</li> <li>It takes more time to clean up cut &amp; paste code than doing right in the first     place</li> <li>Just make a function out of the code and call it!</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#avoid_wall-of-code_cell","title":"Avoid \"wall-of-code\" cell","text":"<ul> <li>Obvious</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#avoid_data_biases","title":"Avoid data biases","text":"<ul> <li>Try to compute statistics on the entire data set so that results are     representative and not dependent on a particular slice of the data</li> <li>You can sample the data and check stability of the results</li> <li>If it takes too long to compute the statistics on the entire data set, report     the problem and we can think of how to speed it up</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#avoid_hardwired_constants","title":"Avoid hardwired constants","text":"<ul> <li>Don't use hardwired constants</li> <li>Try to parametrize the code</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#explain_where_data_is_coming_from","title":"Explain where data is coming from","text":"<ul> <li>If you are using data from a file (e.g., <code>/data/wd/RP_1yr_13_companies.pkl</code>),     explain in a comment how the file was generated<ul> <li>Ideally report a command line to regenerate the data</li> </ul> </li> <li>The goal is for other people to be able to re-run the notebook from scratch</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#fix_warnings","title":"Fix warnings","text":"<ul> <li>A notebook should run without warnings</li> <li>Warnings can't be ignored since they indicate that the code is relying on a     feature that will change in the future, e.g.,     <code>FutureWarning: Sorting because non-concatenation axis is     not aligned. A future version of pandas will change to not sort by     default.     To accept the future behavior, pass 'sort=False'.     To retain the current behavior and silence the warning, pass 'sort=True'.</code></li> <li>Another example: after a cell execution the following warning appears:     <code>A value is trying to be set on a copy of a slice from a DataFrame     See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy</code><ul> <li>This is a typical pandas warning telling us that we created a view on a     dataframe (e.g., by slicing) and we are modifying the underlying data     through the view</li> <li>This is dangerous since it can create unexpected side effects and coupling     between pieces of code that can be painful to debug and fix</li> </ul> </li> <li>If we don't fix the issue now, the next time we create a conda environment the     code might either break or (even worse) have a different behavior, i.e.,     silent failure</li> <li>It's better to fix the warning now that we can verify that the code does what     we want to do, instead of fixing it later when we don't remember anymore what     exactly we were doing</li> <li>If you have warnings in your code or notebook you can't be sure that the code     is doing exactly what you think it is doing<ul> <li>For what we know your code might be deleting your hard-disk, moving money     from your account to mine, starting World War 3, ...</li> <li>You don't ever want to program by coincidence</li> </ul> </li> <li>Typically the warnings are informative and tell us what's the issue and how to     fix it, so please fix your code<ul> <li>If it's not obvious how to interpret or fix a warning, file a bug reporting     clearly a repro case and the error message</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#make_cells_idempotent","title":"Make cells idempotent","text":"<ul> <li>Try to make a notebook cell able of being executed multiple times without     changing its output value, e.g.,<ul> <li>Bad <code>df[\"id\"] = df[\"id\"] + 1</code><ul> <li>This computation is not idempotent, since if you execute it multiple times     is going to increment the column <code>id</code> at every iteration</li> </ul> </li> <li>Good <code>df[\"incremented_df\"] = df[\"id\"] + 1</code><ul> <li>A better approach is to always create a new \"copy\"</li> </ul> </li> </ul> </li> <li>Another example:<ul> <li>Bad <code>tmp = normalize(tmp)</code></li> <li>Good <code>tmp_after_normalize = normalize(tmp)</code><ul> <li>In this way it's easy to add another stage in the pipeline without     changing everything</li> <li>Of course the names <code>tmp_1</code>, <code>tmp_2</code> are a horrible idea since they are     not self-explanatory and adding a new stage screws up the numbering</li> </ul> </li> </ul> </li> <li>For data frames and variables is a good idea to create copies of the data     along the way:     <code>df_without_1s = df[df[\"id\"] != 1].copy()</code></li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#always_look_at_the_discarded_data","title":"Always look at the discarded data","text":"<ul> <li>Filtering the data is a risky operation since once the data is dropped, nobody     is going to go back and double check what exactly happened</li> <li>Everything downstream (e.g., all the results, all the conclusions, all the     decisions based on those conclusions) rely on the filtering being correct</li> <li>Any time there is a <code>dropna</code> or a filtering / masking operation, e.g.:     <code>compu_data = compu_data.dropna(subset=[\"CIK\"])</code>     or     <code>selected_metrics = [...]     compu_data = compu_data[compu_data[\"item\"].apply(lambda x : x in selected_metrics)]     compu_data = compu_data[compu_data[\"datadate\"].apply(date_is_quarter_end)]</code></li> <li>Always count what percentage of the rows you dropped (e.g., do a back of the     envelope check that you are dropping what you would expect)     <code>import helpers.printing as hprint     ...     n_rows = compu_form_df.shape[0]     compu_form_df = compu_form_df.drop_duplicates()     n_rows_after = compu_form_df.shape[0]     _LOG.debug(\"After dropping duplicates kept: %s\", hprint.perc(n_rows_after, n_rows))</code></li> <li>Make absolutely sure you are not dropping important data<ul> <li>E.g., has the distribution of the data changed in the way you would expect?</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#use_a_progress_bar","title":"Use a progress bar","text":"<ul> <li>Always use progress bars (even in notebooks) so that user can see how long it     will take for a certain computation.</li> <li>It is also possible to let <code>tqdm</code> automatically choose between console or     notebook versions by using     <code>from tqdm.autonotebook import tqdm</code></li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#notebooks_and_libraries","title":"Notebooks and libraries","text":"<ul> <li>It's ok to use functions in notebooks when building the analysis to leverage     notebook interactivity</li> <li>Once the notebook is \"stable\", often it's better to move the code in a     library, i.e., a Python file.</li> <li>Make sure you add autoreload modules in <code>Imports</code> section     <code>%load_ext autoreload     %autoreload 2</code><ul> <li>Otherwise, if you change a function in the lib, the notebook will not pull     this change and use the old version of the function</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#pros","title":"Pros","text":"<ul> <li>The same notebook code can be used for different notebooks<ul> <li>E.g., the function to read the data from disk is an obvious example</li> </ul> </li> <li>More people can reuse the same code for different analyses</li> <li>If one changes the code in a library, Git can help tracking changes and     merging, while notebooks are difficult to diff / merge</li> <li>Cleaning up / commenting / untangling the code can help reason carefully about     the assumptions to find issues</li> <li>The notebook becomes more streamlined and easy to understand since now it's a     sequence of functions <code>do_this_and_that</code> and presenting the results</li> <li>One can speed up / parallelize analyses with multiprocessing<ul> <li>Notebooks are not great for this</li> <li>E.g., when one does the analyses on a small subset of the data and then     wants to run on the entire large dataset</li> </ul> </li> <li>The exploratory analysis can be moved towards modeling and then production</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#cons","title":"Cons","text":"<ul> <li>One has to scroll back and forth between the notebook and the libraries to     execute the cell with the functions and fix all the possible mistakes</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#recommendations_for_plots","title":"Recommendations for plots","text":""},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#use_the_proper_y-scale","title":"Use the proper y-scale","text":"<ul> <li>If one value can vary from -1.0 to 1.0, force the y-scale between those limits     so that the values are absolutes, unless this would squash the plot</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#make_each_plot_self-explanatory","title":"Make each plot self-explanatory","text":"<ul> <li>Make sure that each plot has a descriptive title, x and y label</li> <li>Explain the set-up of a plot / analysis<ul> <li>E.g., what is the universe of stocks used? What is the period of time?</li> <li>Add this information also to the plots</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#avoid_wall-of-text_tables","title":"Avoid wall-of-text tables","text":"<ul> <li>Try to use plots summarizing the results besides the raw results in a table</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#use_common_axes_to_allow_visual_comparisons","title":"Use common axes to allow visual comparisons","text":"<ul> <li>Try to use same axes for multiple graphs when possible to allow visual     comparison between graphs</li> <li>If that's not possible or convenient, make individual plots with different     scales and add a plot with multiple graphs inside on the same axis (e.g., with     y-log)</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#use_the_right_plot","title":"Use the right plot","text":"<ul> <li>Pick the right type of graph to make your point<ul> <li><code>pandas</code>, <code>seaborn</code>, <code>matplotlib</code> are your friends</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#useful_plugins","title":"Useful plugins","text":"<ul> <li>You can access the extensions menu:<ul> <li><code>Edit -&gt; nbextensions config</code></li> <li><code>http://localhost:XYZ/nbextensions/</code></li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#vim_bindings","title":"Vim bindings","text":"<ul> <li>VIM binding     will change your life</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#table_of_contents","title":"Table of contents","text":"<ul> <li>To see the entire logical flow of the notebook, when you use the headers     properly</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#executetime","title":"ExecuteTime","text":"<ul> <li>To see how long each cell takes to execute</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#spellchecker","title":"Spellchecker","text":"<ul> <li>To improve your English!</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#autosavetime","title":"AutoSaveTime","text":"<ul> <li>To save the code automatically every minute</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#notify","title":"Notify","text":"<ul> <li>Show a browser notification when kernel becomes idle</li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#jupytext","title":"Jupytext","text":"<ul> <li>We use Jupytext as standard part of our development flow</li> <li>See     <code>all.jupytext.how_to_guide.md</code></li> </ul>"},{"location":"tools/notebooks/all.jupyter_notebook.how_to_guide.html#gspread","title":"Gspread","text":"<ul> <li>Allow to read g-sheets in Jupyter Notebook</li> <li>First, one needs to configure Google API, just follow the instructions from     here</li> <li>Useful links:<ul> <li>Examples of gspread Usage</li> <li>Enabling Gsheet API</li> <li>Adding service email if it's not working</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html","title":"All.jupytext.how to guide","text":""},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#jupytext","title":"Jupytext","text":""},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#why_jupytext","title":"Why Jupytext?","text":"<ul> <li> <p>In few words Jupytext associates a     Python representation to a notebook, which is kept in sync with the notebook     in a sensible way</p> </li> <li> <p>Jupytext allows to:</p> <ul> <li>Edit notebooks with your favorite editor (hopefully) vi or PyCharm</li> <li>Use a Python version of your notebook to run long computations from shell     instead of using a notebook</li> <li>Do a code review, diff changes, resolve conflicts using the Python code</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#reference_documentation","title":"Reference documentation","text":"<ul> <li>https://github.com/mwouts/jupytext</li> <li>https://jupytext.readthedocs.io/en/latest/</li> <li>https://jupytext.readthedocs.io/en/latest/faq.html</li> </ul>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#installation","title":"Installation","text":"<ul> <li> <p>Check what version you have:</p> <p>```bash</p> <p>jupytext --version 1.2.1 ```</p> </li> <li> <p>Check if you have a Jupyter config:</p> <p>```bash</p> <p>ls ~/.jupyter/jupyter_notebook_config.py ```</p> </li> <li> <p>If you don't have a config, generate it with:</p> <p>```bash</p> <p>jupyter notebook --generate-config ```</p> </li> <li> <p>Edit <code>~/.jupyter/jupyter_notebook_config.py</code> and append the following:</p> <p>```python</p> </li> <li> <p>Now you need to restart the notebook server to pick up Jupytext</p> </li> <li>We use the \"percent\" format where cells are delimited by a <code>%%</code> comment<ul> <li>Pycharm, black, and other tools understand / respect that this is a     delimiter for jupyter cells</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#------------------------------------------------------------------------------","title":"------------------------------------------------------------------------------","text":""},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#jupytext_1","title":"Jupytext","text":""},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#------------------------------------------------------------------------------_1","title":"------------------------------------------------------------------------------","text":"<p>c.NotebookApp.contents_manager_class = \"jupytext.TextFileContentsManager\"</p>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#always_pair_ipynb_notebooks_to_py_files","title":"Always pair ipynb notebooks to py files","text":"<p>c.ContentsManager.default_jupytext_formats = \"ipynb,py\"</p>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#use_the_percent_format_when_saving_as_py","title":"Use the percent format when saving as py","text":"<p>c.ContentsManager.preferred_jupytext_formats_save = \"py:percent\" c.ContentsManager.outdated_text_notebook_margin = float(\"inf\") ```</p>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#using_jupytext","title":"Using Jupytext","text":"<ul> <li>Now when you <code>git add</code> a <code>.ipynb</code> file you always need to add also the paired     <code>.py</code> file</li> <li>Same thing if you rename with <code>git mv</code> or delete a notebook<ul> <li>You need to explicitly take care of renaming and deleting also the <code>.py</code>     file</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#example_of_uses","title":"Example of uses","text":""},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#test_that_the_conversion_works_test-that-the-conversion-works","title":"Test that the conversion works {#test-that-the-conversion-works}","text":"<ul> <li>Jupytext keeps a notebook and the paired <code>.py</code> file in sync     <code>bash     &gt; jupytext Task22.ipynb --test --to py:percent --stop     &gt; jupytext Task22.ipynb --test-strict --to py:percent</code></li> </ul>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#manual_sync","title":"Manual sync","text":"<pre><code>    ```bash\n    &gt; jupytext --sync --to py:percent XYZ.ipynb\n    ```\n</code></pre>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#automatic_syncing_when_using_the_jupyter_server","title":"Automatic syncing when using the Jupyter server","text":"<ul> <li> <p>After you have installed the jupytext extension, open a notebook with your     Jupyter server</p> <ul> <li>You should see that there is a <code>.py</code> file close to the <code>.ipynb</code> file you     opened</li> <li>Open the <code>.py</code> file</li> <li>You can see that there is the code in the cells separated by <code>%%</code></li> </ul> </li> <li> <p>Changes to the notebook are reflected in the file:</p> <ul> <li>Modify the notebook and save the notebook</li> <li>Open the <code>.py</code></li> <li>Note that the cell that you modified in the notebook has changed in the     <code>.py</code> file</li> </ul> </li> <li> <p>Changes to the file are reflected in the notebook:</p> <ul> <li>Modify the <code>.py</code> file, e.g., changing one cell</li> <li>Go to the jupyter notebook and reload it</li> <li>The cell you modified has changed!</li> </ul> </li> </ul>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#convert_a_notebook_to_script","title":"Convert a notebook to script","text":"<pre><code>    ```bash\n    &gt; jupytext --to py:percent XYZ.ipynb\n    ```\n</code></pre>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#convert_a_script_into_a_notebook","title":"Convert a script into a notebook","text":"<pre><code>&gt; jupytext --to notebook XYZ.py\n</code></pre>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#remove_metadata_from_a_notebook","title":"Remove metadata from a notebook","text":"<ul> <li>This is equivalent to transforming the paired <code>.py</code> file in a notebook     <code>bash     &gt; jupytext --to notebook XYZ.py</code></li> </ul>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#linter","title":"Linter","text":"<ul> <li>Linter automatically reformats the <code>.py</code> files and then updates the <code>.ipynb</code>     without losing the formatting     <code>bash     &gt; i lint --files=\"XYZ.py\"     &gt; jupytext --sync --to py:percent XYZ.py</code></li> </ul>"},{"location":"tools/notebooks/all.jupytext.how_to_guide.html#refresh_all_the_scripts","title":"Refresh all the scripts","text":"<ul> <li> <p>The script <code>dev_scripts/notebooks/process_jupytext.py</code> automates some of the     workflow in Jupytext (see the help)</p> </li> <li> <p>The script <code>dev_scripts/notebooks/process_all_jupytext.sh</code> applies     <code>process_jupytext.py</code> to all the <code>ipynb</code> files     <code>bash     &gt; dev_scripts/notebooks/process_all_jupytext.sh &lt;ACTION&gt;</code></p> </li> </ul>"},{"location":"tools/notebooks/all.plotting.how_to_guide.html","title":"All.plotting.how to guide","text":""},{"location":"tools/notebooks/all.plotting.how_to_guide.html#best_practices_for_writing_plotting_functions","title":"Best practices for writing plotting functions","text":""},{"location":"tools/notebooks/all.plotting.how_to_guide.html#cosmetic_requirements","title":"Cosmetic requirements","text":"<p>A plot should be easy to interpret. This means:</p> <ul> <li>It should have a title</li> <li>Axes should have labels</li> <li>Ticks should be placed in an interpretable way; if there are few of them,     place their labels with zero rotation</li> <li>If there are several lines on the plot, it should have a legend</li> </ul>"},{"location":"tools/notebooks/all.plotting.how_to_guide.html#technical_requirements","title":"Technical requirements","text":"<ul> <li>Do not add <code>plt.show()</code> at the end of the function. It prevents the user from     using this plot as a subplot and from tweaking it.</li> <li>Expose the <code>ax</code> parameter</li> </ul>"},{"location":"tools/notebooks/all.plotting.how_to_guide.html#multiple_plots","title":"Multiple plots","text":"<ul> <li>If the function plots multiple plots, it is usually better to create a single     figure for them. This way, the output is more concise and can be copied as one     image.</li> <li>In <code>amp/core/plotting.py</code>, there is a helper called <code>get_multiple_plots()</code>     that is used for generating a figure and axes</li> <li>Add <code>plt.tight_layout()</code> in the end only if you are sure this figure will not     be wrapped inside another figure</li> <li>If there is a possibility the figure will be wrapped, try passing in a list of     axes</li> <li>To combine multiple figures with subplots into one, use     GridSpec</li> </ul>"},{"location":"tools/notebooks/all.publish_notebook.how_to_guide.html","title":"All.publish notebook.how to guide","text":""},{"location":"tools/notebooks/all.publish_notebook.how_to_guide.html#publishing_a_notebook","title":"Publishing a notebook","text":""},{"location":"tools/notebooks/all.publish_notebook.how_to_guide.html#what_it_is_about","title":"What it is about","text":"<ul> <li><code>publish_notebook.py</code> is a little tool that allows to:<ol> <li>Opening a notebook in your browser (useful for read-only mode)</li> <li>E.g., without having to use Jupyter notebook (which modifies the file in    your client) or github preview (which is slow or fails when the notebook    is too large)</li> <li>Sharing a notebook with others in a simple way</li> <li>Pointing to detailed documentation in your analysis Google docs</li> <li>Reviewing someone's notebook</li> <li>Comparing multiple notebooks against each other in different browser  windows</li> <li>Taking a snapshot / checkpoint of a notebook as a backup or before making  changes</li> <li>This is a lightweight alternative to \"unit testing\" to capture the    desired behavior of a notebook</li> <li>One can take a snapshot and visually compare multiple notebooks    side-by-side for changes</li> </ol> </li> </ul> <p>You can get details by running: <code>dev_scripts/notebooks/publish_notebook.py -h</code></p>"},{"location":"tools/notebooks/all.publish_notebook.how_to_guide.html#opening_a_notebook","title":"Opening a notebook","text":"<ul> <li>Inside the dev container     <code>docker&gt; FILE=datapull/common/universe/notebooks/Master_universe_analysis.ipynb; publish_notebook.py --file $FILE --action convert</code></li> <li>This converts the <code>ipynb</code> file into a <code>HTML</code></li> <li>Then you can open it from outside your container     <code>&gt; open Master_universe_analysis.20231214-081257.html</code>     or pointing the browser to it</li> </ul>"},{"location":"tools/notebooks/all.publish_notebook.how_to_guide.html#what_has_changed","title":"What has changed","text":"<p>We've deployed the new service for storing notebooks in HTML format</p> <ul> <li>From now on <code>publish_notebook.py</code> will work from the Docker container. The new     version of <code>publish_notebook.py</code> works using HTTP protocol and does not     require ssh key authorization as it was before</li> <li>We've synchronized all documents. So all old docs already available on the new     service</li> </ul>"},{"location":"tools/notebooks/all.run_jupyter_notebook.how_to_guide.html","title":"All.run jupyter notebook.how to guide","text":""},{"location":"tools/notebooks/all.run_jupyter_notebook.how_to_guide.html#run_jupyter_notebook","title":"Run Jupyter notebook","text":""},{"location":"tools/notebooks/all.run_jupyter_notebook.how_to_guide.html#general","title":"General","text":"<p>Every notebook should have a config that controls its parameters.</p> <p>A notebook can be run in two ways:</p> <ol> <li>Automatically: via a Python script</li> <li>Manually: via Jupyter from a web-browser</li> </ol>"},{"location":"tools/notebooks/all.run_jupyter_notebook.how_to_guide.html#running_a_notebook_using_a_python_script","title":"Running a notebook using a Python script","text":"<p>Use <code>run_notebook.py</code> to run the notebook via script.</p> <p>Here is an example of the run command inside the dev container, <code>cmamp</code> repository:</p> <pre><code>/app/dev_scripts/notebooks/run_notebook.py \\\n        --notebook /app/oms/notebooks/Master_broker_portfolio_reconciliation.ipynb \\\n        --config_builder 'amp.oms.execution_analysis_configs.get_broker_portfolio_reconciliation_configs_Cmtask5690(\"/shared_data/ecs/test/system_reconciliation/C11a/prod/20240320_134000.20240320_143500/system_log_dir.manual/process_forecasts\")' \\\n        --dst_dir . \\\n        --tee --no_suppress_output --num_threads 'serial' \\\n        --publish_notebook -v DEBUG\n</code></pre> <ul> <li>The script propagates a pointer to a config builder function via environment     variables <code>__CONFIG_BUILDER__</code>, <code>__CONFIG_IDX__</code>, <code>__CONFIG_DST_DIR__</code></li> <li>Given the environment variables above, a notebooks builds a config from a     config builder function</li> </ul> <p>The script also saves a config to a Pickle file. Path to a config file is stored in the <code>__NOTEBOOK_CONFIG_PATH__</code> environment variable which is displayed in a notebook so that a user knows where a config file is stored.</p>"},{"location":"tools/notebooks/all.run_jupyter_notebook.how_to_guide.html#run_a_notebook_manually","title":"Run a notebook manually","text":"<ol> <li> <p>Load <code>Config</code> from a file</p> </li> <li> <p>Use case: when re-running a notebook that was already run using the Python     script</p> </li> <li>How to get a path to a config file: in a published notebook, search for the     <code>__NOTEBOOK_CONFIG_PATH__</code> environment variable and copy its value</li> <li> <p>Where to use a path: pass the value as <code>config_file_name</code> in     <code>get_notebook_config()</code></p> <ul> <li>Note: Make sure to set <code>replace_ecs_tokyo = True</code> to replace the <code>ecs_tokyo</code>     with <code>ecs</code> in the actual pathh</li> </ul> </li> <li> <p>Specify a config builder function manually</p> </li> <li> <p>Use case: there is no config file saved</p> </li> <li>This way one needs to specify all of the parameters of a config builder     function manually</li> </ol>"},{"location":"tools/notebooks/all.run_jupyter_notebook.how_to_guide.html#example_of_the_run_flow","title":"Example of the run flow","text":"<pre><code>flowchart TD\nsubgraph A1[Run a notebook via the `run_notebook.py`]\n        A[Pass the string pointer to a config builder function\n        to the notebook as a parameter] --&gt; B[Save the Config object to a file]\n        B --&gt; C[Display path to a config file inside a notebook]\nend\nsubgraph B1[Run a notebook manually]\n        D{Config source}\n        E[Load the Config object from a file]\n        F[Manually define the Config object]\n        D --&gt; E &amp; F\nend\nA1 ==&gt; B1\nC ..-&gt;|copy-paste\\nfile path| E\n</code></pre>"},{"location":"tools/thin_environment/all.gh_and_thin_env_requirements.reference.html","title":"All.gh and thin env requirements.reference","text":""},{"location":"tools/thin_environment/all.gh_and_thin_env_requirements.reference.html#required_packages_for_the_thin_environment_and_gh_actions","title":"Required Packages for the thin environment and GH Actions","text":""},{"location":"tools/thin_environment/all.gh_and_thin_env_requirements.reference.html#thin_environment","title":"Thin environment","text":"<p>File location:</p> <ul> <li>requirements.txt</li> </ul>"},{"location":"tools/thin_environment/all.gh_and_thin_env_requirements.reference.html#packages","title":"Packages","text":"<ul> <li> <p><code>boto3</code></p> <ul> <li>Interacts with the AWS services:</li> <li><code>boto3</code> import in the <code>haws</code></li> <li><code>haws</code> usage in the <code>lib_tasks_docker_release.py</code></li> </ul> </li> <li> <p><code>invoke</code></p> <ul> <li>Need for running the invoke targets:</li> <li>_run_tests</li> </ul> </li> <li> <p><code>poetry</code></p> <ul> <li>Manage dependencies in the dev image:</li> <li>docker_build_local_image</li> </ul> </li> <li> <p><code>pytest</code></p> <ul> <li>To run <code>Docker image QA tests</code>:</li> <li>_run_qa_tests</li> </ul> </li> <li> <p><code>tqdm</code></p> <ul> <li>Widely used for showing the progress of the process for example:</li> <li>_fix_invalid_owner</li> </ul> </li> <li> <p><code>s3fs</code></p> <ul> <li>Needed for some invoke targets, for example:</li> <li>docker_update_prod_task_definition</li> </ul> </li> <li> <p><code>requests</code></p> <ul> <li>Dependency for the <code>docker</code>, for now pinned to the version <code>2.31.0</code> since     the versions &gt;=<code>2.32.1</code> is causing the issue with the <code>docker-compose</code>:     https://github.com/psf/requests/issues/6707</li> <li>See the https://github.com/cryptokaizen/cmamp/issues/8340 for details</li> </ul> </li> </ul>"},{"location":"tools/thin_environment/all.gh_and_thin_env_requirements.reference.html#candidate_packages_to_remove","title":"Candidate Packages to remove","text":"<ul> <li><code>docker</code> and <code>docker-compose</code> should be moved to OS installation     https://github.com/cryptokaizen/cmamp/issues/6498</li> </ul>"},{"location":"tools/thin_environment/all.gh_and_thin_env_requirements.reference.html#gh_actions","title":"GH Actions","text":"<p>File location:</p> <ul> <li>gh_requirements.txt</li> </ul>"},{"location":"tools/thin_environment/all.gh_and_thin_env_requirements.reference.html#packages_1","title":"Packages","text":"<ul> <li><code>invoke</code></li> <li><code>poetry</code></li> <li><code>pytest</code></li> <li><code>tqdm</code></li> <li><code>s3fs</code></li> <li><code>requests</code></li> </ul> <p>For above packages, see descriptions in the Thin environment/Packages section.</p>"},{"location":"tools/thin_environment/all.gh_and_thin_env_requirements.reference.html#candidate_packages_to_remove_1","title":"Candidate Packages to remove","text":"<ul> <li><code>docker</code> and <code>docker-compose</code> see in the     Thin environment section</li> </ul>"},{"location":"tools/thin_environment/all.gh_thin_env_dependencies.how_to_guide.html","title":"All.gh thin env dependencies.how to guide","text":""},{"location":"tools/thin_environment/all.gh_thin_env_dependencies.how_to_guide.html#thin_environment_dependencies","title":"Thin Environment Dependencies","text":""},{"location":"tools/thin_environment/all.gh_thin_env_dependencies.how_to_guide.html#description","title":"Description","text":"<ul> <li> <p>We have 3 sources of package requirements in the project:</p> <ol> <li>The thin environment to run <code>invoke</code> targets outside the container</li> <li>/dev_scripts/client_setup/requirements.txt</li> <li>This is managed with <code>pip</code></li> <li>GitHub requirements used for GitHub Actions specifically</li> <li>/.github/gh_requirements.txt</li> <li>This is managed with <code>pip</code></li> <li>Requirements necessary for the container:</li> <li>/devops/docker_build/pyproject.toml</li> <li>This is managed with <code>poetry</code></li> </ol> </li> <li> <p>We want to keep the thin environment as \"thin\" as possible (i.e., with fewer     dependencies)</p> </li> <li> <p>The thin environment and GitHub requirements have to be in sync</p> <ul> <li>The only difference is that the GitHub requirements have some limitations     due to the GitHub Actions environment</li> <li>TODO(Vlad): Still not clear what exact difference between the two     requirements files</li> </ul> </li> <li> <p>This document provides a step-by-step guide for adding or make any changes in     the requirements file of both the thin env and GitHub</p> </li> </ul>"},{"location":"tools/thin_environment/all.gh_thin_env_dependencies.how_to_guide.html#change_in_requirements_file","title":"Change in Requirements File","text":"<ul> <li>Some reasons for updating/changing the <code>requirements.txt</code> file are:<ul> <li>A new feature requires a new package outside the container, e.g., a new or     updated <code>invoke</code> target</li> <li>Upgrading the package version since the current one is outdated</li> <li>Removing a package since it is not used anymore</li> </ul> </li> </ul>"},{"location":"tools/thin_environment/all.gh_thin_env_dependencies.how_to_guide.html#confirm_with_build_team","title":"Confirm with Build Team","text":"<ul> <li>Changes in any of the requirement files should be confirmed with the Build     team before merging the PR<ul> <li>Is the new dependencies really needed?</li> <li>If the new dependencies is really needed, can we limit the scope of the     dependency? E.g.,<ul> <li>Move the related imports to where it is strictly needed in the code</li> <li>Do a try-catch <code>ImportError</code></li> </ul> </li> </ul> </li> </ul> <p>Example:</p> <ul> <li>The /helpers/lib_tasks_gh.py module has some     <code>invoke</code> targets that are executed only in the container</li> <li>If the new package is needed for the <code>invoke</code> target only in the container, we     should move the import to the function where it is strictly needed</li> <li>See the <code>gh_publish_buildmeister_dashboard_to_s3()</code> in the     /helpers/lib_tasks_gh.py     for reference.</li> </ul>"},{"location":"tools/thin_environment/all.gh_thin_env_dependencies.how_to_guide.html#update_requirements_file","title":"Update Requirements File","text":"<ul> <li>Update both the requirements file if relevant     /dev_scripts/client_setup/requirements.txt     and /.github/gh_requirements.txt<ul> <li>This file should be changed in every repository (e.g., <code>cmamp</code>,     <code>kaizenflow</code>, <code>orange</code>)</li> </ul> </li> <li>After adding the new requirements the build team will run all the tests     locally as well as on GitHub</li> </ul>"},{"location":"tools/thin_environment/all.gh_thin_env_dependencies.how_to_guide.html#update_documentation","title":"Update Documentation","text":"<ul> <li>Update the     /docs/tools/thin_environment/all.gh_and_thin_env_requirements.reference.md</li> </ul>"},{"location":"tools/thin_environment/all.gh_thin_env_dependencies.how_to_guide.html#notify_team","title":"Notify Team","text":"<p>In the <code>#team-eng</code> Slack channel, notify the team about the new package and ask them to rebuild the thin env.</p> <p>Example:</p> <pre><code>Hi! In the PR: https://github.com/cryptokaizen/cmamp/pull/6800 we removed\nunused packages from the thin environment.\n\nYou need to update the thin environment by running:\n\n&gt; cd ~/src/cmamp1\n&gt; dev_scripts/client_setup/build.sh\n</code></pre> <p>Last review: GP on 2024-05-07</p>"},{"location":"tools/thin_environment/all.thin_environment.reference.html","title":"All.thin environment.reference","text":""},{"location":"tools/thin_environment/all.thin_environment.reference.html#basic_concepts","title":"Basic concepts","text":"<p>Each repo relies on several concept:</p> <ul> <li>\"thin environment\": a Python environment that contains the minimal number of     packages to run commands and start the development system in Docker</li> <li>\"set env\" script: it is used to configured a shell inside the thin environment</li> <li>\"tmux\" script: it is used to create a <code>tmux</code> session with our standard setup     for the repo (e.g., multiple windows on different repo and sub-repos)</li> <li>\"Docker environment\": it contains all the libraries, packages, and     dependencies for the actual development (e.g., run the tools, run the unit     tests)</li> <li>\"devops\": it contains all the code to build containers for the specific repo</li> </ul>"},{"location":"tools/thin_environment/all.thin_environment.reference.html#naming_objects_composed_across_repos","title":"Naming objects composed across repos","text":"<ul> <li>We add a suffix representing the name of the repo any time we need to     distinguish different objects<ul> <li>E.g., <code>dev_scripts_helpers</code> vs <code>dev_scripts_cmamp</code></li> </ul> </li> <li>For runnable dirs that are not top level, the suffix includes the name of the     repo followed by the name of the runnable dir<ul> <li>E.g., <code>dev_scripts_cmamp_infra</code>, <code>dev_scripts_tutorial_tensorflow</code></li> </ul> </li> <li> <p>We can't always use the <code>.</code> to separate the fields since <code>.</code> creates problems     when importing Python files, and so we resort to using <code>_</code></p> </li> <li> <p>On the one hand, we agree that the directory structure should help distinguish     different objects</p> <ul> <li>E.g., <code>//helpers/.../dev_scripts_helpers</code> vs <code>//cmamp/.../dev_scripts_cmamp</code></li> </ul> </li> <li> <p>We keep files named in the same way underneath different directories</p> </li> <li> <p>Pros</p> <ul> <li>It's easy to diff files in corresponding dirs</li> </ul> </li> <li>Cons<ul> <li>The name is not enough to disambiguate files, but one needs to account for     the full path of each file across different repos / directories</li> </ul> </li> </ul>"},{"location":"tools/thin_environment/all.thin_environment.reference.html#files","title":"Files","text":"<ul> <li>The files involved are:     <code>bash     &gt; ls -l -1 dev_scripts_helpers/thin_client/     __init__.py     build.py     requirements.txt     setenv.sh     sync_repo_thin_client.sh     test_thin_client.sh     thin_client_utils.py     thin_client_utils.sh     tmux.py</code></li> </ul>"},{"location":"tools/thin_environment/all.thin_environment.reference.html#general_utils","title":"General utils","text":"<ul> <li><code>thin_client_utils.sh</code>: contain common Bash code for the scripts in this dir</li> <li><code>thin_client_utils.py</code>: contain common Python code for the scripts in this dir</li> </ul>"},{"location":"tools/thin_environment/all.thin_environment.reference.html#building_the_thin_environment","title":"Building the thin environment","text":"<ul> <li><code>build.py</code>: build the thin environment</li> <li><code>requirements.txt</code>: list of Python packages needed in the thin environment</li> </ul>"},{"location":"tools/thin_environment/all.thin_environment.reference.html#configuring_a_shell","title":"Configuring a shell","text":"<ul> <li><code>setenv.sh</code>: configure a shell to run the thin environment</li> </ul>"},{"location":"tools/thin_environment/all.thin_environment.reference.html#tmux_session","title":"Tmux session","text":"<ul> <li><code>tmux.py</code>: a script to create the standard <code>tmux</code> set up</li> </ul>"},{"location":"tools/thin_environment/all.thin_environment.reference.html#how_to_test","title":"How to test","text":"<ul> <li> <p>The script     <code>/dev_scripts_helpers/thin_client/test_helpers.sh</code>     tests <code>helpers</code></p> </li> <li> <p>Build thin environment     <code>bash     &gt; dev_scripts_helpers/thin_client/build.py</code></p> </li> <li>Source setenv     <code>bash     &gt; source dev_scripts_helpers/thin_client/setenv.sh</code></li> <li>Create the tmux session     <code>bash     &gt; dev_scripts_helpers/thin_client/tmux.py --create_global_link     &gt; dev_scripts_helpers/thin_client/tmux.py --index 1</code></li> </ul>"},{"location":"tools/thin_environment/all.thin_environment.reference.html#build_container","title":"Build container","text":"<ul> <li> <p>Build the container</p> <p>```bash</p> <p>i docker_build_local_image --version 1.0.0 i docker_tag_local_image_as_dev --version 1.0.0 ```</p> </li> <li> <p>Run the container     <code>bash     &gt; i docker_bash --skip-pull     &gt; i docker_jupyter</code></p> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html","title":"All.run unit tests.how to guide","text":""},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_unit_tests","title":"Run Unit Tests","text":""},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_unit_tests_1","title":"Run Unit Tests","text":"<ul> <li>We use <code>pytest</code> and <code>unittest</code> as testing framework</li> <li>Before any PR (and ideally after a few commits), we want to run all the unit     tests to make sure we didn't introduce any new bugs</li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#test_lists","title":"Test Lists","text":"<ul> <li>We have different test set lists:<ul> <li><code>fast</code><ul> <li>Tests that are quick to execute (typically less than 5 secs per test     class)</li> <li>We want to run these tests before/after every commit/PR to make sure     things are not broken</li> </ul> </li> <li><code>slow</code><ul> <li>Tests that we don't want to run all the times because they are:<ul> <li>Slow (typically less than 20 seconds per test)</li> <li>They are related to pieces of code that don't change often<ul> <li>E.g., external APIs we don't want to hit continuously</li> </ul> </li> </ul> </li> </ul> </li> <li><code>superslow</code><ul> <li>Tests that run long workload, e.g., running a production model, a long     simulation</li> <li>No time limit but we need to be judicious with length<ul> <li>Anything above 5-15 mins is problematic</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#using_invoke","title":"Using <code>Invoke</code>","text":"<ul> <li><code>invoke</code> is a task execution framework which     allows to execute some typical workflows in a simple way</li> <li>In our config script <code>invoke</code> is aliased to <code>i</code> to reduce typing</li> <li> <p>E.g., we use it to run the test suites:</p> <p>```bash</p> </li> <li> <p>To see the options use <code>--help</code> option, e.g. <code>i --help run_fast_tests</code>:</p> <p>```bash Usage: inv[oke] [--core-opts] run_fast_tests [--options] [other tasks here ...]</p> <p>Docstring:     Run fast tests.     :param stage: select a specific stage for the Docker image     :param pytest_opts: option for pytest     :param skip_submodules: ignore all the dir inside a submodule     :param coverage: enable coverage computation     :param collect_only: do not run tests but show what will be executed     :param tee_to_file: save output of pytest in <code>tmp.pytest.log</code>     :param kwargs: kwargs for <code>ctx.run</code></p> <p>Options:     -c, --coverage     -k, --skip-submodules     -o, --collect-only     -p STRING, --pytest-opts=STRING     -s STRING, --stage=STRING     -t, --tee-to-file     -v STRING, --version=STRING ```</p> </li> <li> <p>Docker image stage and version</p> <ul> <li> <p>To select a specific stage for Docker image use the <code>--stage</code> option. E.g.,     this might be useful when a user wants to run regressions on the local     Docker image to verify that nothing is broken before promoting it to <code>dev</code>     image.</p> <p>```bash</p> <p>i run_fast_tests --stage local <code>- To run the tests on the specific version of a Docker image, use the `--version` option. - E.g., this might be useful when releasing a new version of an image.</code>bash     &gt; i run_fast_tests --stage local --version 1.0.4     ```</p> </li> </ul> </li> <li> <p>Specifying <code>pytest</code> options</p> <ul> <li>With the option <code>--pytest-opts</code> it is possible to pass any <code>pytest</code> option     to <code>invoke</code>.</li> </ul> </li> <li> <p>Running in debug mode</p> <ul> <li>If a user wants to run the tests in debug mode to show the output     <code>bash     &gt; i run_fast_tests -s --dbg</code></li> <li>This is equivalent to specifying <code>-v DEBUG</code> through the command line of one     of the executables</li> </ul> </li> <li> <p>Save test output to a file</p> <ul> <li>To save the output of <code>pytest</code> to <code>tmp.pytest.log</code> use the <code>--tee-to-file</code>     option.     <code>bash     &gt; i run_fast_tests --tee-to-file</code></li> </ul> </li> <li> <p>Show the tests but do not run</p> <ul> <li>To list, but not run, the tests that will be executed, use <code>--collect-only</code>.     <code>bash     &gt; i run_fast_tests --collect-only</code></li> </ul> </li> <li> <p>Skip submodules</p> <ul> <li>To skip running tests in submodules, use the <code>--skip-submodules</code> option.</li> <li>This option is useful in repos with Git submodules so that you can run only     the tests specific to the repo, skipping the tests in the submodule<ul> <li>E.g., to run only the tests in <code>cmamp</code> but not in <code>helpers</code> (which is a     submodule)     <code>bash     &gt; cd cmamp1     &gt; i run_fast_tests --skip-submodules</code></li> </ul> </li> </ul> </li> <li> <p>Compute test coverage</p> <ul> <li> <p>Use the <code>--coverage</code> option</p> <p>```bash</p> <p>i run_fast_tests --coverage ```</p> </li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_only_fast_tests","title":"Run only fast tests.","text":"<p>i run_fast_tests</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_only_slow_tests","title":"Run only slow tests.","text":"<p>i run_slow_tests</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_only_superslow_tests","title":"Run only superslow tests.","text":"<p>i run_superslow_tests ```</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#timeout","title":"Timeout","text":"<ul> <li>We use the <code>pytest-timeout</code>     package to limit durations of fast, slow, and superslow tests</li> <li>The timeout restricts the running time of the test methods, including     <code>set_up_test()</code> and <code>tear_down_test()</code> time, if they are run at the     beginning/end of the methods</li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#rerunning_timed_out_tests","title":"Rerunning Timed Out Tests","text":"<ul> <li>Running tests can take different amounts of time depending on workload and     machine</li> <li>Because of this, we rerun failing tests using     <code>pytest-rerunfailures</code></li> <li><code>pytest-rerunfailures</code> is not completely compatible with <code>pytest-timeout</code>.     This is why we have to add the <code>-o timeout_func_only=true</code> flag to     <code>pytest-timeout</code>. See     https://github.com/pytest-dev/pytest-rerunfailures/issues/99     for more information</li> <li>We rerun time outed fast tests twice and time outed slow and superslow tests     once</li> <li>There is a     way     to provide a rerun delay for individual tests. However, we can't use it for     now due to     #693 (comment)</li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#compute_test_coverage","title":"Compute Test Coverage","text":"<ul> <li> <p>The documentation for the Python package <code>coverage</code> is     here.</p> </li> <li> <p>Run a set of unit tests enabling coverage:</p> <p>```bash</p> </li> <li> <p>This generates and runs a pytest command inside Docker like:</p> <p><code>bash docker&gt; pytest -m \"not slow and not superslow\" oms/test/test_broker.py::TestSimulatedBroker1 --cov=. --cov-branch --cov-report term-missing --cov-report html</code></p> <p>which generates: - A binary <code>.coverage</code> file that contains the coverage information - A default coverage report - An <code>htmlcov</code> directory with a browsable HTML coverage report</p> </li> <li> <p>You can post-process the coverage report in different ways using the command     <code>coverage</code> inside a docker container, since the code was run (as always)     inside the Docker container that contains all the dependencies</p> <p>```bash</p> <p>coverage -h</p> <p>Coverage.py, version 5.5 with C extension Measure, collect, and report on code coverage in Python programs.</p> <p>usage: <code>coverage &lt;command&gt;</code> [options] [args]</p> <p>Commands:         annotate    Annotate source files with execution information.         combine     Combine a number of data files.         debug       Display information about the internals of coverage.py         erase       Erase previously collected coverage data.         help        Get help on using coverage.py.         html        Create an HTML report.         json        Create a JSON report of coverage results.         report      Report coverage stats on modules.         run         Run a Python program and measure code execution.         xml         Create an XML report of coverage results.</p> <p>Use <code>coverage help &lt;command&gt;</code> for detailed help on any command. Full documentation is at https://coverage.readthedocs.io ```</p> <p>```bash</p> <p>coverage report -h</p> <p>Usage: coverage report [options] [modules]</p> <p>Report coverage statistics on modules.</p> <p>Options:     --contexts=REGEX1,REGEX2,...                                                 Only display data from lines covered in the given                                                 contexts. Accepts Python regexes, which must be                                                 quoted.     --fail-under=MIN      Exit with a status of 2 if the total coverage is less                                                 than MIN.     -i, --ignore-errors   Ignore errors while reading source files.     --include=PAT1,PAT2,...                                                 Include only files whose paths match one of these                                                 patterns. Accepts shell-style wildcards, which must be                                                 quoted.     --omit=PAT1,PAT2,...  Omit files whose paths match one of these patterns.                                                 Accepts shell-style wildcards, which must be quoted.     --precision=N         Number of digits after the decimal point to display                                                 for reported coverage percentages.     --sort=COLUMN         Sort the report by the named column: name, stmts,                                                 miss, branch, brpart, or cover. Default is name.     -m, --show-missing    Show line numbers of statements in each module that                                                 weren't executed.     --skip-covered        Skip files with 100% coverage.     --no-skip-covered     Disable --skip-covered.     --skip-empty          Skip files with no code.     --debug=OPTS          Debug options, separated by commas. [env:                                                 COVERAGE_DEBUG]     -h, --help            Get help on this command.     --rcfile=RCFILE       Specify configuration file. By default '.coveragerc',                                                 'setup.cfg', 'tox.ini', and 'pyproject.toml' are                                                 tried. [env: COVERAGE_RCFILE] ```</p> </li> <li> <p>Report the coverage for all the files under <code>oms</code> using the workload above     (i.e., the fast tests under <code>oms/test/test_broker.py::TestSimulatedBroker1</code>)</p> <p>```bash</p> <p>i docker_bash docker&gt; coverage report --include=\"oms/*\" Name                                    Stmts   Miss Branch BrPart  Cover</p> <p>oms/init.py                             0      0      0      0   100% oms/api.py                                154     47     36      2    70% oms/broker.py                             200     31     50      9    81% oms/broker_example.py                      23      0      4      1    96% oms/call_optimizer.py                      31      0      0      0   100% oms/devops/init.py                      0      0      0      0   100% oms/devops/docker_scripts/init.py       0      0      0      0   100% oms/locates.py                              7      7      2      0     0% oms/mr_market.py                           55      1     10      1    97% oms/oms_db.py                              47      0     10      3    95% oms/oms_lib_tasks.py                       64     39      2      0    38% oms/oms_utils.py                           34     34      6      0     0% oms/order.py                              101     30     22      0    64% oms/order_example.py                       26      0      0      0   100% oms/place_orders.py                       121      8     18      6    90% oms/pnl_simulator.py                      326     42     68      8    83% oms/portfolio.py                          309     21     22      0    92% oms/portfolio_example.py                   32      0      0      0   100% oms/tasks.py                                3      3      0      0     0% oms/test/oms_db_helper.py                  29     11      2      0    65% oms/test/test_api.py                      132     25     12      0    83% oms/test/test_broker.py                    33      5      4      0    86% oms/test/test_mocked_portfolio.py           0      0      0      0   100% oms/test/test_mr_market.py                 46      0      2      0   100% oms/test/test_oms_db.py                   114     75     14      0    38% oms/test/test_order.py                     24      0      4      0   100% oms/test/test_place_orders.py              77      0      4      0   100% oms/test/test_pnl_simulator.py            235      6     16      0    98% oms/test/test_portfolio.py                135      0      6      0   100%</p> <p>TOTAL                                    2358    385    314     30    82% ```</p> </li> <li> <p>To exclude the test files, which could inflate the coverage</p> <p>```bash</p> <p>coverage report --include=\"oms/\" --omit=\"/test_*.py\"</p> </li> <li> <p>To open the line coverage from outside Docker, go with your browser to     <code>/htmlcov/index.html</code></p> </li> <li> <p>The <code>htmlcov</code> is re-written with every coverage run with the     <code>--cov-report html</code> option</p> <ul> <li>If you move out <code>index.html</code> from <code>htmlcov</code> dir some html features (e.g.,     filtering) will not work.</li> </ul> <p>```bash</p> </li> <li> <p>By clicking on a file you can see which lines are not covered</p> <p></p> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_the_coverage_for_a_single_test","title":"Run the coverage for a single test:","text":"<p>i run_fast_tests --coverage -p oms/test/test_broker.py::TestSimulatedBroker1</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_coverage_for_an_entire_module_like_oms","title":"Run coverage for an entire module like <code>oms</code>:","text":"<p>i run_fast_tests --coverage -p oms ```</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#name_stmts_miss_branch_brpart_cover","title":"Name                                    Stmts   Miss Branch BrPart  Cover","text":"<p>oms/init.py                             0      0      0      0   100% oms/api.py                                154     47     36      2    70% oms/broker.py                             200     31     50      9    81% oms/broker_example.py                      23      0      4      1    96% oms/call_optimizer.py                      31      0      0      0   100% oms/devops/init.py                      0      0      0      0   100% oms/devops/docker_scripts/init.py       0      0      0      0   100% oms/locates.py                              7      7      2      0     0% oms/mr_market.py                           55      1     10      1    97% oms/oms_db.py                              47      0     10      3    95% oms/oms_lib_tasks.py                       64     39      2      0    38% oms/oms_utils.py                           34     34      6      0     0% oms/order.py                              101     30     22      0    64% oms/order_example.py                       26      0      0      0   100% oms/place_orders.py                       121      8     18      6    90% oms/pnl_simulator.py                      326     42     68      8    83% oms/portfolio.py                          309     21     22      0    92% oms/portfolio_example.py                   32      0      0      0   100% oms/tasks.py                                3      3      0      0     0% oms/test/oms_db_helper.py                  29     11      2      0    65%</p> <p>TOTAL                                    1562    274    252     30    80% ```</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#on_macos","title":"On macOS:","text":"<p>open htmlcov/index.html ```</p> <p></p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#an_example_coverage_session","title":"An Example Coverage Session","text":"<ul> <li> <p>You want to measure the unit test coverage of <code>oms</code> component from both <code>fast</code>     and <code>slow</code> test suites</p> </li> <li> <p>Start by running the fast tests for the <code>oms</code> dir</p> <p>```bash</p> <p>i run_fast_tests --coverage -p oms collected 66 items / 7 deselected / 59 selected ... ```</p> </li> <li> <p>Compute the coverage for the module sorting by coverage</p> <p>```bash docker&gt; coverage report --include=\"oms/\" --omit=\"/test_*.py\" --sort=cover</p> </li> <li> <p>You see that some files have low coverage, so you want to see what exactly is     not covered</p> </li> <li> <p>Generate the same report in a browsable format:</p> <p>```bash docker&gt; rm -rf htmlcov; coverage html --include=\"oms/\" --omit=\"/test_*.py\"</p> </li> <li> <p>Now, we run the coverage for the slow tests</p> <p>```bash</p> </li> <li> <p>We see that the coverage from the slow tests is only 23% for 7 tests</p> <p><code>bash docker&gt; coverage combine .coverage_fast_tests .coverage_slow_tests Combined data file .coverage_fast_tests Combined data file .coverage_slow_tests</code></p> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#name_stmts_miss_branch_brpart_cover_1","title":"Name                                    Stmts   Miss Branch BrPart  Cover","text":"<p>oms/locates.py                              7      7      2      0     0% oms/oms_utils.py                           34     34      6      0     0% oms/tasks.py                                3      3      0      0     0% oms/oms_lib_tasks.py                       64     39      2      0    38% oms/order.py                              101     30     22      0    64% oms/test/oms_db_helper.py                  29     11      2      0    65% oms/api.py                                154     47     36      2    70% oms/broker.py                             200     31     50      9    81% oms/pnl_simulator.py                      326     42     68      8    83% oms/place_orders.py                       121      8     18      6    90% oms/portfolio.py                          309     21     22      0    92% oms/oms_db.py                              47      0     10      3    95% oms/broker_example.py                      23      0      4      1    96% oms/mr_market.py                           55      1     10      1    97% oms/init.py                             0      0      0      0   100% oms/call_optimizer.py                      31      0      0      0   100% oms/devops/init.py                      0      0      0      0   100% oms/devops/docker_scripts/init.py       0      0      0      0   100% oms/order_example.py                       26      0      0      0   100% oms/portfolio_example.py                   32      0      0      0   100%</p> <p>TOTAL                                    1562    274    252     30    80% ```</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#wrote_html_report_to_htmlcovindexhtml","title":"Wrote HTML report to <code>htmlcov/index.html</code>","text":"<p>open htmlcov/index.html <code>`` - The low coverage for</code>tasks.py<code>and</code>oms_lib_tasks.py<code>is due to the fact     that we are running code through invoke that doesn't allow</code>coverage` to     track it</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#save_the_coverage_from_the_fast_tests_run","title":"Save the coverage from the fast tests run","text":"<p>cp .coverage .coverage_fast_tests</p> <p>i run_slow_tests --coverage -p oms collected 66 items / 59 deselected / 7 selected</p> <p>cp .coverage .coverage_slow_tests</p> <p>coverage report --include=\"oms/\" --omit=\"/test_*.py\" --sort=Cover</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#name_stmts_miss_branch_brpart_cover_2","title":"Name                                    Stmts   Miss Branch BrPart  Cover","text":"<p>oms/locates.py                              7      7      2      0     0% oms/oms_utils.py                           34     34      6      0     0% oms/tasks.py                                3      3      0      0     0% oms/pnl_simulator.py                      326    280     68      1    13% oms/place_orders.py                       121    100     18      0    15% oms/mr_market.py                           55     44     10      0    17% oms/portfolio.py                          309    256     22      0    18% oms/call_optimizer.py                      31     25      0      0    19% oms/broker.py                             200    159     50      0    20% oms/order.py                              101     78     22      0    20% oms/order_example.py                       26     19      0      0    27% oms/broker_example.py                      23     14      4      0    33% oms/portfolio_example.py                   32     21      0      0    34% oms/api.py                                154    107     36      0    36% oms/oms_lib_tasks.py                       64     39      2      0    38% oms/oms_db.py                              47      5     10      2    84% oms/init.py                             0      0      0      0   100% oms/devops/init.py                      0      0      0      0   100% oms/devops/docker_scripts/init.py       0      0      0      0   100% oms/test/oms_db_helper.py                  29      0      2      0   100%</p> <p>TOTAL                                    1562   1191    252      3    23% ```</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#an_example_with_customized_pytest-cov_html_run","title":"An Example with Customized <code>Pytest-Cov</code> Html Run","text":"<ul> <li> <p>You want to measure unit test coverage specifically for one test in     <code>datapull/common/data/transform/</code> and to save generated <code>htmlcov</code> in the same     directory.</p> </li> <li> <p>Run the command in the container:</p> <p>```bash</p> <p>i docket_bash docker&gt; pytest --cov-report term-missing     --cov=datapull/common/data/transform/ datapull/common/data/transform/test/test_transform_utils.py     --cov-report html:datapull/common/data/transform/htmlcov</p> <p>... ---------- coverage: platform linux, python 3.8.10-final-0 ----------- Name                                                              Stmts Miss Cover Missing</p> <p>datapull/common/data/transform/convert_csv_to_pq.py                  55    55   0%    2-159 datapull/common/data/transform/extract_data_from_db.py               55    55   0%    2-125 datapull/common/data/transform/pq_convert.py                         126   126  0%    3-248 datapull/common/data/transform/transform_pq_by_date_to_by_asset.py   131   131  0%    2-437 datapull/common/data/transform/transform_utils.py                    22    0    100%</p> <p>TOTAL                                                             389   367  6% Coverage HTML written to dir datapull/common/data/transform/htmlcov ```</p> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#generate_coverage_report_with_invoke","title":"Generate Coverage Report with <code>Invoke</code>","text":"<ul> <li> <p>You can compute test coverage for a specified directory and generate text and     HTML reports automatically using <code>invoke task run_coverage_report</code></p> <p>```bash</p> <p>i --help run_coverage_report INFO: &gt; cmd='/data/grisha/src/venv/amp.client_venv/bin/invoke --help run_coverage_report'</p> <p>Usage: inv[oke] [--core-opts] run_coverage_report [--options] [other tasks here ...]</p> <p>Docstring:</p> <pre><code>Compute test coverage stats.\n</code></pre> <p>:param target_dir: directory to compute coverage stats for running-unit-tests :param generate_html_report: whether to generate HTML coverage report or not :param publish_html_on_s3: whether to publish HTML coverage report or not :param aws_profile: the AWS profile to use for publishing HTML report</p> <p>Options:     -a STRING, --aws-profile=STRING     -g, --[no-]generate-html-report     -p, --[no-]publish-html-on-s3     -t STRING, --target-dir=STRING ```</p> </li> <li> <p>Compute coverage for <code>market_data</code> dir, generate text and HTML reports and     publish HTML report on S3</p> <p>```bash</p> <p>i run_coverage_report --target-dir market_data ... Name                                   Stmts   Miss Branch BrPart  Cover</p> <p>market_data/real_time_market_data.py     100     81     32      0    16% market_data/replayed_market_data.py      111     88     24      0    19% market_data/abstract_market_data.py      177    141     24      0    19% market_data/market_data_example.py       124     97     10      0    20% market_data/market_data_im_client.py      66     50     18      0    21% market_data/init.py                    5      0      0      0   100%</p> <p>TOTAL                                    583    457    108      0    19% Wrote HTML report to htmlcov/index.html</p> <p>20:08:53 - INFO  lib_tasks.py _publish_html_coverage_report_on_s3:3679  HTML coverage report is published on S3: path=<code>s3://cryptokaizen-html/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project</code> ```</p> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#publishing_html_report_on_s3","title":"Publishing HTML Report on S3","text":"<ul> <li>To make a dir with the report unique, you decorate the dir with a linux user     and a Git branch name, e.g.,     <code>html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project</code><ul> <li><code>html_coverage</code> is the common dir on S3 for coverage reports</li> </ul> </li> <li>After publishing the report, you can open the report via a local web browser<ul> <li>See the details in     htmlcov server</li> <li>E.g.     http://172.30.2.44/html_coverage/grisha_CmTask1038_Tool_to_extract_the_dependency_from_a_project/</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#running_pytest_directly","title":"Running <code>Pytest</code> Directly","text":""},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#basic_rules","title":"Basic Rules","text":"<ul> <li>Always run <code>pytest</code> from the Docker container to ensure consistency in our     environments     <code>bash     &gt; i docker_bash     # Then from Docker, run pytest.     docker &gt; pytest &lt;test_file_name&gt;</code></li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#usage_and_invocations_reference","title":"Usage and Invocations Reference","text":"<ul> <li>See <code>pytest</code> documentation</li> <li> <p>Some examples of useful command lines:</p> <p>```bash</p> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#stop_at_first_failure","title":"Stop at first failure","text":"<p>pytest -x</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_a_single_test_class","title":"Run a single test class","text":"<p>pytest -k TestPcaFactorComputer1</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_a_single_test_method","title":"Run a single test method","text":"<p>pytest core/test/test_core.py::TestPcaFactorComputer1::test_linearize_eigval_eigvec</p> <pre><code># Run all tests in a file\n</code></pre> <p>pytest core/test/test_core.py</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#remove_cache_artifacts","title":"Remove cache artifacts","text":"<p>find . -name \"pycache\" -o -name \".pytest_cache\" ./.pytest_cache ./dev_scripts/test/Test_linter_py1.test_linter1/tmp.scratch/pycache ./dev_scripts/test/pycache ./dev_scripts/pycache</p> <p>find . -name \"pycache\" -o -name \".pytest_cache\" | xargs rm -rf</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_with_a_clear_cache","title":"Run with a clear cache","text":"<p>pytest --cache-clear</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_the_tests_that_last_failed_this_data_is_stored_in_pytest_cachevcachelastfailed","title":"Run the tests that last failed (this data is stored in .pytest_cache/v/cache/lastfailed)","text":"<p>pytest --last-failed ```</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#custom_pytest_options_behaviors","title":"Custom <code>Pytest</code> Options Behaviors","text":"<ul> <li> <p>Enable logging</p> <ul> <li> <p>To enable logging of <code>_LOG.debug</code> for a single test run:</p> <p>```bash</p> </li> </ul> </li> <li> <p>Update golden outcomes</p> <ul> <li> <p>This switch allows to overwrite the golden outcomes that are used as     reference in the unit tests to detect failures</p> <p>```bash</p> <p>pytest --update_outcomes ```</p> </li> </ul> </li> <li> <p>Incremental test mode</p> <ul> <li>This switch allows to reuse artifacts in the test directory and to skip the     clean up phase</li> <li> <p>It is used to rerun tests from the middle when they are very long and one     wants to debug them</p> <p>```bash</p> <p>pytest --incremental ```</p> </li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#enable_debug_info","title":"Enable debug info","text":"<p>pytest oms/test/test_broker.py::TestSimulatedBroker1 -s --dbg ```</p>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#debugging_notebooks","title":"Debugging Notebooks","text":"<ol> <li>Run a failing test with <code>-s --dbg</code> to get detailed logs</li> <li>E.g., <code>&gt; pytest core/plotting/test/test_gallery_notebook.py -s --dbg</code></li> <li>From the logs take a <code>run_notebook.py</code> script command that was run by the    test</li> <li>E.g., starting like      <code>/app/dev_scripts/notebooks/run_notebook.py --notebook ...</code></li> <li>Append <code>--no_suppress_output</code> to this command and run it again directly from    the bash</li> <li>E.g., like      <code>&gt; /app/dev_scripts/notebooks/run_notebook.py --notebook ... --no_suppress_output</code></li> <li>Scroll up the logs and see a report about the problem, notebooks failures    will be displayed as well</li> <li>E.g.,      </li> </ol>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#running_tests_on_gh_actions","title":"Running Tests on GH Actions","text":"<ul> <li>The official documentation is     https://docs.github.com/en/actions</li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#how_to_run_a_single_test_on_gh_action","title":"How to Run a Single Test on GH Action","text":"<ul> <li> <p>Unfortunately, there is no way to log in and run interactively on GH machines.     This is a feature requested but not implemented by GH yet.</p> </li> <li> <p>All the code to run GH Actions is in the <code>.github</code> directory in <code>lemonade</code> and     <code>amp</code>.</p> </li> <li> <p>E.g., to run a single test in the fast test target, instead of the entire     regression suite</p> <ul> <li> <p>You can modify <code>.github/workflows/fast_tests.yml</code>, by replacing</p> <p>```bash</p> </li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.run_unit_tests.how_to_guide.html#run_invoke_run_fast_tests","title":"run: invoke run_fast_tests","text":"<p>run: invoke run_fast_tests --pytest-opts=\"helpers/test/test_git.py::Test_git_modified_files1::test_get_modified_files_in_branch1 -s --dbg\" ``` - Note that the indentation matters since it's a YAML file</p> <pre><code>&lt;img src=\"../../code_guidelines/figs/unit_tests/image_3.png\"&gt;\n</code></pre> <ul> <li>The <code>-s --dbg</code> is to show <code>_LOG.debug</code> in case you care about that to get     more information<ul> <li>Commit the code to your branch (not in master, please) since GH runs tests independently per branch</li> <li>Kick-off manually the fast test through the GH interface</li> <li>After debugging, you can revert the change from your branch to <code>master</code> and move along with the usual PR flow</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html","title":"All.write unit tests.how to guide","text":""},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#write_unit_tests","title":"Write Unit Tests","text":""},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#guidelines_about_writing_unit_tests","title":"Guidelines about writing unit tests","text":""},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#what_is_a_unit_test","title":"What is a unit test?","text":"<ul> <li>A unit test is a small, self-contained test of a (public) function or method     of a library</li> <li>The test specifies the given inputs, any necessary state, and the expected     output</li> <li>Running the test ensures that the actual output agrees with the expected     output</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#why_is_unit_testing_important","title":"Why is unit testing important?","text":"<ul> <li>Good unit testing improves software quality by:<ul> <li>Eliminating bugs (obvious)</li> <li>Clarifying code design and interfaces (\"Design to Test\")</li> <li>Making refactoring safer and easier (\"Refactor Early, Refactor Often\")</li> <li>Documenting expected behavior and usage</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#the_pragmatic_programming_and_unit_testing","title":"The Pragmatic Programming and unit testing","text":"<ul> <li> <p>Unit testing is an integral part of     The Pragmatic Programming     approach</p> </li> <li> <p>Some of the tips that relate to unit testing are:</p> <ul> <li>Design with Contracts</li> <li>Refactor Early, Refactor Often</li> <li>Test Your Software, or Your Users Will</li> <li>Coding Ain't Done Till All the Tests Run</li> <li>Test State Coverage, Not Code Coverage</li> <li>You Can't Write Perfect Software</li> <li>Crash Early</li> <li>Design to Test</li> <li>Test Early. Test Often. Test Automatically.</li> <li>Use Saboteurs to Test Your Testing</li> <li>Find Bugs Once</li> </ul> </li> <li> <p>Read these wisdom pearls carefully and you will have made another step towards     programming mastery</p> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#unit_testing_tips","title":"Unit testing tips","text":""},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#test_one_thing","title":"Test one thing","text":"<ul> <li>A good unit test tests only one thing<ul> <li>A test class should test only one function / class</li> <li>A test method should only test a single case (e.g., \"for these inputs the     function responds with this output\"</li> </ul> </li> <li>Testing one thing keeps the unit test simple, relatively easy to understand,     and helps isolate the root cause when the test fails</li> <li>How do you test more than one thing? By having more than one unit test!</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#keep_tests_self-contained","title":"Keep tests self-contained","text":"<ul> <li>A unit test should be independent of all other unit tests</li> <li>Each test should be self-sufficient</li> <li>One should never assume that unit tests will be executed in a particular order</li> <li>A corollary of keeping tests self-contained is to keep all information needed     to understand the test within the test itself<ul> <li>Specify the data explicitly in the test where it is used</li> <li>This makes the test easier to understand and easier to debug when it fails</li> <li>If multiple unit tests use or can use the same initialization data, do not     hesitate repeating it in each test (or consider using parameterized testing)</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#only_specify_data_related_to_what_is_being_tested","title":"Only specify data related to what is being tested","text":"<ul> <li>Specify the minimum of what is required to test what is being tested</li> <li>E.g., if a function that is being tested supports optional arguments, but     those optional arguments are not needed for a particular unit test, then do     not specify them in the test</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#test_realistic_corner_cases","title":"Test realistic corner cases","text":"<ul> <li>Can the function receive an empty list?</li> <li>Can it return an empty Series?</li> <li>What happens if it receives a numerical value outside of an expected range?<ul> <li>How should the function behave in those cases? Should it crash? Should it     return a reasonable default value?</li> </ul> </li> <li>Expect these questions to come up in practice and think through what the     appropriate behavior should be. Then, test for it.</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#test_a_typical_scenario","title":"Test a typical scenario","text":"<ul> <li>In ensuring that corner cases are covered, do not overlook testing basic     functionality for typical cases</li> <li>This is useful for verifying current behavior and to support refactoring.</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#test_executable_scripts_end-to-end","title":"Test executable scripts end-to-end","text":"<ul> <li>In some cases, like scripts, it is easy to get lost chasing the coverage %<ul> <li>E.g., covering every line of the original, including the parser</li> </ul> </li> <li>This is not always necessary<ul> <li>If you can run a script with all arguments present, it means that the parser     works correctly</li> <li>So an end-to-end smoke test will also cover the parser</li> <li>This saves a little time and reduces the bloat</li> </ul> </li> <li>If you need to test the functionality, consider factoring out as much code as     possible from <code>_main()</code><ul> <li>A good practice is to have a <code>_run()</code> function that does all the job and     <code>_main()</code> only brings together the parser and the executable part</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#test_from_the_outside-in","title":"Test from the outside-in","text":"<ul> <li>Prefer testing end-to-end behaviors first, rather than starting with internal     implementation details.</li> <li>This helps focus tests on what the system should do, not how it's implemented.</li> <li>It also lowers maintenance costs when internal code is refactored, since     interface-level tests don't need to change.</li> <li>Start with public-facing methods or behaviors, and only add tests for internal     helpers if truly necessary.</li> <li>This approach supports flexible and safe refactoring, encourages clear     interfaces, and reduces test fragility.</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#conventions","title":"Conventions","text":""},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#naming_and_placement_conventions","title":"Naming and placement conventions","text":"<ul> <li> <p>We follow conventions that happen to be mostly the default to <code>pytest</code></p> </li> <li> <p>A directory <code>test</code> contains all the test code and artifacts</p> <ul> <li>The <code>test</code> directory contains all the <code>test_*.py</code> files and all inputs and     outputs for the tests.</li> <li>A unit test file should be close to the library / code it tests</li> </ul> </li> <li> <p>The test class should make clear reference to the code that is tested</p> <ul> <li>To test a class <code>FooBar</code>, the corresponding test class is named     <code>TestFooBar</code>, i.e. we use the CamelCase for the test classes</li> <li>To test a function <code>generate_html_tables()</code>, the corresponding test class is     named <code>Test_generate_html_tables</code></li> <li>To test a method <code>method_a()</code> of the class <code>FooBar</code>, the corresponding test     class is named <code>TestFooBar</code> and the test method in this class is named     <code>test_method_a</code></li> <li>To test a protected method <code>_gozilla()</code> of <code>FooBar</code>, the corresponding test     method is named <code>test__gozilla</code> (note the double underscore). This is needed     to distinguish testing the public method <code>FooBar.gozilla()</code> from     <code>FooBar._gozilla()</code></li> </ul> </li> <li> <p>Numbers can be used to differentiate between separate test cases clearly</p> <ul> <li>A number can be added to the test class name, e.g., <code>TestFooBar1()</code>, if     there are multiple test classes that are testing the code in different ways     (e.g., with different set up and tear down actions)</li> <li>We prefer to name classes <code>TestFooBar1</code> and methods <code>TestFooBar1.test1()</code>,     even if there is a single class / method, to make it easier to add another     test class, without having to rename class and <code>check_string</code> files</li> <li>We are OK with using suffixes like <code>01</code>, <code>02</code>, ... , when we believe it's     important that methods are tested in a certain order (e.g., from the     simplest to the most complex)</li> </ul> </li> <li> <p>A single test class can have multiple test methods, e.g., for     <code>FooBar.method_a()</code> and <code>FooBar.method_b()</code>, the test class contains the     following methods:</p> <p>```python class TestFooBar1(unittest2.TestCase):     def test_method_a(self):             ...</p> <pre><code>def test_method_b(self):\n        ...\n</code></pre> <p>```</p> </li> <li> <p>Split test classes and methods in a reasonable way so each one tests one     single thing in the simplest possible way</p> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#keep_testing_code_in_sync_with_tested_code","title":"Keep testing code in sync with tested code","text":"<ul> <li>If you change the name of a tested class, also the test should be changed</li> <li>If you change the name of a file also the name of the file with the testing     code should be changed</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#test_code_is_not_second-class_citizen","title":"Test code is not second-class citizen","text":"<ul> <li> <p>Test code is not second-class citizen, even though it's auxiliary to the code</p> </li> <li> <p>Add comments and docstring explaining what the code is doing</p> </li> <li> <p>Avoid repetition in test code, but use helper to factor out common code</p> </li> <li>Abhor copy-paste and keep the code DRY</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#testing_code_layout","title":"Testing code layout","text":"<ul> <li>The layout of a test dir should look like:     <code>bash     &gt; ls -1 helpers/test/     Test_dassert1.test2     Test_dassert1.test3     Test_dassert1.test4     ...     Test_dassert_misc1.test6     Test_dassert_misc1.test8     Test_system1.test7     test_dbg.py     test_helpers.py     test_system_interaction.py</code></li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#our_framework_to_test_using_input_output_data","title":"Our framework to test using input / output data","text":"<ul> <li><code>/helpers/hunit_test.py</code> has some utilities to     easily create input and output dirs storing data for unit tests</li> <li> <p><code>hunitest.TestCase</code> has various methods to help you create dirs</p> <ul> <li><code>get_input_dir()</code>: return the name of the dir used to store the inputs</li> <li><code>get_scratch_space()</code>: return the name of a scratch dir to keep artifacts of     the test</li> <li><code>get_output_dir()</code>: probably not interesting for the user</li> </ul> </li> <li> <p>The directory structure enforced by the out <code>TestCase</code> is like:</p> <p>```bash</p> <p>tree -d edg/form_8/test/ edg/form_8/test/ \u00c3\u00a2\u00c2\u0094\u00c2\u0094\u00c3\u00a2\u00c2\u0094\u00c2\u0080\u00c3\u00a2\u00c2\u0094\u00c2\u0080 TestExtractTables1.test1         \u00c3\u00a2\u00c2\u0094\u00c2\u009c\u00c3\u00a2\u00c2\u0094\u00c2\u0080\u00c3\u00a2\u00c2\u0094\u00c2\u0080 input         \u00c3\u00a2\u00c2\u0094\u00c2\u0094\u00c3\u00a2\u00c2\u0094\u00c2\u0080\u00c3\u00a2\u00c2\u0094\u00c2\u0080 output ```</p> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#use_text_and_not_pickle_files_as_inputoutputs","title":"Use text and not pickle files as input/outputs","text":"<ul> <li> <p>The problems with pickle files are the usual ones</p> <ul> <li>Pickle files are not stable across different versions of libraries</li> <li>Pickle files are not human-readable</li> </ul> </li> <li> <p>Prefer to use text file</p> <ul> <li>E.g., use a CSV file</li> </ul> </li> <li> <p>If the data used for testing is generated in a non-complicated way</p> <ul> <li>Document how it was generated</li> <li>Even better, add a test that generates the data</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#small_testing_data_is_best","title":"Small testing data is best","text":"<ul> <li> <p>Use a subset of the input data</p> </li> <li> <p>The smaller, the better for everybody</p> <ul> <li>Fast tests</li> <li>Easier to debug</li> <li>More targeted unit test</li> </ul> </li> <li> <p>Do not check in 1 megabyte of test data!</p> </li> </ul> <p>Last review: GP on 2024-05-13</p>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#check_string_vs_selfassertequal","title":"<code>check_string</code> vs <code>self.assertEqual</code>","text":"<ul> <li>Use <code>self.assert_equal()</code> when you expect a simple, small string comparison.</li> <li>Use <code>self.check_string()</code> when:<ul> <li>The expected string is too large to inline in code.</li> <li>You want a frozen reference output stored on disk.</li> <li>The test output may change less frequently but still needs stable diffing.</li> </ul> </li> <li>Rule of thumb:<ul> <li>If it's simple \u2192 <code>assert_equal()</code></li> <li>If it's large, externalized, or behavioral regression \u2192 <code>check_string()</code>.</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#use_selfassert_equal","title":"Use <code>self.assert_equal()</code>","text":"<ul> <li>This is a function that helps you understand what the mismatches are</li> <li>It works on <code>str</code></li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#how_to_split_unit_test_code_in_files","title":"How to split unit test code in files","text":"<ul> <li>The two extreme approaches are:<ul> <li>All the test code for a directory goes in one file     <code>foo/bar/test/test_$DIRNAME.py</code> (or <code>foo/bar/test/test_all.py</code>)</li> <li>Each file <code>foo/bar/$FILENAME</code> with code gets its corresponding     <code>foo/bar/test/test_$FILENAME.py</code><ul> <li>It should also be named according to the library it tests</li> <li>For example, if the library to test is called <code>pnl.py</code>, then a     corresponding unit test should be called <code>test_pnl.py</code></li> </ul> </li> </ul> </li> <li>Pros of 1) vs 2)<ul> <li>Less maintenance churn<ul> <li>It takes work to keep the code and the test files in sync, e.g.,<ul> <li>If you change the name of the code file, you don't have to change other     file names</li> <li>If you move one class from one file to another, you might not need to     move test code</li> </ul> </li> </ul> </li> <li>Fewer files opened in your editor</li> <li>Avoid many files with a lot of boilerplate code</li> </ul> </li> <li>Cons of 1) vs 2)<ul> <li>The single file can become huge!</li> </ul> </li> <li>Compromise solution: Start with a single file     <code>test_$DIRNAME.py</code>(or<code>test*dir_name.py</code>) * In the large file add a framed     comment like:     ```python</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#_1","title":"All.write unit tests.how to guide","text":""},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#unit_tests_for","title":"Unit tests for ...","text":""},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#_2","title":"All.write unit tests.how to guide","text":"<pre><code>```\n- So it's easy to find which file is tested were using grep\n</code></pre> <ul> <li>Then split when it becomes too big using <code>test_$FILENAME.py</code></li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#template_for_unit_test","title":"Template for unit test","text":"<ul> <li>Interesting unit tests are in <code>helpers/test</code></li> <li>A unit test looks like:     <code>python import helpers.hunit_test as hunitest class Test...(hunitest.TestCase):         def test...(self):                 ...</code></li> <li><code>pytest</code> will take care of running the code so you don't need:     <code>python if __name__ == '__main__': unittest.main()</code></li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#use_consistent_comments_in_test_methods","title":"Use consistent comments in test methods","text":"<p>We strongly encourage adding short, consistent comments at the start of each main action within your test methods to establish a unified structure:</p> <pre><code>import helpers.hunit_test as hunitest\n\nclass TestFooBar1(hunitest.TestCase):\n        def test_method_a(self):\n                # Prepare inputs.\n                input_value = 42\n                # Run.\n                actual = foo_bar_function(input_value)\n                # Check.\n                expected = \"some_expected_value\"\n                self.assert_equal(str(actual), expected)\n</code></pre> <p>This makes it easy to scan what the test is doing and quickly understand its three core steps. It also improves readability and consistency across all our test code.</p>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#hierarchical_testcase_approach","title":"Hierarchical <code>TestCase</code> approach","text":"<ul> <li>Whenever there is a hierarchy in classes, we also create a hierarchy of test     classes</li> <li>A parent test class looks like:     <code>python import helpers.hunit_test as hunitest class SomeClientTestCase(hunitest.TestCase):         def _test...1(self):                 ...         def _test...2(self):                 ...</code></li> <li>While a child test class looks like this, where test methods use the     corresponding methods from the parent test class:     <code>python class TestSomeClient(SomeClientTestCase):         def test...1(self):                 ...         def test...2(self):                 ...</code></li> <li>Each <code>TestCase</code> tests a \"behavior\" like a set of related methods</li> <li>Each <code>TestCase</code> is under the test dir</li> <li>Each derived class should use the proper <code>TestCase</code> classes to reach a decent     coverage</li> <li>It is OK to use non-private methods in test classes to ensure that the code is     in order of dependency so that the reader doesn't have to jump back / forth</li> <li> <p>We want to separate chunks of unit test code using:</p> <p>```python</p> </li> <li> <p>It is OK to skip a <code>TestCase</code> method if it is not meaningful, when coverage is     enough</p> </li> <li>As an example, see <code>datapull/common/data/client/test/im_client_test_case.py</code>     and <code>datapull/ccxt/data/client/test/test_ccxt_clients.py</code></li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#_3","title":"All.write unit tests.how to guide","text":"<p>```</p> <p>putting all the methods used by that chunk at the beginning and so on</p>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#use_the_appropriate_selfassert","title":"Use the appropriate <code>self.assert*</code>","text":"<ul> <li>When you get a failure, you don't want to get something like \"True is not     False\", rather an informative message like \"5 is not &lt; 4\"</li> <li>Bad <code>self.assertTrue(a &lt; b)</code></li> <li>Good <code>self.assertLess(a, b)</code></li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#do_not_use_hdbgdassert_in_testing","title":"Do not use <code>hdbg.dassert</code> in testing","text":"<ul> <li><code>dassert</code>s are for checking the self-consistency of the code</li> <li>The invariant is that you can remove <code>dbg.dassert</code> without changing the code's     behavior. Of course, you can't remove the assertion and get unit tests to work</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#always_explain_selfassertraises","title":"Always explain <code>self.assertRaises</code>","text":"<ul> <li>When testing for an assertion, always use the following idiom to clearly     explain what exception is expected and why:</li> </ul> <pre><code>with self.assertRaises(AssertionError) as cm:\n        hlitagit.git_patch_create(\n                ctx, mode, modified, branch, last_commit, files\n        )\nact = str(cm.exception)\nexp = r\"\"\"\n#### Failed assertion * '0' == '1' Specify only one among --modified,\n    --branch, --last-commit\n\"\"\"\nself.assert_equal(act, exp, fuzzy_match=True)\n</code></pre>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#test_from_the_outside-in_1","title":"Test from the outside-in","text":"<ul> <li>We want to start testing from the end-to-end methods towards the constructor     of an object</li> <li>Rationale: often, we start testing the constructor very carefully and then we     get tired / run out of time when we finally get to test the actual behavior</li> <li>Also, testing the important behavior automatically tests building the objects</li> <li>Use the code coverage to see what's left to test once you have tested the     \"most external\" code</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#we_dont_need_to_test_all_the_assertions","title":"We don't need to test all the assertions","text":"<ul> <li>Focus on behavioral correctness, not line coverage.</li> <li>We don't need to exhaustively test every <code>dassert</code> or input validation unless     it's part of the system's observable behavior.</li> <li>Behavioral coverage gives better ROI than aiming for 100% line coverage.</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#use_strings_to_compare_output_instead_of_data_structures","title":"Use strings to compare output instead of data structures","text":"<ul> <li> <p>Often, it's easier to do a check like:</p> <p>```python</p> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#better","title":"Better:","text":"<p>expected = str(...) expected = pprint.pformat(...)</p>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#worse","title":"Worse:","text":"<p>expected = [\"a\", \"b\", { ... }]     ```</p> <pre><code>rather than building the data structure\n</code></pre> <ul> <li>Some purists might not like this, but<ul> <li>It's much faster to use a string (which is or should be one-to-one to the     data structure), rather than the data structure itself<ul> <li>By extension, many of the more complex data structure have a built-in     string representation</li> </ul> </li> <li>It is often more readable and easier to diff (e.g., <code>self.assertEqual</code> vs     <code>self.assert_equal</code>)</li> <li>In case of mismatch, it's easier to update the string with copy-paste rather     than creating a data structure that matches what was created</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#use_selfcheck_string_for_things_that_we_care_about_not_changing_or_are_too_big_to_have_as_strings_in_the_code","title":"Use <code>self.check_string()</code> for things that we care about not changing (or are too big to have as strings in the code)","text":"<ul> <li>Use <code>self.assert_equal()</code> for things that should not change (e.g., 1 + 1 = 2)</li> <li>When using <code>check_string</code> still try to add invariants that force the code to     be correct</li> <li>E.g., if we want to check the PnL of a model, we can freeze the output with     <code>check_string()</code>, but we want to add a constraint like there are more     timestamps than 0 to avoid the situation where we update the string to     something malformed</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#each_test_method_should_test_a_single_test_case","title":"Each test method should test a single test case","text":"<ul> <li>Rationale: we want each test to be clear, simple, fast</li> <li>If there is repeated code we should factor it out (e.g., builders for objects)</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#each_test_should_be_crystal_clear_on_how_it_is_different_from_the_others","title":"Each test should be crystal clear on how it is different from the others","text":"<ul> <li>Often, you can factor out all the common logic into a helper method</li> <li>Copy-paste is not allowed in unit tests in the same way it's not allowed in     production code</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#in_general_you_want_to_budget_the_time_to_write_unit_tests","title":"In general, you want to budget the time to write unit tests","text":"<ul> <li> <p>E.g., \"I'm going to spend 3 hours writing unit tests\". This is going to help     you focus on what's important to test and force you to use an iterative     approach rather than incremental (remember the Monalisa)</p> <p></p> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#write_a_template_of_unit_tests_and_ask_for_a_review_if_you_are_not_sure_how_what_to_test","title":"Write a template of unit tests and ask for a review if you are not sure how what to test","text":"<ul> <li>Aka \"testing plan\"</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#interesting_testing_functions","title":"Interesting testing functions","text":"<ul> <li>List of useful testing functions are:<ul> <li>General Python</li> <li>Numpy</li> <li>Pandas</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#use_set_up_test_tear_down_test","title":"Use set_up_test / tear_down_test","text":"<ul> <li>If you have a lot of repeated code in your tests, you can make them shorter by     moving this code to <code>set_up_test/tear_down_test</code> methods:<ul> <li>These methods are our preferred alternative to <code>setUp()</code> and <code>tearDown()</code>     methods that are standardly used in the <code>unittest</code> framework. As a general     rule, we should avoid defining custom <code>setUp()</code> and <code>tearDown()</code> methods in     our test classes. The reason is that the <code>unittest</code>/<code>pytest</code> framework     relies on its built-in <code>setUp()</code> and <code>tearDown()</code> methods, and if we     introduce additional operations in these methods in the individual test     classes (e.g., mocking, opening DBs), it can interfere with the inner     workings of the framework.</li> <li>Instead of <code>setUp()</code>, we define a <code>set_up_test()</code> method within the test     class and then run it at the beginning of each test method.</li> <li>Similarly, instead of <code>tearDown()</code>, we define <code>tear_down_test()</code> and run it     at the end of each test method.</li> <li> <p>To make sure both <code>set_up_test()</code> and <code>tear_down_test()</code> run even if the     test itself fails, we wrap them in a     pytest fixture.</p> <p>Bad:</p> <p>```python def setUp(self) -&gt; None: super().setUp() ... ... # custom code ...</p> </li> </ul> </li> </ul> <p>def tearDown(self) -&gt; None:         ...         ... # custom code         ...         super().tearDown()</p> <p>def test1(self) -&gt; None:         ...         ```</p> <pre><code>    **Good**:\n\n    ```python\n</code></pre>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#this_will_be_run_before_and_after_each_test","title":"This will be run before and after each test.","text":"<p>@pytest.fixture(autouse=True) def setup_teardown_test(self):         # Run before each test.         self.set_up_test()         yield         # Run after each test.         self.tear_down_test()</p> <p>def set_up_test(self) -&gt; None:         ...         ... # custom code         ...</p> <p>def tear_down_test(self) -&gt; None:         ...         ... # custom code         ...</p> <p>def test1(self) -&gt; None:         ...         <code>``     - If there is nothing left in</code>setUp()<code>/</code>tearDown()<code>after removing</code>super().setUp()<code>/</code>super().tearDown()<code>, then</code>setUp()<code>/</code>tearDown()` can be         discarded completely.</p>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#nested_set_up_test_tear_down_test","title":"Nested set_up_test / tear_down_test","text":"<ul> <li>In nested inheritance:<ul> <li>The parent class uses <code>set_up_test()</code> / <code>tear_down_test()</code>.</li> <li>The child class uses <code>set_up_test2()</code> / <code>tear_down_test2()</code>, calling its     parent.</li> <li>Each further nested level adds a numeric suffix: <code>set_up_test3()</code>, etc.</li> </ul> </li> <li>This makes nested setup logic explicit and avoids framework collisions.</li> <li> <p>The following cases are possible with regard to the <code>setUp()</code>/<code>tearDown()</code>     configuration:</p> <ul> <li>Both of TestParent and TestChild have separate <code>setUp()</code>/<code>tearDown()</code>     methods. Then, in TestParent, <code>setUp()</code>/<code>tearDown()</code> should be replaced by     <code>set_up_test()</code>/<code>tear_down_test()</code> as described above; in TestChild,     <code>setUp()</code>/<code>tearDown()</code> should be replaced by     <code>set_up_test2()</code>/<code>tear_down_test2()</code>, which call     <code>set_up_test()</code>/<code>tear_down_test()</code> from the parent class. For example:</li> </ul> <p>```python class TestParent(hunitest.TestCase):     def setUp(self) -&gt; None:             super().setUp()             ...</p> <pre><code>def tearDown(self) -&gt; None:\n        ...\n        super().tearDown()\n\ndef test1(self) -&gt; None:\n        ...\n</code></pre> </li> </ul> <p>class TestChild(TestParent):         def setUp(self) -&gt; None:                 super().setUp()                 ...</p> <pre><code>    def tearDown(self) -&gt; None:\n            ...\n            super().tearDown()\n\n    def test1(self) -&gt; None:\n            ...\n```\n\nshould be replaced by:\n\n```python\n</code></pre> <p>class TestParent(hunitest.TestCase):         @pytest.fixture(autouse=True)         def setup_teardown_test(self):                 # Run before each test.                 self.set_up_test()                 yield                 # Run after each test.                 self.tear_down_test()</p> <pre><code>    def set_up_test(self) -&gt; None:\n            ...\n\n    def tear_down_test(self) -&gt; None:\n            ...\n\n    def test1(self) -&gt; None:\n            ...\n</code></pre> <p>class TestChild(TestParent):         @pytest.fixture(autouse=True)         def setup_teardown_test(self):                 # Run before each test.                 self.set_up_test2()                 yield                 # Run after each test.                 self.tear_down_test2()</p> <pre><code>    def set_up_test2(self) -&gt; None:\n            self.set_up_test()\n            ...\n\n    def tear_down_test2(self) -&gt; None:\n            ...\n            self.tear_down_test()\n\n    def test1(self) -&gt; None:\n            ...\n```\n- TestParent has `setUp()` and `tearDown()`, while TestChild does not. Then,\n    in TestParent, `setUp()`/`tearDown()` should be replaced by\n    `set_up_test()`/`tear_down_test()` as described above; in TestChild,\n    `set_up_test()`/`tear_down_test()` will run in the test methods\n    automatically via the inherited fixture. For example:\n\n```python\n</code></pre> <p>class TestParent(hunitest.TestCase):         def setUp(self) -&gt; None:                 super().setUp()                 ...</p> <pre><code>    def tearDown(self) -&gt; None:\n            ...\n            super().tearDown()\n\n    def test1(self) -&gt; None:\n            ...\n</code></pre> <p>class TestChild(TestParent):         def test1(self) -&gt; None:                 ...     ```</p> <pre><code>should be replaced by:\n\n```python\n</code></pre> <p>class TestParent(hunitest.TestCase):         @pytest.fixture(autouse=True)         def setup_teardown_test(self):                 # Run before each test.                 self.set_up_test()                 yield                 # Run after each test.                 self.tear_down_test()</p> <pre><code>    def set_up_test(self) -&gt; None:\n            ...\n\n    def tear_down_test(self) -&gt; None:\n            ...\n\n    def test1(self) -&gt; None:\n            ...\n</code></pre> <p>class TestChild(TestParent):         def test1(self) -&gt; None:                 ...     <code>``     - If TestParent only has one of the two, either</code>setUp()<code>or</code>tearDown()<code>,         then in both TestParent and TestChild, the test methods should run (from the         fixture), respectively, either</code>set_up_test()<code>or</code>tear_down_test()`, e.g.:</p> <pre><code>            ```python\n</code></pre> <p>class TestParent(hunitest.TestCase):         @pytest.fixture(autouse=True)         def setup_teardown_test(self):                 # Run before each test.                 self.set_up_test()                 yield</p> <pre><code>    def set_up_test(self) -&gt; None:\n            ...\n\n    def test1(self) -&gt; None:\n            ...\n</code></pre> <p>class TestChild(TestParent):         def test1(self) -&gt; None:                 ...                 <code>``     - A combination of the previous two options: TestParent has</code>setUp()<code>and</code>tearDown()<code>, and TestChild has one of them but not the other. Then, in         TestParent,</code>setUp()<code>/</code>tearDown()<code>should be replaced by</code>set_up_test()<code>/</code>tear_down_test()<code>as described above. In TestChild, the         method that was present,</code>setUp()<code>or</code>tearDown()<code>, should be replaced by</code>set_up_test2()<code>or</code>tear_down_test2()<code>, which should call</code>set_up_test()<code>or</code>tear_down_test()<code>from TestParent; the other method from TestParent,         which was absent in TestChild, should be added directly to TestChild's         fixture to run in its test methods. For example, for the case when TestChild         has</code>setUp()<code>but not</code>tearDown()`:</p> <pre><code>```python\n</code></pre> <p>class TestParent(hunitest.TestCase):         def setUp(self) -&gt; None:                 super().setUp()                 ...</p> <pre><code>    def tearDown(self) -&gt; None:\n            ...\n            super().tearDown()\n\n    def test1(self) -&gt; None:\n            ...\n</code></pre> <p>class TestChild(TestParent):         def setUp(self) -&gt; None:                 super().setUp()                 ...</p> <pre><code>    def test1(self) -&gt; None:\n            ...\n```\n\nshould be replaced by:\n\n```python\n</code></pre> <p>class TestParent(hunitest.TestCase):         @pytest.fixture(autouse=True)         def setup_teardown_test(self):                 # Run before each test.                 self.set_up_test()                 yield                 # Run after each test.                 self.tear_down_test()</p> <pre><code>    def set_up_test(self) -&gt; None:\n            ...\n\n    def tear_down_test(self) -&gt; None:\n            ...\n\n    def test1(self) -&gt; None:\n            ...\n</code></pre> <p>class TestChild(TestParent):         @pytest.fixture(autouse=True)         def setup_teardown_test(self):                 # Run before each test.                 self.set_up_test2()                 yield                 # Run after each test.                 self.tear_down_test()</p> <pre><code>    def set_up_test2(self) -&gt; None:\n            self.set_up_test()\n            ...\n\n    def test1(self) -&gt; None:\n            ...\n```\n- TestChild has `setUp()` and `tearDown()` (or just one of the two), while\n    TestParent does not. Then, in TestChild, `setUp()`/`tearDown()` should be\n    replaced by `set_up_test()`/`tear_down_test()` as described above;\n    TestParent undergoes no changes. For example:\n\n```python\n</code></pre> <p>class TestParent(hunitest.TestCase):         def test1(self) -&gt; None:                 ...</p> <p>class TestChild(TestParent):         def setUp(self) -&gt; None:                 super().setUp()                 ...</p> <pre><code>    def tearDown(self) -&gt; None:\n            ...\n            super().tearDown()\n\n    def test1(self) -&gt; None:\n            ...\n```\n\nshould be replaced by:\n\n```python\n</code></pre> <p>class TestParent(hunitest.TestCase):         def test1(self) -&gt; None:                 ...</p> <p>class TestChild(TestParent):         @pytest.fixture(autouse=True)         def setup_teardown_test(self):                 # Run before each test.                 self.set_up_test()                 yield                 # Run after each test.                 self.tear_down_test()</p> <pre><code>    def set_up_test(self) -&gt; None:\n            ...\n\n    def tear_down_test(self) -&gt; None:\n            ...\n\n    def test1(self) -&gt; None:\n            ...\n```\n</code></pre>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#use_setupclass_teardownclass","title":"Use setUpClass / tearDownClass","text":"<ul> <li> <p>If you need some expensive code parts to be done once for the whole test     class, such as opening a database connection, opening a temporary file on the     filesystem, loading a shared library for testing, etc., you can use     <code>setUpClass/tearDownClass</code> methods:</p> <ul> <li> <p><code>setUpClass()</code></p> <p>A class method called before tests in an individual class are run. <code>setUpClass</code> is called with the class as the only argument and must be decorated as a classmethod:</p> <p><code>python @classmethod def setUpClass(cls): ...</code>     - <code>tearDownClass()</code></p> <p>A class method called after tests in an individual class have run. <code>tearDownClass</code> is called with the class as the only argument and must be decorated as a classmethod:</p> <p><code>python @classmethod def tearDownClass(cls): ...</code></p> </li> </ul> </li> <li> <p>For more information, see     official unittest docs</p> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#update_test_tags","title":"Update test tags","text":"<ul> <li>In the root of each repo there is a <code>pytest.ini</code> file with the list of tests'     tags</li> <li>In order to update the tags:<ul> <li>In the <code>markers</code> section, add a name of a new tag</li> <li>After a <code>:</code> add a short description</li> <li>Keep tags in the alphabetical order</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#mocking","title":"Mocking","text":""},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#refs","title":"Refs","text":"<ul> <li>Introductory article is     https://realpython.com/python-mock-library/ </li> <li>Official Python documentation for the mock package can be seen here     unit test mock</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#our_philosophy_about_mocking","title":"Our Philosophy about mocking","text":""},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#mock_only_external_dependencies","title":"Mock only external dependencies","text":"<ul> <li> <p>Typically we want to mock interactions with only external components, e.g.,</p> <ul> <li>3rd party provider<ul> <li>CCXT</li> </ul> </li> <li>Cloud infra (e.g., AWS)<ul> <li>S3 (e.g., see <code>/helpers/hmoto.py)</code>)</li> <li>AWS Secrets ...</li> </ul> </li> <li>DataBase</li> <li>GitHub</li> <li>...</li> </ul> </li> <li> <p>E.g., assume there is a class that is interfacing with an external data     provider and our code places requests and gets values back</p> </li> <li> <p>We want to replace the provider with an object that responds to the requests     with the actual response of the provider</p> </li> <li> <p>If we want to interactions with GitHub we should mock the GitHub library and     not our API on top of it (since we want to test it)</p> <p>Good:</p> <p><code>python @umock.patch(\"github.Github\") def test_github_labels(self, mock_github):    # Mock only the external provider.    mock_repo = umock.Mock()    mock_github.return_value.get_repo.return_value = mock_repo    ...</code></p> <p>Bad:</p> <p>```python @umock.patch(\"helpers.get_labels\") def test_github_labels(self, mock_helper):</p> </li> <li> <p>We want our mock object to look just real enough for the code to run</p> <ul> <li> <p>Include only the attributes or return values your function actually uses</p> <p>Bad:</p> <p>```python mock_label = umock.Mock() mock_label.name = \"bug\"</p> </li> </ul> </li> </ul> <p>def test_process_labels(mock_repo):     labels = mock_repo.get_labels()     self.assert_equal(labels[0].name, \"bug\")         ```</p> <pre><code>    **Good**:\n\n    ```python\n</code></pre> <p>mock_label = umock.Mock()</p>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#do_not_mock_internal_helper","title":"Do not mock internal helper.","text":"<p>... ```</p>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#dont_add_unused_fields","title":"Don't add unused fields.","text":"<p>mock_label.color = \"f29513\" mock_label.description = \"Something is not working\" mock_label.created_at = \"2024\u00c3\u00a2\u00c2\u0080\u00c2\u009101\u00c3\u00a2\u00c2\u0080\u00c2\u009101\" mock_label.updated_at = \"2024\u00c3\u00a2\u00c2\u0080\u00c2\u009101\u00c3\u00a2\u00c2\u0080\u00c2\u009102\" ....</p>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#add_only_the_fields_the_function_uses","title":"Add only the fields the function uses.","text":"<p>mock_label.name = \"bug\" mock_repo = umock.Mock() mock_repo.get_labels.return_value = [mock_label] ...</p> <p>def test_process_labels(mock_repo):     labels = mock_repo.get_labels()     self.assert_equal(labels[0].name, \"bug\")         ```</p> <ul> <li>Do not test external providers themselves. E.g., when using OpenAI, we don't     need to verify that OpenAI's API behaves according to spec\u2014only that our code     handles the expected inputs/outputs.</li> <li>This avoids redundant tests and focuses coverage on our code, not third-party     behavior.</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#do_not_mock_internal_dependencies","title":"Do not mock internal dependencies","text":"<ul> <li>In general we don't want to mock any code that is inside our repo, since<ul> <li>We want to actually test the interaction of different pieces of our code</li> <li>It creates maintenance problems</li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#testing_end-to-end","title":"Testing end-to-end","text":"<ul> <li> <p>Often we want to test public methods of our class (and a few private methods)</p> <ul> <li>In other words, we want to test the end-to-end behavior and not how things     are achieved</li> <li>Rationale: if we start testing \"how\" things are done and not \"what\" is done,     we can't change how we do things (even if it doesn't affect the interface     and its behavior), without updating tons of methods</li> </ul> </li> <li> <p>We want to test the minimal amount of behavior that enforces what we care     about</p> <p>Bad:</p> <p><code>python @umock.patch(\"docker.build_container\") @umock.patch(\"helpers.get_labels\") @umock.patch(\"github.Github\") def test_github_labels(self, mock_get_labels, mock_build_container, mock_github):    # Don't mock too much, like internal Docker and helper functions.    mock_get_labels.return_value = [\"bug\"]    mock_build_container.return_value = \"image123\"    mock_repo = umock.Mock()    mock_github.return_value.get_repo.return_value = mock_repo</code></p> <p>Good:</p> <p><code>python @umock.patch(\"github.Github\") def test_GitHub_labels(self, mock_github):    # Mock only the behavior that needs to be tested.    mock_repo = umock.Mock()    mock_github.return_value.get_repo.return_value = mock_repo    ...</code></p> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#object_patch_with_return_value","title":"Object patch with return value","text":"<pre><code>import unittest.mock as umock\nimport datapull.ccxt.data.extract.extractor as ivcdexex\n\n@umock.patch.object(ivcdexex.hsecret, \"get_secret\")\ndef test_function_call1(self, mock_get_secret: umock.MagicMock):\n        mock_get_secret.return_value = \"dummy\"\n</code></pre> <ul> <li>Function <code>get_secret</code> in <code>helpers/hsecret.py</code> is mocked<ul> <li>Pay attention on where is <code>get_secret</code> mocked:<ul> <li>It is mocked in <code>datapull.ccxt.data.extract.extractor</code> as \"get_secret\" is     called there in function that is being tested</li> </ul> </li> <li><code>@umock.patch.object(hsecret, \"get_secret\")</code> will not work as mocks are     applied after all modules are loaded, hence the reason for using exact     location<ul> <li>If we import s module in test itself it will work as mock is applied</li> <li>For modules outside of test function it is too late as they are loaded     before mocks for test are applied</li> </ul> </li> </ul> </li> <li>On every call, it returns string \"dummy\"</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#path_patch_with_multiple_return_values","title":"Path patch with multiple return values","text":"<pre><code>import unittest.mock as umock\n\n@umock.patch(\"helpers.hsecret.get_secret\")\ndef test_function_call1(self, mock_get_secret: umock.MagicMock):\n\nmock_get_secret.side_effect = [\"dummy\", Exception]\n</code></pre> <ul> <li>On first call, string <code>dummy</code> is returned</li> <li>On second, <code>Exception</code> is raised</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#ways_of_calling_patch_and_patchobject","title":"Ways of calling <code>patch</code> and <code>patch.object</code>","text":"<ul> <li>Via decorator     <code>python @umock.patch(\"helpers.hsecret.get_secret\") def test_function_call1(self, mock_get_secret: umock.MagicMock):         pass</code></li> <li>In actual function     <code>python get_secret_patch = umock.patch(\"helpers.hsecret.get_secret\") get_secret_mock = get_secret_patch.start()</code><ul> <li>This is the only approach in which you need to start/stop patch!<ul> <li>The actual mock is returned as the return value of <code>start()</code> method!</li> </ul> </li> <li>In other two approaches, start/stop is handled under the hood and we are     always interacting with <code>MagicMock</code> object</li> </ul> </li> <li>Via <code>with</code> statement (also in function)     <code>python with umock.patch(\"\"helpers.hsecret.get_secret\"\") as get_secret_mock:         pass</code><ul> <li>One of the use cases for this is if we are calling a different function     inside a function that is being mocked<ul> <li>Mostly because it is easy for an eye if there are to much patches via     decorator and we do not need to worry about reverting the patch changes as     that is automatically done at the end of with statement</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#mock_object_state_after_test_run","title":"Mock object state after test run","text":"<pre><code>@umock.patch.object(exchange_class._exchange, \"fetch_ohlcv\")\ndef test_function_call1(self, fetch_ohlcv_mock: umock.MagicMock):\n        self.assertEqual(fetch_ohlcv_mock.call_count, 1)\n        actual_args = tuple(fetch_ohlcv_mock.call_args)\n        expected_args = (\n                        (\"BTC/USDT\",),\n                        {\"limit\": 2, \"since\": 1, \"timeframe\": \"1m\"},\n        )\n        self.assertEqual(actual_args, expected_args)\n</code></pre> <ul> <li>After <code>fetch_ohlcv</code> is patched, <code>Mock</code> object is passed to test<ul> <li>In this case, it is <code>fetch_ohlcv_mock</code></li> </ul> </li> <li> <p>From sample we can see that function is called once</p> <ul> <li>First value in a tuple are positional args passed to <code>fetch_ohlcv</code> function</li> <li>Second value in a tuple are keyword args passed to <code>fetch_ohlcv</code> function</li> <li> <p>As an alternative, <code>fetch_ohlcv_mock.call_args.args</code> and     <code>fetch_ohlcv_mock.call_args.kwargs</code> can be called for separate results of     args/kwargs</p> <p><code>python self.assertEqual(fetch_ohlcv_mock.call_count, 3) actual_args = str(fetch_ohlcv_mock.call_args_list) expected_args = r\"\"\" [call('BTC/USDT', since=1645660800000, bar_per_iteration=500), call('BTC/USDT', since=1645690800000, bar_per_iteration=500), call('BTC/USDT', since=1645720800000, bar_per_iteration=500)] \"\"\" self.assert_equal(actual_args, expected_args, fuzzy_match=True)</code></p> </li> </ul> </li> <li> <p>In sample above, that is continuation of previous sample,     <code>fetch_ohlcv_mock.call_args_list</code> is called that returns all calls to mocked     function regardless of how many times it is called</p> </li> <li>Useful for verifying that args passed are changing as expected</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#mock_common_external_calls_in_hunitesttestcase_class","title":"Mock common external calls in <code>hunitest.TestCase</code> class","text":"<pre><code>class TestCcxtExtractor1(hunitest.TestCase):\n        # Mock calls to external providers.\n        get_secret_patch = umock.patch.object(ivcdexex.hsecret, \"get_secret\")\n        ccxt_patch = umock.patch.object(ivcdexex, \"ccxt\", spec=ivcdexex.ccxt)\n\n        def set_up_test(self) -&gt; None:\n                self.get_secret_mock: umock.MagicMock = self.get_secret_patch.start()\n                self.ccxt_mock: umock.MagicMock = self.ccxt_patch.start()\n                # Set dummy credentials for all tests.\n                self.get_secret_mock.return_value = {\"apiKey\": \"test\", \"secret\": \"test\"}\n\n        def tear_down_test(self) -&gt; None:\n                self.get_secret_patch.stop()\n                self.ccxt_patch.stop()\n</code></pre> <ul> <li>For every unit test we want to isolate external calls and replace them with     mocks<ul> <li>This way tests are much faster and not influenced by external factors we can     not control</li> <li>Mocking them in <code>set_up_test()</code> (which is run at the beginning of the test     methods) will make the tests using this class simpler and ready out of the     box</li> </ul> </li> <li>In current sample we are mocking AWS secrets and <code>ccxt</code> library<ul> <li><code>umock.patch.object</code> is creating <code>patch</code> object that is not yet activated<ul> <li><code>patch.start()/stop()</code> is activating/deactivating patch for each test     where <code>set_up_test()</code> and <code>tear_down_test()</code> are run</li> <li><code>patch.start()</code> is returning a standard <code>MagicMock</code> object we can use to     check various states as mentioned in previous examples and control return     values<ul> <li><code>call_args</code>, <code>call_count</code>, <code>return_value</code>, <code>side_effect</code>, etc.</li> </ul> </li> </ul> </li> </ul> </li> <li>Note: Although patch initialization in static variables belongs to     <code>set_up_test()</code>, when this code is moved there patch is created for each test     separately. We want to avoid that and only start/stop same patch for each     test.</li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#mocks_with_specs","title":"Mocks with specs","text":"<pre><code>## Regular mock and external library `ccxt` is replaced with `MagicMock`\n@umock.patch.object(ivcdexex, \"ccxt\")\n## Only `ccxt` is spec'd, not actual components that are \"deeper\" in the `ccxt` library.\n@umock.patch.object(ivcdexex, \"ccxt\", spec=ivcdexex.ccxt)\n## Everything is spec'd recursively , including returning values/instances of `ccxt`\n## functions and returned values/instances of returned values/instances, etc.\n@umock.patch.object(ivcdexex, \"ccxt\", autospec=True)\n</code></pre> <ul> <li>First mock is not tied to any spec and we can call any attribute/function     against the mock and the call will be memorized for inspection and the return     value is new <code>MagicMock</code>.<ul> <li><code>ccxt_mock.test(123)</code> returns new <code>MagicMock</code> and raises no error</li> </ul> </li> <li>In second mock <code>ccxt.test(123)</code> would fail as such function does not exists<ul> <li>We can only call valid exchange such as <code>ccxt_mock.binance()</code> that will     return <code>MagicMock</code>, as exchange is not part of the spec</li> </ul> </li> <li>In third mock everything needs to be properly called<ul> <li><code>ccxt_mock.binance()</code> will return <code>MagicMock</code> with <code>ccxt.Exchange</code> spec_id     (in mock instance as meta)<ul> <li>As newly <code>exchange</code> instance is with spec, we can only call real     functions/attributes of <code>ccxt.Exchange</code> class</li> </ul> </li> </ul> </li> </ul>"},{"location":"tools/unit_test/all.write_unit_tests.how_to_guide.html#caveats","title":"Caveats","text":"<pre><code>## `datetime.now` cannot be patched directly, as it is a built-in method.\n## Error: \"can't set attributes of built-in/extension type 'datetime.datetime'\"\ndatetime_patch = umock.patch.object(imvcdeexut, \"datetime\", spec=imvcdeexut.datetime)\n</code></pre> <ul> <li> <p>Python built-in methods can not be patched</p> <p><code>python class TestExtractor1(hunitest.TestCase):     # Mock `Extractor`'s abstract functions.     abstract_methods_patch = umock.patch.object(             imvcdexex.MarketDataExtractor, \"__abstractmethods__\", new=set()     )     ohlcv_patch = umock.patch.object(             imvcdexex.MarketDataExtractor,             \"_download_ohlcv\",             spec=imvcdexex.MarketDataExtractor._download_ohlcv,     )</code></p> </li> <li> <p>Patching <code>__abstractmethods__</code> function of an abstract class enables us to     instantiate and test abstract class as any regular class</p> </li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html","title":"All.buildmeister.how to guide","text":""},{"location":"work_organization/all.buildmeister.how_to_guide.html#buildmeister","title":"Buildmeister","text":""},{"location":"work_organization/all.buildmeister.how_to_guide.html#buildmeister_process","title":"Buildmeister Process","text":""},{"location":"work_organization/all.buildmeister.how_to_guide.html#general","title":"General","text":"<ul> <li>The Buildmeister rotates according to the schedule<ul> <li>To see who is the Buildmeister now refer to     Buildmeister gsheet</li> <li>Each rotation should be confirmed following the     Handover Process which includes a status report from     the outgoing Buildmeister and acknowledgment from the new one</li> </ul> </li> <li>The Buildmeister is responsible for:<ul> <li>Check build status using the     buildmeister dashboard everyday</li> <li>Pushing team members to fix broken tests</li> <li>Conducting post-mortem analysis<ul> <li>Why did the break happen?</li> <li>How can we avoid the problem next time, through process and automation?</li> </ul> </li> </ul> </li> <li>Refer to <code>.github</code> dir in the repo for update schedule of GH actions</li> <li>Additional information about tests:<ul> <li>write tests</li> <li>run tests</li> </ul> </li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#notification_system","title":"Notification System","text":"<ul> <li><code>Kaizen GitHub actions app</code> notifies the team about breaks via Slack channel     #build-notifications</li> <li>A notification contains:<ul> <li>Failing tests type: fast/slow/super-slow</li> <li>Repo</li> <li>Branch</li> <li>Event</li> <li>Link to a failing run</li> </ul> </li> </ul> <p>Example:</p> <ul> <li></li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#buildmeister_instructions","title":"Buildmeister Instructions","text":"<ul> <li>You receive a break notification from <code>Kaizen GitHub actions app</code></li> <li>Have a look at the message<ul> <li>Do it right away, this is always your highest priority task</li> </ul> </li> <li> <p>Notify the team</p> <ul> <li>If the break happened in <code>lemonade</code> repo, ping GP or Paul, since they are     the only ones with write access</li> </ul> </li> <li> <p>Post on the     #build-notifications Slack     channel what tests broke, e.g.,     <code>FAILED knowledge_graph/vendors/test/test_utils.py::TestClean::test_clean</code></p> <ul> <li>If unsure about the cause of failure (there is a chance that a failure is     temporary):<ul> <li>Do a quick run locally for the failed test</li> <li>If the test is specific and can not be run locally, rerun the regressions</li> </ul> </li> <li>Ask if somebody knows what is the problem<ul> <li>If you know who is in charge of that test (you can use <code>git blame</code>) ask     directly</li> </ul> </li> <li>If the offender says that it's fixing the bug right away, let him/her do it</li> <li>Otherwise, file a bug to track the issue</li> </ul> </li> <li> <p>File an Issue on GH to report the failing tests and the errors</p> <ul> <li>Example:     https://github.com/cryptokaizen/cmamp/issues/4386</li> <li>Issue title template <code>Build fail - {repo} {test type} ({run number})</code><ul> <li>Example: <code>Build fail - Cmamp fast_tests (1442077107)</code></li> </ul> </li> <li>Paste the URL of the failing run<ul> <li>Example:     https://github.com/causify-ai/cmamp/actions/runs/12738914526</li> </ul> </li> <li>Provide as much information as possible to give an understanding of the     problem</li> <li>List all the tests with FAILED status in a GitHub run, e.g.,     <code>bash     FAILED knowledge_graph/vendors/test/test_p1_utils.py::TestClean::test_clean     FAILED knowledge_graph/vendors/nbsc/test/test_nbsc_utils.py::TestExposeNBSCMetadata::test_expose_nbsc_metadata</code></li> <li>Stack trace or part of it (if it's too large)     <code>bash     Traceback (most recent call last): File     \"/.../automl/hypotheses/test/test*rh_generator.py\", line 104, in test1     kg_metadata, * = p1ut.load_release(version=\"0.5.2\") File     \"/.../knowledge_graph/vendors/utils.py\", line 53, in load_release % version,     File \"/.../amp/helpers/dbg.py\", line 335, in dassert_dir_exists \\_dfatal(txt,     msg, \\*args) File \"/.../amp/helpers/dbg.py\", line 97, in \\_dfatal     dfatal(dfatal_txt) File \"/.../amp/helpers/dbg.py\", line 48, in dfatal raise     assertion_type(ret) AssertionError:     ##############################################################################     Failed assertion \\*         dir='/fsx/research/data/kg/releases/timeseries_db/v0.5.2' doesn't exist or         it's not a dir The requested version 0.5.2 has directory associated with it.</code></li> <li>If the failures are not connected to each other, file separate issues for     each of the potential root cause</li> <li>Keep issues grouped according to the codebase organization</li> </ul> </li> <li> <p>Post the issue reference on Slack channel     #build-notifications</p> <ul> <li>You can quickly discuss there who will take care of the broken tests, assign     that person</li> <li>Otherwise, assign it to the person who can reroute</li> </ul> </li> <li> <p>Our policy is \"fix it or revert\"</p> <ul> <li>The build needs to go back to green within 1 hr<ul> <li>Either the person responsible for the break fixes the issue within 1 hour,     or you need to push the responsible person to disable the test</li> <li>Do not make the decision about disabling the test yourself!</li> <li>First, check with the responsible person, and if he / she is ok with     disabling, do it</li> <li>IMPORTANT: Disabling a test is not the first choice, it's a measure of     last resort!</li> </ul> </li> </ul> </li> <li> <p>Regularly check issues related to build breaks.</p> <ul> <li>You have to update the break issues if the problem was solved or partially     solved.</li> <li>Pay special attention to the failures which resulted in disabling tests.</li> </ul> </li> <li> <p>When your time of the Buildmeister duties is over, confirm the rotation with     the next responsible person in the related Slack channel.</p> </li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#update_helpers_submodule_fails","title":"<code>Update_Helpers_Submodule</code> Fails","text":"<ul> <li> <p>When this happens, the first thing to do is attempt to update the <code>helpers</code>     pointer manually</p> </li> <li> <p>Instructions:     <code>bash     &gt; cd src/cmamp1     &gt; git checkout master     &gt; git pull --recurse-submodules     &gt; cd helpers_root     &gt; git checkout master     &gt; git pull origin master     &gt; cd ..     &gt; git add helpers_root     &gt; git commit -m \"Update helpers pointer\"</code></p> </li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#daily_reporting_and_handover_process","title":"Daily Reporting and Handover Process","text":"<ul> <li> <p>The current Buildmeister must send a daily status report to eng@ at the start     of each workday with the TODO email</p> <ul> <li>The email subject should follow the format:     <code>[BM Report] Build Status - YYYY-MM-DD</code></li> <li>The email should include:<ul> <li>Current status of all builds (green/red)</li> <li>For any red (failing) builds:<ul> <li>Which tests are failing</li> <li>Why they are failing (if known)</li> <li>Who is responsible for fixing them</li> <li>Expected timeline for fixes</li> </ul> </li> <li>Confirmation that all breaks are tracked with GitHub issues<ul> <li>Include links to all open issues related to build breaks</li> </ul> </li> <li>A screenshot of the current Buildmeister dashboard<ul> <li>This provides a visual overview of the build status</li> </ul> </li> <li>Any additional relevant information or concerns</li> </ul> </li> </ul> </li> <li> <p>The current Buildmeister must also save the status report in a Log file at     Buildmesietr Log Doc</p> </li> <li> <p>When a new Buildmeister takes over the role:</p> <ul> <li>The new Buildmeister must respond to the most recent status report email</li> <li>The response should:<ul> <li>Acknowledge receipt of the handover</li> <li>Confirm understanding of current build status</li> <li>Include \"Acknowledged\" in the subject line</li> <li>CC the previous Buildmeister and eng@</li> </ul> </li> </ul> </li> <li> <p>Example email format:</p> <p>```text Subject: BuildMeister Status</p> <p>Build Status Summary: - cmamp/master: RED (2 failing tests) - helpers/master: GREEN - lemonade/master: GREEN</p> <p>Point each break to a git Issue: - https://github.com/cryptokaizen/cmamp/issues/4386</p> <p>Dashboard screenshot attached.</p> <p>Additional notes if needed: - The failing tests have been occurring since yesterday's deployment - We may need to update the data path configuration in our CI environment ```</p> </li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#buildmeister_dashboard","title":"Buildmeister Dashboard","text":"<p>The Buildmeister dashboard is a tool that provides a quick overview of the current state of the results of all GitHub Actions workflows. See run and publish the buildmeister dashboard for detailed information.</p> <p></p>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#logic_used_to_determine_workflow_status","title":"Logic Used to Determine Workflow Status","text":"<p>The dashboard uses the following logic to determine the latest and most relevant status of each GitHub Actions workflow:</p> <ul> <li> <p>Workflow Run Selection:</p> <ul> <li>For each workflow in each repo, the dashboard fetches the 10 most recent     workflow runs on the <code>master</code> branch</li> <li>For workflows such as <code>\"Gitleaks Scan\"</code>, the dashboard prioritizes the     latest scheduled run (<code>event == \"schedule\"</code>) as these are used for CI health     checks</li> <li>If no scheduled run is found (or the workflow does not have scheduled runs),     it falls back to the most recent completed run with a <code>conclusion</code> of     <code>\"success\"</code> or <code>\"failure\"</code></li> </ul> </li> <li> <p>Overall Repository Status:</p> <ul> <li>A repository is marked as <code>Failed</code> if any of its workflows have a     <code>conclusion</code> of <code>failure</code></li> <li>Otherwise, it is marked as <code>Success</code></li> <li>In Jupyter notebooks, these statuses are color-coded:<ul> <li><code>Green</code>: <code>Success</code></li> <li><code>Red</code>: <code>Failure</code></li> </ul> </li> </ul> </li> <li> <p>Workflow Data and Display:</p> <ul> <li>For each workflow, the dashboard shows:<ul> <li><code>repo_name</code>: repository name (e.g., <code>cryptokaizen/cmamp</code>)</li> <li><code>workflow_name</code>: name of the workflow (e.g., <code>\"Allure fast tests\"</code>)</li> <li><code>conclusion</code>: overall status (<code>\"success\"</code> or <code>\"failure\"</code>)</li> <li><code>url</code>: link to the specific workflow run</li> </ul> </li> <li>All URLs in the dashboard are rendered as clickable links</li> <li>A timestamp indicating when the dashboard was last generated is included for     reference</li> </ul> </li> <li> <p>Handling Missing or Sparse Runs:</p> <ul> <li>If fewer than 10 runs are available for a workflow, the dashboard logs a     warning and skips the workflow</li> <li>This ensures that workflows which haven't run recently do not affect the     overall dashboard accuracy</li> </ul> </li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#allure_reports_analysis","title":"Allure Reports Analysis","text":"<ul> <li> <p>For a background on Allure, refer to these docs</p> <ul> <li>Detailed info can be found in the official     docs</li> <li>Allure Explanantion</li> <li>Allure How to Guide</li> </ul> </li> <li> <p>For now, the Buildmeister can get the link to the Allure reports by navigating     GitHub Actions page     https://github.com/cryptokaizen/cmamp/actions</p> <ul> <li>Select a particular workflow (Allure fast tests, Allure slow tests, Allure     superslow tests) based on the test types</li> <li>Click on the particular run for which to get the report. Latest is on the     top</li> <li>Access the report URL by clicking <code>Report URL</code> in the run link. For e.g.:     https://github.com/cryptokaizen/cmamp/actions/runs/7210433549/job/19643566697</li> <li>The report URL looks like:     http://172.30.2.44/allure_reports/cmamp/fast/report.20231212_013147</li> </ul> </li> <li> <p>Once a week the Buildmeister manually inspects     graph section of the     report</p> <ul> <li>The overall goal is to:<ul> <li> <p>Monitor the amount of skipped, failed, and broken tests using the     <code>Trend Chart</code>. It shows how a certain value changed over time. Each     vertical line corresponds to a certain version of the test report, with     the last line on the right corresponding to the current version</p> <p>         - Monitor the <code>Duration Trend</code> to check the time taken to the run all tests comparing to historical trends</p> <p>         - Monitor the <code>Duration Distribution</code>, where all the tests are divided into groups based on how long it took to complete them, and manually compare with the last week results</p> <p>         - Monitor the <code>Retries Trend</code> to check the number of retries occurred in a particular run</p> <p>         - The idea is to make sure it doesn't have drastic change in the values</p> </li> </ul> </li> </ul> </li> <li> <p>Steps to perform if a test fails, timeouts or breaks</p> <ul> <li>When a particular test fails, timeouts or breaks, Buildmeister should look     in report for<ul> <li>How long it was the case, e.g., did it occur in the past? Include this     info when filing an issue</li> <li>The very first run when it happened and add that info to the issue. This     could be useful for debugging purposes</li> <li>These info can be extracted by navigating the <code>Packages</code> section of the     report for that test. Any particular test has <code>history</code> and <code>retries</code>     section which shows the history of success and number of retries occured     for that test</li> </ul> </li> <li>The goal here is to provide more context when filing an issue so that we can     make better decisions</li> </ul> </li> </ul>"},{"location":"work_organization/all.buildmeister.how_to_guide.html#post-mortem_analysis_tbd","title":"Post-Mortem Analysis (TBD)","text":"<ul> <li> <p>We want to understand on why builds are broken so that we can improve the     system to make it more robust</p> <ul> <li>In order to do that, we need to understand the failure modes of the system</li> <li>For this reason we keep a log of all the issues and what was the root cause</li> </ul> </li> <li> <p><code>Date</code> column:</p> <ul> <li>Enter the date when the break took place</li> <li>Keep the bug ordered in reverse chronological order (i.e., most recent dates     first)</li> </ul> </li> <li> <p><code>Repo</code> column:</p> <ul> <li>Specify the repo where break occurred<ul> <li><code>amp</code></li> <li>...</li> </ul> </li> </ul> </li> <li> <p><code>Test type</code> column:</p> <ul> <li>Specify the type of the failing tests<ul> <li>Fast</li> <li>Slow</li> <li>Super-slow</li> </ul> </li> </ul> </li> <li> <p><code>Link</code> column:</p> <ul> <li>Provide a link to a failing run</li> </ul> </li> <li> <p><code>Reason</code> column:</p> <ul> <li>Specify the reason of the break<ul> <li>Merged a branch with broken tests</li> <li>Master was not merged in a branch</li> <li>Merged broken slow tests without knowing that</li> <li>Underlying data changed</li> </ul> </li> </ul> </li> <li> <p><code>Issue</code> column:</p> <ul> <li>Provide the link to the GH issue with the break description</li> </ul> </li> <li> <p><code>Solution</code> column:</p> <ul> <li>Provide the solution description of the problem<ul> <li>Problem that led to the break was solved</li> <li>Failing tests were disabled, i.e. problem was not solved</li> </ul> </li> </ul> </li> </ul>"},{"location":"work_organization/all.contributor_feedback.how_to_guide.html","title":"All.contributor feedback.how to guide","text":""},{"location":"work_organization/all.contributor_feedback.how_to_guide.html#contributor_feedback","title":"Contributor Feedback","text":""},{"location":"work_organization/all.contributor_feedback.how_to_guide.html#feedback_principles","title":"Feedback principles","text":"<ul> <li> <p>We want to evaluate and provide feedback to our contributors on different     aspects of their work</p> </li> <li> <p>As a way to formalize giving feedback, we assign numerical scores on a variety     of metrics</p> </li> <li> <p>Each metric is scored between 1 (poor), 3 (average) and 5 (excellent)</p> <ul> <li>We consider 4 as acceptable, anything less than 4 as problematic and needs     to improve</li> </ul> </li> <li> <p>We don't take non-perfect scores personally but just as a way to understand     what to improve</p> </li> <li> <p>Scoring is anonymous</p> </li> <li> <p>Everyone should be scored by at least 2 people</p> </li> <li> <p>Frequency:</p> <ul> <li>Every 2 weeks for interns</li> <li>Every month for permanent team members</li> </ul> </li> </ul>"},{"location":"work_organization/all.contributor_feedback.how_to_guide.html#scoring_metrics","title":"Scoring metrics","text":""},{"location":"work_organization/all.contributor_feedback.how_to_guide.html#general","title":"General","text":"<ul> <li> <p>Metrics should be independent</p> </li> <li> <p>We should provide</p> <ul> <li>Concrete questions to assess how people do on each metric</li> <li>Ways to improve the score (e.g., \"read this book!\", \"do more of this and     less of that\")</li> </ul> </li> <li> <p>Along with the numerical scores, there should be a possibility to add a     textual note that can be used to provide rationale of the feedback and to     suggest improvements</p> </li> </ul>"},{"location":"work_organization/all.contributor_feedback.how_to_guide.html#metrics_for_interns","title":"Metrics for interns","text":"<ul> <li>Sends good TODO emails<ul> <li>Doesn't forget to send one</li> <li>Follows our     formatting requirements</li> <li>Sets realistic ETAs</li> </ul> </li> <li>Reads docs with attention<ul> <li>Internalizes our conventions described in the docs</li> </ul> </li> <li>Able to follow procedures<ul> <li>Issue and PR-related workflows</li> <li>Org processes</li> </ul> </li> <li>Independence<ul> <li>Provides solutions rather than questions</li> <li>Doesn't need a lot of guidance</li> <li>Asks only \"good\" questions (not something that they should be able to solve     on their own)</li> </ul> </li> <li>Attention to detail<ul> <li>Doesn't forget to do small things, including but not limited to:<ul> <li>Follow style conventions</li> <li>Apply fixes everywhere appropriate</li> <li>Keep the branch up to date</li> <li>Make sure there are no tmp files checked in</li> </ul> </li> <li>Thinks about corner cases while writing code and tests</li> </ul> </li> <li>Git / GitHub knowledge<ul> <li>Doesn't run into problems with branches/PRs</li> </ul> </li> <li>Python knowledge / coding ability<ul> <li>Writes effective and beautiful code</li> </ul> </li> <li>Commitment to the project<ul> <li>Puts in the hours<ul> <li>This is a minor point: the number of hours doesn't really matter as long     as stuff is done</li> <li>On the other hand, if somebody consistently doesn't put enough time to get     stuff done, it can become a problem</li> </ul> </li> <li>Willing to learn and contribute</li> <li>Willing to solve problems</li> </ul> </li> <li>Productivity<ul> <li>Quick to successfully complete tasks</li> </ul> </li> <li>Learns from reviews<ul> <li>Doesn't repeat the same mistake twice</li> </ul> </li> </ul>"},{"location":"work_organization/all.contributor_feedback.how_to_guide.html#metrics_for_permanent_team_members","title":"Metrics for permanent team members","text":"<ul> <li>Quality of code<ul> <li>Writes elegant code</li> <li>Follows our standards and conventions</li> </ul> </li> <li>Quality of design<ul> <li>Designs beautiful but simple abstractions</li> <li>Adds abstractions only when needed</li> <li>Orchestrates software components properly</li> <li>Uses design patterns when needed</li> </ul> </li> <li>Attention to details<ul> <li>Thinks in terms of corner cases</li> <li>Debugs things carefully</li> <li>Takes pride in a well-done product (e.g., code, documentation)</li> </ul> </li> <li>Productivity<ul> <li>Closes issues effectively without unnecessary iterations</li> </ul> </li> <li>Makes and achieves ETAs<ul> <li>Accurately estimates complexity of issues</li> <li>Thinks of risks and unknown unknowns, best / average / worst ETAs</li> <li>Resolves issues in set ETAs</li> <li>Puts in a sufficient amount of hours to make progress</li> </ul> </li> <li>Autonomy<ul> <li>Understands specs</li> <li>Needs an acceptable level of supervision to execute the tasks</li> <li>Does what's right according to our shared way of doing things without     reminders</li> </ul> </li> <li>Follows our PR process<ul> <li>Learns from reviews and doesn't make the same mistakes</li> <li>Runs Linter consistently before each iteration</li> <li>Does a PR / day (even a draft)</li> </ul> </li> <li>Follows our organizational process<ul> <li>Sends a daily TODO email</li> <li>Updates their issues regularly</li> <li>Curates GitHub</li> </ul> </li> <li>Team work<ul> <li>Helps others on the team when others need help / supervision</li> <li>Takes the initiative and goes the extra mile when needed</li> <li>Sacrifices for the greater good (e.g., doing stuff that is not fun to do)</li> </ul> </li> <li>Communication<ul> <li>Files issues with clear specs</li> <li>Explains technical issues and gives updates properly and with clarity</li> <li>Reports problems and solutions with proper context</li> <li>Speaks and writes English well</li> </ul> </li> <li>Ability to run a team<ul> <li>Can juggle multiple topics at once</li> <li>Can split the work in issues</li> <li>Can provide clear and extensive specs</li> <li>Is ok with being interrupted to help team members</li> </ul> </li> <li>Positive energy<ul> <li>Has an upbeat approach to working even if sh*t doesn't work (since things     never work)</li> <li>Isn't a     Negative Nelly</li> </ul> </li> <li>Dev %, Data scientist %, Devops %<ul> <li>This measures how much of each role the team member can cover</li> </ul> </li> </ul>"},{"location":"work_organization/all.contributor_feedback.how_to_guide.html#roles","title":"Roles","text":"<ul> <li> <p>We want to determine how comfortable the team member is engaging in different     types of activities</p> <ul> <li>This is helpful to understand which roles a new hire can play</li> </ul> </li> <li> <p>Current roles:</p> <ul> <li>Data science<ul> <li>Example of activities:<ul> <li>Write notebooks</li> <li>Do research</li> <li>Debug data</li> </ul> </li> </ul> </li> <li>Dev<ul> <li>Example of activities:<ul> <li>Write code</li> <li>Refactor code</li> <li>Architecture code</li> <li>Debug code</li> <li>Unit test code</li> </ul> </li> </ul> </li> <li>DevOps<ul> <li>Example of activities:<ul> <li>Manage / supervise infra</li> <li>Airflow</li> <li>Docker</li> <li>AWS</li> <li>Administer Linux</li> </ul> </li> </ul> </li> </ul> </li> <li> <p>E.g., X is a data scientist and has Data science=5, Dev=3, DevOps=1</p> </li> <li>Roles are not mutually exclusive<ul> <li>A jack-of-all-trades can get a high score for all the roles</li> </ul> </li> </ul>"},{"location":"work_organization/all.contributor_feedback.how_to_guide.html#scoring_process","title":"Scoring process","text":"<ul> <li> <p>The process is organized and guided by</p> <ul> <li>HiringMeister for interns</li> <li>FeedbackMeister for permanent team members</li> </ul> </li> <li> <p>Scoring is done via a Google Form, which is distributed to the scorers in     Asana</p> </li> <li> <p>Scores for each metric are averaged in a spreadsheet, which is then made     available to people as feedback</p> <ul> <li>If there are textual notes accompanying numerical scores, their summary is     also provided</li> </ul> </li> <li> <p>Contributors receive an email which includes the feedback and a link to this     doc to help interpret the metrics</p> </li> </ul>"},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html","title":"all.datapull DAGmeister.how to guide","text":""},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html#datapull_dagmeister","title":"Datapull Dagmeister","text":""},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html#datapull_dagmeister_process","title":"DataPull DagMeister process","text":""},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html#general","title":"General","text":"<ul> <li>The DagMeister rotates every 2 weeks<ul> <li>To see who is the DagMeister now refer to     DataPull_DagMeister gsheet</li> <li>Each rotation should be confirmed by a 'handshake' between the outgoing     DagMeister and the new one in the related Slack channel     <code>#notifs-preprod-datapull</code></li> <li>Transfer the assignee of     #8785 to new DagMeister</li> </ul> </li> <li>The DagMeister is responsible for:<ul> <li>Check the Slack channel for any failures from preprod DAGs.</li> <li>Raising the issue on Github for that failure by debugging the root cause of     the failure.<ul> <li>If the issue is already raised, comment the link of the failure in the     issue citing same reason.</li> </ul> </li> <li>Tag team leader in the issue to confirm if the issue needs to be fixed with     highest priority or not.</li> <li>All the failures from region <code>tokyo</code> are of highest priority and needs to be     resolved ASAP.</li> </ul> </li> </ul>"},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html#notification_system","title":"Notification system","text":"<ul> <li><code>@CK_Airflow_bot</code> notifies the team about breaks via Slack channel     <code>Kaizen Preprod Datapull Notifications</code></li> <li>A notification contains:<ul> <li>DAG start timestamp</li> <li>Link fo broken DAG</li> </ul> </li> </ul>"},{"location":"work_organization/all.datapull_DAGmeister.how_to_guide.html#dagmeister_instructions","title":"DagMeister instructions","text":"<ul> <li>You receive a break notification from <code>@CK_Airflow_bot</code></li> <li>Have a look at the message<ul> <li>Do it right away, this is always your highest priority task</li> </ul> </li> <li> <p>Notify the team</p> <ul> <li>If the break happened in <code>tokyo</code> region for <code>bid_ask</code> or <code>OHLCV</code> DAGs ping     the channel by tagging the team leader.</li> <li>Reply on the failure to notify you are already looking into this.</li> <li>After the issue is raised reply back with the issue number.<ul> <li>There could be multiple failure due to the same reason so just reply with     same issue number.</li> </ul> </li> </ul> </li> <li> <p>File an Issue in GH to report the failing tests and the errors</p> <ul> <li>Paste the URL of the failing run<ul> <li>Example: #9110</li> </ul> </li> <li>Provide as much information as possible to give an understanding of the     problem</li> <li>Stack trace or part of it (if it's too large)</li> <li>Paste the link of QA notebook if QA failed.</li> </ul> </li> <li> <p>Fixing the issue</p> <ul> <li>If the bug is obvious and can be fixed easily. Fix it with highest priority.</li> <li>If fixing will require debugging time, tag the team leader to ask for     priority.<ul> <li>IMPORTANT: Disabling a DAG is not the first choice, it's a measure of last     resort! and should oly be done after the approval from the team leader.</li> </ul> </li> </ul> </li> <li> <p>When your time of the DAGMeister duties is over, confirm the rotation with the     next responsible person in the related Slack chat.</p> </li> </ul>"},{"location":"work_organization/all.github_projects_process.reference.html","title":"All.github projects process.reference","text":""},{"location":"work_organization/all.github_projects_process.reference.html#github_projects_process","title":"GitHub Projects process","text":""},{"location":"work_organization/all.github_projects_process.reference.html#overview","title":"Overview","text":"<ul> <li> <p>We use GitHub projects to track progress on individual     components/products/services</p> </li> <li> <p>The entrypoint to preview available projects is     Causify-ai GitHub Projects     TODO(Juraj): this link will work only for people who are directly members of     the organization (not collaborators)</p> </li> </ul>"},{"location":"work_organization/all.github_projects_process.reference.html#gh_project_structure","title":"GH project structure","text":"<ul> <li> <p>A GH project represents a long-term project that is composed of GH Issues,     executed by multiple team members according to Sprints</p> </li> <li> <p>Each GH project window consists of tabs called \"views\"</p> </li> <li> <p>A view can provide a specific perspective on the issues in the project, e.g.,     filter certain issues by their attributes, visualize them in a board/list</p> </li> <li> <p>We use a base     template     for all of our projects to ensure consistency between projects</p> </li> <li> <p>The base template consists of the following views:</p> <ul> <li>All issues: provides a table view of all issues present in the project</li> <li>Current sprint: provides a table view of issues in the currently running     sprint</li> <li>Next sprint: provides a table view of issues in the next sprint</li> <li>Backlog: list of a non-finished issues in a given project without a sprint     assigned</li> <li>Team capacity: aggregates \"estimate point\" for each collaborator in the     project</li> </ul> </li> </ul>"},{"location":"work_organization/all.github_projects_process.reference.html#issue_fields","title":"Issue fields","text":"<ul> <li> <p>Each view within a project contains specific fields:</p> <ul> <li>Assignee<ul> <li>Names of people the issue is assigned to</li> </ul> </li> <li>Status<ul> <li>What phase of the development the issue is in</li> <li>We have 7 status options available that should be in sync within all     projects<ul> <li>Todo (This item hasn't been started)</li> <li>In Progress (This is actively being worked on)</li> <li>Paused (Work was paused)</li> <li>Done (This has been completed)</li> <li>In Review (This is awaiting review or validation)</li> <li>PR back to author (Author needs to apply review feedback)</li> <li>QA (Change being tested in QA)</li> </ul> </li> </ul> </li> <li>Label (Bug, P0, P1)<ul> <li>Label assigned to the specific issue</li> <li>Current labels synced to all repos are located at     <code>/dev_scripts_helpers/github/labels/gh_issues_labels.yml</code></li> </ul> </li> <li>Estimate in terms of \"uninterrupted hours of work\"<ul> <li>What is the issues' estimated implementation difficulty</li> <li>We want to become good at estimating complexity and overcome being     uncomfortable in providing ETAs</li> <li>Estimating complexity in terms of hours of work is done to the best of our     ability acknowledging that higher priority issues come up, we get     interrupted sometimes, debugging or implementing a feature might take more     time than expected</li> <li>We are ok with doing our best to estimate and we are ok failing to meet     our expectations, so that we can learn and improve</li> </ul> </li> <li>Sprint<ul> <li>Which weekly sprint this issue belongs to</li> </ul> </li> <li>Linked Pull Request<ul> <li>The pull requests linked to the specific issue</li> </ul> </li> </ul> </li> <li> <p>It's possible to add new fields; the list above includes some of the agreed     upon fields shared across all projects</p> </li> </ul>"},{"location":"work_organization/all.github_projects_process.reference.html#how_to_set_up_a_new_project","title":"How to set up a new project","text":"<ul> <li> <p>Click on <code>Projects</code> tab in the desired repository, e.g.     causify-ai/cmamp</p> </li> <li> <p>Click on the green <code>New Project</code> button</p> </li> <li> <p>A new window with a pop-up should appear, in the pop-up choose     <code>Project templates</code> &gt; <code>From your organization</code> and choose ** [TEMPLATE]     Causify Project**</p> </li> <li> <p>Change the default name <code>@yourusername untitled project</code> to a relevant name</p> <ul> <li>The name should be short and representative of what needs to be done</li> <li>Ideally we want to complete a Project in 2-3 months, since we don't want to     have long-running projects linger forever on the board. Each project should     have a clear goal</li> <li>Long-running projects should be broken in smaller projects to show and     measure progress</li> <li>If a project can't be easily described in a phrase starting with a verb, it     means that the scope is too big</li> <li>Good names are:<ul> <li>\"CustomerXYZ Proof-of-concept\", e.g., \"ITSavvy Proof-of-concept\"</li> <li>\"Split cmamp into separate repos\"</li> <li>\"FAR trading\"</li> <li>\"Implement v0.1 of XYZ\", \"Deploy XYZ\", ...</li> </ul> </li> <li>Bad names are:<ul> <li>\"Config\". A better name is \"Make config robust\" or \"Config - Q1-2024     Goals\"</li> </ul> </li> </ul> </li> <li> <p>Click on the <code>Project details</code> button in the upper right corner next to the     three dots <code>...</code></p> <ul> <li>Provide a short description of the project, e.g., who is the customer</li> <li>What is the overarching goal of the project</li> <li>Who is the team leader</li> <li>Provide links to the key resources (Slack/Asana channel)</li> </ul> </li> <li> <p>Add team members to the project</p> <ul> <li>Click on the three dots <code>...</code> (View more button) and go to     <code>Settings &gt; Manage access &gt; Invite collaborators</code></li> <li>Set write access if you want a team member to be able to add new issues to     the project and manage the state of the issues</li> </ul> </li> <li> <p>Open Slack and create a channel for the project</p> <ul> <li>The naming convention is <code>proj-&lt;&lt;project-name&gt;&gt;</code></li> <li>The Slack channel is used to communicate with the team working on that     project, using our conventions of what's so urgent to require an IM</li> <li>Click on <code>Add Canvas</code> and create a <code>Channel Overview</code> canvas<ul> <li>Provide a short description (can be the same as in GitHub project)</li> <li>Include links to the GitHub and Asana</li> </ul> </li> </ul> </li> <li> <p>Open Asana and create a new project</p> <ul> <li>The naming convention is <code>Proj - &lt;&lt;project name&gt;&gt;</code></li> <li>The Asana project is used to<ul> <li>Track non-technical issues (i.e., track work that doesn't easily map in     terms of coding)</li> <li>To track the project at higher level than GH</li> </ul> </li> <li>The Asana board is typically used by the project Team leader to communicate     status to the other departments and Team leaders</li> <li>E.g., on Asana we can do high level planning and track work that is not     ready yet to be filed as GH Issues, similar to what we do with Google Docs</li> </ul> </li> <li> <p>In general there is not a clear cut distinction between what goes in GH     Project, GH Issues, Asana tasks, Google Docs planning</p> <ul> <li>We agree that a GH Project includes many GH Issues</li> <li>We agree that things can go in GH Project or in Asana tasks or in a Google     Doc planning docs</li> <li>We let people a bit of flexibility to do things however they are more     comfortable</li> <li>E.g., GP likes Google Docs to sketch out a project and track issues, Asana     to track at high-level, and one Issue per \"feature\" when the Issue is ready     to be executed</li> </ul> </li> </ul>"},{"location":"work_organization/all.github_projects_process.reference.html#special_types_of_project","title":"Special types of project","text":"<ul> <li> <p>Not all project need to have an explicit start and finish date</p> </li> <li> <p>Each team (Infra/Devops, DataPull, ML ops etc.) should have their catch-all     project to track the issues</p> </li> </ul>"},{"location":"work_organization/all.github_projects_process.reference.html#sprint_process","title":"Sprint process","text":"<ol> <li> <p>Sprints run on Sunday to Saturday schedule         - This is due to the fact that Friday is the preferred day to complete             status updates         - GitHub issue <code>sprint</code> field adheres to this convention</p> </li> <li> <p>Project owner creates a GitHub status update (by clicking on the Status         button on the upper right in a given project) until 11 AM ET each Friday         - The 11 AM deadline was chosen to facilitate easier completion of Asana             status updates which have a 12 AM deadline             - In practice the GitHub project status updates will often serve as the                 source of information for the Asana update         - The status update should include the following sections:             - Summary (what was accomplished, what kind of obstacles did the team run                 into)             - Next steps (explain work for the next 1-2 sprints)             - Example update:</p> <pre><code>        ```text\n            Summary\n            - We have finished updating ETL pipeline for historical bid/ask data\n                for additional symbols (the work started two sprints ago)\n            - We have started to run daily scheduled live trading runs (low\n                volume, short runs) using one of our off-the-shelf models\n                - This is a major step forward for the team as it involved\n                    collaboration between all engineering departments\n                - The runs were completed successfully from start to finish for the\n                    last couple of days\n            - The orderbook related ETL and backtest pipelines encountered some\n                unexpected obstacles (code-base related). It took a bit more time\n                than expected to resolve them, but it didn't take us off track.\n\n            Next steps\n            - Increase the length of daily low-volume trading to exercise the\n                whole system from start to finish in a more rigorous manner and\n                catch some corner-case behavior (mainly with regards interaction\n                with exchange API)\n            - Run back-test(s) for orderbook-based model(s)\n            - Kick-off work on a new model\n        ```\n</code></pre> </li> <li> <p>The status update is sent to customers (either external or internal sending         to all@)</p> </li> <li> <p>Project owner (or a TL) can ask other collaborators in the project to also         add status updates if needed, before broadcasting, to make sure we manage         expectations.</p> </li> </ol>"},{"location":"work_organization/all.github_projects_process.reference.html#automation","title":"Automation","text":""},{"location":"work_organization/all.github_projects_process.reference.html#built-in_workflows","title":"Built-in workflows","text":"<ul> <li> <p>GitHub offers a handful of     built-in workflows     to automate frequently run actions within projects</p> <ul> <li>For example: when an issue is closed, set the status to done</li> </ul> </li> <li> <p>To open the workflows menu, click on the <code>...</code> button in the upper right     corner of the project view and select \"Workflows\"</p> </li> <li> <p>In our     base template     we currently use     these automated     workflows</p> </li> </ul>"},{"location":"work_organization/all.github_projects_process.reference.html#github_actions_automations","title":"Github Actions automations","text":"<ul> <li>For more customized automated workflows we use GitHub actions</li> <li>A GitHub Action     sprint_iteration.yml     is set up to move items in active projects from last sprint to current sprint     at the beginning of each sprint.</li> <li>We have     sync_gh_issue_labels.py     to synchronize GitHub issue labels from a label inventory manifest file</li> <li>We also have     sync_gh_projects.py     to compare a source GitHub Project (the template) with a destination project     and ensures all global fields and views from the template exist in the     destination</li> </ul>"},{"location":"work_organization/all.github_projects_process.reference.html#parent_and_sub-issues","title":"Parent and sub-issues","text":"<ul> <li>GitHub offers     sub-issues     to breakdown complex or higher-level task into smaller chunks of works</li> <li>Our current convention is not to use this feature</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html","title":"All.inframeister.how to guide","text":""},{"location":"work_organization/all.inframeister.how_to_guide.html#inframeister_responsibilities","title":"InfraMeister Responsibilities","text":""},{"location":"work_organization/all.inframeister.how_to_guide.html#overview","title":"Overview","text":"<p>The InfraMeister role is critical for maintaining the operational health of our infrastructure. This role entails proactive monitoring, alert management, cost control, and specific oversight of infrastructure-related incidents.</p>"},{"location":"work_organization/all.inframeister.how_to_guide.html#1_monitoring_and_alert_management","title":"1. Monitoring and Alert Management","text":""},{"location":"work_organization/all.inframeister.how_to_guide.html#review_alerts","title":"Review Alerts","text":"<ul> <li>AlertManager Overview: Prometheus AlertManager, is a tool integrated with     Prometheus to handle alerts sent by client applications such as Prometheus     server. It takes care of deduplicating, grouping, and routing them to the     correct receiver integrations such as email or Slack. It also ensures alert     silencing and inhibition logic.<ul> <li>Regularly check alerts generated by AlertManager to identify critical issues     that require immediate attention.</li> <li>Evaluate alerts to determine their impact and urgency, prioritizing actions     based on severity.</li> </ul> </li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#general_approach_to_critical_alerts","title":"General Approach to Critical Alerts","text":""},{"location":"work_organization/all.inframeister.how_to_guide.html#immediate_response","title":"Immediate Response","text":"<ul> <li>Acknowledge the Alert: Immediately confirm receipt of the alert to     mitigate escalations and inform the team that the issue is being addressed.</li> <li>Assess the Situation: Swiftly evaluate the metrics or logs relevant to the     alert to understand its immediacy and potential impact. For instance, if an     alert indicates high CPU usage, immediately access the CPU usage logs and     metrics on the Grafana dashboard to understand the severity (e.g., whether the     CPU usage is consistently above 90%).</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#diagnosis","title":"Diagnosis","text":"<ul> <li>Review Data Trends: Analyze historical data to identify growth trends and     potential points of failure. For instance, review historical data available in     Grafana, to identify trends such as disk usage, memory leaks, or load patterns     that can forecast potential points of failure.</li> <li>Check for Anomalies: Investigate any unusual spikes or patterns that could     signify underlying problems, such as a sudden increase in response time or     failure rates.</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#mitigation","title":"Mitigation","text":"<ul> <li>Implement Quick Fixes: Apply temporary solutions if necessary to stabilize     the environment while long-term solutions are devised.</li> <li>Optimize System Performance: Make adjustments to reduce the strain on     resources, which could include purging unnecessary data or tweaking system     settings.</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#documentation_and_reporting","title":"Documentation and Reporting","text":"<ul> <li>Report Actions and Outcomes: Log all actions taken and outcomes achieved     in resolving the alert.<ul> <li>GitHub: Use for detailed incident tracking and resolution. File a GitHub     issue or update an existing one with comprehensive details of the incident     management process, including what was done, what was discovered, and how     the issue was resolved or mitigated.</li> <li>Google Docs: Maintain a running document for each major incident for     real-time collaboration and information sharing during the incident     response.</li> <li>Slack For immediate communications and quick updates during an incident.</li> </ul> </li> <li>Communicate with Stakeholders: Update all pertinent parties, including     management and technical leads, about the incident's status and resolution.     Use our designated Slack channels for real-time updates and discussions during     active incident management. This allows for immediate feedback and quick     coordination among the involved teams.</li> <li>Conduct Reviews: Hold post-mortem meetings if necessary to discuss the     root cause and lessons learned. Provide updates during weekly sprint meetings     with tech leads. Use this time for a more detailed discussion of recent     incidents, resolved issues, and any lessons learned.</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#aws_rds_database_alarms","title":"AWS RDS Database Alarms","text":"<ul> <li>Immediate Expansion: If the database is critically low on storage (e.g.     90% of capacity), temporarily increase storage capacity to prevent service     disruption. Note that due to the nature of AWS RDS, any increase in storage     capacity is permanent as AWS does not support reducing allocated storage once     it is expanded. Therefore, it is crucial to carefully evaluate the necessity     of increasing storage.</li> <li>Data Management: Evaluate data retention policies and execute data     clean-up or archiving strategies to free up space. For example, evaluate     Kubernetes CronJobs or Airflow archiving tasks that automatically compress and     move older logs and data out of primary storage every month, ensuring system     efficiency and compliance with data policies.</li> <li>System Adjustment: Review and adjust the database configuration to     optimize storage utilization and performance.</li> <li>Long-term Storage Scaling: Plan for scalable storage solutions based on     the anticipated data growth and usage patterns.</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#airflow_alerts","title":"Airflow Alerts","text":"<ul> <li>Test Stage DAGs: Alerts prefixed with <code>test.</code> generally require no action     unless specified.</li> <li>Preprod Stage DAGs: Investigate alerts through the Airflow UI to pinpoint     the underlying issues, focusing on whether they arise from task failures,     development glitches, or infrastructure problems.<ul> <li>If related to task failures or development issues:<ul> <li>Contact the responsible individual (e.g., Sonaal for DataPull DAGs, Nina     for Trading DAGs).</li> <li>File a GitHub issue detailing the problem and assign it to the responsible     party. Use     Troubleshoot Airflow DagRun scheduling delay #8705     as an example template for how to document and file these issues.</li> </ul> </li> <li>If stemming from infrastructure issues (e.g., Pod failures):<ul> <li>Coordinate with Shayan for resolution and follow-up actions.</li> </ul> </li> </ul> </li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#refinement_of_alerts","title":"Refinement of Alerts","text":"<ul> <li>Continuously improve monitoring tools and alerting mechanisms to minimize     irrelevant alerts and enhance operational efficiency.</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#2_infrastructure_incident_management","title":"2. Infrastructure Incident Management","text":""},{"location":"work_organization/all.inframeister.how_to_guide.html#incident_response_protocol","title":"Incident Response Protocol","text":""},{"location":"work_organization/all.inframeister.how_to_guide.html#initial_assessment","title":"Initial Assessment","text":"<ul> <li>Identify the Alert: Quickly determine the scope and potential impact of     the alert.</li> <li>Prioritize the Incident: Classify the incident according to its severity     and potential impact on operations.</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#investigation_and_troubleshooting","title":"Investigation and Troubleshooting","text":"<ul> <li>Gather Data: Collect relevant logs, metrics, and configurations that     pertain to the affected systems.</li> <li>Analyze Symptoms: Correlate the collected data to identify patterns or     anomalies that could indicate the root cause.</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#resolution","title":"Resolution","text":"<ul> <li>Implement Fix: Apply the necessary fixes or workarounds to resolve the     incident.</li> <li>Monitor Outcomes: Closely monitor the system following the fix to ensure     stability and that the issue does not recur.</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#review_and_documentation","title":"Review and Documentation","text":"<ul> <li>Document the Incident: Update the issue with a detailed incident report     outlining the cause, response, and resolution.</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#filing_infrastructure_issues","title":"Filing Infrastructure Issues","text":"<ul> <li>File and Track Issues: Systematically file and track all     infrastructure-related issues under the dedicated EPIC     Infrastructure Alert and Failure Resolution #9721.     Use issue #9606 as an     example template for documenting and filing new issues to ensure consistency     and thoroughness.</li> <li>Incident Report: Ensure comprehensive report of each incident, including     its impact assessment, investigative steps, and resolution measures.</li> <li>Communication and Collaboration: Work with relevant technical leads to     develop and implement solutions.</li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#3_grafana_dashboard","title":"3. Grafana Dashboard","text":""},{"location":"work_organization/all.inframeister.how_to_guide.html#dashboard_reviews","title":"Dashboard Reviews","text":"<ul> <li>Regularly analyze Grafana dashboards to assess system performance and health.</li> <li>Identify trends, potential bottlenecks, or anomalies.</li> <li> <p>Suggest improvements or adjustments to dashboards to better reflect     operational status and metrics.</p> </li> <li> <p>Link to Grafana Dashboards: Access the Grafana dashboards directly through     this link.</p> </li> <li> <p>Link to AWS Dashboards: Review AWS dashboards for basic resource     utilization metrics through     this link.</p> </li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#4_cost_management","title":"4. Cost Management","text":""},{"location":"work_organization/all.inframeister.how_to_guide.html#aws_cost_monitoring","title":"AWS Cost Monitoring","text":"<ul> <li>Check AWS billing and usage every Monday and Friday for any anomalies or     unexpected charges. An anomaly is defined as any charge that exceeds a ~15%     increase compared to the average of the previous three months' spending on the     same service.</li> <li>Proactively suggest cost optimization strategies and implement approved     measures to manage cloud expenditure.</li> <li>AWS Cost Anomaly Detection: Utilize AWS Cost Anomaly Detection to     automatically monitor and alert on unexpected increases in spending. This tool     helps in identifying and addressing anomalies in AWS costs without manual     monitoring.<ul> <li>Regularly review alerts generated by AWS Cost Anomaly Detection which are     being sent to the <code>infra@causify.ai</code> e-mail address by the     <code>Alert subscription</code>.     The alerts are sent once a week if in that week there was a cost anomaly     which exceeded 15% and $120 above expected spend. Analyze any detected     anomalies in the <code>Overview</code> section of the     <code>Cost Anomaly Detection</code>     to determine their cause\u2014whether they are due to expected changes in usage     or inefficiencies that need correction.</li> </ul> </li> </ul>"},{"location":"work_organization/all.inframeister.how_to_guide.html#issue_reporting","title":"Issue Reporting","text":"<ul> <li>File sub-issues under the EPIC     AWS Cost Optimizations #8940     and within the Infra Project on GitHub for any significant deviations that     warrant further investigation or evaluation.</li> </ul>"},{"location":"work_organization/all.issue_workflow.explanation.html","title":"All.issue workflow.explanation","text":"<p>TODO(Grisha): consider merging the current doc with <code>/docs/work_organization/all.use_github.how_to_guide.md</code> into something like \"ck.planning.how_to_guide.md\".</p>"},{"location":"work_organization/all.issue_workflow.explanation.html#issue_workflow","title":"Issue workflow","text":"<pre><code>Idea -&gt; Project -&gt; Task (Issue)\n</code></pre>"},{"location":"work_organization/all.issue_workflow.explanation.html#idea","title":"Idea","text":"<p>An idea is often the starting point for new initiatives and can be abstract and unstructured.</p> <p>Examples:</p> <ul> <li>\"What if we expand our current trading universe?\"</li> <li>\"It would be nice to get historical bid/ask data\"</li> </ul> <p>We use Google Docs (Gdocs) to keep track of ideas.</p> <p>Example flow:</p> <ul> <li>You have an idea and you do not have time to think it through/execute at the     moment</li> <li>However, you would like to keep track of what could be done in the future</li> <li>You put a note in a Gdoc for your future self</li> <li>Use suggestion mode so that other people can track that (once in a while we     accept all the changes)</li> </ul> <p>This is helpful because:</p> <ul> <li>We keep track of work that could be done</li> <li>Simplifies the planning<ul> <li>E.g., if someone is blocked, he/she can go to a Gdoc and choose a project</li> </ul> </li> <li>People are notified of that change and see the text in suggestion mode</li> <li>It is quick to edit a Gdoc and has low friction</li> <li>Since an idea is abstract and unstructured it is easier to edit it later in a     Gdoc rather than in a GitHub issue</li> </ul>"},{"location":"work_organization/all.issue_workflow.explanation.html#project","title":"Project","text":"<p>Unlike ideas, projects have clear goals, defined scope, tangible outcomes and time constraints.</p> <p>We use GitHub Projects to track projects <code>/docs/work_organization/all.use_github.how_to_guide.md#projects</code>.</p> <p>To go from \"idea\" to \"project\" one needs to perform a through analysis:</p> <ul> <li>Identify a problem</li> <li>Suggest solutions for the problem</li> <li>Split the work into actionable tasks</li> <li>Estimate the complexity</li> </ul> <p>Team leaders are responsible for converting \"ideas\" into \"projects\". The outcome of the analysis is a GitHub Project with clear objectives, start/end dates, assignees and filed GitHub issues.</p>"},{"location":"work_organization/all.issue_workflow.explanation.html#task_issue","title":"Task (Issue)","text":"<p>A task is a specific, actionable item that needs to be completed, often as part of a project. Tasks are usually smaller in scope and more focused on single activities or steps within a project.</p> <p>We use GitHub Issues to track tasks. Every issue should belong to a GH Project <code>/docs/work_organization/all.use_github.how_to_guide.md#issue</code>.</p> <p>Last review: GP on 2024-05-25</p>"},{"location":"work_organization/all.organize_your_work.how_to_guide.html","title":"All.organize your work.how to guide","text":""},{"location":"work_organization/all.organize_your_work.how_to_guide.html#untitled","title":"Untitled","text":""},{"location":"work_organization/all.organize_your_work.how_to_guide.html#golden_rules","title":"Golden rules","text":"<ul> <li>Please <code>watch</code>, <code>star</code> and <code>fork</code> the repo.</li> <li>\"Remember to treat others the way you want to be treated\"</li> <li>Everybody comes from a different place and different skill-level, somebody has     a job, somebody has a full-time work</li> <li>Let's be polite, helpful, and supportive to each other</li> <li>I'll remind you an adage that I tell myself when my teammates drive me crazy:     \"If you want to go fast, go alone; if you want to go far, go together\"</li> <li>While being assigned a warm-up issue:<ul> <li>We highly value the importance of following instructions to ensure efficient     task completion. We intentionally select straightforward and simple issues     as the first tasks for you. So as your first step you should put primary     focus on following the rules. Failure to do so can result in wasting more of     our time than saving it.</li> <li>A pledge to put in the time you committed and not disappear</li> <li>If we don't find that you can meaningfully contribute, we will give you a     warning and help you improve, we will suggest you how to fill in your     knowledge gaps, and start not putting the time to help since we don't have     enough resources</li> <li>If we decide that you are not ready to contribute, we will drop you, without     discussions: this is our sole decision for the good of the project and the     other contributors</li> <li>You can study and try again in 6 months</li> </ul> </li> </ul>"},{"location":"work_organization/all.organize_your_work.how_to_guide.html#roles","title":"Roles","text":""},{"location":"work_organization/all.organize_your_work.how_to_guide.html#tech_leads","title":"Tech leads","text":"<ul> <li> <p>Tech leads are contributors that have put more effort in KaizenFlow and / or     have more experience in research and development</p> </li> <li> <p>The same project (e.g., Web3, Model, Arbitrage, NLP) can have multiple tech     leads</p> <ul> <li>One becomes a tech lead by technical excellence and dedication, and for no     other reason (e.g., seniority or degree)</li> </ul> </li> <li> <p>What tech leads do:</p> <ul> <li>Organize the work in their specific project together with GP / Paul and the     other full-time contributors</li> <li>Help young team members, e.g., answering their questions, showing how we do     things</li> <li>Answer other team members questions, even in different projects</li> <li>Following our process, advertising it, and explaining it to new team members</li> <li>Understand what other people in your team are doing and coordinate with them</li> <li>Make sure code, notebooks, and data are reused and shared</li> </ul> </li> <li> <p>What tech leads gain from being a tech lead:</p> <ul> <li>Learn how to run a research / dev team</li> <li>Practice leadership skills (which mainly entail technical excellence and     patience)</li> <li>Work closely with GP / Paul</li> <li>Material benefits (e.g., higher KaizenFlow bonuses once the reward system is     in place)</li> </ul> </li> </ul>"},{"location":"work_organization/all.organize_your_work.how_to_guide.html#developers","title":"Developers","text":"<ul> <li>People on teams working on the same project or on related projects can:<ol> <li>Work together on the same tasks (you can coordinate on GitHub, Slack, or  do a Zoom)</li> <li>Work together and split the work (still you need coordination)</li> <li>Each can \"replicate\" the work so that both of you understand (no  coordination)</li> </ol> </li> </ul>"},{"location":"work_organization/all.organize_your_work.how_to_guide.html#how_to_organize_your_research","title":"How to organize your research","text":"<ul> <li> <p>The GH issues are for:</p> <ul> <li>Reporting issues (e.g., \"I am getting this error. What should I do?\")</li> <li>Updates (e.g., \"I've made a bunch of progress and the details are in the     gdoc )</li> </ul> </li> <li> <p>Commit the code in a PR (even the same PR) so that it's easier to review the     details together</p> <ul> <li>You can say \"this is what I'm doing, what do you think?\"</li> <li>E.g., create a dir like 'SorrIssueXYZ_...' under     https://github.com/kaizen-ai/kaizenflow/tree/master/sorrentum_sandbox/examples/ml_projects</li> <li>E.g., see the PR under https://github.com/kaizen-ai/kaizenflow/pull/31</li> </ul> </li> <li> <p>Work in the Docker container so that you use the standard flow and it's easy     to collaborate</p> <ul> <li>It's ok if you want to use your different flow, but still try to commit     often</li> </ul> </li> <li> <p>Save all the files in     Project dir</p> <ul> <li>You can use Google Desktop to keep     the dir synced locally on your computer so when you work on your project the     data is always in sync with what others view</li> </ul> </li> <li> <p>Keep a detailed research log in a Google Doc in the     Project dir</p> <ul> <li>Document what you are doing, take screenshot, explain the results</li> <li>You can make progress only if you are organized and consistent</li> </ul> </li> <li> <p>Avoid emails any time possible</p> <ul> <li>Communication should happen on GitHub around specific Issues</li> <li>Read     General Rules of Collaboration     for more details</li> </ul> </li> <li> <p>Use Slack channel/DMs when you are blocked on something</p> <ul> <li>Try to use GitHub Issues when not urgent to provide context</li> <li>Don't abuse Slack, learning what is urgent and what's not</li> </ul> </li> <li> <p>On GitHub</p> <ul> <li>Provide context about the problem</li> <li>Explain clearly what is the problem and what you are trying to do</li> <li>Writing good bug updates is an art that takes time</li> </ul> </li> <li> <p>When you are done with your task and you need more work ping your project chat</p> </li> <li> <p>Work with your team (especially if you are a tech lead)</p> <ul> <li>Answer other people questions</li> <li>Understand what other people in your team are doing</li> <li>Coordinate with them</li> <li>We can only make progress together as a team and not as a single person (get     over being shy)</li> </ul> </li> </ul>"},{"location":"work_organization/all.organize_your_work.how_to_guide.html#about_the_project_you_choose","title":"About the project you choose","text":"<ul> <li>We let you pick the project you like because we believe that one needs to be     excited about something to work on that through the late night, on Sat and     Sunday</li> <li> <p>On the other side, in 15 years of leading, mentoring, hiring, and firing     people I've never found one person not complaining about their project being     \"boring\"</p> <ul> <li>People working on exploratory analysis want to do machine learning</li> <li>People want to do \"machine learning\" when 95% of the machine learning work     is building data pipelines and doing exploratory analysis</li> <li>People running models complain about that being uninspiring and would rather     to do exploratory analysis</li> <li>People writing code want to do machine learning (since that's the cool     stuff!)</li> <li>Software engineers would like to be lawyers (since they want to interact     with people and computers), and lawyers want to be engineers (since they     can't take dealing with people anymore)</li> </ul> </li> <li> <p>The problem is that everything is interesting and boring at the same time. It     depends on your mindset</p> <ul> <li>If you say \"this is boring\", it will be boring for sure</li> <li>If you look for finding interesting pieces in what you need to do, it will     become suddenly super interesting</li> <li>The people that succeed are the ones that do their best at their job no     matter what, that find joy in solving whatever problem brings value</li> </ul> </li> </ul>"},{"location":"work_organization/all.rollout.how_to_guide.html","title":"All.rollout.how to guide","text":""},{"location":"work_organization/all.rollout.how_to_guide.html#rollout","title":"Rollout","text":""},{"location":"work_organization/all.rollout.how_to_guide.html#roll-out_process","title":"Roll-out process","text":"<ul> <li>Implement</li> <li>Prepare documentation</li> <li>Dogfood by RPs or subset of target audience</li> <li>Initial roll-out</li> <li>Full rollout: Distribute it your team</li> <li> <p>Deprecate the old system</p> <ul> <li>The deadline is blah: do it!</li> <li>Shut down from old</li> </ul> </li> <li> <p>File an Issue with the content of the email</p> <ul> <li>The assignee is the person in charge of making sure the rollout is done</li> </ul> </li> <li>Send an ORG email with the same content of the Issue</li> </ul>"},{"location":"work_organization/all.rollout.how_to_guide.html#roll-out_documentation","title":"Roll-out documentation","text":"<ul> <li>A roll-out should address the following points:<ul> <li>Short summary</li> <li>Who is the intended audience</li> <li>What you need to do</li> <li>Where is the reference documentation</li> <li>What has changed</li> <li>Why is it important</li> <li>Whom to ask for help</li> </ul> </li> </ul>"},{"location":"work_organization/all.rollout.how_to_guide.html#an_example_of_roll-out_email","title":"An example of roll-out email","text":"<pre><code>Hello team,\n\n### Intended audience\n\nAnybody using Jupyter notebooks\n\n### What it is about\n\n- `publish_notebook.py` is a little tool that allows to:\n    1. Opening a notebook in your browser (useful for read-only mode)\n     - E.g., without having to use Jupyter notebook (which modifies the file in\n       your client) or github preview (which is slow or fails when the notebook\n       is too large)\n    2. Sharing a notebook with others in a simple way\n    3. Pointing to detailed documentation in your analysis Google docs\n    4. Reviewing someone's notebook\n    5. Comparing multiple notebooks against each other in different browser\n     windows\n    6. Taking a snapshot / checkpoint of a notebook as a backup or before making\n     changes\n     - This is a lightweight alternative to \"unit testing\" to capture the\n       desired behavior of a notebook\n     - One can take a snapshot and visually compare multiple notebooks\n       side-by-side for changes\n\nYou can get details by running: `dev_scripts/notebooks/publish_notebook.py -h`\n\n### What you need to do\n\nPlease update your branches from the `master` for all the submodules.\n\nYou can use our shortcut:\n\n&gt; make git_pull\n\n### What has changed\n\nWe've deployed the new service for storing notebooks in HTML format. From now on\n`publish_notebook.py` will work from the Docker container. The new version of\n`publish_notebook.py` works using HTTP protocol and does not require ssh key\nauthorization as it was before We've synchronized all documents. So all old docs\nalready available on the new service The old links http://research:8077/...\nwon't work from now on, we need to replace them with the new ones\n(http://notebook-keeper.p1/...) If you see any link starts with\nhttp://research:8077 replace them with http://notebook-keeper.p1 .\n\n### Reference documentation\n//amp/docs/coding/all.publish_notebook.how_to_guide.md\n</code></pre>"},{"location":"work_organization/all.scrum.explanation.html","title":"All.scrum.explanation","text":""},{"location":"work_organization/all.scrum.explanation.html#refs","title":"Refs","text":"<ul> <li>From \"Lacey, The Scrum Field Guide: Practical Advice for Your First Year,     2012\"</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#roles","title":"Roles","text":""},{"location":"work_organization/all.scrum.explanation.html#goal_of_scrum_methodology","title":"Goal of Scrum methodology","text":"<ul> <li>Work in the interests of customers and stakeholders to turn the vision into a     working product</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#metaphor_for_the_roles_in_terms_of_a_race_car","title":"Metaphor for the roles in terms of a race car","text":"<ul> <li> <p>ProductOwner = driver</p> </li> <li> <p>DevTeam = engine</p> </li> <li> <p>ScrumMaster = lubricants and sensors</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#scrummaster","title":"ScrumMaster","text":"<ul> <li> <p>Identify when the team is not performing to its ability</p> </li> <li> <p>Assist in correcting the issues</p> </li> <li> <p>Notice non-verbal cues</p> </li> <li> <p>Is comfortable with conflict</p> </li> <li> <p>Can build trust and earn respect</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#productowner","title":"ProductOwner","text":"<ul> <li> <p>Represent the customers</p> </li> <li> <p>Point the car in the correct direction</p> </li> <li> <p>Adjust the car direction to stay on course</p> </li> <li> <p>Make decisions about official release</p> </li> <li> <p>Ultimately he is responsible for success or failure of the projects</p> </li> <li> <p>Decide:</p> <ul> <li>What is developed</li> <li>When it is developed</li> <li>Whether the product meets expectations</li> </ul> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#devteam","title":"DevTeam","text":"<ul> <li> <p>Aka Team, Development team, Core team</p> </li> <li> <p>Developers, testers, architects, designers</p> <ul> <li>Cross-functionality is a good thing</li> </ul> </li> <li> <p>The ideal team size is 6 plus / minus 2</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#artifacts","title":"Artifacts","text":""},{"location":"work_organization/all.scrum.explanation.html#product_backlog","title":"Product backlog","text":"<ul> <li> <p>= master list of all features and functionalities needed to implement the     vision into the product</p> </li> <li> <p>The ProductOwner keeps the backlog:</p> <ul> <li>Prioritized</li> <li>Up to date</li> <li>Clear</li> </ul> </li> <li> <p>The backlog is never complete:</p> <ul> <li>Items are added and removed</li> <li>Reordered based on priority, value, or risk</li> </ul> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#product_backlog_items","title":"Product backlog items","text":"<ul> <li> <p>Aka PBI</p> </li> <li> <p>E.g., bugs, features, enhancements, non-functional requirements</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#complexity_of_pbi","title":"Complexity of PBI","text":"<ul> <li> <p>ProductOwner and the DevTeam estimate the size of each task</p> </li> <li> <p>The complexity of each task can be expressed in different ways:</p> <ul> <li>Points</li> <li>T-shirt size (S, M, L, XL)</li> </ul> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#high-priority_vs_lower-priority_tasks","title":"High-priority vs lower-priority tasks","text":"<ul> <li> <p>High-priority stories should be small and clear</p> <ul> <li>So they can be brought into the sprint</li> </ul> </li> <li> <p>Lower-priority items can be large and fuzzy</p> <ul> <li>Bigger stories are decomposed into smaller chunks</li> </ul> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#sprint_backlog","title":"Sprint backlog","text":"<ul> <li> <p>= output of the planning meeting</p> </li> <li> <p>List of tasks that need to complete during the sprint</p> </li> <li> <p>Sprint backlog tasks have an estimate in hours</p> </li> <li> <p>The DevTeam keeps the sprint backlog up to date</p> </li> <li> <p>During a sprint</p> <ul> <li>New tasks are discovered</li> <li>Tasks are adjusted (in terms of description or estimated hours)</li> <li>Tasks are marked as done</li> </ul> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#the_burndown","title":"The burndown","text":"<ul> <li> <p>Communicate how much work is remaining and what is the team velocity</p> </li> <li> <p>It is updated at the end of each day</p> </li> <li> <p>Plot the number of hours remaining (y-axis) against the number of days     remaining (x-axis)</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#the_meetings","title":"The meetings","text":""},{"location":"work_organization/all.scrum.explanation.html#planning_meeting","title":"Planning meeting","text":"<ul> <li>Each sprint begins with a sprint planning attended by the team, ScrumMaster,     ProductOwner<ul> <li>Typically one needs two hours per number of weeks to plan the sprint</li> <li>For a 1-month sprint, 8 hours of meeting</li> <li>For 2-week sprint, 4 hours of meeting</li> </ul> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#part_one_of_sprint_planning_meeting","title":"Part one of sprint planning meeting","text":"<ul> <li> <p>Review of potential product backlog items for the sprint</p> </li> <li> <p>ProductOwner describes what the goal of the meeting is</p> </li> <li> <p>DevTeam asks questions to drive away ambiguity</p> </li> <li> <p>Outcome is one-sentence description of the desired outcome of the sprint</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#part_two_of_sprint_planning_meeting","title":"Part two of sprint planning meeting","text":"<ul> <li> <p>Many DevTeams discuss how to implement the tasks</p> </li> <li> <p>The ProductOwner doesn't need to be present</p> </li> <li> <p>The ScrumMaster can be present facilitating the process</p> </li> <li> <p>The DevTeam discusses and decides the implementation of the tasks</p> </li> <li> <p>Decompose backlog items into work tasks</p> </li> <li> <p>Estimate tasks in terms of hours</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#daily_scrum","title":"Daily scrum","text":"<ul> <li> <p>Aka daily stand-up</p> </li> <li> <p>Give the DevTeam the opportunity to sync daily, at the same time, and at the     same place</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#daily_scrum_questions","title":"Daily scrum: questions","text":"<ul> <li>The 3 most frequent questions are:<ul> <li>What have you accomplished since the last meeting?</li> <li>What will you accomplish today?</li> <li>What obstacles are in your way?</li> </ul> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#what_the_daily_scrum_is_not","title":"What the daily scrum is not","text":"<ul> <li> <p>The daily scrum is not a deep-dive problem-solving meeting</p> <ul> <li>Any other issues need to be taken offline</li> </ul> </li> <li> <p>It is not a status report meeting to the ScrumMaster</p> <ul> <li>The purpose is for the DevTeam members to talk to each other</li> </ul> </li> <li> <p>The ProductOwner is in \"listen-only\" mode</p> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#sprint_review","title":"Sprint review","text":"<ul> <li> <p>On the last day of the sprint, the DevTeam holds a sprint review</p> </li> <li> <p>Everybody should join</p> <ul> <li>ScrumMaster</li> <li>ProductOwner</li> <li>DevTeam</li> <li>Customers, key stakeholders</li> <li>Executives</li> </ul> </li> <li> <p>DevTeam</p> <ul> <li>Recaps the goal of the sprint</li> <li>Presents the work done</li> </ul> </li> <li> <p>Customers</p> <ul> <li>Review the progress made on the project</li> <li>Accept changes</li> <li>Ask for changes</li> </ul> </li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#sprint_retrospective","title":"Sprint retrospective","text":"<ul> <li>After the sprint review, the retrospective is a way to identify how to improve     process and execution</li> </ul>"},{"location":"work_organization/all.scrum.explanation.html#sprint_retrospective_questions","title":"Sprint retrospective: questions","text":"<ul> <li> <p>What went well during the sprint?</p> </li> <li> <p>What could be improved in the next sprint?</p> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html","title":"All.team collaboration.how to guide","text":""},{"location":"work_organization/all.team_collaboration.how_to_guide.html#general_rules_of_collaboration","title":"General Rules of Collaboration","text":"<ul> <li>We are not going to discuss and debate the rationale behind the following     suggestions, but we assume that all is self-evident truth</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#ask_somebody_if_you_have_any_doubts","title":"Ask somebody if you have any doubts","text":"<ul> <li>If you have doubts on how to do something you want to do:<ul> <li>Look in the documentation and our Google drive folders     Process     and Tech</li> <li>Try Google search/ChatGPT</li> </ul> </li> <li>If you still can't find the answer, ask your team members<ul> <li>Learn     how to ask questions     first</li> <li>Note that often people tell you his / her interpretation or their workaround     for a problem, which might not be the best approach, so be careful and     always think for yourself</li> <li>Don't hesitate to ask anyone, including GP &amp; Paul</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#ping_team_leaders_when_you_are_out_of_tasks","title":"Ping Team Leaders when you are out of tasks","text":"<ul> <li>When you're close to being out of tasks or all your ongoing PRs are waiting     for review and are close to being merged, feel free to ping us in the IM and     ask for more issues<ul> <li>IM = Telegram for interns / Slack for permanent team members</li> </ul> </li> <li>In this way, Team Leaders can quickly assign you another issue, before you run     out of work</li> <li>Everyone should have 2 issues to work on at the same time to avoid getting     blocked on others</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#collaboration_for_problem_solving","title":"Collaboration for problem solving","text":"<ul> <li>Identify a problem and describe it in the issue</li> <li>Try your best to find a solution independently and outline the design in the     issue</li> <li>If in doubt, seek guidance from a more experienced team member</li> <li>Communicate with the team and reach consensus on the solution</li> <li>NB:<ul> <li>Not every step requires explicit approval. In our team members, we value     autonomy and ability to get things done</li> <li>Keep our usual conventions and workflows in mind. For example, usually there     is no need to ask \"Do we need to add unit tests for this?\" or \"Do we also     want this documented?\" - the answer is almost always \"YES\", so just doing it     saves time, shows proactivity and ability to follow processes</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#collaboration","title":"Collaboration","text":""},{"location":"work_organization/all.team_collaboration.how_to_guide.html#why_do_we_need_to_follow_this_handbook","title":"Why do we need to follow this handbook?","text":""},{"location":"work_organization/all.team_collaboration.how_to_guide.html#learning_from_each_other","title":"Learning from each other","text":"<ul> <li>Proper research and software engineering practices allow us to:<ul> <li>Learn from each other</li> <li>Accumulate and distill the wisdom of experts</li> <li>Share lessons learned from our mistakes along the way</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#consistency_and_process","title":"Consistency and process","text":"<ul> <li>Consistency is a crucial enabler to make teams faster</li> <li>Productivity increases when team members \"work in the same way\", i.e., there     is a single official way of performing a task, so that it's possible and easy     to:<ul> <li>Re-use research and software components</li> <li>Help each other in debugging issues</li> <li>Add/transfer new people to teams</li> <li>Work on multiple projects simultaneously</li> <li>Learn from each other's experience and mistakes</li> <li>Review each other's work looking for errors and improvements</li> <li>...</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#sync-ups","title":"Sync-ups","text":"<ul> <li>We meet regularly every week and with different audiences to check on the     progress of the projects we work on</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#company_town-hall_meetings","title":"Company town-hall meetings","text":"<ul> <li>They usually happen on the first Monday of each month and have the following     goals:<ul> <li>Summarize ongoing projects and their status<ul> <li>Share the achieved milestones</li> <li>Discuss blocking tasks across projects</li> </ul> </li> <li>Discuss topics of general interest<ul> <li>E.g., organization, process</li> </ul> </li> <li>Talk about the team, hiring, customers</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#technical_sync-ups","title":"Technical sync-ups","text":"<ul> <li>We meet one or two times per week for each of the projects (e.g., IM, WEB3)</li> <li>Please check your calendar to make sure the times work and the invited     participants are correct</li> <li>The people running the day-to-day project should update the meeting agenda in     the Gdoc<ul> <li>Try to do it one day before so that everybody knows ahead of time what we     need to talk about and can come prepared</li> </ul> </li> <li>Typically 2-3 issues are more than enough to fill one hour of discussion<ul> <li>Give priority to tasks that are controversial, blocking, or finished</li> <li>No reason to linger on the successes or the easy stuff</li> </ul> </li> <li>Send an email or tag a comment to Gdocs to broadcast the agenda</li> <li>It's ok to skip a meeting when the agenda is empty, or keep it short when     there is not much to discuss<ul> <li>We don't have to fill one hour every time</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#ad-hoc_meetings","title":"Ad-hoc meetings","text":"<ul> <li>Don't hesitate to ask for a quick meeting if you are unsure about:<ul> <li>What exactly needs to be done in a GitHub Issue</li> <li>How to set-up something (e.g., environment, docker)</li> </ul> </li> <li>Better safe than sorry</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#org_emails","title":"Org emails","text":"<ul> <li>GP &amp; Paul may send emails with the subject starting with \"ORG:\" pointing to     interesting docs that are of general interest and relevance</li> <li>Please make sure to read the docs carefully and internalize what we suggest to     do and, especially, the rationale of the proposed solutions</li> <li>It's ok to acknowledge the email replying to <code>eng@causify.ai</code></li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#synchronization_point","title":"Synchronization point","text":"<ul> <li>We understand that most of the time everybody is head-down making progress on     their tasks<ul> <li>This is great!</li> </ul> </li> <li>However, sometimes we need synchronization:<ul> <li>We need to stop the progress for a bit when requested</li> <li>Do an urgent task</li> <li>Acknowledge that the task is done</li> <li>Go back to pushing</li> </ul> </li> <li>The procedure is:<ul> <li>One of us (e.g., GP or Paul) creates a GitHub task, with:<ul> <li>Detailed instructions</li> <li>The list of all of the persons in charge of executing the task</li> </ul> </li> <li>Send a ping with the link on Slack if the task is urgent</li> <li>Everybody does what's asked</li> <li>Mark on the GitHub task your name</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#morning_todo_email","title":"Morning TODO email","text":"<p>The idea is to send a morning TODO email to broadcast:</p> <ul> <li>Issues you will be working on</li> <li>Working hours</li> <li> <p>Blocking issues/PRs</p> </li> <li> <p>E-mail template:     ```     To: eng@causify.ai</p> <p>Subject: TODO</p> <p>Hi all,</p> <p>Today I am going to work</p> <p>Hours: - ...</p> <p>Issues: - GitHub issue title and number with a hyperlink  - ETA: YYYY-MM-DD - GitHub issue title and number with a hyperlink  - Original ETA: YYYY-MM-DD  - ETA: YYYY-MM-DD  - Reason for ETA update: ...  - Blocked on: ... (ok to omit if not blocked) ```</p> </li> <li> <p>Good example:     ```     To: eng@causify.ai</p> <p>Subject: TODO</p> <p>Hi all,</p> <p>Today I am going to work</p> <p>Hours: - 8</p> <p>Issues: - Create matching service #261         - ETA: today (2023-05-25)         - Original ETA: yesterday (2023-05-24)         - Reason for ETA update: it was more complex than what we thought         - Blocked on: PR review from Grisha - Create linear supply/demand curves #177         - ETA: tomorrow (2023-05-26) - Unit test <code>compute_share_prices_and_slippage()</code> #8482         - ETA: 2024-06-19 ```</p> </li> <li> <p>Bad example:     ```     To: eng@causify.ai</p> <p>Subject: to-do</p> <p>Hi all,</p> <p>Today I am going to work 2-6 hours. - Calls - PR reviews - Finish the trading report analysis ```</p> </li> <li> <p>The goal is:</p> <ul> <li>Think about what you are going to work on for the day, so you have a clear     plan</li> <li>Let Team Leaders know that you're going to work today and what is your     workload</li> <li>Make sure people blocked on your tasks know that / whether you are working     on those tasks</li> <li>Broadcast if you are blocked or if you don't have tasks</li> <li>A TODO email replaces stand-up meetings</li> </ul> </li> <li>When to send an email:<ul> <li>Within the first hour of beginning the day<ul> <li>There is no hard deadline, given that we have people working from     different time zones. Doing it first thing in the morning ensures one     doesn't forget</li> </ul> </li> <li>It is recommended to check the inbox (email, GitHub) first to reflect all     the update in the plan for the day</li> </ul> </li> <li>Send an email to:<ul> <li>For permanent team members/people on a 3-month trial: <code>eng@causify.ai</code></li> <li>For interns or collaborators: <code>contributors@causify.ai</code></li> </ul> </li> <li>Email subject:<ul> <li><code>TODO</code> (all caps)</li> <li>It makes it easier to filter emails by subject</li> </ul> </li> <li>Reply to your previous TODO email so that at least last week of TODOs is     included</li> <li>Specify how many hours are you going to work today<ul> <li>On Monday we also specify an estimation for a week</li> <li>No need to be too specific, give just an approximation</li> </ul> </li> <li>List all the tasks you're going to work during the day in priority order<ul> <li>Add a hyperlink to the corresponding GitHub issue to each task in order to     ease the navigation </li> <li>For each task provide an ETA<ul> <li>You always estimate the ETA yourself, based on the task's difficulty,     priority and your workload</li> <li>No reason to be optimistic: complex things take time to be done correctly</li> <li>Use a date in the YYYY-MM-DD format, e.g. 2023-05-20</li> <li>When possible, add \"today\", \"yesterday\", \"tomorrow\", \"end of week\" so that     it's easier to parse<ul> <li>\"End of week\" generally means Friday, although it can be extended if you     are planning to work on the weekend (provide the date to clarify)</li> <li>If nothing is applicable, it's okay to just provide the date</li> </ul> </li> <li>If the originally set ETA has not been changed yet, omit \"Original ETA\"     and \"Reason for ETA update\"</li> <li>If your original ETA needs to be updated (e.g., you thought that you would     have finished a task by yesterday, but it's taking longer), keep the older     ETA (as \"Original ETA\") and add the new one (as \"ETA\")</li> </ul> </li> </ul> </li> <li>Report the PR/issue blocked</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#communication","title":"Communication","text":""},{"location":"work_organization/all.team_collaboration.how_to_guide.html#use_the_right_form_of_communication","title":"Use the right form of communication","text":"<ul> <li>GitHub<ul> <li>This is a major form of communication about technical details, so if you     have any questions about any particular issue or PR, discuss it there first,     e.g.:<ul> <li>Clarify issue specs</li> <li>Ask for help with debugging</li> <li>PR reviews</li> </ul> </li> </ul> </li> <li>Asana<ul> <li>Is concerned with all the non-technical issues in general, e.g.:<ul> <li>Work organization tasks</li> <li>Marketing and funding</li> <li>On-boarding process</li> </ul> </li> </ul> </li> <li>Slack<ul> <li>This is our messenger for tight interaction (like a debug session) or     immediacy (e.g., \"are you ready for the sync up?\")</li> <li>Please, avoid discussing anything that can be discussed at GitHub or Asana<ul> <li>You often need to reference some comments and ideas in other places like     issues or messages and it is impossible to reference a Slack message     outside of it</li> <li>It is much easier for all reviewers to catch up with all the thinking     process if it is logged at one place - Slack is never a place for this</li> </ul> </li> </ul> </li> <li>Jupyter notebooks<ul> <li>Generally used to implement and describe research in detail<ul> <li><code>Master</code> notebooks are intended to be used as tools for demonstrative     analysis with visible stats and plots</li> </ul> </li> </ul> </li> <li>Markdown files<ul> <li>Document instructions, process, design closely related to code</li> <li>Notes that need to be close to the code itself</li> <li>Documents that need to be authoritative and long-term (e.g., reviewed,     tracked carefully)</li> </ul> </li> <li>Google docs<ul> <li>Document research in a descriptive way</li> <li>Explain what are the results independently on how they were reached</li> </ul> </li> <li>Emails<ul> <li><code>TODO</code>s primarily</li> <li>Rarely used for any other purpose</li> <li>Exceptions are to send non-urgent information to everybody</li> </ul> </li> <li>There should be little replication among these forms of documentation<ul> <li>It's not ok to file a bug and then ping on Slack unless it's urgent</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#dry_also_applies_to_documentation","title":"DRY also applies to documentation","text":"<ul> <li>DRY! Do not Repeat     Yourself</li> <li>E.g., it's not a good idea to cut &amp; paste pieces of Gdocs in a GitHub bug,     rather just point to the relevant session on Gdocs from the GitHub bug</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#avoid_write-once_code_and_research","title":"Avoid write-once code and research","text":"<ul> <li>Code and research is:<ul> <li>Written once by a few people</li> <li>Read many times by many people</li> </ul> </li> <li>Therefore it is essential to invest in the process of writing it heavily</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#consistency","title":"Consistency","text":"<ul> <li>Coding/research across our group is done with consistent procedures, code     layout, and naming conventions</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#training_period","title":"Training period","text":"<ul> <li>When you start working with us, you need to go through a period of training in     following the procedures and conventions described in this handbook</li> <li>We understand that this is a painful process for you:<ul> <li>You need to change your old habits for new habits that you might disagree     with, or not comprehend</li> <li>You need to rework your code/notebooks that are already correctly working     until it adheres to the new conventions</li> </ul> </li> <li>Understand that this is also a painful process for the reviewers:<ul> <li>On top of their usual workload, they need to:<ul> <li>Invest time to explain to you how we do things</li> <li>Answer your questions</li> <li>Try to convey the sense of why these procedures are important</li> </ul> </li> </ul> </li> <li>In a few words, nobody enjoys this process, and yet it is necessary,     mandatory, and even beneficial</li> <li>The process acquaintance can take several days if you are open and patient,     but months if you resist or treat it as an afterthought<ul> <li>Our suggestion is to accept these rules as the existence of gravity</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#go_slowly_to_go_faster","title":"Go slowly to go faster","text":"<ul> <li>Once you reach proficiency, you will be moving much faster and make up for the     invested time<ul> <li>In fact, everyone will be much quicker, because everyone will be able to     look at any part of the codebase or any notebook and get oriented quickly</li> </ul> </li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#vacationsooto_time","title":"Vacations/OOTO time","text":"<ul> <li>We use     vacation calendar     to announce time off</li> <li>If you are a part of @all mailing group you should be able to access this     calendar with your company email</li> <li>Create an event in it, whenever you have planned time off in order to let your     colleagues know in advance</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#improve_your_english","title":"Improve your English!","text":"<ul> <li>Make sure you have English checker in all your tools:<ul> <li>Pycharm: you can use     this plugin</li> <li>Vim: <code>set spell</code></li> <li>Google docs: Grammarly</li> <li>GitHub and web: Grammarly</li> <li>Email client: TBD</li> </ul> </li> <li>These tools are going to help you improve since you can see the mistake as you     go</li> <li>Feel free to use Google Translate when you     are not sure about a word or a phrase</li> <li>What's the point of doing an excellent job if you can't communicate it?</li> </ul>"},{"location":"work_organization/all.team_collaboration.how_to_guide.html#study_an_english_grammar_book","title":"Study an English grammar book","text":"<ul> <li>I used     this     when I learned English (late in life at 25 starting from     no-English-whatsoever, so you can do it too)</li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html","title":"All.use github.how to guide","text":""},{"location":"work_organization/all.use_github.how_to_guide.html#use_github","title":"Use Github","text":""},{"location":"work_organization/all.use_github.how_to_guide.html#introduction","title":"Introduction","text":"<ul> <li>In the following we use the abbreviations below:<ul> <li>GH = GitHub</li> <li>PR = Pull Request</li> <li>RP = Responsible party (aka Team Leader)</li> </ul> </li> <li>Everything we work on comes in the form of a GH Issues<ul> <li>We call GH Issues \"issues\", and \"tasks\", (sometimes \"tickets\")     interchangeably</li> <li>We avoid to call them bugs since many times we use GH to track ideas,     activities, and improvements, and not only defects in the code</li> <li>We file tasks, prioritize them, and distribute the workload across the team</li> <li>We try to always work on high priority (aka, P0) tasks</li> </ul> </li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#concepts","title":"Concepts","text":""},{"location":"work_organization/all.use_github.how_to_guide.html#sprints","title":"Sprints","text":"<ul> <li>Sprints are weekly, Monday - Friday and consist of the Issues worked on during     the week</li> <li>Sprints help us answer the questions<ul> <li>What work should the team be focusing during this week?</li> <li>What did the team achieve last week?</li> </ul> </li> <li>Anything worked on during a week is added to that week's Sprint</li> <li>Issues added to a Sprint but not worked on or completed during the week should     not be removed (Issues can belong to more than one Sprint, and not removing     helps measure how \"overloaded\" a Sprint was)</li> <li>Each week's Sprint has Issues added to it by Team Leaders before Monday's work     begins</li> <li>Every Issue in a Sprint should be linked to a GH project</li> <li>To support adaptability and flexibility, Issues may be added to a Sprint     mid-week (but Issues should not be removed). While this may require     sacrificing other Issues in the Sprint, the point is to make the trade-off     apparent</li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#projects","title":"Projects","text":"<ul> <li>GitHub Projects are thematic groups of Issues that are somehow related by     their topic<ul> <li>It may take multiple Sprints to complete all the Issues in an Project</li> <li>Most Projects are created around software components or user workflows     (which may cross software components)</li> </ul> </li> <li>For more information on GH Projects, see     all.github_projects_process.reference.md</li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#issue","title":"Issue","text":"<ul> <li>Each Issue is a piece of work to be done</li> <li>Issues are combined into Projects by topic</li> <li>An Issue has certain characteristics, i.e. labels</li> <li>An Issue has a progress status, (e.g., <code>Todo</code>, <code>In progress</code>, <code>Done</code>)</li> <li>PRs are linked to work needed to complete an Issue</li> <li>An issue might not have an assignee but before execution of course it needs to     be resolved</li> <li>An issue may be labeled as \"Epic\" if it has associated sub-issues.<ul> <li>However, this is not a standard in our workflow.</li> </ul> </li> <li>See also     all.issue_workflow.explanation.md     for the description of the concept of Issues as opposed to Ideas/Projects.</li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#gh_actions","title":"GH Actions","text":"<ul> <li>GitHub Actions are used to automate tasks like running tests, checking     coverage, and deploying artifacts across our repositories</li> <li>Each workflow is defined in <code>.yml</code> files located under <code>.github/workflows/</code> in     each repo</li> <li>These actions run automatically based on defined triggers such as pushes, pull     requests, or scheduled events</li> </ul> <p>Disabling a GitHub Actions Workflow</p> <p>To disable a particular GitHub Actions workflow without deleting or commenting out the code:</p> <ul> <li>Rename the file so that GitHub no longer recognizes it as a valid workflow:     <code>bash     mv .github/workflows/ci.yml .github/workflows/ci.yml.DISABLED</code></li> <li>This prevents GitHub from executing the workflow, while preserving the file     for future use or reference</li> <li>We follow this convention consistently across repos to make it visually     obvious when a workflow is intentionally deactivated</li> <li> <p>Note: Only files ending in <code>.yml</code> or <code>.yaml</code> are treated as valid workflows by     GitHub</p> <p>For example, read about of our repos with workflows: - Helpers:     all.gitleaks_workflow.explanation.md</p> </li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#label","title":"Label","text":"<ul> <li>Labels are attributes of an issue (or PR), e.g., <code>good first issue</code>,     <code>PR_for_reviewers</code>, <code>duplicate</code>, etc.</li> <li>See the current list of labels and their descriptions at     Git Issue Labels<ul> <li>The repos should always have labels in sync</li> </ul> </li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#list_of_labels","title":"List of Labels","text":"<ul> <li><code>Blocking</code>: This issue needs to be worked on immediately</li> <li><code>Bug</code>: Something isn't working</li> <li><code>Cleanup</code></li> <li><code>Design</code></li> <li><code>Documentation</code>: Improvements or additions to documentation</li> <li><code>Enhancement</code>: New feature or request</li> <li><code>Epic</code>: A high-level issue, possibly encompassing sub-issues</li> <li><code>good first issue</code>: Good for newcomers     &gt; TODO(gp): <code>Good_first_issue</code></li> <li><code>Outsource</code>: Anybody can do it</li> <li><code>P0</code>: Issue with a high priority</li> <li><code>P1</code>: Issue is important but has less priority</li> <li><code>P2</code>: Issue which is not urgent and has the lowest priority</li> <li><code>Paused</code>: An issue was started and then stopped</li> <li><code>PR_for_authors</code>: The PR needs authors to make changes</li> <li><code>PR_for_reviewers</code>: The PR needs to be reviewed by team leaders     &gt; TODO(gp): -&gt; <code>PR_for_team_leaders</code></li> <li><code>PR_for_integrators</code>: The PR needs to be reviewed by Integrators and possibly     merged</li> <li><code>Readings</code>: Reading a book, article and getting familiar with code</li> <li><code>To close</code>: An issue can be potentially closed     &gt; TODO(gp): -&gt; To_close</li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#status","title":"Status","text":"<ul> <li>Each Issue in a Project is assigned a progress status</li> <li>We have the following status options:<ul> <li><code>Todo</code><ul> <li>Issues that are new, where the work has not started yet</li> <li><code>assignee</code> should be the tech lead of that area</li> </ul> </li> <li><code>In Progress</code><ul> <li>Issues that we are currently being worked on</li> </ul> </li> <li><code>In Review</code><ul> <li>Issues opened for review and testing</li> <li>Code is ready to be merged pending feedback</li> </ul> </li> <li><code>PR back to author</code><ul> <li>Feedback has been given on a PR, awaiting updates from the author</li> </ul> </li> <li><code>QA</code></li> <li><code>Done</code><ul> <li>Issues that are done and are waiting for closing</li> <li>Integrators / team leaders are responsible for closing</li> </ul> </li> </ul> </li> </ul> <pre><code>stateDiagram\n        [*] --&gt; Todo\n        Todo --&gt; In_Progress\n        In_Progress --&gt; In_Review\n        In_Review --&gt; PR_back_to_author\n        PR_back_to_author --&gt; In_Review\n        In_Review --&gt; QA\n        QA --&gt; Done\n        Done --&gt; [*]\n</code></pre>"},{"location":"work_organization/all.use_github.how_to_guide.html#pr","title":"PR","text":"<ul> <li>A pull request (PR) is an event where a contributor asks to review code they     want to merge into a project</li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#issue_workflows","title":"Issue Workflows","text":""},{"location":"work_organization/all.use_github.how_to_guide.html#naming_an_issue","title":"Naming an Issue","text":"<ul> <li>Use an informative description, typically in the form of an action<ul> <li>E.g., \"Do this and that\"</li> </ul> </li> <li>Do not put too many details in the title making it too long<ul> <li>E.g., the full path to the file where the changes should be made is better     suited for the starting post, not the title</li> </ul> </li> <li>We don't use a period at the end of the title</li> <li> <p>We prefer to avoid too much capitalization to make the Issue title easy to     read and for consistency with the rest of the bugs</p> <p>Good</p> <p><code>text Optimize Prometheus configuration for enhanced Kubernetes monitoring</code></p> <p>Bad</p> <p><code>text Optimize Prometheus Configuration for Enhanced Kubernetes Monitoring</code></p> </li> <li> <p>They are equivalent, but the first one is more readable</p> </li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#filing_a_new_issue","title":"Filing a New Issue","text":"<ul> <li>If it is a \"serious\" problem (bug) put as much information about the Issue as     possible, e.g.,:<ul> <li>What you are trying to achieve</li> <li>Command line you ran, e.g.,     <code>bash     &gt; i lint -f defi/tulip/test/test_dao_cross_sol.py</code></li> <li>Copy-paste the error and the stack trace from the command line, no     screenshots, e.g.,     <code>bash     Traceback (most recent call last):         File \"/venv/bin/invoke\", line 8, in &lt;module&gt;             sys.exit(program.run())         File \"/venv/lib/python3.8/site-packages/invoke/program.py\", line 373, in run             self.parse_collection()     ValueError: One and only one set-up config should be true:</code></li> <li>The log of the run<ul> <li>Maybe the same run using <code>-v DEBUG</code> to get more info on the problem</li> </ul> </li> <li>What the problem is</li> <li>Why the outcome is different from what you expected</li> </ul> </li> <li>Use check boxes for \"small\" actions that need to be tracked in the Issue (not     worth their own Issue)<ul> <li>An issue should be closed only after all the checkboxes have been addressed     OR the remaining checkboxes were either transformed into new issues in their     own right (e.g. if the implementation turned out to be more complex than     initially thought) OR have a reason for not being implemented</li> <li>These things should be mentioned explicitly before closing the issue     (Element of Least Surprise.).</li> </ul> </li> <li>We use the <code>FYI @...</code> syntax to add \"watchers\"<ul> <li>E.g., <code>FYI @cryptomtc</code> so that he receives notifications for this issue</li> <li>Authors and assignees receive all the emails in any case</li> <li>In general everybody should be subscribed to receiving all the notifications     and you can quickly go through them to know what's happening around you</li> </ul> </li> <li>Assign an Issue to the right person<ul> <li>There should be a single assignee to a Issue so we know who needs to do the     work</li> <li>Assign to yourself if you are going to work on it</li> <li>Assign Integrators / Team leaders if not sure</li> </ul> </li> <li>If you are not sure, leave it unassigned but <code>@tag</code> Integrators / team leaders     to make sure we can take care of it</li> <li>Assign an Issue to the right Project and Label<ul> <li>Use <code>Blocking</code> label when an issue needs to be handled immediately, i.e. it     prevents you from making progress</li> <li>If you are unsure then you can leave it empty, but <code>@tag</code> Integrator / team     leaders to make sure we can re-route and improve the Projects/Labels</li> </ul> </li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#updating_an_issue","title":"Updating an Issue","text":"<ul> <li>For large or complex Issues, there should be a design phase (in the form of GH     Issue, Google Doc, or design PR) before starting to write a code<ul> <li>A team leader / integrator should review the design</li> </ul> </li> <li>When you start working on an Issue, change its status to <code>In Progress</code><ul> <li>Try to use <code>In Progress</code> only for Issues you are actively working on</li> <li>A rule of thumb is that you should not have more than 2-3 <code>In Progress</code>     Issues</li> <li>Give priority to Issues that are close to being completed, rather than     starting a new Issue</li> </ul> </li> <li>Update an Issue on GH often, like at least once a day of work<ul> <li>Show the progress to the team with quick updates</li> <li>Update your Issue with pointers to gdocs, PRs, notebooks</li> <li>If you have questions, post them on the bug and tag people</li> </ul> </li> <li>Once the task, in your opinion, is done, change the status to <code>To Review</code> so     that Integrator / team leaders can review it</li> <li>If we decide to stop the work, add a <code>Paused</code> label</li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#closing_an_issue","title":"Closing an Issue","text":"<ul> <li>A task is closed when PR has been reviewed and merged into <code>master</code></li> <li>When, in your opinion, there is no more work to be done on your side on an     Issue, change its status to <code>Done</code> but do not close it<ul> <li>Integrators / team leaders will close it after review</li> </ul> </li> <li>If you made specific assumptions, or if there are loose ends, etc., add a     <code>TODO(user)</code> or file a follow-up Issue</li> <li>Done means that something is \"DONE\", not \"99% done\"<ul> <li>\"DONE\" means that the code is tested, readable, and usable by other     teammates</li> </ul> </li> <li>Together we can decide that 99% done is good enough, but it should be a     conscious decision and not comes as a surprise</li> <li>There should be a reason when closing an issue<ul> <li>E.g. - closing as PR is merged</li> <li>E.g. - closing since obsolete</li> </ul> </li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#pr_workflows","title":"PR Workflows","text":""},{"location":"work_organization/all.use_github.how_to_guide.html#pr_labels","title":"PR Labels","text":"<ul> <li><code>PR_for_authors</code><ul> <li>There are changes to be addressed by an author of a PR</li> </ul> </li> <li><code>PR_for_reviewers</code><ul> <li>PR is ready for review by team leaders</li> </ul> </li> <li><code>PR_for_integrators</code><ul> <li>PR is ready for the final round of review by Integrators, i.e. close to     merge</li> </ul> </li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#filing_a_new_pr","title":"Filing a New PR","text":""},{"location":"work_organization/all.use_github.how_to_guide.html#general_tips","title":"General Tips","text":"<ul> <li>Implement a feature in a branch (not <code>master</code>), once it is ready for review     push it and file a PR via GH interface</li> <li>We have <code>invoke</code> tasks to automate some of these tasks:     <code>bash     &gt; i git_branch_create -i 828     &gt; i git_branch_create -b Cmamp723_hello_world     &gt; i gh_create_pr</code></li> <li>If you want to make sure you are going in a right direction or just to confirm     the interfaces you can also file a PR to discuss</li> <li>Mark PR as draft if it is not ready, use the <code>Convert to draft</code> button<ul> <li> <p>Draft PR should be filed when there is something to discuss with and     demonstrate to the reviewer, but the feature is not completely implemented</p> <p></p> </li> </ul> </li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#filing_process","title":"Filing Process","text":"<ul> <li>The title of the PR should match the name of the branch<ul> <li>Look at the existing PRs in the repo for examples</li> </ul> </li> <li>Add a description to help reviewers to understand what it is about and what     you want the focus to be</li> <li>Add a pointer in the description to the issue that PR is related to - this     will ease the GH navigation for you and reviewers<ul> <li>Do not link the PR to the issue through the dedicated GH functionality.     The reason is that when they are linked, GH automatically closes the issue     when the PR is merged, and this is inconvenient if there is more work to do.     We always want to have control over when the issue is closed.</li> <li>Sometimes GH creates these links itself - usually when the PR description     contains keywords like \"Fixes #\". So please avoid such     phrasing and instead use something more abstract like \"Related to     #\". <li>Put yourself in the \"Assignees\" field</li> <li>Add reviewers to the reviewers list<ul> <li>For optional review just do <code>FYI @username</code> in the description</li> </ul> </li> <li>Add a corresponding label<ul> <li>Usually the first label in the filed PR is <code>PR_for_reviewers</code></li> <li>If it is urgent/blocking, use the <code>Blocking</code> label</li> </ul> </li> <li>Make sure that the corresponding tests pass</li> <li>Always lint (and commit files modified by Linter) before asking for a review</li>"},{"location":"work_organization/all.use_github.how_to_guide.html#review","title":"Review","text":"<ul> <li>A reviewer should check the code:<ul> <li>Architecture</li> <li>Conformity with specs</li> <li>Code style conventions</li> <li>Interfaces</li> <li>Mistakes</li> <li>Readability</li> </ul> </li> <li>There are 2 possible outcomes of a review:<ul> <li>There are changes to be addressed by author<ul> <li>A reviewer leaves comments to the code</li> <li>Marks PR as <code>PR_for_authors</code></li> </ul> </li> <li>A PR is ready to be merged:<ul> <li>Pass it to integrators and mark it as <code>PR_for_integrators</code><ul> <li>Usually is placed by team leaders after they approve PR</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"work_organization/all.use_github.how_to_guide.html#addressing_a_comment","title":"Addressing a Comment","text":"<ul> <li>If the reviewer's comment is clear to the author and agreed upon:<ul> <li>The author addresses the comment with a code change and after changing the     code (everywhere the comment applies) marks it as <code>RESOLVED</code> on the GH     interface</li> <li>Here we trust the authors to do a good job and to not skip / lose comments</li> <li>If the comment needs further discussion, the author adds a note explaining     why he/she disagrees and the discussion continues until consensus is reached</li> </ul> </li> <li>Once all comments are addressed:<ul> <li>Re-request the review</li> <li>Mark it as <code>PR_for_reviewers</code></li> </ul> </li> </ul>"}]}